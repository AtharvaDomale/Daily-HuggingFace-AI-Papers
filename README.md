<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-38-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-554+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">38</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">230</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">603</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">554+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 21, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Kling-Omni Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16776) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16776) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16776)

> We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse vi...

</details>

<details>
<summary><b>2. Adaptation of Agentic AI</b> â­ 274</summary>

<br/>

**ğŸ‘¥ Authors:** XueqiangXu, p-song1, Gabshi, linjc16, pat-jj

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16301) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16301) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16301)

**ğŸ’» Code:** [â­ Code](https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI)

> Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation b...

</details>

<details>
<summary><b>3. LLaDA2.0: Scaling Up Diffusion Language Models to 100B</b> â­ 168</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.15745) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.15745) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.15745)

**ğŸ’» Code:** [â­ Code](https://github.com/inclusionAI/LLaDA2.0)

> This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deploym...

</details>

<details>
<summary><b>4. Next-Embedding Prediction Makes Strong Vision Learners</b> â­ 69</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16922) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16922) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16922)

**ğŸ’» Code:** [â­ Code](https://github.com/SihanXU/nepa)

> Make SSL great again.

</details>

<details>
<summary><b>5. StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</b> â­ 43</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16915) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16915) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16915)

**ğŸ’» Code:** [â­ Code](https://github.com/KlingTeam/StereoPilot)

> StereoPilot replaces the fragile "Depth-Warp-Inpaint" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo datase...

</details>

<details>
<summary><b>6. Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13507) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13507) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13507)

> Seedance 1.5 pro Technical Report

</details>

<details>
<summary><b>7. Generative Refocusing: Flexible Defocus Control from a Single Image</b> â­ 43</summary>

<br/>

**ğŸ‘¥ Authors:** Yu-Lun Liu, Jia-Bin Huang, rayray9999

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16923) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16923) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16923)

**ğŸ’» Code:** [â­ Code](https://github.com/rayray9999/Genfocus)

> Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Curren...

</details>

<details>
<summary><b>8. Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Wenxuan Lu, Dizhe Zhang, Meixi Song, Xin Lin, haodongli

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16913) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16913) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16913)

> In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale datas...

</details>

<details>
<summary><b>9. DeContext as Defense: Safe Image Editing in Diffusion Transformers</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16625) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16625) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16625)

**ğŸ’» Code:** [â­ Code](https://github.com/LinghuiiShen/DeContext)

> âœ¨ Image editing is awesome; but it can leak user information! ğŸ›¡ï¸ Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. ğŸ“„ Paper: https://arxiv.org/abs/2512.16625 ğŸ’» Code: https://github.co...

</details>

<details>
<summary><b>10. Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jiarong Ou, Miao Yang, Xi Chen, Yang Zhou, Kaixin Ding

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16905) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16905) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16905)

> data selection

</details>

<details>
<summary><b>11. REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Giorgos Sfikas, Theodoros Giannakopoulos, Bill Psomas, Christos Sgouropoulos, Giorgos Petsangourakis

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16636) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16636) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16636)

**ğŸ’» Code:** [â­ Code](https://github.com/giorgospets/reglue)

> Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sam...

</details>

<details>
<summary><b>12. The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</b> â­ 67</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16924) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16924) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16924)

**ğŸ’» Code:** [â­ Code](https://github.com/pPetrichor/WorldCanvas)

> Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas

</details>

<details>
<summary><b>13. N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</b> â­ 24</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16561) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16561) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16561)

**ğŸ’» Code:** [â­ Code](https://github.com/W-Ted/N3D-VLM)

> Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM

</details>

<details>
<summary><b>14. JustRL: Scaling a 1.5B LLM with a Simple RL Recipe</b> â­ 73</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16649) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16649) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16649)

**ğŸ’» Code:** [â­ Code](https://github.com/thunlp/JustRL)

> âœ¨What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2Ã— less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.

</details>

<details>
<summary><b>15. EasyV2V: A High-quality Instruction-based Video Editing Framework</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16920) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16920) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16920)

> While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and ...

</details>

<details>
<summary><b>16. AdaTooler-V: Adaptive Tool-Use for Images and Videos</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Zhixun Li, Zhongyu Wang, Dongyang Chen, Kaituo Feng, Chaoyang Wang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16918) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16918) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16918)

**ğŸ’» Code:** [â­ Code](https://github.com/CYWang735/AdaTooler-V)

> Project page: https://github.com/CYWang735/AdaTooler-V

</details>

<details>
<summary><b>17. Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16912) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16912) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16912)

> This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathem...

</details>

<details>
<summary><b>18. FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</b> â­ 83</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16900) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16900) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16900)

**ğŸ’» Code:** [â­ Code](https://github.com/Francis-Rings/FlashPortrait)

> Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-len...

</details>

<details>
<summary><b>19. Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</b> â­ 25</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16899) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16899) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16899)

**ğŸ’» Code:** [â­ Code](https://github.com/facebookresearch/MMRB2/tree/main)

> Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta...

</details>

<details>
<summary><b>20. RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</b> â­ 15</summary>

<br/>

**ğŸ‘¥ Authors:** Yuqi Liu, Longxiang Tang, Xiaohang Zhan, Lei Ke, TainU

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16864) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16864) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16864)

**ğŸ’» Code:** [â­ Code](https://github.com/dvlab-research/RePlan)

> ğŸš§ The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenesâ€”a challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level g...

</details>

<details>
<summary><b>21. VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16501) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16501) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16501)

> GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a si...

</details>

<details>
<summary><b>22. ModelTables: A Corpus of Tables about Models</b> â­ 12</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16106) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16106) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16106)

**ğŸ’» Code:** [â­ Code](https://github.com/RJMillerLab/ModelTables)

> ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and con...

</details>

<details>
<summary><b>23. Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</b> â­ 13</summary>

<br/>

**ğŸ‘¥ Authors:** Carlos Escolano, VilÃ©m Zouhar, zhopto3, javi8979, spapi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16378) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16378) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16378)

**ğŸ’» Code:** [â­ Code](https://github.com/sarapapi/hearing2translate)

> Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challengin...

</details>

<details>
<summary><b>24. Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16921) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16921) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16921)

> Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively d...

</details>

<details>
<summary><b>25. Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16615) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16615) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16615)

**ğŸ’» Code:** [â­ Code](https://github.com/SingleZombie/LLSA)

> Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA

</details>

<details>
<summary><b>26. Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11251) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11251) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11251)

> Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptio...

</details>

<details>
<summary><b>27. FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Hendrik P. A. Lensch, Ole Beisswenger, JDihlmann

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16670) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16670) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16670)

> FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination ...

</details>

<details>
<summary><b>28. Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16767) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16767) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16767)

**ğŸ’» Code:** [â­ Code](https://github.com/jasongzy/Make-It-Poseable)

> No abstract available.

</details>

<details>
<summary><b>29. Coupled Variational Reinforcement Learning for Language Model General Reasoning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ben He, Hongyu Lin, Yanjiang Liu, Jie Lou, Aunderline

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12576) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12576) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12576)

> While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabil...

</details>

<details>
<summary><b>30. Bidirectional Normalizing Flow: From Data to Noise and Back</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Qiao Sun, Yiyang Lu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10953) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10953) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10953)

> Bidirectional Normalizing Flow: From Data to Noise and Back

</details>

<details>
<summary><b>31. MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16909) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16909) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16909)

> Project Page: https://hybridrobotics.github.io/MomaGraph/

</details>

<details>
<summary><b>32. TabReX : Tabular Referenceless eXplainable Evaluation</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Vivek Gupta, Aparna Garimella, Juhna Park, Tejas Anvekar

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.15907) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.15907) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.15907)

**ğŸ’» Code:** [â­ Code](https://github.com/CoRAL-ASU/TabReX)

> Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a...

</details>

<details>
<summary><b>33. Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.15489) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.15489) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.15489)

> Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS com...

</details>

<details>
<summary><b>34. Vibe Spaces for Creatively Connecting and Expressing Visual Concepts</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yutong Bai, Michael D. Grossberg, Andrew Lu, Katherine Xu, Huzheng Yang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.14884) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.14884) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.14884)

**ğŸ’» Code:** [â­ Code](https://github.com/huzeyann/VibeSpace)

> what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.

</details>

<details>
<summary><b>35. Improving Recursive Transformers with Mixture of LoRAs</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12880) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12880) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12880)

> Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected sh...

</details>

<details>
<summary><b>36. Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12623) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12623) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12623)

> ğŸŒ Website: https://mllm-dmlr.github.io ğŸ“„ Paper: https://arxiv.org/abs/2512.12623

</details>

<details>
<summary><b>37. EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Can Ma. Yu Zhou, Dongbao Yang, Daiqing Wu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.15528) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.15528) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.15528)

**ğŸ’» Code:** [â­ Code](https://github.com/wdqqdw/EmoCaliber)

> Update the paper.

</details>

<details>
<summary><b>38. Sharing State Between Prompts and Programs</b> â­ 3</summary>

<br/>

**ğŸ‘¥ Authors:** Michael Carbin, Tian Jin, Logan Weber, ellieyhc

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.14805) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.14805) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.14805)

**ğŸ’» Code:** [â­ Code](https://github.com/psg-mit/nightjarpy/)

> No abstract available.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 38 |
| ğŸ“… Today | [`2025-12-21.json`](data/daily/2025-12-21.json) | 38 |
| ğŸ“† This Week | [`2025-W50.json`](data/weekly/2025-W50.json) | 230 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 603 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-21 | 38 | [View JSON](data/daily/2025-12-21.json) |
| ğŸ“„ 2025-12-20 | 37 | [View JSON](data/daily/2025-12-20.json) |
| ğŸ“„ 2025-12-19 | 30 | [View JSON](data/daily/2025-12-19.json) |
| ğŸ“„ 2025-12-18 | 38 | [View JSON](data/daily/2025-12-18.json) |
| ğŸ“„ 2025-12-17 | 41 | [View JSON](data/daily/2025-12-17.json) |
| ğŸ“„ 2025-12-16 | 21 | [View JSON](data/daily/2025-12-16.json) |
| ğŸ“„ 2025-12-15 | 25 | [View JSON](data/daily/2025-12-15.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W50 | 230 | [View JSON](data/weekly/2025-W50.json) |
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 603 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
