<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-21-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-370+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">21</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">46</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">419</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">370+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 16, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. EgoX: Egocentric Video Generation from a Single Exocentric Video</b> â­ 17</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08269) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08269) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08269)

**ğŸ’» Code:** [â­ Code](https://github.com/KEH0T0/EgoX)

> No abstract available.

</details>

<details>
<summary><b>2. DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yanchao Li, Junjie Zhao, Jiaming Zhang, Zhenyang Cai, CocoNutZENG

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11558) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11558) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11558)

> Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability f...

</details>

<details>
<summary><b>3. SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</b> â­ 40</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11749) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11749) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11749)

**ğŸ’» Code:** [â­ Code](https://github.com/KlingTeam/SVG-T2I)

> Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diff...

</details>

<details>
<summary><b>4. V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</b> â­ 40</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11799) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11799) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11799)

**ğŸ’» Code:** [â­ Code](https://github.com/Aleafy/V-RGBX)

> Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., a...

</details>

<details>
<summary><b>5. Sliding Window Attention Adaptation</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10411) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10411) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10411)

**ğŸ’» Code:** [â­ Code](https://github.com/yuyijiong/sliding-window-attention-adaptation)

> We propose a set of practical recipes that can let a full-attention LLM use sliding window attention to improve efficiency. For example, some can achieve nearly 100% acceleration of LLM long-context inference speed with 90% accuracy retainment; so...

</details>

<details>
<summary><b>6. PersonaLive! Expressive Portrait Image Animation for Live Streaming</b> â­ 206</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11253) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11253) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11253)

**ğŸ’» Code:** [â­ Code](https://github.com/GVCLab/PersonaLive)

> Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming...

</details>

<details>
<summary><b>7. Exploring MLLM-Diffusion Information Transfer with MetaCanvas</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11464) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11464) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11464)

> Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to globa...

</details>

<details>
<summary><b>8. Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Qifeng Chen, Jingyuan Liu, George Stoica, Tim666, sunfly

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11792) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11792) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11792)

> We introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX).

</details>

<details>
<summary><b>9. MeshSplatting: Differentiable Rendering with Opaque Meshes</b> â­ 273</summary>

<br/>

**ğŸ‘¥ Authors:** Matheus Gadelha, Daniel Rebain, Renaud Vandeghen, Sanghyun Son, Jan Held

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06818) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06818) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06818)

**ğŸ’» Code:** [â­ Code](https://github.com/meshsplatting/mesh-splatting)

> MeshSplatting introduces a differentiable rendering approach that reconstructs connected, fully opaque triangle meshes for fast, memory efficient, high quality novel view synthesis.

</details>

<details>
<summary><b>10. LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10605) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10605) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10605)

**ğŸ’» Code:** [â­ Code](https://github.com/LegendLeoChen/LEO-RobotAgent)

> A general-purpose robotic agent framework based on LLMs. The LLM can independently reason, plan, and execute actions to operate diverse robot types across various scenarios to complete unpredictable, complex tasks.

</details>

<details>
<summary><b>11. Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</b> â­ 7</summary>

<br/>

**ğŸ‘¥ Authors:** elandy

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11150) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11150) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11150)

**ğŸ’» Code:** [â­ Code](https://github.com/cimo-labs/cje)

> LLM-as-judge evals are convenient, but meaningful (fixable) failure modes lurk beneath the surface. CJE treats LLM-judge evaluation as a statistics problem: â€¢ calibrate a cheap judge to a small oracle slice of high-quality labels â€¢ quantify uncert...

</details>

<details>
<summary><b>12. Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}</b> â­ 14</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.02901) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.02901) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.02901)

**ğŸ’» Code:** [â­ Code](https://github.com/PKULab1806/Fairy2i-W2)

> Is it possible to run LLMs at 2-bit with virtually NO loss in accuracy? ğŸ¤” No with Real numbers, but Yes with Complex ones! ğŸš€ Meet Fairy2i-W2(2bit): QAT from LLaMA-2 7B with Complex Phase quant PPL: 7.85 (vs FP16's 6.63) Accuracy: 62.00% (vs FP16's...

</details>

<details>
<summary><b>13. CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11437) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11437) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11437)

> First and largest multilingual trustworthiness benchmark for healthcare

</details>

<details>
<summary><b>14. Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</b> â­ 111</summary>

<br/>

**ğŸ‘¥ Authors:** Akash Karnatak, Gleb Zarin, IliaLarchenko

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06951) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06951) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06951)

**ğŸ’» Code:** [â­ Code](https://github.com/IliaLarchenko/behavior-1k-solution)

> We present our 1st place solution to the 2025 NeurIPS BEHAVIOR Challenge, where a single Vision-Language-Action robotics policy is trained to perform 50 household manipulation tasks in a photorealistic simulator. The approach builds on Pi0.5 with ...

</details>

<details>
<summary><b>15. Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching</b> â­ 45</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11130) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11130) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11130)

**ğŸ’» Code:** [â­ Code](https://github.com/NVlabs/Fast-FoundationStereo)

> A real-time foundation model for stereo depth estimation, which is crucial for robotics/humanoid 3D spatial perception.

</details>

<details>
<summary><b>16. Scaling Behavior of Discrete Diffusion Language Models</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10858) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10858) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10858)

**ğŸ’» Code:** [â­ Code](https://github.com/dvruette/gidd-easydel)

> We scale diffusion language models up to 3B (masked and uniform diffusion) and 10B (uniform diffusion) parameters,  pre-trained on a pure diffusion objective (mixture of unconditional and conditional) via Nemotron-CC. ğŸ¤– GitHub: https://github.com/...

</details>

<details>
<summary><b>17. CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Enzo Ferrante, Rodrigo Echeveste, Nicolas Gaggion, Matias Cosarinsky

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10715) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10715) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10715)

**ğŸ’» Code:** [â­ Code](https://github.com/mcosarinsky/CheXmask-U)

> We present CheXmask-U , a framework for quantifying uncertainty in landmark-based anatomical segmentation models on chest X-rays and release the CheXmask-U dataset providing per-node uncertainty estimates to support research in robust and safe med...

</details>

<details>
<summary><b>18. The N-Body Problem: Parallel Execution from Single-Person Egocentric Video</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Dima Damen, Yoichi Sato, Yifei Huang, Zhifan Zhu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11393) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11393) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11393)

> Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks obse...

</details>

<details>
<summary><b>19. Sharp Monocular View Synthesis in Less Than a Second</b> â­ 139</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10685) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10685) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10685)

**ğŸ’» Code:** [â­ Code](https://github.com/apple/ml-sharp)

> Sharp Monocular View Synthesis in Less Than a Second https://huggingface.co/papers/2512.10685 Real-time photorealistic view synthesis from a single image. Given a single photograph, regresses the parameters of a 3D Gaussian representation of the d...

</details>

<details>
<summary><b>20. Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</b> â­ 9</summary>

<br/>

**ğŸ‘¥ Authors:** Neel Nanda, Lewis Smith, Lisa Dunlap, Xiaoqing Sun, Nick Jiang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10092) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10092) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10092)

**ğŸ’» Code:** [â­ Code](https://github.com/nickjiang2378/interp_embed)

> Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating datas...

</details>

<details>
<summary><b>21. Particulate: Feed-Forward 3D Object Articulation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Joan Lasenby, Christian Rupprecht, Chuanxia Zheng, Yuxin Yao, Ruining Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11798) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11798) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11798)

> No abstract available.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 21 |
| ğŸ“… Today | [`2025-12-16.json`](data/daily/2025-12-16.json) | 21 |
| ğŸ“† This Week | [`2025-W50.json`](data/weekly/2025-W50.json) | 46 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 419 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-16 | 21 | [View JSON](data/daily/2025-12-16.json) |
| ğŸ“„ 2025-12-15 | 25 | [View JSON](data/daily/2025-12-15.json) |
| ğŸ“„ 2025-12-14 | 25 | [View JSON](data/daily/2025-12-14.json) |
| ğŸ“„ 2025-12-13 | 24 | [View JSON](data/daily/2025-12-13.json) |
| ğŸ“„ 2025-12-12 | 21 | [View JSON](data/daily/2025-12-12.json) |
| ğŸ“„ 2025-12-11 | 25 | [View JSON](data/daily/2025-12-11.json) |
| ğŸ“„ 2025-12-10 | 29 | [View JSON](data/daily/2025-12-10.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W50 | 46 | [View JSON](data/weekly/2025-W50.json) |
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 419 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
