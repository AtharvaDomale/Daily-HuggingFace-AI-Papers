<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-32-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1270+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">32</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">103</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">532</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1270+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 22, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</b> â­ 265</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.12993) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.12993) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.12993)

**ğŸ’» Code:** [â­ Code](https://github.com/BeingBeyond/Being-H)

> We scale human-centric robot learning with Being-H0.5 toward cross-embodiment generalization. Building on over 35,000 hours data, we unify human hand motion and diverse robot embodiments with a Unified Action Space, and train all heterogeneous sup...

</details>

<details>
<summary><b>2. Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey</b> â­ 40</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11655) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11655) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11655)

**ğŸ’» Code:** [â­ Code](https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution)

> ğŸš€ Awesome issue resolution: a comprehensive survey! This paper surveyed 175+ works to construct the first unified taxonomy serving as the comprehensive roadmap for issue resolution.

</details>

<details>
<summary><b>3. Think3D: Thinking with Space for Spatial Reasoning</b> â­ 32</summary>

<br/>

**ğŸ‘¥ Authors:** Yuhan Wu, JeremyYin, sunz525, luciasnowblack, MrBean2024

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13029) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13029) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13029)

**ğŸ’» Code:** [â­ Code](https://github.com/zhangzaibin/spagent)

> We introduce Think3D, a framework that enables VLM agents to think in 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through ...

</details>

<details>
<summary><b>4. OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</b> â­ 54</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14250) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14250) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14250)

**ğŸ’» Code:** [â­ Code](https://github.com/PangzeCheung/OmniTransfer)

> Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spa...

</details>

<details>
<summary><b>5. Toward Efficient Agents: Memory, Tool learning, and Planning</b> â­ 27</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14192) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14192) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14192)

**ğŸ’» Code:** [â­ Code](https://github.com/yxf203/Awesome-Efficient-Agents)

> This paper surveys efficiency-oriented methods for agentic systems across memory, tool learning, and planning, distills shared design principles, and summarizes how recent methods and benchmarks measure efficiency, which hopes to guide the develop...

</details>

<details>
<summary><b>6. FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13836) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13836) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13836)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenMOSS/FutureOmni)

> FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs ğŸ”— Paper: https://arxiv.org/pdf/2601.13836 ğŸ’» Code: https://github.com/OpenMOSS/FutureOmni ğŸŒ Project: https://openmoss.github.io/FutureOmni ğŸ¬ Datasets: https://hug...

</details>

<details>
<summary><b>7. MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models</b> â­ 4</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11969) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11969) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11969)

**ğŸ’» Code:** [â­ Code](https://github.com/LCM-Lab/MemRewardBench)

> Check our code: https://github.com/LCM-Lab/MemRewardBench and Benchmark: https://huggingface.co/datasets/LCM-Lab/MemRewardBench

</details>

<details>
<summary><b>8. Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** qiaw99, WANGYIWEI, zunhai, mingyang26, hengyuanya

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14004) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14004) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14004)

> Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models

</details>

<details>
<summary><b>9. UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation</b> â­ 19</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11522) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11522) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11522)

**ğŸ’» Code:** [â­ Code](https://github.com/ZrH42/UniX)

> We introduce UniX, a unified foundation model for Chest X-Ray that combines Autoregression (for understanding) and Diffusion (for generation) within a decoupled dual-branch architecture! ğŸ¥âœ¨ Why UniX? Current unified models often face a conflict be...

</details>

<details>
<summary><b>10. ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.12294) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.12294) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.12294)

> ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents

</details>

<details>
<summary><b>11. Aligning Agentic World Models via Knowledgeable Experience Learning</b> â­ 21</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13247) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13247) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13247)

**ğŸ’» Code:** [â­ Code](https://github.com/zjunlp/WorldMind)

> WorldMind helps language models stop making physically impossible plans by learning real-world rules from feedback and successful experiences, rather than retraining the model itself.

</details>

<details>
<summary><b>12. Agentic-R: Learning to Retrieve for Agentic Search</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Daiting Shi, Yuchen Li, Yutao Zhu, Xinyu Ma, Wenhan Liu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11888) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11888) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11888)

> Agentic-R: Learning to Retrieve for Agentic Search

</details>

<details>
<summary><b>13. A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13288) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13288) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13288)

> Rather than adding another model to the stack, this work reuses computation already paid for in the serving LLMâ€™s forward pass by training compact probes on hidden states. It frames the problem as principled selection across tokens and layers (not...

</details>

<details>
<summary><b>14. KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</b> â­ 7</summary>

<br/>

**ğŸ‘¥ Authors:** Aleksandr I. Panov, Alexey K. Kovalev, Daniil Zelezetsky, Egor Cherepanov

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14232) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14232) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14232)

**ğŸ’» Code:** [â­ Code](https://github.com/CognitiveAISystems/kage-bench)

> Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduc...

</details>

<details>
<summary><b>15. LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14251) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14251) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14251)

> We present LightOnOCR-2-1B , a 1B-parameter end-to-end multilingual vision-language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillat...

</details>

<details>
<summary><b>16. PRiSM: Benchmarking Phone Realization in Speech Models</b> â­ 4</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14046) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14046) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14046)

**ğŸ’» Code:** [â­ Code](https://github.com/changelinglab/prism)

> Main take-aways PRiSM is the first fully-open benchmark that evaluates Phone-Recognition systems on both intrinsic (phone-transcription) and extrinsic (down-stream) tasks across 12 datasets covering clinical, L2-learning and multilingual settings....

</details>

<details>
<summary><b>17. FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13976) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13976) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13976)

**ğŸ’» Code:** [â­ Code](https://github.com/Fantasy-AMAP/fantasy-vln) â€¢ [â­ Code](https://github.com/FoundationVision/VAR)

> FantasyVLN is a unified multimodal Chain-of-Thought (CoT) reasoning framework that enables efficient and precise navigation based on natural language instructions and visual observations. FantasyVLN combines the benefits of textual, visual, and mu...

</details>

<details>
<summary><b>18. DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13761) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13761) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13761)

**ğŸ’» Code:** [â­ Code](https://github.com/RUCBM/DARC)

> In this work, we introduce the DARC framework, which adopts decoupled training and asymmetric self-distillation to stabilize self-evolving. We hope this work provides useful insights for LLM self-evolution. avXiv: https://arxiv.org/abs/2601.13761 ...

</details>

<details>
<summary><b>19. Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14249) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14249) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14249)

**ğŸ’» Code:** [â­ Code](https://github.com/UmeanNever/RankSurprisalRatio)

> Code: https://github.com/UmeanNever/RankSurprisalRatio

</details>

<details>
<summary><b>20. InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14209) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14209) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14209)

> Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces ...

</details>

<details>
<summary><b>21. Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13697) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13697) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13697)

> Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either...

</details>

<details>
<summary><b>22. On the Evidentiary Limits of Membership Inference for Copyright Auditing</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Marten van Dijk, Kaleel Mahmood, Min Chen, emirhanboge, bilgehanertan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.12937) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.12937) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.12937)

> ğŸ§‘â€âš–ï¸ğŸ“„ This paper shows that membership inference attacks are not reliable technical evidence for copyright infringement in court. Even with strong MIAs, semantics-preserving paraphrasing breaks the signal while keeping utility, making them brittle...

</details>

<details>
<summary><b>23. Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.10237) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.10237) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.10237)

> This paper quantifies a fundamental lower bound on the noise required for differentially private stochastic gradient descent (DP-SGD) to maintain strong privacy, revealing that even with massive datasets and both shuffled and Poisson subsampling, ...

</details>

<details>
<summary><b>24. DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13591) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13591) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13591)

> This paper introduce the DSAEval, evaluating LLM based Data Agent in a wide-range of real world problems.

</details>

<details>
<summary><b>25. A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ã–zay Ezerceli, Mehmet Emin Buldur, MElHuseyni, etosun

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13253) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13253) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13253)

> Addressing data scarcity in low-resource languages, this paper introduces a cost-effective ($65) pipeline for generating large-scale semantic datasets. By integrating FastText clustering, Gemini 2.5-Flash labeling, and dictionary curation, the aut...

</details>

<details>
<summary><b>26. Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ã–zay Ezerceli, Mehmet Emin Buldur, MElHuseyni, etosun

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13251) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13251) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13251)

> This paper addresses the inability of neural embeddings to distinguish synonyms from antonyms. The authors introduce a soft-to-hard clustering algorithm that prevents semantic drift and a 3-way relation discriminator (90% F1). Validated against a ...

</details>

<details>
<summary><b>27. METIS: Mentoring Engine for Thoughtful Inquiry & Solutions</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13075) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13075) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13075)

> Students have immense research potential, but enough mentors for them. What if we could design an AI system to mentor them? We introduce METIS (Mentoring Engine for Thoughtful Inquiry & Solutions), a stage-aware research mentor.

</details>

<details>
<summary><b>28. SciCoQA: Quality Assurance for Scientific Paper--Code Alignment</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.12910) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.12910) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.12910)

**ğŸ’» Code:** [â­ Code](https://github.com/UKPLab/scicoqa) â€¢ [â­ Code](https://github.com/ukplab/scicoqa)

> We introduce the SciCoQA dataset for evaluating models on detecting discrepancies between paper and code. Find all resources here: Paper: arXiv Data: Hugging Face Dataset Code: GitHub Demo: Hugging Face Space Project Page : UKPLab/scicoqa

</details>

<details>
<summary><b>29. LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.10700) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.10700) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.10700)

**ğŸ’» Code:** [â­ Code](https://github.com/GilatToker/Liberty-benchmark)

> The paper addresses the lack of reliable ground-truth benchmarks for evaluating concept-based explainability in Large Language Models. The authors introduce LIBERTy, a framework that generates "structural counterfactuals" by explicitly defining St...

</details>

<details>
<summary><b>30. Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging</b> â­ 11</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13677) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.19183) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13677)

**ğŸ’» Code:** [â­ Code](https://github.com/MIC-DKFZ/nnActive/tree/nnActive_v2)

> ğŸš€ Building on nnActive , an evaluation framework for active learning in 3D biomedical imaging, this paper proposes a simple and effective method that consistently outperforms strong random baselines.

</details>

<details>
<summary><b>31. Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yu He, Weiping Fu, Zhiyuan Wang, Zhangqi Wang, Jian Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13481) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13481) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13481)

> We propose APOLO (Automated Prompt Optimization for Linguistic emOtion diagnosis), a framework that systematically explores a broader and finer-grained prompt space to enhance diagnostic efficiency and robustness.

</details>

<details>
<summary><b>32. RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11898) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11898) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11898)

**ğŸ’» Code:** [â­ Code](https://github.com/yilmazkorkmaz1/RemoteVAR)

> https://github.com/yilmazkorkmaz1/RemoteVAR

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 32 |
| ğŸ“… Today | [`2026-01-22.json`](data/daily/2026-01-22.json) | 32 |
| ğŸ“† This Week | [`2026-W03.json`](data/weekly/2026-W03.json) | 103 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 532 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-22 | 32 | [View JSON](data/daily/2026-01-22.json) |
| ğŸ“„ 2026-01-21 | 11 | [View JSON](data/daily/2026-01-21.json) |
| ğŸ“„ 2026-01-20 | 22 | [View JSON](data/daily/2026-01-20.json) |
| ğŸ“„ 2026-01-19 | 38 | [View JSON](data/daily/2026-01-19.json) |
| ğŸ“„ 2026-01-18 | 38 | [View JSON](data/daily/2026-01-18.json) |
| ğŸ“„ 2026-01-17 | 38 | [View JSON](data/daily/2026-01-17.json) |
| ğŸ“„ 2026-01-16 | 27 | [View JSON](data/daily/2026-01-16.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W03 | 103 | [View JSON](data/weekly/2026-W03.json) |
| ğŸ“… 2026-W02 | 232 | [View JSON](data/weekly/2026-W02.json) |
| ğŸ“… 2026-W01 | 156 | [View JSON](data/weekly/2026-W01.json) |
| ğŸ“… 2026-W00 | 41 | [View JSON](data/weekly/2026-W00.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 532 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
