<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-31-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-738+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">31</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">52</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">787</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">738+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 31, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23447) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23447) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23447)

> We propose the Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the routerâ€™s decisions with expert capabilities. Unlike prior coupling methods that scale with the number of tokens (often millions per batch), the...

</details>

<details>
<summary><b>2. LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</b> â­ 81</summary>

<br/>

**ğŸ‘¥ Authors:** Steffi Chern, Jiadi Su, Bohao Tang, Zhulin Hu, Ethan Chern

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23576) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23576) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23576)

**ğŸ’» Code:** [â­ Code](https://github.com/GAIR-NLP/LiveTalk)

> Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models...

</details>

<details>
<summary><b>3. Yume-1.5: A Text-Controlled Interactive World Generation Model</b> â­ 426</summary>

<br/>

**ğŸ‘¥ Authors:** Kaining Ying, Xiaojie Xu, Chuanhao Li, Zhen Li, Xiaofeng Mao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22096) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22096) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22096)

**ğŸ’» Code:** [â­ Code](https://github.com/stdstu12/YUME)

> Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inferen...

</details>

<details>
<summary><b>4. SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22322) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22322) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22322)

> We introduce SmartSnap , a paradigm shift that transforms GUI agentsğŸ“±ğŸ’»ğŸ¤– from passive task executors into proactive self-verifiers. By empowering agents to curate their own evidence of success through the 3C Principles (Completeness, Conciseness, C...

</details>

<details>
<summary><b>5. Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</b> â­ 94</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23705) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23705) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23705)

**ğŸ’» Code:** [â­ Code](https://github.com/Daniellli/DKT)

> Abstract Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimate...

</details>

<details>
<summary><b>6. Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Po-Fan Yu, Chi-Wei Hsiao, Zhixiang Wang, Chin-Yang Lin, Hau-Shiang Shiu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23709) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23709) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23709)

> Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally...

</details>

<details>
<summary><b>7. Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</b> â­ 41</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22615) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22615) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22615)

**ğŸ’» Code:** [â­ Code](https://github.com/DreamLM/Dream-VLX)

> Building on the success of Dream 7B, we introduce Dream-VL and Dream-VLA, open VL and VLA models that fully unlock discrete diffusionâ€™s advantages in long-horizon planning, bidirectional reasoning, and parallel action generation for multimodal tasks.

</details>

<details>
<summary><b>8. SpotEdit: Selective Region Editing in Diffusion Transformers</b> â­ 48</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22323) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22323) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22323)

**ğŸ’» Code:** [â­ Code](https://github.com/Biangbiang0321/SpotEdit)

> ğŸ¯ SpotEdit: Edit Only What Needs to Be Edited Why regenerate the entire background just to add a scarf to the dog in your photo? This is a frustrating limitation facing many current AI image editing models. Existing methods typically perform a ful...

</details>

<details>
<summary><b>9. GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.15560) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.15560) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.15560)

> The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of...

</details>

<details>
<summary><b>10. Act2Goal: From World Model To General Goal-conditioned Policy</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23541) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23541) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23541)

> Project page: https://act2goal.github.io/ Abs: Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goa...

</details>

<details>
<summary><b>11. Web World Models</b> â­ 17</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23676) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23676) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23676)

**ğŸ’» Code:** [â­ Code](https://github.com/Princeton-AI2-Lab/Web-World-Models)

> In this work, we introduce the Web World Model (WWM), a middle ground where world state and physics are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisi...

</details>

<details>
<summary><b>12. DiRL: An Efficient Post-Training Framework for Diffusion Language Models</b> â­ 113</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22234) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22234) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22234)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenMOSS/DiRL)

> Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remai...

</details>

<details>
<summary><b>13. Training AI Co-Scientists Using Rubric Rewards</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23707) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23707) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23707)

> How to train language models at generating research plans given diverse open-ended research goals?

</details>

<details>
<summary><b>14. Video-BrowseComp: Benchmarking Agentic Video Research on Open Web</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Kaixin Liang, Minghao Qin, Xiangrui Liu, Yan Shu, Zhengyang Liang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23044) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23044) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23044)

> Introduces Video-BrowseComp, a benchmark of 210 open-web agentic video questions requiring temporal visual evidence to test proactive video reasoning in grounded retrieval.

</details>

<details>
<summary><b>15. OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jian Liu, Weiqiang Wang, Bohan Yu, Wenjie Du, Keda Tao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23646) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23646) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23646)

> Website: https://kd-tao.github.io/OmniAgent/

</details>

<details>
<summary><b>16. YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23273) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23273) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23273)

> Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all input...

</details>

<details>
<summary><b>17. VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xihui Liu, Jinming Xu, Meng Wei, Shaohao Zhu, Wensi Huang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22342) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22342) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22342)

> VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs

</details>

<details>
<summary><b>18. Nested Browser-Use Learning for Agentic Information Seeking</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23647) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23647) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23647)

> Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer ...

</details>

<details>
<summary><b>19. SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23162) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23162) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23162)

> Proposes SurgWorld world model to learn surgical robot policies from unlabeled videos via synthetic pseudokinematics, enabling data-efficient VLA policies from SATA data.

</details>

<details>
<summary><b>20. Monadic Context Engineering</b> â­ 6</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22431) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22431) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22431)

**ğŸ’» Code:** [â­ Code](https://github.com/yifanzhang-pro/monadic-context-engineering)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A Declarative Language for Building And Orchestrating LLM-Powered Agent Wor...

</details>

<details>
<summary><b>21. An Information Theoretic Perspective on Agentic System Design</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.21720) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.21720) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.21720)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference (20...

</details>

<details>
<summary><b>22. Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.20927) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.20927) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.20927)

> project page: https://jaesung-choe.github.io/qrender/index.html

</details>

<details>
<summary><b>23. Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation</b> â­ 21</summary>

<br/>

**ğŸ‘¥ Authors:** Yuheng Ji, Zixiao Wang, Yijie Xu, Sixiang Chen, Huajie Tan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23703) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23703) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23703)

**ğŸ’» Code:** [â­ Code](https://github.com/FlagOpen/Robo-Dopamine)

> Upload Robo-Dopamine

</details>

<details>
<summary><b>24. ProGuard: Towards Proactive Multimodal Safeguard</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jing Shao, Lu Sheng, Chenyang Si, Lijun Li, Shaohan Yu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23573) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23573) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23573)

**ğŸ’» Code:** [â­ Code](https://github.com/yushaohan/ProGuard)

> The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that i...

</details>

<details>
<summary><b>25. Bridging Your Imagination with Audio-Video Generation via a Unified Director</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23222) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23222) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23222)

> UniMAGE unifies script writing and keyframe generation for long-context video creation using Mixture-of-Transformers and a two-stage interleaving/disentangling training paradigm.

</details>

<details>
<summary><b>26. Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.21734) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.21734) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.21734)

> We propose Knot Forcing , a streaming framework for real-time portrait animation that enables high-fidelity, temporally consistent, and interactive video generation from dynamic inputs such as reference images and driving signals. Unlike diffusion...

</details>

<details>
<summary><b>27. KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23236) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23236) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23236)

> Excited to share our recent work on KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta . We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across g...

</details>

<details>
<summary><b>28. Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22100) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22100) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22100)

> We proudly present our brand new Turkish NLP benchmarking sets, TrGLUE. Unlike previous work, TrGLUE is not based on translation of original GLUE tasks but tailored for Turkish vocabulary, syntax, semantics and cultural heritage.

</details>

<details>
<summary><b>29. Self-Evaluation Unlocks Any-Step Text-to-Image Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22374) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22374) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22374)

> No abstract available.

</details>

<details>
<summary><b>30. Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22255) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22255) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22255)

> Training on synthetic CoT traces, even with wrong final answers, improves reasoning due to aligning with the model's distribution and leveraging partial reasoning steps, outperforming human-annotated data. In our paper we explore this interesting ...

</details>

<details>
<summary><b>31. Reverse Personalization</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Nicu Sebe, Tuomas Varanka, Han-Wei Kung

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22984) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22984) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22984)

**ğŸ’» Code:** [â­ Code](https://github.com/hanweikung/reverse-personalization)

> https://github.com/hanweikung/reverse-personalization

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 31 |
| ğŸ“… Today | [`2025-12-31.json`](data/daily/2025-12-31.json) | 31 |
| ğŸ“† This Week | [`2025-W52.json`](data/weekly/2025-W52.json) | 52 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 787 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-31 | 31 | [View JSON](data/daily/2025-12-31.json) |
| ğŸ“„ 2025-12-30 | 14 | [View JSON](data/daily/2025-12-30.json) |
| ğŸ“„ 2025-12-29 | 7 | [View JSON](data/daily/2025-12-29.json) |
| ğŸ“„ 2025-12-28 | 7 | [View JSON](data/daily/2025-12-28.json) |
| ğŸ“„ 2025-12-27 | 7 | [View JSON](data/daily/2025-12-27.json) |
| ğŸ“„ 2025-12-26 | 17 | [View JSON](data/daily/2025-12-26.json) |
| ğŸ“„ 2025-12-25 | 18 | [View JSON](data/daily/2025-12-25.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W52 | 52 | [View JSON](data/weekly/2025-W52.json) |
| ğŸ“… 2025-W51 | 132 | [View JSON](data/weekly/2025-W51.json) |
| ğŸ“… 2025-W50 | 230 | [View JSON](data/weekly/2025-W50.json) |
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
