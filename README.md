<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-45-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1519+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">45</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">169</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">781</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1519+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 31, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives</b> â­ 54</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20833) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20833) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20833)

**ğŸ’» Code:** [â­ Code](https://github.com/AgentAlphaAGI/Idea2Paper)

> arXivLens breakdown of this paper ğŸ‘‰ https://arxivlens.com/PaperView/Details/idea2story-an-automated-pipeline-for-transforming-research-concepts-into-complete-scientific-narratives-2345-6407a884 Executive Summary Detailed Breakdown Practical Applic...

</details>

<details>
<summary><b>2. Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</b> â­ 93</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20354) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20354) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20354)

**ğŸ’» Code:** [â­ Code](https://github.com/AMAP-ML/SpatialGenEval)

> A very interesting benchmark (ICLR2026) for T2I models!

</details>

<details>
<summary><b>3. Scaling Embeddings Outperforms Scaling Experts in Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21204) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21204) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21204)

> Embedding scaling can outperform mixture of experts for sparse language models, aided by system optimizations and speculative decoding, with LongCat-Flash-Lite achieving strong competitiveness.

</details>

<details>
<summary><b>4. DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</b> â­ 48</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22153) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22153) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22153)

**ğŸ’» Code:** [â­ Code](https://github.com/hzxie/DynamicVLA)

> TL; DR: DynamicVLA enables open-ended dynamic object manipulation by pairing a compact 0.4B VLM with low-latency Continuous Inference and Latent-aware Action Streaming, evaluated at scale through the new DOM benchmark in both simulation and the re...

</details>

<details>
<summary><b>5. OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</b> â­ 13</summary>

<br/>

**ğŸ‘¥ Authors:** Liming Zheng, Wenkang Han, Xuanle Zhao, Lei Chen, Albert-Zhong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21639) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21639) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21639)

**ğŸ’» Code:** [â­ Code](https://github.com/DocTron-hub/OCRVerse)

> OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models

</details>

<details>
<summary><b>6. MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21821) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21821) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21821)

> Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer ...

</details>

<details>
<summary><b>7. ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation</b> â­ 6</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21420) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21420) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21420)

**ğŸ’» Code:** [â­ Code](https://github.com/ZihaoHuang-notabot/ConceptMoE)

> ConceptMoE shifts language model processing from uniform token-level to adaptive concept-level computation. By learning to merge semantically similar tokens into unified concepts while preserving fine-grained granularity for complex tokens, it per...

</details>

<details>
<summary><b>8. PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22046) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22046) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22046)

> PLANING introduces a loosely coupled triangle-Gaussian representation and a monocular streaming framework that jointly achieves accurate geometry, high-fidelity rendering, and efficient planar abstraction for embodied AI applications.

</details>

<details>
<summary><b>9. Qwen3-ASR Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21337) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21337) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21337)

> Qwen3-ASR delivers two all-in-one ASR models with 52-language support and a non-autoregressive forced-aligner; achieves competitive SOTA accuracy, fast TTFT, and open-source Apache 2.0 release.

</details>

<details>
<summary><b>10. AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20730) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20730) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20730)

**ğŸ’» Code:** [â­ Code](https://github.com/euReKa025/AgentLongBench)

> The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the compl...

</details>

<details>
<summary><b>11. Exploring Reasoning Reward Model for Agents</b> â­ 14</summary>

<br/>

**ğŸ‘¥ Authors:** Zhixun Li, Tianshuo Peng, Manyuan Zhang, Kaituo Feng, bunny127

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22154) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22154) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22154)

**ğŸ’» Code:** [â­ Code](https://github.com/kxfan2002/Reagent)

> Github: https://github.com/kxfan2002/Reagent Paper: https://arxiv.org/pdf/2601.22154

</details>

<details>
<summary><b>12. LoL: Longer than Longer, Scaling Video Generation to Hour</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xiaojie Li, Tao Yang, Ming Li, Jie Wu, Justin Cui

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16914) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16914) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16914)

**ğŸ’» Code:** [â­ Code](https://github.com/justincui03/LoL)

> Scaling up video generation to hour long, please checkout our paper at: https://arxiv.org/abs/2601.16914 Project Page and code will released at: https://github.com/justincui03/LoL

</details>

<details>
<summary><b>13. Language-based Trial and Error Falls Behind in the Era of Experience</b> â­ 7</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21754) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21754) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21754)

**ğŸ’» Code:** [â­ Code](https://github.com/Harry-mic/SCOUT)

> While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch be...

</details>

<details>
<summary><b>14. Discovering Hidden Gems in Model Repositories</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yedid Hoshen, Eliahu Horwitz, Jonathan Kahana

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22157) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22157) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22157)

> An investigation of the available fine-tunes of popular foundation models. While over 90% of downloads are directed to the official base versions the paper shows the existence of other, rarely downloaded fine-tunes that significantly outperform them.

</details>

<details>
<summary><b>15. Latent Adversarial Regularization for Offline Preference Optimization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22083) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22083) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22083)

**ğŸ’» Code:** [â­ Code](https://github.com/enyijiang/GANPO)

> Most offline preference optimization methods (e.g., DPO) constrain policy updates using token-level divergences. However, token-space similarity is often a weak proxy for semantic or structural behavior. We propose GANPO, a plug-and-play regulariz...

</details>

<details>
<summary><b>16. Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Haitham Bou Ammar, Matthieu Zimmer, Rasul Tutunov, xtongji

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21590) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21590) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21590)

> What if RL isnâ€™t teaching LLMs how to reason, but just sharpening whatâ€™s already there? Most recent progress in LLM reasoning comes from RL post-training (GRPO, verifiers, rewards). But thereâ€™s growing evidence that these gains may come less from ...

</details>

<details>
<summary><b>17. Shaping capabilities with token-level data filtering</b> â­ 10</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21571) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21571) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21571)

**ğŸ’» Code:** [â­ Code](https://github.com/neilrathi/token-filtering)

> Key Findings: 1. Token-level Filtering vs Document-level Filtering (Figure 3) Token filtering Pareto-dominates document filtering : Can achieve equal reduction in undesired capabilities (equal medical loss) at lower cost to desired capabilities (l...

</details>

<details>
<summary><b>18. Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21051) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21051) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21051)

> Model card: https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning

</details>

<details>
<summary><b>19. Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18129) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18129) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18129)

**ğŸ’» Code:** [â­ Code](https://github.com/scb-10x/typhoon-s)

> Code: https://github.com/scb-10x/typhoon-s Artifact: https://huggingface.co/collections/typhoon-ai/typhoon-s

</details>

<details>
<summary><b>20. VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning</b> â­ 12</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22069) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22069) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22069)

**ğŸ’» Code:** [â­ Code](https://github.com/w-yibo/VTC-R1)

> We propose VTC-R1, an efficient long-context reasoning paradigm that integrates vision-text compression into iterative reasoning. By rendering previous reasoning segments into compact visual representations, VTC-R1 replaces long textual contexts w...

</details>

<details>
<summary><b>21. MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yong Man Ro, Youngchae Chee, Se Yeon Kim, topyun

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21181) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21181) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21181)

> Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interacti...

</details>

<details>
<summary><b>22. DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20975) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20975) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20975)

> Proposes DeepSearchQA, a 900-prompt benchmark across 17 fields to test long-horizon search, info synthesis, deduplication, and stopping criteria for open-web research agents.

</details>

<details>
<summary><b>23. EEG Foundation Models: Progresses, Benchmarking, and Open Problems</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17883) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17883) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17883)

**ğŸ’» Code:** [â­ Code](https://github.com/Dingkun0817/EEG-FM-Benchmark)

> We propose fair and comprehensive benchmarking for open source EEG foundation models.

</details>

<details>
<summary><b>24. Beyond Imitation: Reinforcement Learning for Active Latent Planning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Wee Sun Lee, zz1358m

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21598) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21598) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21598)

> Our recent work on Latent Reasoning

</details>

<details>
<summary><b>25. One-step Latent-free Image Generation with Pixel Mean Flows</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Zhicheng Jiang, Hanhong Zhao, Qiao Sun, Susie Lu, Yiyang Lu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22158) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22158) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22158)

> One-step Latent-free Image Generation with Pixel Mean Flows

</details>

<details>
<summary><b>26. Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22156) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22156) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22156)

**ğŸ’» Code:** [â­ Code](https://github.com/thunlp/hybrid-linear-attention) â€¢ [â­ Code](https://www.github.com/THUNLP/hybrid-linear-attention)

> Code: https://www.github.com/THUNLP/hybrid-linear-attention

</details>

<details>
<summary><b>27. FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22146) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22146) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22146)

> @ AjayP13 and @ craffel really interesting work and approach, do you plan to add support for multilingual instructions ğŸ¤”

</details>

<details>
<summary><b>28. KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Danilo Mandic, Giorgos Iacovides, Yuxuan Gu, WuyangZzzz

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21579) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21579) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21579)

**ğŸ’» Code:** [â­ Code](https://github.com/wz1119/KromHC)

> KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices

</details>

<details>
<summary><b>29. Self-Improving Pretraining: using post-trained models to pretrain better models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21343) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21343) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21343)

> Streaming pretraining uses a strong post-trained model to judge next-token generations with RL, improving quality, safety, and factuality earlier in training.

</details>

<details>
<summary><b>30. ECO: Quantized Training without Full-Precision Master Weights</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22101) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22101) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22101)

> We present Error-Compensating Optimizer (ECO), which integrates with standard optimizers and, for the first time, enables quantized training of large-scale LLMs without requiring high-precision master weights.

</details>

<details>
<summary><b>31. MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</b> â­ 41</summary>

<br/>

**ğŸ‘¥ Authors:** Jianxun Cui, Xuancheng Zhang, Donglin Di, Baorui Ma, yjh001

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22054) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22054) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22054)

**ğŸ’» Code:** [â­ Code](https://github.com/metric-anything/metric-anything)

> Project Page: https://metric-anything.github.io/metric-anything-io/ Code: https: https://github.com/metric-anything/metric-anything

</details>

<details>
<summary><b>32. Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21996) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21996) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21996)

**ğŸ’» Code:** [â­ Code](https://github.com/chenjianhuii/Mechanistic-Data-Attribution)

> We introduce Mechanistic Data Attribution (MDA), a new paradigm that shifts the focus of mechanistic interpretability from post-hoc circuit analysis to the causal formation of these mechanisms during training.

</details>

<details>
<summary><b>33. Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Guanhua Chen, Yong Wang, Kangrui Cen, Hongyang Wei, Zihan Su

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21406) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21406) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21406)

**ğŸ’» Code:** [â­ Code](https://github.com/Sugewud/UniMRG)

> Paper: https://arxiv.org/abs/2601.21406 Github: https://github.com/Sugewud/UniMRG Project: https://sugewud.github.io/UniMRG-Project/

</details>

<details>
<summary><b>34. BMAM: Brain-inspired Multi-Agent Memory Framework</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Mingkun Xu, Yujie Wu, Yusong Wang, Jiaxiang Liu, innovation64

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20465) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20465) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20465)

**ğŸ’» Code:** [â­ Code](https://github.com/innovation64/BMAM)

> We introduce BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture designed to solve "soul erosion"â€”the loss of temporal grounding and consistency in long-term agent interactions. ğŸ§  Key Innovations: Cognitive-inspired Arc...

</details>

<details>
<summary><b>35. JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Urska Jelercic, Matan Ben Yosef, Tavi Halperin, Naomi Ken Korem, Anthony Chen

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22143) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22143) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22143)

> No abstract available.

</details>

<details>
<summary><b>36. FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.19001) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.19001) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.19001)

> ICLR2026

</details>

<details>
<summary><b>37. Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jesse Roberts, Micah Rentschler

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21268) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21268) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21268)

> We present Reinforcement Learning from Meta-Evaluation (RLME), a label-free RL framework that trains LLMs using evaluator judgments to natural-language meta-questions, achieving performance comparable to supervised rewards while scaling to ambiguo...

</details>

<details>
<summary><b>38. Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20103) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20103) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20103)

> We show that contrasting reward hacks in an outlier detection setting helps LLMs detect code hacking behaviors. We further show that a cluster's benign-to-hacked trajectory ratio influences this detection rate. Finally we perform thorough QA and s...

</details>

<details>
<summary><b>39. Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Melody Ma, Iram Kamdar, Yunyan Ouyang, Ziling Gong, Franck-Dernoncourt

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17690) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17690) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17690)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Atten...

</details>

<details>
<summary><b>40. PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Stefano Petrangeli, Yu Shen, Sunav Choudhary, Huaxiaoyue Wang, Franck-Dernoncourt

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11747) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11747) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11747)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing (2026) Styles...

</details>

<details>
<summary><b>41. WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21872) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21872) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21872)

> Accepted at ICLR 2026

</details>

<details>
<summary><b>42. Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Liming Chen, Emmanuel DellandrÃ©a, Bruno Machado, Beegbrain

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21416) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21416) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21416)

> The ability of visuomotor policies to generalize across tasks and environments critically depends on the structure of the underlying visual representations. While most state-of-the-art robot policies rely on either global or dense features from pr...

</details>

<details>
<summary><b>43. WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Pranay Boreddy, Ayush Agrawal, Jim Solomon, Howard Zhang, Rishi Upadhyay

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21282) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21282) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21282)

> WorldBench provides a disentangled, concept-specific video benchmark to rigorously evaluate physical reasoning in world models and their video generation.

</details>

<details>
<summary><b>44. STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Liming Chen, Emmanuel DellandrÃ©a, Beegbrain

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20381) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20381) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20381)

> We introduce a slot-based object-centric method with a "task-awareness" alignment in order to learn robotic manipulation. Our method obtains strong generalization improvements over existing VFM by simply adding a few layers of structure and keepin...

</details>

<details>
<summary><b>45. Flow-based Extremal Mathematical Structure Discovery</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18005) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18005) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18005)

> No abstract available.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 45 |
| ğŸ“… Today | [`2026-01-31.json`](data/daily/2026-01-31.json) | 45 |
| ğŸ“† This Week | [`2026-W04.json`](data/weekly/2026-W04.json) | 169 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 781 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-31 | 45 | [View JSON](data/daily/2026-01-31.json) |
| ğŸ“„ 2026-01-30 | 21 | [View JSON](data/daily/2026-01-30.json) |
| ğŸ“„ 2026-01-29 | 21 | [View JSON](data/daily/2026-01-29.json) |
| ğŸ“„ 2026-01-28 | 37 | [View JSON](data/daily/2026-01-28.json) |
| ğŸ“„ 2026-01-27 | 18 | [View JSON](data/daily/2026-01-27.json) |
| ğŸ“„ 2026-01-26 | 27 | [View JSON](data/daily/2026-01-26.json) |
| ğŸ“„ 2026-01-25 | 27 | [View JSON](data/daily/2026-01-25.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W04 | 169 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |
| ğŸ“… 2026-W02 | 232 | [View JSON](data/weekly/2026-W02.json) |
| ğŸ“… 2026-W01 | 156 | [View JSON](data/weekly/2026-W01.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 781 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
