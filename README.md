<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-40-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1649+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">40</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">85</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">130</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1649+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** February 03, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas</b> â­ 84</summary>

<br/>

**ğŸ‘¥ Authors:** Hao Zhou, Shuaiting Chen, Haotian Wang, jade0101, Emperorizzis

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21558) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21558) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21558)

**ğŸ’» Code:** [â­ Code](https://github.com/LianjiaTech/astra)

> ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas

</details>

<details>
<summary><b>2. Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22813) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22813) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22813)

**ğŸ’» Code:** [â­ Code](https://github.com/IST-DASLab/Quartet-II)

> A SOTA NVFP4 LLM pre-training method based on MS-EDEN unbiased gradient estimation. Code is available on GitHub .

</details>

<details>
<summary><b>3. Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22975) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22975) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22975)

> TL;DR: We introduce Golden Goose ğŸ¦¢, a simple method that synthesizes unlimited RLVR tasks from unverifiable internet text by constructing multiple-choice fill-in-the-middle problems. This enables the use of reasoning-rich unverifiable corpora typi...

</details>

<details>
<summary><b>4. THINKSAFE: Self-Generated Safety Alignment for Reasoning Models</b> â­ 3</summary>

<br/>

**ğŸ‘¥ Authors:** Minki Kang, Gyeongman Kim, YuminChoi, Sangsang, Seanie-lee

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23143) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23143) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23143)

**ğŸ’» Code:** [â­ Code](https://github.com/seanie12/ThinkSafe.git)

> THINKSAFE: Self-Generated Safety Alignment for Reasoning Models

</details>

<details>
<summary><b>5. TTCS: Test-Time Curriculum Synthesis for Self-Evolving</b> â­ 19</summary>

<br/>

**ğŸ‘¥ Authors:** Chengsong Huang, Zongpei Teng, Yunbo Tang, Zhishang Xiang, ChengyiYang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22628) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22628) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22628)

**ğŸ’» Code:** [â­ Code](https://github.com/XMUDeepLIT/TTCS)

> TTCS, a new paradigm for self-evolving

</details>

<details>
<summary><b>6. PaperBanana: Automating Academic Illustration for AI Scientists</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23265) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23265) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23265)

> PaperBanana automates publication-ready AI research illustrations via an agentic framework using VLMs and image models, orchestrating reference retrieval, planning, rendering, and self-critique with a benchmarking suite.

</details>

<details>
<summary><b>7. Do Reasoning Models Enhance Embedding Models?</b> â­ 5</summary>

<br/>

**ğŸ‘¥ Authors:** Elton Chun-Chai Li, Kwun Hang Lau, Huihao Jing, Shaojin Chen, lucaswychan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21192) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21192) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21192)

**ğŸ’» Code:** [â­ Code](https://github.com/HKUST-KnowComp/Reasoning-Embedding)

> Our analysis revealed a phenomenon we term Manifold Realignment. RLVR is a Trajectory Optimizer : We found that RLVR irreversibly reorganizes the local geometry of the latent manifold but largely preserves the global manifold geometry (the overall...

</details>

<details>
<summary><b>8. FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23182) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23182) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23182)

**ğŸ’» Code:** [â­ Code](https://github.com/ShirleYoung/FourierSampler)

> Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectr...

</details>

<details>
<summary><b>9. Causal World Modeling for Robot Control</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ruilin Wang, Shuai Yang, Yiming Luo, Qihang Zhang, Lin Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21998) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21998) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21998)

> No abstract available.

</details>

<details>
<summary><b>10. ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought</b> â­ 15</summary>

<br/>

**ğŸ‘¥ Authors:** Zhifeng Gao, Hongteng Xu, Guojiang Zhao, Haotian Liu, FanmengWang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23184) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23184) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23184)

**ğŸ’» Code:** [â­ Code](https://github.com/FanmengWang/ReGuLaR)

> Introduces ReGuLaR, a variational latent reasoning framework that renders reasoning as images to regularize posterior inference, achieving efficient multimodal reasoning beyond traditional chain of thought.

</details>

<details>
<summary><b>11. DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21716) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21716) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21716)

> Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal moti...

</details>

<details>
<summary><b>12. SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Bolin Ni, Fangzhi Xu, Yuhao Shen, Jinyang Wu, thkelper

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22491) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22491) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22491)

> No abstract available.

</details>

<details>
<summary><b>13. DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20218) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20218) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20218)

> A dense reward for RL in flow matching models.

</details>

<details>
<summary><b>14. Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22636) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22636) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22636)

> Real-world jailbreak attackers donâ€™t usually try once. They try many times, in parallel, until the model slips. Thatâ€™s why adversarial risk canâ€™t be captured by attack success rate on a single attempt (ASR@1). As the number of attempts N grows, ri...

</details>

<details>
<summary><b>15. DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jong Chul Ye, Byunghee Cha, Hun Chang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22904) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22904) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22904)

> DINO-SAE bridges semantic directions and pixel fidelity via spherical latent diffusion with hierarchical patch embedding and cosine alignment, achieving state-of-the-art reconstruction while preserving semantic alignment.

</details>

<details>
<summary><b>16. PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Zelun Zhang, Tingquan Gao, Suyin Liang, sunflowerting78, ChengCui

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21957) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21957) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21957)

> No abstract available.

</details>

<details>
<summary><b>17. RM -RF: Reward Model for Run-Free Unit Test Evaluation</b> â­ 5</summary>

<br/>

**ğŸ‘¥ Authors:** Mikhail Klementev, doooori, rmndrnts, dangrebenkin, brucheselena

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13097) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13097) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13097)

**ğŸ’» Code:** [â­ Code](https://github.com/trndcenter/RM-RF-unit-tests)

> RM -RF: Reward Model for Run-Free Unit Test Evaluation proposes a novel lightweight reward model  that predicts unit test quality without compiling or executing code by inferring three execution-derived signals directly from source and test code: ...

</details>

<details>
<summary><b>18. DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23161) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23161) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23161)

**ğŸ’» Code:** [â­ Code](https://github.com/NKU-HLT/DIFFA)

> DIFFA-2 provides a practical diffusion-based large audio language model with semantic/acoustic adapters and a four-stage curriculum, improving general audio understanding under practical budgets.

</details>

<details>
<summary><b>19. NativeTok: Native Visual Tokenization for Improved Image Generation</b> â­ 4</summary>

<br/>

**ğŸ‘¥ Authors:** Zhendong Mao, Weinan Jia, Mengqi Huang, Bin Wu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22837) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22837) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22837)

**ğŸ’» Code:** [â­ Code](https://github.com/wangbei1/Nativetok)

> Introduces native visual tokenization with causal dependencies, via NativeTok (MIT and MoCET) and hierarchical training for efficient, coherent image reconstruction with relational token constraints.

</details>

<details>
<summary><b>20. Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22642) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22642) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22642)

> No abstract available.

</details>

<details>
<summary><b>21. MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yuxin Chen, Wenyu Mao, Yu Yang, Shugui Liu, Yaorui Shi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21468) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21468) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21468)

> MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.

</details>

<details>
<summary><b>22. TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Vadim Alperovich, dangrebenkin, rmndrnts, doooori, brucheselena

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18241) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18241) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18241)

**ğŸ’» Code:** [â­ Code](https://github.com/trndcenter/TAM-Eval)

> ğŸ§ª TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance Whatâ€™s new: Large Language Models (LLMs) have been widely explored for unit test generation , but real-world test suite maintenance â€” like creating, updating, and repairing tests as c...

</details>

<details>
<summary><b>23. Scaling Multiagent Systems with Process Rewards</b> â­ 43</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23228) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23228) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23228)

**ğŸ’» Code:** [â­ Code](https://github.com/ltjed/multiagent-coaching)

> Define and train your own multiagent system @ our github repo !

</details>

<details>
<summary><b>24. Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21358) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21358) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21358)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Forest Before Trees: Latent Superposition for Efficient Visual Reasoning (2...

</details>

<details>
<summary><b>25. Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23188) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23188) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23188)

> ğŸš€ New Research Alert! ğŸ§ âœ¨ Weâ€™re excited to share our latest work: Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience ğŸ” Whatâ€™s the key idea? Deep search agents powered by LLMs excel at multi-step reasoning and...

</details>

<details>
<summary><b>26. Revisiting Diffusion Model Predictions Through Dimensionality</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Chaoyang Wang, Qing Jin

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21419) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21419) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21419)

> not sure which to choose: x0 prediction or velocity prediction? this paper provides a universal solution to find the optimal solution for you

</details>

<details>
<summary><b>27. Real-Time Aligned Reward Model beyond Semantics</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jianbin Zheng, Yuxi Ren, Xin Xia, Yikunb, hzxllll

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22664) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22664) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22664)

> RLHF is central to aligning LLMs with human preferences, but it often suffers from reward overoptimization: the policy learns to game the reward model instead of truly following human intent. A key reason? Distribution shiftâ€”the policy keeps chang...

</details>

<details>
<summary><b>28. LMK > CLS: Landmark Pooling for Dense Embeddings</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yulong Li, Parul Awasthy, Aashka Trivedi, vishwajeetkumar, meetdoshi90

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21525) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21525) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21525)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in De...

</details>

<details>
<summary><b>29. Continual GUI Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20732) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20732) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20732)

> Some of the observations founded are :- -- Static GUI training breaks under real world change : GUI agents trained on fixed datasets degrade badly when UI domains (mobile --> desktop --> web) or resolutions (1080p -> 4K) shift, mainly due to unsta...

</details>

<details>
<summary><b>30. Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Bin Liang, Zezhong Wang, Rui Wang, Zhiwei Zhang, Hiiamein

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15625) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15625) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15625)

> Robust Tool Use via FISSION-GRPO: Learning to Recover from Execution Errors

</details>

<details>
<summary><b>31. Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Michal Byra, Alberto Presta, GrzegorzStefanski

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22141) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22141) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22141)

> This work challenges a core assumption of the Lottery Ticket Hypothesis: that a single sparse subnetwork can serve all data. The authors show that under heterogeneity, multiple specialized winning tickets outperform a universal one, reframing prun...

</details>

<details>
<summary><b>32. Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving</b> â­ 32</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22032) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22032) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22032)

**ğŸ’» Code:** [â­ Code](https://github.com/linhanwang/Drive-JEPA)

> End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. Th...

</details>

<details>
<summary><b>33. Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xialiang Tong, Yinqi Bai, Xing Li, Jie Wang, Qingyue Yang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21709) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21709) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21709)

> We systematically analyze attention patterns from a unified temporal perspective and find that embedding temporal self-similarity and RoPE are key factors underlying streaming, retrieval, seasonal, and reaccess attention patterns. We further apply...

</details>

<details>
<summary><b>34. Memorization Dynamics in Knowledge Distillation for Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15394) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15394) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15394)

> We show that knowledge distillation in language models can give both improved generalization and reduced memorization.

</details>

<details>
<summary><b>35. Machine Learning for Energy-Performance-aware Scheduling</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Yifei Shi, Peter2023HuggingFace

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.23134) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.23134) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.23134)

**ğŸ’» Code:** [â­ Code](https://github.com/PeterHUistyping/ml-cpu-sched)

> Machine Learning for Energy-Performance-aware Scheduling. @ misc {HuShi2026mlcpusched,
      title={Machine Learning for Energy-Performance-aware Scheduling}, 
      author={Zheyuan Hu and Yifei Shi},
      year={2026},
      eprint={2601.23134},
...

</details>

<details>
<summary><b>36. Visual Personalization Turing Test</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Kuan-Chieh Jackson Wang, Sergey Tulyakov, James Burgess, Rameen Abdal

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22680) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22680) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22680)

> No abstract available.

</details>

<details>
<summary><b>37. ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22666) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22666) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22666)

> Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit sup...

</details>

<details>
<summary><b>38. Value-Based Pre-Training with Downstream Feedback</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22108) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22108) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22108)

> Weâ€™re entering the age of research, not just the age of scaling. Bigger models gave us horsepower. But pretraining still has almost no steering wheel. Todayâ€™s foundation models learn in an open loop: pick a proxy objective (nextâ€‘token / fixed augm...

</details>

<details>
<summary><b>39. SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21666) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21666) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21666)

> SONIC-O1: A Real-World Benchmark for Evaluating Multimodal LLMs on Audio-Video Understanding SONIC-O1 is a fully human-verified benchmark for real-world audioâ€“video conversations: 13 conversational domains, 4,958 annotated instances, plus demograp...

</details>

<details>
<summary><b>40. KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization</b> â­ 56</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21526) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21526) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21526)

**ğŸ’» Code:** [â­ Code](https://github.com/Leeroo-AI/kapso)

> We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 40 |
| ğŸ“… Today | [`2026-02-03.json`](data/daily/2026-02-03.json) | 40 |
| ğŸ“† This Week | [`2026-W05.json`](data/weekly/2026-W05.json) | 85 |
| ğŸ—“ï¸ This Month | [`2026-02.json`](data/monthly/2026-02.json) | 130 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-02-03 | 40 | [View JSON](data/daily/2026-02-03.json) |
| ğŸ“„ 2026-02-02 | 45 | [View JSON](data/daily/2026-02-02.json) |
| ğŸ“„ 2026-02-01 | 45 | [View JSON](data/daily/2026-02-01.json) |
| ğŸ“„ 2026-01-31 | 45 | [View JSON](data/daily/2026-01-31.json) |
| ğŸ“„ 2026-01-30 | 21 | [View JSON](data/daily/2026-01-30.json) |
| ğŸ“„ 2026-01-29 | 21 | [View JSON](data/daily/2026-01-29.json) |
| ğŸ“„ 2026-01-28 | 37 | [View JSON](data/daily/2026-01-28.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W05 | 85 | [View JSON](data/weekly/2026-W05.json) |
| ğŸ“… 2026-W04 | 214 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |
| ğŸ“… 2026-W02 | 232 | [View JSON](data/weekly/2026-W02.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-02 | 130 | [View JSON](data/monthly/2026-02.json) |
| ğŸ—“ï¸ 2026-01 | 781 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
