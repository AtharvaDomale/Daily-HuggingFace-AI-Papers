<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-21-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-275+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">21</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">137</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">324</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">275+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 12, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Guixun Luo, Hanwen Liang, Longfei Li, yuyangyin, KXingLab

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09363) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09363) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09363)

> StereoWorld presents geometry-aware monocular-to-stereo video generation using a pretrained video generator with geometry regularization and tiling for high-resolution, consistent stereo videos.

</details>

<details>
<summary><b>2. BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** tamarott, Antoniotorralbaborruel, yuvalgolbari, mcosarinsky, navvew

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08560) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08560) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08560)

> We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex.

</details>

<details>
<summary><b>3. OmniPSD: Layered PSD Generation with Diffusion Transformer</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Cheng Liu, AnalMom, wanghaofan, yiren98

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09247) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09247) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09247)

> OmniPSD presents a diffusion-transformer framework for text-to-PSD generation and image-to-PSD decomposition, enabling layered, transparent PSDs with hierarchical, editable channels via in-context learning.

</details>

<details>
<summary><b>4. Composing Concepts from Images and Videos via Concept-prompt Binding</b> â­ 45</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09824) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09824) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09824)

**ğŸ’» Code:** [â­ Code](https://github.com/refkxh/bico)

> We introduce Bind & Compose (BiCo), a one-shot method that enables flexible visual concept composition by binding visual concepts with the corresponding prompt tokens and composing the target prompt with bound tokens from various sources. ğŸŒ Projec...

</details>

<details>
<summary><b>5. InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</b> â­ 30</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08829) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08829) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08829)

**ğŸ’» Code:** [â­ Code](https://github.com/hustvl/InfiniteVL)

> Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation ...

</details>

<details>
<summary><b>6. HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</b> â­ 17</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09928) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09928) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09928)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenHelix-Team/HiF-VLA)

> Code and checkpoints are available! Github: https://github.com/OpenHelix-Team/HiF-VLA Project page: https://hifvla.github.io/

</details>

<details>
<summary><b>7. Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yang Zhang, guokan-shang, mvazirg, amr-mohamed

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.02892) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.02892) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.02892)

> SchED introduces a training-free, early-exit decoding criterion for diffusion LLMs , halting sampling once a smooth, progress-adaptive confidence threshold is satisfied. SchED achieves up to ~4Ã— decoding speedups on average with â‰¥99â€“100% performan...

</details>

<details>
<summary><b>8. Rethinking Chain-of-Thought Reasoning for Videos</b> â­ 4</summary>

<br/>

**ğŸ‘¥ Authors:** Liwei Wang, Yin Li, Zi-Yuan Hu, Yiwu Zhong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09616) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09616) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09616)

**ğŸ’» Code:** [â­ Code](https://github.com/LaVi-Lab/Rethink_CoT_Video)

> Rethinking Chain-of-Thought Reasoning for Videos

</details>

<details>
<summary><b>9. EtCon: Edit-then-Consolidate for Reliable Knowledge Editing</b> â­ 5</summary>

<br/>

**ğŸ‘¥ Authors:** Chenglin Li, Wenhong Zhu, Ruilin Li, Rethinker, CodeGoat24

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04753) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04753) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04753)

**ğŸ’» Code:** [â­ Code](https://github.com/RlinL/EtCon)

> EtCon: Edit-then-Consolidate for Reliable Knowledge Editing

</details>

<details>
<summary><b>10. UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09864) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09864) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09864)

> Proposes UniUGP, a unified framework integrating scene understanding, video generation, and trajectory planning for autonomous driving with visual reasoning.

</details>

<details>
<summary><b>11. WonderZoom: Multi-Scale 3D World Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jiajun Wu, Hong-Xing Yu, Jin Cao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09164) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09164) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09164)

> WonderZoom enables multi-scale 3D world generation from a single image via scale-adaptive Gaussian surfels and progressive detail synthesis for zoomed-in realism.

</details>

<details>
<summary><b>12. Learning Unmasking Policies for Diffusion Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09106) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09106) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09106)

> Trains a lightweight RL-based policy to unmask tokens in masked diffusion LMs, achieving competitive performance with heuristics and generalizing to new models and longer sequences.

</details>

<details>
<summary><b>13. Towards a Science of Scaling Agent Systems</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Samuel Schmidgall, Chunjong Park, Chanwoo Park, Ken Gu, Yubin Kim

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08296) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08296) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08296)

> No abstract available.

</details>

<details>
<summary><b>14. IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09663) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09663) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09663)

> Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the f...

</details>

<details>
<summary><b>15. TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.05446) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.05446) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.05446)

> Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and effic...

</details>

<details>
<summary><b>16. Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS</b> â­ 7</summary>

<br/>

**ğŸ‘¥ Authors:** Morteza Abolghasemi, hrrabiee, ZahraDehghanian97, dninvb, MahtaFetrat

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08006) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08006) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08006)

**ğŸ’» Code:** [â­ Code](https://github.com/MahtaFetrat/Piper-with-LCA-Phonemizer)

> Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with ...

</details>

<details>
<summary><b>17. VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04519) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04519) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04519)

> We introduce VideoSSM, an AR video diffusion model equipped with a novel hybrid memory architecture that combines a causal sliding-window local lossless cache with an SSM-based global compressed memory for long video generation.

</details>

<details>
<summary><b>18. GimbalDiffusion: Gravity-Aware Camera Control for Video Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09112) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09112) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09112)

> No abstract available.

</details>

<details>
<summary><b>19. Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07222) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07222) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07222)

**ğŸ’» Code:** [â­ Code](https://github.com/michaeltian108/FDA)

> We had an interesting yet explorable observation that lowering the attention on function words of VLMs increaes robustness and zero-shot performance on several datasets/models/tasks, casuing little or no performance drops , surpasing SOTA adversar...

</details>

<details>
<summary><b>20. Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.05402) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.05402) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.05402)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasti...

</details>

<details>
<summary><b>21. Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Hengshu Zhu, Hongke Zhao, ChuangZhao, likang03, zxq1942461723

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.01453) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.01453) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.01453)

**ğŸ’» Code:** [â­ Code](https://github.com/xqz614/Awesome-Agentic-Clinical-Dialogue)

> Fresh medical LLM survey

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 21 |
| ğŸ“… Today | [`2025-12-12.json`](data/daily/2025-12-12.json) | 21 |
| ğŸ“† This Week | [`2025-W49.json`](data/weekly/2025-W49.json) | 137 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 324 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-12 | 21 | [View JSON](data/daily/2025-12-12.json) |
| ğŸ“„ 2025-12-11 | 25 | [View JSON](data/daily/2025-12-11.json) |
| ğŸ“„ 2025-12-10 | 29 | [View JSON](data/daily/2025-12-10.json) |
| ğŸ“„ 2025-12-09 | 24 | [View JSON](data/daily/2025-12-09.json) |
| ğŸ“„ 2025-12-08 | 38 | [View JSON](data/daily/2025-12-08.json) |
| ğŸ“„ 2025-12-07 | 38 | [View JSON](data/daily/2025-12-07.json) |
| ğŸ“„ 2025-12-06 | 38 | [View JSON](data/daily/2025-12-06.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W49 | 137 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 324 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
