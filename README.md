<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-37-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1432+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">37</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">82</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">694</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1432+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 28, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs</b> â­ 644</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17058) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17058) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17058)

**ğŸ’» Code:** [â­ Code](https://github.com/weAIDB/awesome-data-llm)

> Please refer to our repository for more details: https://github.com/weAIDB/awesome-data-llm .

</details>

<details>
<summary><b>2. daVinci-Dev: Agent-native Mid-training for Software Engineering</b> â­ 22</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18418) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18418) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18418)

**ğŸ’» Code:** [â­ Code](https://github.com/GAIR-NLP/daVinci-Dev)

> Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories.  While post-training...

</details>

<details>
<summary><b>3. The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</b> â­ 228</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17737) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17737) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17737)

**ğŸ’» Code:** [â­ Code](https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent)

> Convert dialogue to script for video generation.

</details>

<details>
<summary><b>4. Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17027) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17027) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17027)

**ğŸ’» Code:** [â­ Code](https://github.com/SciGenBench/SciGenBench)

> While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often p...

</details>

<details>
<summary><b>5. Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</b> â­ 11</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17367) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17367) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17367)

**ğŸ’» Code:** [â­ Code](https://github.com/LCM-Lab/Elastic-Attention)

> Elastic Attention enables models to achieve both strong performance and efficient inference by dynamically allocating computation modes (Full Attention or Sparse Attention) to each attention head through our designed Attention Router, adapting spa...

</details>

<details>
<summary><b>6. iFSQ: Improving FSQ for Image Generation with 1 Line of Code</b> â­ 59</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17124) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17124) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17124)

**ğŸ’» Code:** [â­ Code](https://github.com/Tencent-Hunyuan/iFSQ)

> AR or Diffusion? Itâ€™s been hard to judge because different tokenizers (VQ vs. VAE) Enter iFSQ with just 1 line of code! We found: (1) AR wins on efficiency, but Diffusion hits a higher quality ceiling. (2) The sweet spot for representations is ~4 ...

</details>

<details>
<summary><b>7. Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18778) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18778) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18778)

> Check out our blog post: https://ssundaram21.github.io/soar/ !

</details>

<details>
<summary><b>8. Self-Refining Video Sampling</b> â­ 43</summary>

<br/>

**ğŸ‘¥ Authors:** Sangwon Jang, jaehong31, sainx, harry9704, taekyungki

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18577) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18577) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18577)

**ğŸ’» Code:** [â­ Code](https://github.com/agwmon/self-refine-video)

> [TL;DR] We present self-refining video sampling method that reuses a pre-trained video generator as a denoising autoencoder to iteratively refine latents. With ~50% additional NFEs, it improves physical realism (e.g., motion coherence and physics ...

</details>

<details>
<summary><b>9. VIBEVOICE-ASR Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18184) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18184) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18184)

> VibeVoice-ASR is a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized C...

</details>

<details>
<summary><b>10. DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18137) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18137) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18137)

> DeepPlanning â€” a new benchmark for long-horizon agent planning in real-world scenarios!

</details>

<details>
<summary><b>11. CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15849) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15849) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15849)

**ğŸ’» Code:** [â­ Code](https://github.com/yumeow0122/CGPT)

> General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and queryâ€“table mismatch. Recent LLM-based retrieval a...

</details>

<details>
<summary><b>12. STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15860) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15860) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15860)

**ğŸ’» Code:** [â­ Code](https://github.com/adsl135789/STAR)

> Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment par...

</details>

<details>
<summary><b>13. Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18217) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18217) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18217)

> Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifica...

</details>

<details>
<summary><b>14. AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17761) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17761) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17761)

**ğŸ’» Code:** [â­ Code](https://github.com/ModalityDance/AR-Omni)

> AR-Omni is a single-decoder, single-token-stream autoregressive any-to-any model. It unifies multimodal generation (text, images, and speech) as standard next-token prediction over interleaved sequences. It improves training and inference with tas...

</details>

<details>
<summary><b>15. TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18744) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18744) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18744)

> No abstract available.

</details>

<details>
<summary><b>16. Agentic Very Long Video Understanding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18157) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18157) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18157)

> EGAgent uses entity scene graphs and structured search over long, multimodal video streams to enable cross-modal, temporally coherent reasoning for egocentric video understanding.

</details>

<details>
<summary><b>17. DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal</b> â­ 3</summary>

<br/>

**ğŸ‘¥ Authors:** Jingjun Xu, Yingjie Yu, jiaxuanYou, HakHan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18081) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18081) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18081)

**ğŸ’» Code:** [â­ Code](https://github.com/ulab-uiuc/DRPG-RebuttalAgent/tree/master)

> DRPG - An Agentic Framework for Academic Rebuttal

</details>

<details>
<summary><b>18. IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** yjang43, cfmata, jjh6297, kahnchana, jongwoopark7978

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16207) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16207) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16207)

> IVRA is a training-free, inference-time drop-in that restores spatial structure in VLA models by injecting encoder affinity signals into selected LLM layers (no retraining, no extra parameters, ~3% latency). It generalizes across LLaRA, OpenVLA, a...

</details>

<details>
<summary><b>19. SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18202) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18202) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18202)

> SAGE automatically generates difficulty-controlled deep-search QA pairs via an iterative agent-feedback loop, yielding higher-quality training data that improves deep search agent performance and adaptability.

</details>

<details>
<summary><b>20. SkyReels-V3 Technique Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17323) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17323) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17323)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API KlingAvatar 2.0 Technical Report (2025) YingVideo-MV: Music-Driven Multi-St...

</details>

<details>
<summary><b>21. Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17111) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17111) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17111)

> Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing...

</details>

<details>
<summary><b>22. One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment</b> â­ 4</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18731) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18731) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18731)

**ğŸ’» Code:** [â­ Code](https://github.com/ModalityDance/MRM)

> Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and auto...

</details>

<details>
<summary><b>23. End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Shrikanth Narayanan, Catherine Lord, Somer Bishop, Anfeng Xu, tiantiaf

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17640) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17640) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17640)

**ğŸ’» Code:** [â­ Code](https://github.com/usc-sail/joint-asr-diarization-child-adult)

> Accurate transcription and speaker diarization of childâ€“adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely...

</details>

<details>
<summary><b>24. A Mechanistic View on Video Generation as World Models: State and Dynamics</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17067) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17067) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17067)

> While large-scale video generation models show signs of emergent physical coherence, they remain distinct from true world models. A critical gap persists between modern "stateless" video architectures and the "state-centric" requirements of classi...

</details>

<details>
<summary><b>25. Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.13599) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.13599) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.13599)

> One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering be...

</details>

<details>
<summary><b>26. UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** April Yi Wang, Mustafa Doga Dogan, Xiaotian Su, Junling Wang, HenryLhy

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18759) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18759) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18759)

> UI Remix enables interactive, example-driven design for mobile interfaces using multimodal retrieval-augmented generation to search, adapt, and remix interface components with source transparency.

</details>

<details>
<summary><b>27. Masked Depth Modeling for Spatial Perception</b> â­ 252</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17895) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17895) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17895)

**ğŸ’» Code:** [â­ Code](https://github.com/Robbyant/lingbot-depth)

> Website: technology.robbyant.com/lingbot-depth Code: https://github.com/Robbyant/lingbot-depth

</details>

<details>
<summary><b>28. PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** afaji, gentaiscool, faridlazuarda, hanifmz0711, rifqifarhansyah

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17277) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17277) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17277)

> PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues

</details>

<details>
<summary><b>29. Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control</b> â­ 20</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15015) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15015v1) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15015)

**ğŸ’» Code:** [â­ Code](https://github.com/safe-autonomous-systems/fluidgym)

> FluidGym: Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control There is enormous potential for reinforcement learning and other data-driven control paradigms for controlling large-scale fluid flows. But RL r...

</details>

<details>
<summary><b>30. The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14127) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14127) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14127)

**ğŸ’» Code:** [â­ Code](https://github.com/thu-coai/MIR-SafetyBench)

> As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark ...

</details>

<details>
<summary><b>31. Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Guanhong Tao, Yanjun Zhang, Leo Yu Zhang, Xiaomei Zhang, plll123

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.12042) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.12042) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.12042)

> Visual token compression is widely used to accelerate inference in Large Visionâ€“Language Models (LVLMs), enabling deployment in latency- and resource-constrained settings. This paper reveals that such compression introduces a previously overlooked...

</details>

<details>
<summary><b>32. MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18790) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18790) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18790)

> Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in c...

</details>

<details>
<summary><b>33. HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Junhong Lin, zhoudw, liangshi, yanyujun, xyzeng2000

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18753) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18753) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18753)

> ğŸš€ HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs Accepted at ICLR 2026 In this work, we introduce HalluGuard , a unified, theory-driven framework for hallucination detection in large language models , accepted at ...

</details>

<details>
<summary><b>34. RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yiming Song, Han Wu, larryle, zhiyuanyou, Jize1

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18130) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18130) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18130)

> An efficient mixture-of-agents framework with dynamic routing.

</details>

<details>
<summary><b>35. TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17958) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17958) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17958)

> Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or laye...

</details>

<details>
<summary><b>36. Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.17617) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.17617) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.17617)

> This paper presents a large-scale behavioral analysis of 14.44M agentic search interactions, characterizing how autonomous agents organize sessions by intent, execute query reformulations, and reuse retrieved evidence across multi-step trajectories.

</details>

<details>
<summary><b>37. Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing</b> â­ 11</summary>

<br/>

**ğŸ‘¥ Authors:** Wei Ji, Jiayin Zhu, Qiyuan He, Yicong Li, xiaolul2

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14103) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14103) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14103)

**ğŸ’» Code:** [â­ Code](https://github.com/xiaolul2/Interp3D)

> In this work, we propose Interp3D, a training-free approach that instantiates the progressive alignment principle based on generative priors for textured 3D morphing.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 37 |
| ğŸ“… Today | [`2026-01-28.json`](data/daily/2026-01-28.json) | 37 |
| ğŸ“† This Week | [`2026-W04.json`](data/weekly/2026-W04.json) | 82 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 694 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-28 | 37 | [View JSON](data/daily/2026-01-28.json) |
| ğŸ“„ 2026-01-27 | 18 | [View JSON](data/daily/2026-01-27.json) |
| ğŸ“„ 2026-01-26 | 27 | [View JSON](data/daily/2026-01-26.json) |
| ğŸ“„ 2026-01-25 | 27 | [View JSON](data/daily/2026-01-25.json) |
| ğŸ“„ 2026-01-24 | 27 | [View JSON](data/daily/2026-01-24.json) |
| ğŸ“„ 2026-01-23 | 26 | [View JSON](data/daily/2026-01-23.json) |
| ğŸ“„ 2026-01-22 | 32 | [View JSON](data/daily/2026-01-22.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W04 | 82 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |
| ğŸ“… 2026-W02 | 232 | [View JSON](data/weekly/2026-W02.json) |
| ğŸ“… 2026-W01 | 156 | [View JSON](data/weekly/2026-W01.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 694 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
