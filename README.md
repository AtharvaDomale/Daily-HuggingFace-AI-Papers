<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-41-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-411+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">41</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">87</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">460</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">411+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 17, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</b> â­ 18</summary>

<br/>

**ğŸ‘¥ Authors:** Chongxuan Li, Wei Wu, Jian Guan, JinaLeejnl

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13586) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13586) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13586)

**ğŸ’» Code:** [â­ Code](https://github.com/ML-GSAI/ReFusion)

> ReFusion is a masked diffusion model that achieves superior performance and efficiency, featuring full KV cache reuse while simultaneously supporting any-order generation.

</details>

<details>
<summary><b>2. Towards Scalable Pre-training of Visual Tokenizers for Generation</b> â­ 92</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13687) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13687) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13687)

**ğŸ’» Code:** [â­ Code](https://github.com/hustvl) â€¢ [â­ Code](https://github.com/MiniMax-AI/VTP)

> GitHub codes: https://github.com/MiniMax-AI/VTP Huggingface weights: https://huggingface.co/collections/MiniMaxAI/vtp collaborated with HUST Vision Lab: https://github.com/hustvl

</details>

<details>
<summary><b>3. Memory in the Age of AI Agents</b> â­ 115</summary>

<br/>

**ğŸ‘¥ Authors:** Jeryi, zstanjj, KYLN24, Liusc2020, namespace-ERI

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13564) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13564) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13564)

**ğŸ’» Code:** [â­ Code](https://github.com/Shichun-Liu/Agent-Memory-Paper-List)

> Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing work...

</details>

<details>
<summary><b>4. QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management</b> â­ 312</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12967) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12967) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12967)

**ğŸ’» Code:** [â­ Code](https://github.com/Tongyi-Zhiwen/Qwen-Doc) â€¢ [â­ Code](https://github.com/Tongyi-Zhiwen/Qwen-Doc/tree/main/QwenLong-L1.5)

> We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline:...

</details>

<details>
<summary><b>5. LongVie 2: Multimodal Controllable Ultra-Long Video World Model</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xian Liu, Zhaoxi Chen, Jianxiong Gao, ChenyangSi, JunhaoZhuang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13604) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13604) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13604)

**ğŸ’» Code:** [â­ Code](https://github.com/Vchitect/LongVie)

> Page: https://vchitect.github.io/LongVie2-project/ Github: https://github.com/Vchitect/LongVie Huggingface: https://huggingface.co/Vchitect/LongVie2

</details>

<details>
<summary><b>6. Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13168) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13168) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13168)

> Real-world F&A work is messy, spanning heterogeneous and large-scale artifacts such as spreadsheets and PDFs. It's also long-horizon and knowledge-intensive: workflows interleave multiple tasks and span diverse domains such as budgeting, trading, ...

</details>

<details>
<summary><b>7. NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents</b> â­ 25</summary>

<br/>

**ğŸ‘¥ Authors:** yo37, kkish, YueHou, coffiney, JingzheDing

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12730) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12730) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12730)

**ğŸ’» Code:** [â­ Code](https://github.com/multimodal-art-projection/NL2RepoBench)

> Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations f...

</details>

<details>
<summary><b>8. Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics</b> â­ 27</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12602) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12602) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12602)

**ğŸ’» Code:** [â­ Code](https://github.com/declare-lab/EFLA)

> Error-Free Linear Attention is a Free Lunch!

</details>

<details>
<summary><b>9. KlingAvatar 2.0 Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13313) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13313) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13313)

> Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak p...

</details>

<details>
<summary><b>10. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment</b> â­ 7</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09636) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09636) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09636)

**ğŸ’» Code:** [â­ Code](https://github.com/elsa66666/MentraSuite)

> No abstract available.

</details>

<details>
<summary><b>11. Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge</b> â­ 148</summary>

<br/>

**ğŸ‘¥ Authors:** Jinwei Gu, Qizhi Chen, Yu-Wei Chao, Junjie Bai, delinqu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10071) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10071) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10071)

**ğŸ’» Code:** [â­ Code](https://github.com/mli0603/openpi-comet)

> OpenPi Comet is the submission of Team Comet for the 2025 BEHAVIOR Challenge . We provides a unified framework for pre-training, post-training, data generation and evaluation of Ï€0.5 (Pi05) models on BEHAVIOR-1K. ğŸ“„ Arxiv: https://arxiv.org/pdf/251...

</details>

<details>
<summary><b>12. Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos</b> â­ 10</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13080) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13080) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13080)

**ğŸ’» Code:** [â­ Code](https://github.com/BeingBeyond/VIPA-VLA)

> We propose VIPA-VLA , which learns 2Dâ€“toâ€“3D visualâ€“physical grounding from human videos with Spatial-Aware VLA Pretraining, enabling robot policies with stronger spatial understanding and generalization. Website: https://beingbeyond.github.io/VIPA...

</details>

<details>
<summary><b>13. WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</b> â­ 9</summary>

<br/>

**ğŸ‘¥ Authors:** Md Rizwan Parvez, Mohammed Eunus Ali, Tanzima Hashem, mahirlabibdihan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12692) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12692) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12692)

**ğŸ’» Code:** [â­ Code](https://github.com/kagnlp/WebOperator)

> We are excited to share our recent work titled "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment". ğŸ“ƒ Paper: https://arxiv.org/abs/2512.12692 ğŸ’» Code: https://github.com/kagnlp/WebOperator ğŸ  Homepage: https://kagnlp.git...

</details>

<details>
<summary><b>14. DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</b> â­ 22</summary>

<br/>

**ğŸ‘¥ Authors:** Zining Wang, Siming Yan, Rui Yang, Runhui Huang, happinessqq

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12799) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12799) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12799)

**ğŸ’» Code:** [â­ Code](https://github.com/happinesslz/DrivePI)

> Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper,...

</details>

<details>
<summary><b>15. V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** Kwesi Cobbina, Shweta Bhardwaj, Yijun Liang, zhoutianyi, Fcr09

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11995) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11995) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11995)

**ğŸ’» Code:** [â­ Code](https://github.com/tianyi-lab/VREX)

> While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require mul...

</details>

<details>
<summary><b>16. Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13250) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13250) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13250)

**ğŸ’» Code:** [â­ Code](https://github.com/KAIST-Visual-AI-Group/VG-AVS)

> Project page: https://active-view-selection.github.io Arxiv: https://arxiv.org/abs/2512.13250 Code: https://github.com/KAIST-Visual-AI-Group/VG-AVS

</details>

<details>
<summary><b>17. Image Diffusion Preview with Consistency Solver</b> â­ 17</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13592) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13592) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13592)

**ğŸ’» Code:** [â­ Code](https://github.com/G-U-N/consolver)

> The slow inference process of image diffusion models significantly degrades interactive user experiences. We introduce Diffusion Preview , a novel preview-and-refine paradigm that generates rapid, low-step preliminary outputs for user evaluation, ...

</details>

<details>
<summary><b>18. VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Zihan Meng, Jun Cen, Shuang Liu, Zeyi Liu, Songqiao Hu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11891) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11891) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11891)

> Project Page: https://vlsa-aegis.github.io

</details>

<details>
<summary><b>19. GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Chenxuan Miao, Liping Hou, Yuxiang Lu, Zhe Liu, ANIYA673

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12751) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12751) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12751)

> No abstract available.

</details>

<details>
<summary><b>20. Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Shan Du, Khalad Hasan, Qingyun Qian, weathon

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11883) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11883) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11883)

> Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered valu...

</details>

<details>
<summary><b>21. Towards Interactive Intelligence for Digital Humans</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yifei Huang, Sitong Gong, Xiwei Gao, Xuangeng Chu, Yiyi Cai

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13674) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13674) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13674)

> We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-e...

</details>

<details>
<summary><b>22. RecTok: Reconstruction Distillation along Rectified Flow</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Yujing Wang, Kaidong Yu, Size Wu, BryanW, QingyuShi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13421) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13421) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13421)

**ğŸ’» Code:** [â­ Code](https://github.com/Shi-qingyu/RecTok)

> arXiv: https://arxiv.org/abs/2512.13421 Project: https://shi-qingyu.github.io/rectok.github.io/ Code: https://github.com/Shi-qingyu/RecTok

</details>

<details>
<summary><b>23. Few-Step Distillation for Text-to-Image Generation: A Practical Guide</b> â­ 91</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13006) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13006) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13006)

**ğŸ’» Code:** [â­ Code](https://github.com/alibaba-damo-academy/T2I-Distill.git)

> A Systematic Study of Diffusion Distillation for Text-to-Image Synthesis towards truly applicable few steps distillation, casting existing distillation methods (sCM, MeanFlow and IMM) into a unified framework for fair comparison. Code is available...

</details>

<details>
<summary><b>24. Flowception: Temporally Expansive Flow Matching for Video Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11438) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11438) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11438)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction...

</details>

<details>
<summary><b>25. What matters for Representation Alignment: Global Information or Spatial Structure?</b> â­ 80</summary>

<br/>

**ğŸ‘¥ Authors:** Richard Zhang, Liang Zheng, Zongze Wu, Xingjian Leng, Jaskirat Singh

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10794) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10794) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10794)

**ğŸ’» Code:** [â­ Code](https://github.com/end2end-diffusion/irepa)

> No abstract available.

</details>

<details>
<summary><b>26. CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10655) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10655) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10655)

> Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance ...

</details>

<details>
<summary><b>27. DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jui-Hsien Wang, Zhifei Zhang, Chongjian Ge, Susung Hong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13690) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13690) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13690)

> Project page: https://susunghong.github.io/DiffusionBrowser

</details>

<details>
<summary><b>28. LitePT: Lighter Yet Stronger Point Transformer</b> â­ 31</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13689) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13689) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13689)

**ğŸ’» Code:** [â­ Code](https://github.com/prs-eth/LitePT)

> LitePT: Lighter Yet Stronger Point Transformer LitePT is a lightweight, high-performance 3D point cloud architecture for various point cloud processing tasks. It embodies the simple principle "convolutions for low-level geometry, attention for hig...

</details>

<details>
<summary><b>29. I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Aniket Bera, Yichen Sheng, Yunhao Ge, Lu Ling

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13683) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13683) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13683)

> No abstract available.

</details>

<details>
<summary><b>30. Directional Textual Inversion for Personalized Text-to-Image Generation</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13672) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13672) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13672)

**ğŸ’» Code:** [â­ Code](https://github.com/kunheek/dti)

> Hi everyone! ğŸ‘‹ We investigated why Textual Inversion (TI) often ignores context and traced the issue to embedding norm inflation. We found that standard TI learns tokens with massive magnitudes (often >20) compared to the model's native vocabulary...

</details>

<details>
<summary><b>31. AutoMV: An Automatic Multi-Agent System for Music Video Generation</b> â­ 4</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12196) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12196v1) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12196)

**ğŸ’» Code:** [â­ Code](https://github.com/multimodal-art-projection/AutoMV)

> arxiv: https://arxiv.org/abs/2512.12196v1 GitHub: https://github.com/multimodal-art-projection/AutoMV Website: https://m-a-p.ai/AutoMV/ Apache-2.0 license

</details>

<details>
<summary><b>32. FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos</b> â­ 12</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10927) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10927) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10927)

**ğŸ’» Code:** [â­ Code](https://github.com/Wolfv0/FoundationMotion/tree/main)

> FoundationMotion offers a scalable way to curate detailed motion datasets, enabling effective fine-tuning of diverse models (VLM / VLA / world models) to improve motion and spatial reasoning

</details>

<details>
<summary><b>33. START: Spatial and Textual Learning for Chart Understanding</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07186) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07186) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07186)

**ğŸ’» Code:** [â­ Code](https://github.com/dragonlzm/START)

> Does visual grounding help visual reasoning in Chart Understanding? ğŸ“ŠğŸ§  I am excited to share our latest paper, "START: Spatial and Textual Learning for Chart Understanding," which explores how we can teach Multimodal LLMs (MLLMs) to better underst...

</details>

<details>
<summary><b>34. Inferring Compositional 4D Scenes without Ever Seeing One</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.05272) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.05272) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.05272)

**ğŸ’» Code:** [â­ Code](https://github.com/insait-institute/COM4D)

> Our method turns videos into compositional 4D scenes with explicit meshes.

</details>

<details>
<summary><b>35. FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13330) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.01066) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13330)

**ğŸ’» Code:** [â­ Code](https://github.com/LumiOpen/lm-evaluation-harness/tree/main/lm_eval/tasks/finbench_v2)

> Our paper introduces FIN-bench-v2, a unified and robust benchmark suite for evaluating large language models in Finnish, addressing the scarcity of high-quality evaluation resources for low-resource languages. This new suite modernizes the origina...

</details>

<details>
<summary><b>36. State over Tokens: Characterizing the Role of Reasoning Tokens</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yoav Goldberg, Shauli Ravfogel, Zohar Elyoseph, Mosh Levy

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12777) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12777) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12777)

> One of the most captivating features of recent chatbot models is their apparent transparency when they "think" out loud, generating step-by-step text before their answer. This might suggest we can trust them because we can verify their logic, but ...

</details>

<details>
<summary><b>37. CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12768) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12768) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12768)

> A dual semantic + geometric reasoning framework with octant-based 3D tokens and multi-critic GRPO, achieving SoTA on text-to-3D, image-to-3D, and 3D captioning.

</details>

<details>
<summary><b>38. Rethinking Expert Trajectory Utilization in LLM Post-training</b> â­ 5</summary>

<br/>

**ğŸ‘¥ Authors:** Qi Zhu, Jiyao Yuan, Jiayang Lv, Yuhan Chen, Bowen Ding

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11470) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11470) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11470)

**ğŸ’» Code:** [â­ Code](https://github.com/LINs-lab/RETU)

> The systematic study of expert trajectory utilization in LLM post-training.

</details>

<details>
<summary><b>39. Learning Robot Manipulation from Audio World Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Michael Gienger, Fanzhri

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08405) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08405) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08405)

> Paper page: https://arxiv.org/abs/2409.01083

</details>

<details>
<summary><b>40. Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08400) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08400) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08400)

**ğŸ’» Code:** [â­ Code](https://github.com/msamdk/Fish_Re_Identification.git)

> Link to the AutoFish dataset: https://huggingface.co/datasets/vapaau/autofish

</details>

<details>
<summary><b>41. KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ali Nourbakhsh, Nasrin Sanjari, Erfan-Nourbakhsh

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09069) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09069) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09069)

**ğŸ’» Code:** [â­ Code](https://github.com/erfan-nourbakhsh/KD-OCT)

> KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 41 |
| ğŸ“… Today | [`2025-12-17.json`](data/daily/2025-12-17.json) | 41 |
| ğŸ“† This Week | [`2025-W50.json`](data/weekly/2025-W50.json) | 87 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 460 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-17 | 41 | [View JSON](data/daily/2025-12-17.json) |
| ğŸ“„ 2025-12-16 | 21 | [View JSON](data/daily/2025-12-16.json) |
| ğŸ“„ 2025-12-15 | 25 | [View JSON](data/daily/2025-12-15.json) |
| ğŸ“„ 2025-12-14 | 25 | [View JSON](data/daily/2025-12-14.json) |
| ğŸ“„ 2025-12-13 | 24 | [View JSON](data/daily/2025-12-13.json) |
| ğŸ“„ 2025-12-12 | 21 | [View JSON](data/daily/2025-12-12.json) |
| ğŸ“„ 2025-12-11 | 25 | [View JSON](data/daily/2025-12-11.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W50 | 87 | [View JSON](data/weekly/2025-W50.json) |
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 460 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
