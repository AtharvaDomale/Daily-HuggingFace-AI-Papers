<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-26-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-849+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">26</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">70</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">111</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">849+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 08, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03252) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03252) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03252)

> Depth Beyond Pixels ğŸš€ We Introduce InfiniDepth â€” casting monocular depth estimation as a neural implicit field. ğŸ” Arbitrary-Resolution ğŸ“ Accurate Metric Depth ğŸ“· Single-View NVS under large viewpoints shifts Arxiv: https://arxiv.org/abs/2601.03252 ...

</details>

<details>
<summary><b>2. MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01554) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01554) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01554)

> MOSS Transcribe Diarize ğŸ™ï¸ We introduce MOSS Transcribe Diarize â€” a unified multimodal model for Speaker-Attributed, Time-Stamped Transcription (SATS) . ğŸ” End-to-end SATS in a single pass (transcription + speaker attribution + timestamps) ğŸ§  128k c...

</details>

<details>
<summary><b>3. LTX-2: Efficient Joint Audio-Visual Foundation Model</b> â­ 922</summary>

<br/>

**ğŸ‘¥ Authors:** kvochko, jacobitterman, nisan, benibraz, yoavhacohen

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03233) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03233) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03233)

**ğŸ’» Code:** [â­ Code](https://github.com/Lightricks/LTX-2)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API 3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized...

</details>

<details>
<summary><b>4. SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence</b> â­ 56</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.22334) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.22334) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.22334)

**ğŸ’» Code:** [â­ Code](https://github.com/InternScience/SciEvalKit)

> SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.

</details>

<details>
<summary><b>5. UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</b> â­ 25</summary>

<br/>

**ğŸ‘¥ Authors:** Lin-Chen, lovesnowbest, YuZeng260, CostaliyA, Hungryyan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03193) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03193) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03193)

**ğŸ’» Code:** [â­ Code](https://github.com/Hungryyan1/UniCorn)

> UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision.

</details>

<details>
<summary><b>6. NitroGen: An Open Foundation Model for Generalist Gaming Agents</b> â­ 1.44k</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02427) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02427) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02427)

**ğŸ’» Code:** [â­ Code](https://github.com/MineDojo/NitroGen)

> NitroGen is a vision-action foundation model trained on 40k hours of gameplay across 1,000+ games, enabling cross-game generalization with behavior cloning and benchmarking, achieving strong unseen-game transfer.

</details>

<details>
<summary><b>7. SOP: A Scalable Online Post-Training System for Vision-Language-Action Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03044) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03044) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03044)

> ğŸš€ Website: https://www.agibot.com/research/sop We introduce SOP for online post-training of generalist VLAs in the real world â€” unlocking persistent, reliable deployment of generalist robots in physical environments. ğŸ” 36 hours of continuous cloth...

</details>

<details>
<summary><b>8. DreamStyle: A Unified Framework for Video Stylization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02785) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02785) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02785)

> DreamStyle unifies text-, style-image-, and first-frame-guided video stylization on an I2V backbone, using LoRA with token-specific up matrices to improve style consistency and video quality.

</details>

<details>
<summary><b>9. MiMo-V2-Flash Technical Report</b> â­ 957</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02780) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02780) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02780)

**ğŸ’» Code:** [â­ Code](https://github.com/XiaomiMiMo/MiMo-V2-Flash)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Xiaomi MiMo-VL-Miloco Technical Report (2025) Seer: Online Context Learning...

</details>

<details>
<summary><b>10. CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Aojun Lu, Junjie Xie, Shuhang Chen, JacobYuan, Yunqiu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01874) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01874) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01874)

> Project page: https://shchen233.github.io/cogflow/

</details>

<details>
<summary><b>11. Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** Yao Su, vztu, ZihanJia, fjchendp, roz322

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01321) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01321) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01321)

**ğŸ’» Code:** [â­ Code](https://github.com/rongzhou7/Awesome-Digital-Twin-AI/tree/main)

> This paper systematically analyzes AI integration in Digital Twins through a four-stage framework (modeling â†’ mirroring â†’ intervention â†’ autonomous management), covering LLMs, foundation models, world models, and intelligent agents across 11 appli...

</details>

<details>
<summary><b>12. WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02439) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02439) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02439)

> WebGym creates a large, non-stationary visual web task suite and scalable RL pipeline, enabling fast trajectory rollout and improved vision-language agent performance on unseen websites.

</details>

<details>
<summary><b>13. Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Fatemeh Askari, Sadegh Mohammadian, Mohammadali Banayeeanzade, Hosein Hasani, safinal

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02989) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02989) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02989)

> ğŸ”¢ Overcoming Transformer Depth Limits in Counting Tasks LLMs often fail at counting not because they aren't smart, but because of architectural depth constraints ğŸš§. We propose a simple, effective System-2 strategy ğŸ§© that decomposes counting tasks ...

</details>

<details>
<summary><b>14. Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</b> â­ 12</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03256) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03256) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03256)

**ğŸ’» Code:** [â­ Code](https://github.com/luhexiao/Muses)

> Project page: https://luhexiao.github.io/Muses.github.io/ Code: https://github.com/luhexiao/Muses

</details>

<details>
<summary><b>15. FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Donghao Luo, yanweifuture, chengjie-wang, ChengmingX, ScarletAce

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01720) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01720) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01720)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Le...

</details>

<details>
<summary><b>16. OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs</b> â­ 112</summary>

<br/>

**ğŸ‘¥ Authors:** Yang Yao, Yixu Wang, Juncheng Li, Yunhao Chen, xinwang22

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01592) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01592) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01592)

**ğŸ’» Code:** [â­ Code](https://github.com/AI45Lab/OpenRT)

> Even State-of-the-Art Models Fail to Hold Ground Against Sophisticated Adversaries. Our comprehensive evaluation highlights two key findings. (1) A clear stratification in defense capability: Top-tier models such as Claude Haiku 4.5, GPT-5.2, and ...

</details>

<details>
<summary><b>17. MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23412) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23412) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23412)

**ğŸ’» Code:** [â­ Code](https://github.com/TIMMY-CHAN/MindWatcher)

> In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without rel...

</details>

<details>
<summary><b>18. The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03227) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03227) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03227)

**ğŸ’» Code:** [â­ Code](https://github.com/Rising0321/AGL1K)

> We found the sonar moment in audio language models. We propose the task of audio geo-localization. And amazingly, Gemini 3 Pro can reach the distance error of less than 55km for 25%  samples.

</details>

<details>
<summary><b>19. X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Sai Rithwik Reddy Chirra, Shashivardhan Reddy Koppula, Mohammad Zia Ur Rehman, shwetankssingh, UVSKKR

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03194) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03194) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03194)

**ğŸ’» Code:** [â­ Code](https://github.com/ziarehman30/X-MuTeST)

> Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (explainable Multilingual haTe Speech d...

</details>

<details>
<summary><b>20. Parallel Latent Reasoning for Sequential Recommendation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yuning Jiang, Jian Wu, Wen Chen, Xu Chen, TangJiakai5704

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03153) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03153) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03153)

> Parallel Latent Reasoning (PLR): Sequential Recommendation with Parallel Reasoning ğŸ”¥ ğŸ“‰ Depth-only reasoning often hits performance plateausâ€”PLR mitigates this with parallel latent reasoning. Core Innovation âœ¨ ğŸ¯ Learnable trigger tokens: Build para...

</details>

<details>
<summary><b>21. Unified Thinker: A General Reasoning Modular Core for Image Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yue Cao, Hanqing Yang, Jijin Hu, Qiang Zhou, Sashuai Zhou

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03127) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03127) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03127)

> reasoning-based image generation and editing

</details>

<details>
<summary><b>22. Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02996) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02996) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02996)

**ğŸ’» Code:** [â­ Code](https://github.com/cisnlp/multilingual-latent-reasoner)

> https://github.com/cisnlp/multilingual-latent-reasoner

</details>

<details>
<summary><b>23. ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Vladislav Golyanik, Toshihiko Yamasaki, mapooon

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02359) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02359) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02359)

> Detecting deepfakes with generative AI. We introduce ExposeAnyone â€” a paradigm shift in face forgery detection! ğŸ”ï¸ Fully self-supervised approach ğŸ¥‡ Best average AUC on traditional deepfake benchmarks ğŸ’ª Best AUC even on Sora2 by OpenAI ğŸ’¢ Strong Rob...

</details>

<details>
<summary><b>24. AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules</b> â­ 458</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.00581) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.00581) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.00581)

**ğŸ’» Code:** [â­ Code](https://github.com/torchmd/torchmd-net)

> AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternative...

</details>

<details>
<summary><b>25. U-Net-Like Spiking Neural Networks for Single Image Dehazing</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** Peng Li, Yulong Xiao, Mingzhe Liu, Huibin Li, FengShaner

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23950) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23950) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23950)

**ğŸ’» Code:** [â­ Code](https://github.com/HaoranLiu507/DehazeSNN)

> Title: DehazeSNN â€” U-Net-like Spiking Neural Networks for Single Image Dehazing Short summary: DehazeSNN integrates a U-Net architecture with Spiking Neural Networks to reduce compute while achieving competitive dehazing results. Code: github.com/...

</details>

<details>
<summary><b>26. Steerability of Instrumental-Convergence Tendencies in LLMs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** j-hoscilowic

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01584) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01584) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01584)

**ğŸ’» Code:** [â­ Code](https://github.com/j-hoscilowicz/instrumental_steering/)

> This paper measures how easily â€œinstrumental-convergenceâ€ behaviors (e.g., shutdown avoidance, self-replication) in LLMs can be amplified or suppressed by simple steering, and argues that the common claim â€œas AI capability (often glossed as â€˜intel...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 26 |
| ğŸ“… Today | [`2026-01-08.json`](data/daily/2026-01-08.json) | 26 |
| ğŸ“† This Week | [`2026-W01.json`](data/weekly/2026-W01.json) | 70 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 111 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-08 | 26 | [View JSON](data/daily/2026-01-08.json) |
| ğŸ“„ 2026-01-07 | 24 | [View JSON](data/daily/2026-01-07.json) |
| ğŸ“„ 2026-01-06 | 13 | [View JSON](data/daily/2026-01-06.json) |
| ğŸ“„ 2026-01-05 | 7 | [View JSON](data/daily/2026-01-05.json) |
| ğŸ“„ 2026-01-04 | 7 | [View JSON](data/daily/2026-01-04.json) |
| ğŸ“„ 2026-01-03 | 7 | [View JSON](data/daily/2026-01-03.json) |
| ğŸ“„ 2026-01-02 | 20 | [View JSON](data/daily/2026-01-02.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W01 | 70 | [View JSON](data/weekly/2026-W01.json) |
| ğŸ“… 2026-W00 | 41 | [View JSON](data/weekly/2026-W00.json) |
| ğŸ“… 2025-W52 | 52 | [View JSON](data/weekly/2025-W52.json) |
| ğŸ“… 2025-W51 | 132 | [View JSON](data/weekly/2025-W51.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 111 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
