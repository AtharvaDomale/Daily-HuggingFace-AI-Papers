<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-33-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-902+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">33</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">123</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">164</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">902+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 10, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</b> â­ 64</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05242) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05242) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05242)

**ğŸ’» Code:** [â­ Code](https://github.com/NVlabs/GDPO)

> GDPO is a drop-in replacement for GRPO in verl and TRL â€” only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling ...

</details>

<details>
<summary><b>2. Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers</b> â­ 98</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04890) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04890) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04890)

**ğŸ’» Code:** [â­ Code](https://github.com/tiiuae/falcon-h1)

> Building on the Î¼P multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weigh...

</details>

<details>
<summary><b>3. RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes</b> â­ 12</summary>

<br/>

**ğŸ‘¥ Authors:** Chia-Che Chang, Kuan-Lin Chen, yulunliu, NeilLeeNTU

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05249) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05249) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05249)

**ğŸ’» Code:** [â­ Code](https://github.com/BrianChen1120/RL-AWB)

> Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning fo...

</details>

<details>
<summary><b>4. Token-Level LLM Collaboration via FusionRoute</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Furong Huang, Zhaorun Chen, Hanqing Zeng, Nuoya Xiong, zyhang1998

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05106) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05106) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05106)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Ste...

</details>

<details>
<summary><b>5. RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</b> â­ 8</summary>

<br/>

**ğŸ‘¥ Authors:** Jia-Zeng, ZhaoyangLyu, matthewmao, wuzhi-hao, HikariDawn

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05241) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05241) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05241)

**ğŸ’» Code:** [â­ Code](https://github.com/RoboVIP/RoboVIP_VDM)

> The project webpage is at: https://robovip.github.io/RoboVIP/

</details>

<details>
<summary><b>6. RelayLLM: Efficient Reasoning via Collaborative Decoding</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Haolin Liu, Jinyuan Li, Tong Zheng, shrango, ChengsongHuang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05167) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05167) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05167)

**ğŸ’» Code:** [â­ Code](https://github.com/Chengsong-Huang/RelayLLM)

> Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches,...

</details>

<details>
<summary><b>7. AT^2PO: Agentic Turn-based Policy Optimization via Tree Search</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04767) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04767) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04767)

**ğŸ’» Code:** [â­ Code](https://github.com/zzfoutofspace/ATPO)

> Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical po...

</details>

<details>
<summary><b>8. Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.21815) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.21815) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.21815)

> Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertaint...

</details>

<details>
<summary><b>9. VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</b> â­ 11</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05175) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05175) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05175)

**ğŸ’» Code:** [â­ Code](https://github.com/IVUL-KAUST/VideoAuto-R1/)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizi...

</details>

<details>
<summary><b>10. VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xiaoyu Li, Wenbo Hu, Minghao Yin, yanweifuture, sxzheng

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05138) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05138) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05138)

> Project Page: https://sixiaozheng.github.io/VerseCrafter_page/

</details>

<details>
<summary><b>11. The Illusion of Specialization: Unveiling the Domain-Invariant "Standing Committee" in Mixture-of-Experts Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03425) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03425) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03425)

> Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expe...

</details>

<details>
<summary><b>12. Plenoptic Video Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05239) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05239) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05239)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generatio...

</details>

<details>
<summary><b>13. Agent-as-a-Judge</b> â­ 9</summary>

<br/>

**ğŸ‘¥ Authors:** Meng Liu, Qiancheng Xu, Caiqi Zhang, HongruCai, dd101bb

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05111) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05111) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05111)

**ğŸ’» Code:** [â­ Code](https://github.com/ModalityDance/Awesome-Agent-as-a-Judge)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World...

</details>

<details>
<summary><b>14. CoV: Chain-of-View Prompting for Spatial Reasoning</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05172) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05172) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05172)

**ğŸ’» Code:** [â­ Code](https://github.com/ziplab/CoV)

> We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.

</details>

<details>
<summary><b>15. DocDancer: Towards Agentic Document-Grounded Information Seeking</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05163) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05163) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05163)

> Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end...

</details>

<details>
<summary><b>16. Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Tiankai Hang, Yiji Cheng, eternaldolphin, Zhiminli, hrz2000

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05124) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05124) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05124)

**ğŸ’» Code:** [â­ Code](https://github.com/hrz2000/realign)

> This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explic...

</details>

<details>
<summary><b>17. DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jing Ma, Yuxuan Gu, Shidong Cao, Ziyang, danielhzlin

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03559) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03559) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03559)

> DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.

</details>

<details>
<summary><b>18. ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04754) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04754) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04754)

**ğŸ’» Code:** [â­ Code](https://github.com/chiou1203/ProFuse)

> We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding ...

</details>

<details>
<summary><b>19. Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03362) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03362) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03362)

> Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair...

</details>

<details>
<summary><b>20. One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xuefeng Li, Weixun Wang, Yanan Wu, Zhen Huang, Yiyuan Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03111) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03111) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03111)

> This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned...

</details>

<details>
<summary><b>21. Memorization in 3D Shape Generation: An Empirical Study</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.23628) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.23628) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.23628)

**ğŸ’» Code:** [â­ Code](https://github.com/zlab-princeton/3d-gen-mem)

> Our code is available at https://github.com/zlab-princeton/3d-gen-mem.

</details>

<details>
<summary><b>22. Multi-Scale Local Speculative Decoding for Image Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05149) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05149) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05149)

> Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while...

</details>

<details>
<summary><b>23. PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04792) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04792) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04792)

> We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enabl...

</details>

<details>
<summary><b>24. AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Di Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04620) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04620) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04620)

> Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unst...

</details>

<details>
<summary><b>25. Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing</b> â­ 21</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04575) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04575) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04575)

**ğŸ’» Code:** [â­ Code](https://github.com/elefant-ai/open-p2p)

> We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse acti...

</details>

<details>
<summary><b>26. ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04342) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04342) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04342)

> ğŸš€ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the di...

</details>

<details>
<summary><b>27. Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04300) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04300) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04300)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Pol...

</details>

<details>
<summary><b>28. Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Carl James Debono, Matthew Montebello, Gabriel Hili, Dylan Seychell, mbar0075

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02016) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02016) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02016)

> No abstract available.

</details>

<details>
<summary><b>29. VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.05125) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.05125) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.05125)

**ğŸ’» Code:** [â­ Code](https://github.com/nachoDRT/VrDU-Doctor)

> We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize lat...

</details>

<details>
<summary><b>30. Learning User Preferences Through Interaction for Long-Term Collaboration</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Dilek Hakkani-TÃ¼r, Tal August, Priyanka Kargupta, Shuhaib Mehri

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.02702) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.02702) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.02702)

> Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents...

</details>

<details>
<summary><b>31. Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jian Liu, Jian Lou, Kejia Chen, Jiawen Zhang, ttttonyhe

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01887) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01887) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01887)

> Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but ...

</details>

<details>
<summary><b>32. LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04233) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04233) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04233)

> LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering ...

</details>

<details>
<summary><b>33. Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** YuanFu Yang, ZhenQi Chen, water-fountain

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.24160) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.24160) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.24160)

**ğŸ’» Code:** [â­ Code](https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-)

> No abstract available.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 33 |
| ğŸ“… Today | [`2026-01-10.json`](data/daily/2026-01-10.json) | 33 |
| ğŸ“† This Week | [`2026-W01.json`](data/weekly/2026-W01.json) | 123 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 164 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-10 | 33 | [View JSON](data/daily/2026-01-10.json) |
| ğŸ“„ 2026-01-09 | 20 | [View JSON](data/daily/2026-01-09.json) |
| ğŸ“„ 2026-01-08 | 26 | [View JSON](data/daily/2026-01-08.json) |
| ğŸ“„ 2026-01-07 | 24 | [View JSON](data/daily/2026-01-07.json) |
| ğŸ“„ 2026-01-06 | 13 | [View JSON](data/daily/2026-01-06.json) |
| ğŸ“„ 2026-01-05 | 7 | [View JSON](data/daily/2026-01-05.json) |
| ğŸ“„ 2026-01-04 | 7 | [View JSON](data/daily/2026-01-04.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W01 | 123 | [View JSON](data/weekly/2026-W01.json) |
| ğŸ“… 2026-W00 | 41 | [View JSON](data/weekly/2026-W00.json) |
| ğŸ“… 2025-W52 | 52 | [View JSON](data/weekly/2025-W52.json) |
| ğŸ“… 2025-W51 | 132 | [View JSON](data/weekly/2025-W51.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 164 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
