<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-29-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-229+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">29</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">91</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">278</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">229+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 10, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</b> â­ 18</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07461) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07461) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07461)

**ğŸ’» Code:** [â­ Code](https://github.com/bigai-nlco/Native-Parallel-Reasoner)

> Paper: https://arxiv.org/abs/2512.07461 Code: https://github.com/bigai-nlco/Native-Parallel-Reasoner Model & Data: https://huggingface.co/bigai-NPR Website: https://bigai-nlco.github.io/Native-Parallel-Reasoner

</details>

<details>
<summary><b>2. Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</b> â­ 12</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07525) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07525) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07525)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenMOSS/rope_pp)

> Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real comp...

</details>

<details>
<summary><b>3. Unified Video Editing with Temporal Reasoner</b> â­ 25</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07469) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07469) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07469)

**ğŸ’» Code:** [â­ Code](https://github.com/knightyxp/VideoCoF)

> A Chain of Frames video editing method enbale temporal reasoning and 4x video length extrapolation with just 50k training pairs! ğŸ  Page: videocof.github.io/ ğŸ“„ Paper: arxiv.org/abs/2512.07469 ğŸ’» Code: github.com/knightyxp/VideoCoF

</details>

<details>
<summary><b>4. Voxify3D: Pixel Art Meets Volumetric Rendering</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yu-Lun Liu, chien90190, JiewenChan, YiChuanH

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07834) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07834) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07834)

> Stylized voxel art is widely used in games and digital media, but turning 3D meshes into visually appealing voxel forms remains challenging and often requires manual effort. Existing methods struggle to preserve semantic structure and offer limite...

</details>

<details>
<summary><b>5. Scaling Zero-Shot Reference-to-Video Generation</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06905) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06905) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06905)

**ğŸ’» Code:** [â­ Code](https://github.com/franciszzj/Saber)

> Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-te...

</details>

<details>
<summary><b>6. DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06749) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06749) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06749)

> Project website with an intro video is available at: https://aka.ms/DoVer .

</details>

<details>
<summary><b>7. Distribution Matching Variational AutoEncoder</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07778) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07778) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07778)

**ğŸ’» Code:** [â­ Code](https://github.com/sen-ye/dmvae%7D)

> Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without expl...

</details>

<details>
<summary><b>8. EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</b> â­ 15</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06065) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06065) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06065)

**ğŸ’» Code:** [â­ Code](https://github.com/snap-research/EgoEdit)

> We propose a framework for real-time egocentric video editing. Our system is composed of: EgoEditData, a manually curated dataset of 100k video editing pairs focusing on the egocentric case and featuring object substitution and removal under chall...

</details>

<details>
<summary><b>9. Relational Visual Similarity</b> â­ 14</summary>

<br/>

**ğŸ‘¥ Authors:** Jing Shi, Yilin Wang, Krishna Kumar Singh, Sicheng Mo, thaoshibe

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07833) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07833) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07833)

**ğŸ’» Code:** [â­ Code](https://github.com/thaoshibe/relsim)

> Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and p...

</details>

<details>
<summary><b>10. Multi-view Pyramid Transformer: Look Coarser to See Broader</b> â­ 56</summary>

<br/>

**ğŸ‘¥ Authors:** Jungwoo Kim, Younggeun Lee, Seungtae Nam, Seungkwon Yang, Gynjn

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07806) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07806) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07806)

**ğŸ’» Code:** [â­ Code](https://github.com/Gynjn/MVP)

> We are excited to share our recent work "Multi-view Pyramid Transformer: Look Coarser to See Broader" Paper: https://arxiv.org/abs/2512.07806 Project page: https://gynjn.github.io/MVP/ Code: https://github.com/Gynjn/MVP

</details>

<details>
<summary><b>11. LongCat-Image Technical Report</b> â­ 307</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07584) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07584) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07584)

**ğŸ’» Code:** [â­ Code](https://github.com/meituan-longcat/LongCat-Image)

> We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer acce...

</details>

<details>
<summary><b>12. UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</b> â­ 26</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07831) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07831) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07831)

**ğŸ’» Code:** [â­ Code](https://github.com/dvlab-research/UnityVideo)

> Project Website https://jackailab.github.io/Projects/UnityVideo/

</details>

<details>
<summary><b>13. On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</b> â­ 7</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07783) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07783) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07783)

**ğŸ’» Code:** [â­ Code](https://github.com/Interplay-LM-Reasoning/Interplay-LM-Reasoning)

> We develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves suff...

</details>

<details>
<summary><b>14. SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Nanyun Peng, Swastik Roy, Arpit Gupta, Sruthi Gorantla, Salman Rahman

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03244) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03244) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03244)

> Please find our paper on training process reward models without ground truth by leveraging inference-time scaling methods, enabling reinforcement learning in domains where verifiable answers are unavailable.

</details>

<details>
<summary><b>15. ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation</b> â­ 23</summary>

<br/>

**ğŸ‘¥ Authors:** Taojun Ding, Jiehui Huang, Mantang Guo, wangshx, Iron-lyk

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03621) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03621) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03621)

**ğŸ’» Code:** [â­ Code](https://github.com/Iron-LYK/ReCamDriving)

> Project page: https://recamdriving.github.io/

</details>

<details>
<summary><b>16. Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jiacheng Chen, Ziniu Li, Sheng Tang, Ming Chen, trxcc2002

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06533) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06533) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06533)

> No abstract available.

</details>

<details>
<summary><b>17. VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</b> â­ 11</summary>

<br/>

**ğŸ‘¥ Authors:** Yansong Tang, Haoji Zhang, Jingxuan Niu, Wenlong Liu, VoyageWang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06373) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06373) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06373)

**ğŸ’» Code:** [â­ Code](https://github.com/VoyageWang/VG-Refiner)

> The project page is https://github.com/VoyageWang/VG-Refiner

</details>

<details>
<summary><b>18. OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Simeng Qin, Teng Ma, Qi Guo, Jie Liao, jiaxiaojunQAQ

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06589) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06589) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06589)

> This work presents OmniSafeBench-MM, a unified, open-source benchmark and toolbox designed for comprehensive evaluation of multimodal jailbreak attack and defense methods. It integrates 13 representative attack techniques, 15 defense strategies, a...

</details>

<details>
<summary><b>19. One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07829) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07829) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07829)

> We proposed FAE which adapts pretrained ViT as the latent space for visual generative models

</details>

<details>
<summary><b>20. Group Representational Position Encoding</b> â­ 30</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07805) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07805) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07805)

**ğŸ’» Code:** [â­ Code](https://github.com/model-architectures/GRAPE)

> Introducing GRAPE: Group Representational Position Encoding. Embracing General Relative Law of Position Encoding, unifying and improving Multiplicative and Additive Position Encoding, such as RoPE and Alibi! Better performance with a clear theoret...

</details>

<details>
<summary><b>21. Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06835) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06835) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06835)

> Experiment Results ğŸ“Š We evaluate DoGe on 7 benchmarks covering: General visual reasoning & hallucination (MMMU, MMStar, HallBench) Specialized domain reasoning (MathVision, MathVista, ChemBench, MSEarthMCQ) 3B-level Models Performance Method MMMU ...

</details>

<details>
<summary><b>22. VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yaobo Liang, Zhiying Du, Fangyun Wei, godjiaolongge, ys3197

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06963) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06963) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06963)

> Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding model...

</details>

<details>
<summary><b>23. Rethinking Training Dynamics in Scale-wise Autoregressive Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06421) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06421) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06421)

> Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner....

</details>

<details>
<summary><b>24. Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06791) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06791) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06791)

**ğŸ’» Code:** [â­ Code](https://github.com/AashVed/SmallGainNash)

> Gradient methods in games are usually proven to converge only under strong monotonicity in the Euclidean geometry (Rosen-style assumptions). That fails even for simple coupled quadratic games, yet in practice we still often see convergence. This p...

</details>

<details>
<summary><b>25. Vector Quantization using Gaussian Variational Autoencoder</b> â­ 10</summary>

<br/>

**ğŸ‘¥ Authors:** Wendi Zheng, jerytang, Ya-Qin, jmhernandezlobato, xutongda

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06609) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06609) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06609)

**ğŸ’» Code:** [â­ Code](https://github.com/Stability-AI/generative-models) â€¢ [â­ Code](https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE)

> State-of-the-Art VQ-VAE from Gaussian VAE without Training! We train a Gaussian VAE, convert it into VQ-VAE with almost 100% codebook usage, and keeps reconstruction performance! As flexible to setup as VQ-VAE, supporting: codebook size, codebook ...

</details>

<details>
<summary><b>26. DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** YijunLiao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03704) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03704) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03704)

**ğŸ’» Code:** [â­ Code](https://github.com/lyj20071013/DZ-TDPO)

> ğŸ”¥ Solving "State Inertia" in Long-Context LLMs! We introduce DZ-TDPO, a non-destructive alignment framework. Problem: Standard DPO causes "Alignment Tax" (PPL explosion >100) when updating user states in long context. Solution: Dynamic KL Constrai...

</details>

<details>
<summary><b>27. JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Linsey Pang, Aaron Elkins, Aman Chadha, Christos Constantinou, Georgios Ioannides

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.07168) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.07168) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.07168)

> This paper introduces JEPA+DAAM , a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Gaussian mixtureâ€“based Density Adaptive Attention Mechanism (DAAM) to learn semantically rich and highl...

</details>

<details>
<summary><b>28. Embodied Referring Expression Comprehension in Human-Robot Interaction</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ganesh Nanduru, amanchadha, Anubis91, alexiglad, mmiakashs

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06558) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06558) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06558)

> The paper introduces Refer360 , a comprehensive multimodal dataset for embodied referring expression comprehension in human-robot interaction (HRI), and proposes MuRes , a lightweight guided residual module that selectively reinforces modality-spe...

</details>

<details>
<summary><b>29. The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06032) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06032) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06032)

**ğŸ’» Code:** [â­ Code](https://github.com/Applied-AI-Research-Lab/The-SAM2-to-SAM3-Gap-in-the-Segment-Anything-Model-Family)

> This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3 (also called SAMv2 and SAMv3). We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimoda...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 29 |
| ğŸ“… Today | [`2025-12-10.json`](data/daily/2025-12-10.json) | 29 |
| ğŸ“† This Week | [`2025-W49.json`](data/weekly/2025-W49.json) | 91 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 278 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-10 | 29 | [View JSON](data/daily/2025-12-10.json) |
| ğŸ“„ 2025-12-09 | 24 | [View JSON](data/daily/2025-12-09.json) |
| ğŸ“„ 2025-12-08 | 38 | [View JSON](data/daily/2025-12-08.json) |
| ğŸ“„ 2025-12-07 | 38 | [View JSON](data/daily/2025-12-07.json) |
| ğŸ“„ 2025-12-06 | 38 | [View JSON](data/daily/2025-12-06.json) |
| ğŸ“„ 2025-12-05 | 38 | [View JSON](data/daily/2025-12-05.json) |
| ğŸ“„ 2025-12-04 | 24 | [View JSON](data/daily/2025-12-04.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W49 | 91 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 278 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
