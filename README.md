<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-22-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-614+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">22</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">60</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">663</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">614+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 23, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</b> â­ 56</summary>

<br/>

**ğŸ‘¥ Authors:** Yuhao Zhou, SciYu, VitaCoco, BoKelvin, CoCoOne

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16969) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16969) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16969)

**ğŸ’» Code:** [â­ Code](https://github.com/InternScience/SGI-Bench)

> Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition gro...

</details>

<details>
<summary><b>2. PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16793) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16793) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16793)

> No abstract available.

</details>

<details>
<summary><b>3. When Reasoning Meets Its Laws</b> â­ 15</summary>

<br/>

**ğŸ‘¥ Authors:** Liu Ziyin, Jingyan Shen, Tianang Leng, Yifan Sun, jyzhang1208

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17901) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17901) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17901)

**ğŸ’» Code:** [â­ Code](https://github.com/ASTRAL-Group/LoRe)

> Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents ...

</details>

<details>
<summary><b>4. Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17260) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17260) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17260)

**ğŸ’» Code:** [â­ Code](https://github.com/ByteDance-Seed/Seed-Prover)

> Github: https://github.com/ByteDance-Seed/Seed-Prover

</details>

<details>
<summary><b>5. Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17909) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17909) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17909)

> Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to ad...

</details>

<details>
<summary><b>6. 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17012) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17012) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17012)

> Project page: https://ca-joe-yang.github.io/resource/projects/4D_RGPT We propose 4D-RGPT , a specialized MLLM that perceives 4D information for enhanced video understanding. We propose the P erceptual 4 D D istillation ( P4D ) training framework t...

</details>

<details>
<summary><b>7. Are We on the Right Way to Assessing LLM-as-a-Judge?</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16041) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16041) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16041)

> We argue that evaluating LLM-as-a-Judge is biased by human-annotated ground truth, rethink the evaluation of LLM-as-a-Judge, and design metrics that do not need human annotations.

</details>

<details>
<summary><b>8. An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.11362) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.11362) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.11362)

> Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challeng...

</details>

<details>
<summary><b>9. RadarGen: Automotive Radar Point Cloud Generation from Cameras</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Or Litany, Shengyu Huang, Sanja Fidler, Fangqiang Ding, TomerBo

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17897) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17897) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17897)

**ğŸ’» Code:** [â­ Code](https://github.com/tomerborreda/RadarGen)

> Check out radargen.github.io

</details>

<details>
<summary><b>10. GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17495) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17495) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17495)

> Our new benchmark for evaluating the grounding capabilities of frontier MLLMs.

</details>

<details>
<summary><b>11. Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers</b> â­ 278</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17351) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17351) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17351)

**ğŸ’» Code:** [â­ Code](https://github.com/facebookresearch/PhysicsLM4)

> https://x.com/ZeyuanAllenZhu/status/2000892470306152701 https://physics.allen-zhu.com/part-4-architecture-design/part-4-1

</details>

<details>
<summary><b>12. Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Lihong Li, Meet P. Vadera, Rui Meng, Peng Zhou, ljb121002

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17008) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17008) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17008)

> Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exp...

</details>

<details>
<summary><b>13. HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.14870) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.14870) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.14870)

**ğŸ’» Code:** [â­ Code](https://github.com/DanBenAmi/HERBench)

> ğŸ”— Project page: https://herbench.github.io/ ğŸ“„  arXiv: https://arxiv.org/abs/2512.14870 ğŸ¤—  HF dataset card: https://huggingface.co/datasets/DanBenAmi/HERBench ğŸ–¥  Code (GitHub): https://github.com/DanBenAmi/HERBench

</details>

<details>
<summary><b>14. Animate Any Character in Any World</b> â­ 28</summary>

<br/>

**ğŸ‘¥ Authors:** Yan Lu, Bo Dai, Hongyang Zhang, Fangyun Wei, Yitong Wang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17796) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17796) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17796)

**ğŸ’» Code:** [â­ Code](https://github.com/snowflakewang/AniX)

> Introducing AniX, a system enables users to provide 3DGS scene along with a 3D or multi-view character, enabling interactive control of the character's behaviors and active exploration of the environment through natural language commands. The syst...

</details>

<details>
<summary><b>15. SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17419) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17419) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17419)

> Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug f...

</details>

<details>
<summary><b>16. StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16483) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16483) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16483)

**ğŸ’» Code:** [â­ Code](https://github.com/sen-mao/StageVAR)

> github: https://github.com/sen-mao/StageVAR

</details>

<details>
<summary><b>17. Bolmo: Byteifying the Next Generation of Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.15586) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.15586) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.15586)

> So cool idea to make use of mLSTM and developing this byteifying approach ğŸ˜

</details>

<details>
<summary><b>18. Meta-RL Induces Exploration in Language Agents</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Maria Brbic, Michael Moor, Damien Teney, Liangze Jiang, Yulun Jiang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16848) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16848) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16848)

> ğŸŒŠLaMer, a general Meta-RL framework that enables LLM agents to explore and learn from the environment feedback at test time.

</details>

<details>
<summary><b>19. Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Runtao Liu, Xiaogang Xu, Wei Wei, Jianmin Chen, Jiaqi-hkust

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17532) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17532) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17532)

> Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses...

</details>

<details>
<summary><b>20. 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</b> â­ 33</summary>

<br/>

**ğŸ‘¥ Authors:** Hendrik P. A. Lensch, Tobias Sautter, JDihlmann

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17459) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17459) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17459)

**ğŸ’» Code:** [â­ Code](https://github.com/cgtuebingen/3D-RE-GEN)

> ğŸŒ https://3dregen.jdihlmann.com/ ğŸ“ƒ https://arxiv.org/abs/2512.17459 ğŸ’¾ https://github.com/cgtuebingen/3D-RE-GEN

</details>

<details>
<summary><b>21. A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16978) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16978) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16978)

**ğŸ’» Code:** [â­ Code](https://github.com/mbzuai-oryx/longshot)

> ğŸŒ Website: https://mbzuai-oryx.github.io/LongShOT/ ğŸ’» Github: https://github.com/mbzuai-oryx/longshot ğŸ¤— HuggingFace: https://huggingface.co/datasets/MBZUAI/longshot-bench

</details>

<details>
<summary><b>22. MineTheGap: Automatic Mining of Biases in Text-to-Image Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Tomer Michaeli, Inbar Huberman-Spiegelglas, Nurit Spingarn-Eliezer, Noa Cohen

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.13427) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.13427) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.13427)

> Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These bia...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 22 |
| ğŸ“… Today | [`2025-12-23.json`](data/daily/2025-12-23.json) | 22 |
| ğŸ“† This Week | [`2025-W51.json`](data/weekly/2025-W51.json) | 60 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 663 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-23 | 22 | [View JSON](data/daily/2025-12-23.json) |
| ğŸ“„ 2025-12-22 | 38 | [View JSON](data/daily/2025-12-22.json) |
| ğŸ“„ 2025-12-21 | 38 | [View JSON](data/daily/2025-12-21.json) |
| ğŸ“„ 2025-12-20 | 37 | [View JSON](data/daily/2025-12-20.json) |
| ğŸ“„ 2025-12-19 | 30 | [View JSON](data/daily/2025-12-19.json) |
| ğŸ“„ 2025-12-18 | 38 | [View JSON](data/daily/2025-12-18.json) |
| ğŸ“„ 2025-12-17 | 41 | [View JSON](data/daily/2025-12-17.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W51 | 60 | [View JSON](data/weekly/2025-W51.json) |
| ğŸ“… 2025-W50 | 230 | [View JSON](data/weekly/2025-W50.json) |
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 663 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
