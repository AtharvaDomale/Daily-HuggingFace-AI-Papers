<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-25-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-324+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">25</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">186</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">373</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">324+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 14, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10430) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10430) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10430)

> T-pro 2.0 is an open-weight Russian LLM with hybrid reasoning and fast inference, released with datasets, benchmarks, and an optimized decoding pipeline to support reproducible research and practical applications.

</details>

<details>
<summary><b>2. Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10739) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10739) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10739)

> Due to a user error, the abstract displayed in this paper contains some errors ğŸ˜­ (the abstract in the PDF is correct). The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier...

</details>

<details>
<summary><b>3. Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</b> â­ 45</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10949) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10949) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10949)

**ğŸ’» Code:** [â­ Code](https://github.com/Ivan-Tang-3D/3DGen-R1)

> Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1 . Model is released at https://huggingface.co/IvanTang/3DGen-R1 .

</details>

<details>
<summary><b>4. OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10756) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10756) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10756)

> We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.

</details>

<details>
<summary><b>5. Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10534) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10534) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10534)

> InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. Built o...

</details>

<details>
<summary><b>6. MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Mingxi Xu, DonaldLian, weixia111111, wzy27, kehong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10881) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10881) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10881)

> Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrar...

</details>

<details>
<summary><b>7. BEAVER: An Efficient Deterministic LLM Verifier</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.05439) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.05439) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.05439)

> BEAVER is the first practical framework to formally verify an LLMâ€™s output distribution. It enables rigorous assessment and comparison beyond traditional sampling-based evaluation. BEAVER computes deterministic, sound bounds on the total probabili...

</details>

<details>
<summary><b>8. Thinking with Images via Self-Calling Agent</b> â­ 11</summary>

<br/>

**ğŸ‘¥ Authors:** Qixiang Ye, Fang Wan, callsys, ywenxi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08511) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08511) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08511)

**ğŸ’» Code:** [â­ Code](https://github.com/YWenxi/think-with-images-through-self-calling)

> ğŸ§ ğŸ–¼ï¸ Vision-language models are getting smarterâ€”but also harder to train. Many recent systems â€œthink with images,â€ weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually req...

</details>

<details>
<summary><b>9. From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10867) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10867) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10867)

> This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the pote...

</details>

<details>
<summary><b>10. Stronger Normalization-Free Transformers</b> â­ 31</summary>

<br/>

**ğŸ‘¥ Authors:** Zhuang Liu, Mingjie Sun, Jiachen Zhu, TaiMingLu, Fishloong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10938) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10938) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10938)

**ğŸ’» Code:** [â­ Code](https://github.com/zlab-princeton/Derf)

> Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains e...

</details>

<details>
<summary><b>11. VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2511.23386) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.23386) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2511.23386)

> arXiv: https://arxiv.org/pdf/2511.23386 Overall Architecture

</details>

<details>
<summary><b>12. StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</b> â­ 15</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10959) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10959) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10959)

**ğŸ’» Code:** [â­ Code](https://github.com/prs-eth/stereospace)

> Project page: https://huggingface.co/spaces/prs-eth/stereospace_web

</details>

<details>
<summary><b>13. Evaluating Gemini Robotics Policies in a Veo World Simulator</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10675) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10675) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10675)

> Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and ge...

</details>

<details>
<summary><b>14. MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Won-Sik Cheong, Geonho Kim, shurek20, klavna, sangwoonkwak

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09270) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09270) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09270)

**ğŸ’» Code:** [â­ Code](https://github.com/CMLab-Korea/MoRel-arXiv)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal ...

</details>

<details>
<summary><b>15. The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10791) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10791) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10791)

> We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holis...

</details>

<details>
<summary><b>16. Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10359) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10359) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10359)

**ğŸ’» Code:** [â­ Code](https://github.com/fansunqi/VideoTool)

> Tool-augmented VideoQA system, accepted by NeurIPS'25 main track.

</details>

<details>
<summary><b>17. ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning</b> â­ 4</summary>

<br/>

**ğŸ‘¥ Authors:** SuaLily, whluo, Yanbiao, LewisPan, JacobYuan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09924) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09924) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09924)

**ğŸ’» Code:** [â­ Code](https://github.com/Liuxinyv/ReViSE)

> Code: https://github.com/Liuxinyv/ReViSE

</details>

<details>
<summary><b>18. H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos</b> â­ 13</summary>

<br/>

**ğŸ‘¥ Authors:** Pei Yang, Xiaokang Liu, AnalMom, yiren98, HaiCi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09406) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09406) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09406)

**ğŸ’» Code:** [â­ Code](https://github.com/showlab/H2R-Grounder)

> A framework to translate human object interaction (HOI) videos into grounded robot object interaction (ROI) videos.

</details>

<details>
<summary><b>19. Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xiaodong Gu, Yuchao Qiu, Xiang Chen, lanqz7766, YerbaPage

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.08870) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.08870) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.08870)

> Check this out!

</details>

<details>
<summary><b>20. Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10955) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10955) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10955)

> This work can isolate a specific attribute from any image and merge those selected attributes from multiple images into a coherent generation.

</details>

<details>
<summary><b>21. DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jing Liao, Yiran Xu, Matthew Fisher, Nanxuan Zhao, Peiying Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10894) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10894) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10894)

> We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy...

</details>

<details>
<summary><b>22. Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.10398) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.10398) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.10398)

> Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agent...

</details>

<details>
<summary><b>23. X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04537) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04537) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04537)

> The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training...

</details>

<details>
<summary><b>24. MOA: Multi-Objective Alignment for Role-Playing Agents</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Fei Huang, Ke Wang, Yongbin-Li, yuchuan123, ChonghuaLiao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.09756) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.09756) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.09756)

> Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT...

</details>

<details>
<summary><b>25. DragMesh: Interactive 3D Generation Made Easy</b> â­ 6</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.06424) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.06424) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.06424)

**ğŸ’» Code:** [â­ Code](https://github.com/AIGeeksGroup/DragMesh)

> DragMesh enables real time, physically valid 3D object articulation by decoupling kinematic reasoning from motion generation and producing plausible motions via a dual quaternion based generative model.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 25 |
| ğŸ“… Today | [`2025-12-14.json`](data/daily/2025-12-14.json) | 25 |
| ğŸ“† This Week | [`2025-W49.json`](data/weekly/2025-W49.json) | 186 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 373 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-14 | 25 | [View JSON](data/daily/2025-12-14.json) |
| ğŸ“„ 2025-12-13 | 24 | [View JSON](data/daily/2025-12-13.json) |
| ğŸ“„ 2025-12-12 | 21 | [View JSON](data/daily/2025-12-12.json) |
| ğŸ“„ 2025-12-11 | 25 | [View JSON](data/daily/2025-12-11.json) |
| ğŸ“„ 2025-12-10 | 29 | [View JSON](data/daily/2025-12-10.json) |
| ğŸ“„ 2025-12-09 | 24 | [View JSON](data/daily/2025-12-09.json) |
| ğŸ“„ 2025-12-08 | 38 | [View JSON](data/daily/2025-12-08.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 373 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
