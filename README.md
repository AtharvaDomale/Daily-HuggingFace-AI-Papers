<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-52-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1827+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">52</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">263</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">308</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1827+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** February 06, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. ERNIE 5.0 Technical Report</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** HasuerYu, LLLL, guanwcn, max-zhenyu-zhang, sjy1203

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04705) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04705) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04705)

> good work

</details>

<details>
<summary><b>2. FASA: Frequency-aware Sparse Attention</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03152) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03152) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03152)

> [ICLR26] A very interesting and effective work to speed up the inference of large models!

</details>

<details>
<summary><b>3. WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning</b> â­ 2.38k</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04634) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04634) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04634)

**ğŸ’» Code:** [â­ Code](https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1)

> We introduce WideSeek-R1, a lead-agent-subagent system trained via multi-agent RL to explore width scaling for broad information seeking. ğŸŒ Project Page | ğŸ“„ Paper | ğŸ’» Code | ğŸ“¦ Dataset | ğŸ¤— Models

</details>

<details>
<summary><b>4. Training Data Efficiency in Multimodal Process Reward Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Haolin Liu, Shaoyang Xu, Langlin Huang, Chengsong Huang, jinyuan222

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04145) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04145) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04145)

**ğŸ’» Code:** [â­ Code](https://github.com/JinYuanLi0012/Balanced-Info-MPRM)

> Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies...

</details>

<details>
<summary><b>5. OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yiyan Ji, UnnamedWatcher, xuyang-liu16, Jungang, dingyue1011

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04804) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04804) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04804)

> We present OmniSIFT , which is a modality-asymmetric token compression framework tailored for Omni-LLMs.

</details>

<details>
<summary><b>6. HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03560) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03560) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03560)

> Efficient LLM Architecture, Sparse Attention, Hybrid Architecture

</details>

<details>
<summary><b>7. EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ziyi Bai, Chaojie Li, MingMing Yu, Yu Bai, tellarin

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04515) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04515) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04515)

> EgoActor is one of the key components of project RoboNoid. Project page: https://baai-agents.github.io/EgoActor/

</details>

<details>
<summary><b>8. Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02958) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02958) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02958)

> Efficient Long Video Generation, designed for world models and autoregressive video gen applications

</details>

<details>
<summary><b>9. SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02402) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02402) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02402)

> Project Page: https://city-super.github.io/SoMA/

</details>

<details>
<summary><b>10. TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Qiushi Sun, Fangzhi Xu, Xinyu Che, Hang Yan, VentureZJ

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02196) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02196) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02196)

> First Paper For Diagnostic Evaluation of Test-Time Improvement in LLM Agents

</details>

<details>
<summary><b>11. Residual Context Diffusion Language Models</b> â­ 44</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22954) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22954) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22954)

**ğŸ’» Code:** [â­ Code](https://github.com/yuezhouhu/residual-context-diffusion)

> We introduce Residual Context Diffusion (RCD): a simple idea to boost diffusion LLMsâ€”stop wasting â€œremaskedâ€ tokens. Diffusion LLMs decode in parallel but often lag AR models because low-confidence tokens are discarded each step. RCD turns those d...

</details>

<details>
<summary><b>12. Rethinking the Trust Region in LLM Reinforcement Learning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04879) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04879) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04879)

> No abstract available.

</details>

<details>
<summary><b>13. Learning to Repair Lean Proofs from Compiler Feedback</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02990) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02990) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02990)

> Existing Lean datasets contain correct proofs. Models learn error correction with RL, that's expensive. We release a dataset of 260k erroneous Lean proofs, the compiler feedback, error explanation, proof repair reasoning trace, and the corrected p...

</details>

<details>
<summary><b>14. Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03510) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03510) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03510)

> Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary den...

</details>

<details>
<summary><b>15. HY3D-Bench: Generation of 3D Assets</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03907) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03907) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03907)

> HY3D-Bench provides a unified 3D generation data ecosystem with 250k real assets, 125k synthetic assets, structured part-level decomposition, and a pipeline enabling scalable 3D model training.

</details>

<details>
<summary><b>16. AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03828) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03828) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03828)

**ğŸ’» Code:** [â­ Code](https://github.com/ResearAI/AutoFigure-Edit)

> AutoFigure [Accepted to ICLR 2026] An automated scientific figure-drawing system for controllable generation of paper method diagrams. It is now fully open-sourced. The sketch generation process is user-intervenable and editable, avoiding â€œblack-b...

</details>

<details>
<summary><b>17. Self-Hinting Language Models Enhance Reinforcement Learning</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03143) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03143) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03143)

**ğŸ’» Code:** [â­ Code](https://github.com/BaohaoLiao/SAGE)

> RL for LLMs often stalls under sparse rewards â€” especially with GRPO, where whole rollout groups get identical 0 rewards and learning justâ€¦ dies. ğŸ’¡ SAGE fixes this with a simple but powerful idea: ğŸ‘‰ Let the model give itself hints during training....

</details>

<details>
<summary><b>18. CL-bench: A Benchmark for Context Learning</b> â­ 312</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03587) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03587) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03587)

**ğŸ’» Code:** [â­ Code](https://github.com/Tencent-Hunyuan/CL-bench)

> A benchmark for context learning

</details>

<details>
<summary><b>19. Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04575) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04575) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04575)

> For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a â€œusability ceilingâ€...

</details>

<details>
<summary><b>20. VLS: Steering Pretrained Robot Policies via Vision-Language Models</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03973) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03973) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03973)

**ğŸ’» Code:** [â­ Code](https://github.com/Vision-Language-Steering/code)

> Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation o...

</details>

<details>
<summary><b>21. A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces</b> â­ 39</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03442) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03442) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03442)

**ğŸ’» Code:** [â­ Code](https://github.com/Ayanami0730/arag)

> Existing RAG systems rely on Graph or Workflow paradigms that fail to scale with advances in model reasoning and tool-use capabilities. We introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the mod...

</details>

<details>
<summary><b>22. PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR</b> â­ 21</summary>

<br/>

**ğŸ‘¥ Authors:** Alejandro Lozano, Jan N. Hansen, yuhuizhang, pengxunduo, jmhb

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.18207) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.18207) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.18207)

**ğŸ’» Code:** [â­ Code](https://github.com/jmhb0/PaperSearchQA)

> Project page: https://jmhb0.github.io/PaperSearchQA/ Data: https://huggingface.co/collections/jmhb/papersearchqa Code for data-gen pipelines: https://github.com/jmhb0/PaperSearchQA

</details>

<details>
<summary><b>23. Horizon-LM: A RAM-Centric Architecture for LLM Training</b> â­ 7</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04816) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04816) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04816)

**ğŸ’» Code:** [â­ Code](https://github.com/DLYuanGod/Horizon-LM)

> Horizon-LM: Train hundred-billionâ€“parameter language models without buying more GPUs. We propose a RAM-centric, CPU-master training architecture that treats GPUs as transient compute engines rather than persistent parameter stores, enabling large-...

</details>

<details>
<summary><b>24. From Data to Behavior: Predicting Unintended Model Behaviors Before Training</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04735) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04735) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04735)

**ğŸ’» Code:** [â­ Code](https://github.com/zjunlp/Data2Behavior)

> Can we foresee unintended model behaviors before fine-tuning? We demonstrate that unintended biases and safety risks can be traced back to interpretable latent data statistics that mechanistically influence model activations, without any parameter...

</details>

<details>
<summary><b>25. MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22859) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22859) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22859)

**ğŸ’» Code:** [â­ Code](https://github.com/ernie-research/MEnvAgent)

> Check out this verifiable environment for SWE! Open-sourced dataset, Docker images, and evals!

</details>

<details>
<summary><b>26. Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04284) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04284) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04284)

**ğŸ’» Code:** [â­ Code](https://github.com/usail-hkust/Agent-Omit)

> Efficient LLM Agents.

</details>

<details>
<summary><b>27. D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use</b> â­ 7</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02160) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02160) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02160)

**ğŸ’» Code:** [â­ Code](https://github.com/alibaba/EfficientAI)

> good job , awesome boys !

</details>

<details>
<summary><b>28. SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03916) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03916) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03916)

**ğŸ’» Code:** [â­ Code](https://github.com/SpatiaLab-Reasoning/SpatiaLab)

> We are excited to share that our paper â€œğ’ğ©ğšğ­ğ¢ğšğ‹ğšğ›: ğ‚ğšğ§ ğ•ğ¢ğ¬ğ¢ğ¨ğ§â€“ğ‹ğšğ§ğ ğ®ğšğ ğ ğŒğ¨ğğğ¥ğ¬ ğğğ«ğŸğ¨ğ«ğ¦ ğ’ğ©ğšğ­ğ¢ğšğ¥ ğ‘ğğšğ¬ğ¨ğ§ğ¢ğ§ğ  ğ¢ğ§ ğ­ğ¡ğ ğ–ğ¢ğ¥ğ?â€ is accepted to ICLR 2026 (The Fourteenth International Conference on Learning Representations). SpatiaLab investigates how vision...

</details>

<details>
<summary><b>29. Quantifying the Gap between Understanding and Generation within Unified Multimodal Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02140) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02140) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02140)

> A benchmark focuses on quantifying the gap between understanding and generation in unified multimodal model.

</details>

<details>
<summary><b>30. BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xiaohua Wang, Zisu Huang, Yiyang Lu, Jingwen Xu, fdu-lcz

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02554) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02554) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02554)

> Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework...

</details>

<details>
<summary><b>31. Likelihood-Based Reward Designs for General LLM Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03979) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03979) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03979)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API P2S: Probabilistic Process Supervision for General-Domain Reasoning Questio...

</details>

<details>
<summary><b>32. A2Eval: Agentic and Automated Evaluation for Embodied Brain</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.01640) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.01640) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.01640)

> A2Eval introduces an agentic framework that automates embodied VLM evaluation through two collaborative agents: one that curates balanced benchmarks by identifying capability dimensions, and another that synthesizes executable evaluation pipelines...

</details>

<details>
<summary><b>33. Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yuwei Wang, Kehai Chen, Xuefeng Bai, Yu Zhang, Jinlong Ma

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04486) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04486) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04486)

> GMNER

</details>

<details>
<summary><b>34. MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03359) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03359) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03359)

> We introduce MeKi, a memory-based architecture to scale LLM efficiently. MeKi is able to offload pre-trained token-level expert knowledge to ROM space before deployment. Tested on a Snapdragon mobile platform,  our method achieves superior perform...

</details>

<details>
<summary><b>35. Efficient Autoregressive Video Diffusion with Dummy Head</b> â­ 32</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.20499) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.20499) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.20499)

**ğŸ’» Code:** [â­ Code](https://github.com/csguoh/DummyForcing)

> Dummy Forcing is built on the observation that about 25% attention heads in existing autoregressive video diffusion models are "dummy", attending almost exclusively to the current frame despite access to historical context. Based on this observati...

</details>

<details>
<summary><b>36. No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** dimakarp1996

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04442) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04442) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04442)

> We show that effective machine translation for low-resource Turkic languages requires a tailored approach: fine-tuning works best for languages with some data, while retrieval-augmented LLM prompting is essential for extremely resource-scarce ones.

</details>

<details>
<summary><b>37. Context Learning for Multi-Agent Discussion</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02350) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02350) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02350)

**ğŸ’» Code:** [â­ Code](https://github.com/HansenHua/M2CL-ICLR26)

> Try building your own multi-agent system to solve problems!

</details>

<details>
<summary><b>38. Protein Autoregressive Modeling via Multiscale Structure Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04883) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04883) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04883)

> Protein Autoregressive Modeling via Multiscale Structure Generation (PAR) introduces a coarse-to-fine transformerâ€“flow framework for backbone generation with noisy context learning to mitigate exposure bias.

</details>

<details>
<summary><b>39. Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Shi-Min Hu, Yan-Pei Cao, Meng-Hao Guo, Cheng-Feng Pu, Jia-peng Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04805) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04805) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04805)

> Proposes SkinTokens, a discrete, learnable skinning representation enabling a unified TokenRig autoregressive framework with reinforcement learning fine-tuning to improve rigging accuracy and generalization in 3D animation.

</details>

<details>
<summary><b>40. Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models</b> â­ 7</summary>

<br/>

**ğŸ‘¥ Authors:** Thomas B. SchÃ¶n, Lidong Bing, Lei Wang, Ziqi Jin, weblzw

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.01849) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.01849) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.01849)

**ğŸ’» Code:** [â­ Code](https://github.com/Algolzw/self-rewarding-smc)

> Self-Rewarding SMC improves sampling for diffusion language models without additional training or external reward guidance.

</details>

<details>
<summary><b>41. SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Dipan Maity

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04651) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04651) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04651)

> An alternative to ppo for RLHF.

</details>

<details>
<summary><b>42. RexBERT: Context Specialized Bidirectional Encoders for E-commerce</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04605) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04605) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04605)

> RexBERT Paper is finally out!

</details>

<details>
<summary><b>43. Trust The Typical</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Kanan Gupta, Vikash Singh, Biyao Zhang, Sreehari Sankar, Debargha Ganguly

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04581) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04581) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04581)

> Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply ...

</details>

<details>
<summary><b>44. OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** Cecilia Di Ruberto, Andrea Loddo, Luca Zedda

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04547) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04547) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04547)

**ğŸ’» Code:** [â­ Code](https://github.com/unica-visual-intelligence-lab/OmniRad)

> OmniRad introduces a self-supervised radiological foundation model pretrained on 1.2M medical images thatâ€™s designed for representation reuse across classification, segmentation, and visionâ€“language tasks. The paper shows consistent gains over pri...

</details>

<details>
<summary><b>45. Proxy Compression for Language Modeling</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Lingpeng Kong, Xiachong Feng, Qian Liu, Xinyu Li, Lin Zheng

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04289) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04289) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04289)

**ğŸ’» Code:** [â­ Code](https://github.com/LZhengisme/proxy-compression)

> This work introduces proxy compression, an alternative training scheme for language models that preserves the efficiency benefits of compression (e.g. tokenization) while providing an end-to-end, byte-level interface at inference time.

</details>

<details>
<summary><b>46. SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04271) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04271) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04271)

> ğŸš€ Introducing SkeletonGaussian â€” Editable 4D Generation through Gaussian Skeletonization! (Accepted by CVM 2026) âœ¨ Generate dynamic 3D Gaussians from text, images, or videos ğŸ¦´ Explicit skeleton-driven motion enables intuitive pose editing ğŸ¯ Higher...

</details>

<details>
<summary><b>47. AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.03955) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.03955) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.03955)

**ğŸ’» Code:** [â­ Code](https://github.com/AIFrontierLab/AgentArk)

> Distilling multi-agent intelligence into a single agent. A comprehensive study.

</details>

<details>
<summary><b>48. "I May Not Have Articulated Myself Clearly": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Vlado Keselj, Sijia Han, Fengxiang Cheng, Jinkun Chen

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02863) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02863) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02863)

> Large language models often fail during multi-step reasoning, but the failure is usually only observable at the final answer. This paper introduces an inference-time, training-free diagnostic signal for identifying dynamic instability during reaso...

</details>

<details>
<summary><b>49. Reward-free Alignment for Conflicting Objectives</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Tianyi Lin, Xi Chen, Xiaopeng Li, Peter Chen

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02495) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02495) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02495)

> Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to un...

</details>

<details>
<summary><b>50. LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Desen Meng, Xinhao Li, Zihan Jia, Jiaqi Li, hzp

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02341) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02341) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02341)

**ğŸ’» Code:** [â­ Code](https://github.com/MCG-NJU/LongVPO)

> Code: https://github.com/MCG-NJU/LongVPO

</details>

<details>
<summary><b>51. FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.22596) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.22596) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.22596)

**ğŸ’» Code:** [â­ Code](https://github.com/abdelpy/FOTBCD-datasets)

> We release FOTBCD, a large-scale French aerial building change detection benchmark (0.2 m), including ~28k binary-labeled pairs and 4k instance-level COCO pairs, plus pretrained weights and code for reproducible training and evaluation.

</details>

<details>
<summary><b>52. HalluHard: A Hard Multi-Turn Hallucination Benchmark</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Maksym Andriushchenko, Nicolas Flammarion, Sebastien Delsad, Dongyang Fan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.01031) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.01031) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.01031)

> LLM hallucinations are far from solved!

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 52 |
| ğŸ“… Today | [`2026-02-06.json`](data/daily/2026-02-06.json) | 52 |
| ğŸ“† This Week | [`2026-W05.json`](data/weekly/2026-W05.json) | 263 |
| ğŸ—“ï¸ This Month | [`2026-02.json`](data/monthly/2026-02.json) | 308 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-02-06 | 52 | [View JSON](data/daily/2026-02-06.json) |
| ğŸ“„ 2026-02-05 | 53 | [View JSON](data/daily/2026-02-05.json) |
| ğŸ“„ 2026-02-04 | 73 | [View JSON](data/daily/2026-02-04.json) |
| ğŸ“„ 2026-02-03 | 40 | [View JSON](data/daily/2026-02-03.json) |
| ğŸ“„ 2026-02-02 | 45 | [View JSON](data/daily/2026-02-02.json) |
| ğŸ“„ 2026-02-01 | 45 | [View JSON](data/daily/2026-02-01.json) |
| ğŸ“„ 2026-01-31 | 45 | [View JSON](data/daily/2026-01-31.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W05 | 263 | [View JSON](data/weekly/2026-W05.json) |
| ğŸ“… 2026-W04 | 214 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |
| ğŸ“… 2026-W02 | 232 | [View JSON](data/weekly/2026-W02.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-02 | 308 | [View JSON](data/monthly/2026-02.json) |
| ğŸ—“ï¸ 2026-01 | 781 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
