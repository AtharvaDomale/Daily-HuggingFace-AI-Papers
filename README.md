<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-27-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1091+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">27</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">156</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">353</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1091+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 16, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Controlled Self-Evolution for Algorithmic Code Optimization</b> â­ 79</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.07348) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.07348) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.07348)

**ğŸ’» Code:** [â­ Code](https://github.com/QuantaAlpha/EvoControl)

> arXiv explained breakdown of this paper ğŸ‘‰ https://arxivexplained.com/papers/controlled-self-evolution-for-algorithmic-code-optimization

</details>

<details>
<summary><b>2. DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation</b> â­ 67</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09688) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09688) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09688)

**ğŸ’» Code:** [â­ Code](https://github.com/Infinity-AILab/DeepResearchEval)

> Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation ...

</details>

<details>
<summary><b>3. MAXS: Meta-Adaptive Exploration with LLM Agents</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09259) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09259) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09259)

**ğŸ’» Code:** [â­ Code](https://github.com/exoskeletonzj/MAXS)

> Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead,...

</details>

<details>
<summary><b>4. A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09274) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09274) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09274)

**ğŸ’» Code:** [â­ Code](https://github.com/exoskeletonzj/A3-Bench)

> Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks main...

</details>

<details>
<summary><b>5. Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning</b> â­ 16</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09088) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09088) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09088)

**ğŸ’» Code:** [â­ Code](https://github.com/D2I-ai/dasd-thinking)

> In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific ...

</details>

<details>
<summary><b>6. Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09708) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09708) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09708)

> Project page: https://jasper0314-huang.github.io/fast-thinkact/

</details>

<details>
<summary><b>7. SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09136) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09136) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09136)

> General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge...

</details>

<details>
<summary><b>8. OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09575) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09575) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09575)

> OpenVoxel provides training-free grouping and captioning of sparse voxels for open-vocabulary 3D scene understanding using VLMs/MLLMs and text search, enabling RES and OVS without CLIP embeddings.

</details>

<details>
<summary><b>9. OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09028) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09028) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09028)

> OpenDecoder is a novel framework that directly 'opens' the LLM to modify its decoding process within RAG scenarios by leveraging relevance signals from retrieved documents. Through a robustness-oriented training algorithm, the model learns to perf...

</details>

<details>
<summary><b>10. ExpSeek: Self-Triggered Experience Seeking for Web Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.08605) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.08605) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.08605)

> Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passiv...

</details>

<details>
<summary><b>11. EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09465) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09465) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09465)

> EvoFSM presents a controllable self-evolution framework using a finite state machine to guide adaptive problem-solving, separating macroscopic flow and microscopic skills with critic-guided updates and reusable priors.

</details>

<details>
<summary><b>12. FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.03928) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.03928) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.03928)

**ğŸ’» Code:** [â­ Code](https://github.com/showlab/FocusUI)

> TL;DR: High-res UI screenshots (2K/4K) force VLMs to process thousands of visual tokens. Inspired by human vision, which selects only instruction-relevant image patches, FocusUI teaches VLMs where to look in UI screenshots smartly ğŸ” ğŸ“„ Paper: arXiv...

</details>

<details>
<summary><b>13. Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Chi Zhang, Jiawei Shao, Jiangan Chen, Yiliang Song, Hongjun An

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.06596) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.06596) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.06596)

> This paper treats preference undermining as an experimental object, not a vibe. A clean factorial design isolates manipulation factors and quantifies when truth yields to compliance. Conclusion, stated politely: yes, a large model can be PUA-ed, a...

</details>

<details>
<summary><b>14. TranslateGemma Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09012) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09012) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09012)

> TranslateGemma extends Gemma 3 with two-stage fine-tuning (supervised then RL) for multilingual translation, achieving strong WMT performance and multimodal capabilities.

</details>

<details>
<summary><b>15. Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Wenjie Li, Beichen Guo, Hanlin Wang, Youwei Liu, jwanglvy

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.08955) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.08955) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.08955)

> TL;DR: An agent learning framework via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step "imagined" trajectories. This imagination is conducted via a novel adaptive lookahead mechanism...

</details>

<details>
<summary><b>16. Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ayush Tewari, Joan Lasenby, Jeffrey Hu, Jieying Chen

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09697) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09697) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09697)

> Proposes SRENDER: generate sparse diffusion keyframes for static scenes and render 3D views to produce long videos fast and consistently.

</details>

<details>
<summary><b>17. Geometric Stability: The Missing Axis of Representations</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** pcr2120

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09173) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09173) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09173)

**ğŸ’» Code:** [â­ Code](https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials) â€¢ [â­ Code](https://github.com/prashantcraju/geometric-stability)

> DeepSeek got it half right with their mHC paper: stability matters for scaling. But they only measure stability DURING training. What about the stability of what models LEARN? I built Shesha to measure this - a geometric stability metric with SOTA...

</details>

<details>
<summary><b>18. The AI Hippocampus: How Far are We From Human Memory?</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Tong Wu, Yuxuan Wang, Yipeng Kang, Jiaqi Li, Zixia Jia

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09113) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09113) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09113)

> Survey of memory in LLMs and multimodal models, detailing implicit, explicit, and agentic memory, architectures, benchmarks, and challenges in persistence, alignment, and cross-modal retrieval.

</details>

<details>
<summary><b>19. Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments</b> â­ 5</summary>

<br/>

**ğŸ‘¥ Authors:** Thomas Anderson Keller, Yilun Du, Fangneng Zhan, Benhao Huang, Hansen Jin Lillemark

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.01075) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.01075) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.01075)

**ğŸ’» Code:** [â­ Code](https://github.com/hlillemark/flowm)

> No abstract available.

</details>

<details>
<summary><b>20. DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ruihua Song, Yi Zhao, Wei Bi, Yahui Liu, Qian Cao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09609) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09609) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09609)

> Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding divers...

</details>

<details>
<summary><b>21. Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09536) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09536) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09536)

> This paper proposes a unified generative multimodal reasoning paradigm, using a two-stage SFT+RL framework with perception alignment loss and perception reward, and explores bootstrapping step-wise visualizations from text-only reasoning data when...

</details>

<details>
<summary><b>22. Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xiao Yang, Kaipeng Zhang, Shenghai Yuan, Yuanyang Yin, yfdeng10

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.07287) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.07287) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.07287)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API V-Warper: Appearance-Consistent Video Diffusion Personalization via Value W...

</details>

<details>
<summary><b>23. No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yixia Li, Xingchen Zeng, Yulan Hu, Lingjie Jiang, Zhicong Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.06794) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.06794) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.06794)

> Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, whi...

</details>

<details>
<summary><b>24. SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Yixin Cao, Xinrun Wang, Zhongyuan Peng, Changyi Xiao, SII-Molu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04809) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04809) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04809)

**ğŸ’» Code:** [â­ Code](https://github.com/molumolua/SCALER)

> Scalable Environment Synthesis Given a programming problem (statement + reference solution), SCALER synthesizes a reasoning environment with: Verifiability: deterministic oracle / unit tests provide correctness signals. Difficulty control: explici...

</details>

<details>
<summary><b>25. Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jolanta Mizeria-Pietraszko, lsliwko

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.09282) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.09282) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.09282)

> Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Lang...

</details>

<details>
<summary><b>26. sui-1: Grounded and Verifiable Long-Form Summarization</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.08472) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.08472) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.08472)

> No abstract available.

</details>

<details>
<summary><b>27. SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Aleksey Komissarov, Ekaterina Chelombitko, Iaroslav Chelombitko

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.04469) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.04469) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.04469)

**ğŸ’» Code:** [â­ Code](https://github.com/AragonerUA/SampoNLP)

> The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for mor...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 27 |
| ğŸ“… Today | [`2026-01-16.json`](data/daily/2026-01-16.json) | 27 |
| ğŸ“† This Week | [`2026-W02.json`](data/weekly/2026-W02.json) | 156 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 353 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-16 | 27 | [View JSON](data/daily/2026-01-16.json) |
| ğŸ“„ 2026-01-15 | 24 | [View JSON](data/daily/2026-01-15.json) |
| ğŸ“„ 2026-01-14 | 42 | [View JSON](data/daily/2026-01-14.json) |
| ğŸ“„ 2026-01-13 | 30 | [View JSON](data/daily/2026-01-13.json) |
| ğŸ“„ 2026-01-12 | 33 | [View JSON](data/daily/2026-01-12.json) |
| ğŸ“„ 2026-01-11 | 33 | [View JSON](data/daily/2026-01-11.json) |
| ğŸ“„ 2026-01-10 | 33 | [View JSON](data/daily/2026-01-10.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W02 | 156 | [View JSON](data/weekly/2026-W02.json) |
| ğŸ“… 2026-W01 | 156 | [View JSON](data/weekly/2026-W01.json) |
| ğŸ“… 2026-W00 | 41 | [View JSON](data/weekly/2026-W00.json) |
| ğŸ“… 2025-W52 | 52 | [View JSON](data/weekly/2025-W52.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 353 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
