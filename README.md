<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-41-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-2214+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">41</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">293</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">695</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">2214+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** February 15, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jinyu Hou, Zejian Chen, Songyang Liu, Chaozhuo Li, xunyoyo

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09877) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09877) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09877)

> The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loo...

</details>

<details>
<summary><b>2. Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models</b> â­ 14</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12036) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12036) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12036)

**ğŸ’» Code:** [â­ Code](https://github.com/XinXU-USTC/Composition-RL)

> Models and datasets are available at https://huggingface.co/collections/xx18/composition-rl

</details>

<details>
<summary><b>3. DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing</b> â­ 71</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12205) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12205) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12205)

**ğŸ’» Code:** [â­ Code](https://github.com/DeepGenTeam/DeepGen)

> Models: https://huggingface.co/deepgenteam/DeepGen-1.0 Datasets: https://huggingface.co/datasets/DeepGenTeam/DeepGen-1.0

</details>

<details>
<summary><b>4. Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</b> â­ 13</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12125) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12125) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12125)

**ğŸ’» Code:** [â­ Code](https://github.com/RUCBM/G-OPD)

> We propose G-OPD, a generalized on-policy distillation framework. Building on G-OPD, we propose ExOPD (Generalized On-Policy Distillation with Reward Extrapolation), which enables a unified student to surpass all domain teachers in the multi-teach...

</details>

<details>
<summary><b>5. MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models</b> â­ 84</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10934) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10934) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10934)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenMOSS/MOSS-Audio-Tokenizer)

> Start discussion in this paper

</details>

<details>
<summary><b>6. GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning</b> â­ 2.3k</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12099) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12099) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12099)

**ğŸ’» Code:** [â­ Code](https://github.com/open-gigaai/giga-brain-0)

> GigaBrain-0.5M* is a VLA That Learns From World Model-Based Reinforcement Learning. GigaBrain-0.5M* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure.

</details>

<details>
<summary><b>7. NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09070) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09070) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09070)

> No abstract available.

</details>

<details>
<summary><b>8. LawThinker: A Deep Research Legal Agent in Dynamic Environments</b> â­ 23</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12056) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12056) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12056)

**ğŸ’» Code:** [â­ Code](https://github.com/yxy-919/LawThinker-agent)

> Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to pro...

</details>

<details>
<summary><b>9. Thinking with Drafting: Optical Decompression via Logical Reconstruction</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11731) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11731) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11731)

> The core idea of Thinking with Drafting (TwD) is super refreshing: instead of letting a multimodal model â€œguess the answerâ€ with fluent CoT or pretty-looking diagrams, it forces the model to draft its reasoning into executable structure. Not vibes...

</details>

<details>
<summary><b>10. Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</b> â­ 26</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12280) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12280) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12280)

**ğŸ’» Code:** [â­ Code](https://github.com/stroke-of-surprise/Stroke-Of-Surprise)

> Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformatio...

</details>

<details>
<summary><b>11. Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning</b> â­ 11</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11748) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11748) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11748)

**ğŸ’» Code:** [â­ Code](https://github.com/LINs-lab/LIE)

> ğŸ”— Code: https://github.com/LINs-lab/LIE ğŸ”— Paper: https://arxiv.org/abs/2602.11748

</details>

<details>
<summary><b>12. RISE: Self-Improving Robot Policy with Compositional World Model</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11075) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11075) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11075)

> The first study on leveraging world models as an effective learning environment for challenging real-world manipulation, bootstrapping performance on tasks requiring high dynamics, dexterity, and precision.

</details>

<details>
<summary><b>13. Ï‡_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies</b> â­ 182</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09021) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09021) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09021)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenDriveLab/KAI0)

> ğŸ§¥ Live-stream robotic teamwork that folds clothes. 6 clothes in 3 minutes straight. Ï‡â‚€ = 20hrs data + 8 A100s + 3 key insights: Mode Consistency: align your distributions Model Arithmetic: merge, don't retrain Stage Advantage: pivot wisely ğŸ”— http:...

</details>

<details>
<summary><b>14. dVoting: Fast Voting for dLLMs</b> â­ 22</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12153) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12153) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12153)

**ğŸ’» Code:** [â­ Code](https://github.com/fscdc/dVoting)

> The first efficient test-time scaling strategy for dLLMs. Welcome any discussion!

</details>

<details>
<summary><b>15. EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yinghui Li, Haoran Jiang, Jin Chen, Shijia Peng, Modi Shi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10106) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10106) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10106)

> Project page: https://opendrivelab.com/EgoHumanoid

</details>

<details>
<summary><b>16. Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</b> â­ 35</summary>

<br/>

**ğŸ‘¥ Authors:** Yukuan Xu, Yuxian Li, Li Chen, Siqi Liang, Hai Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.05827) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.05827) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.05827)

**ğŸ’» Code:** [â­ Code](https://github.com/opendrivelab/sparsevideonav)

> SparseVideoNav introduces video generation models to real-world beyond-the-view vision-language navigation for the first time. It achieves sub-second trajectory inference with a sparse future spanning a 20-second horizon, yielding a remarkable 27Ã—...

</details>

<details>
<summary><b>17. DeepSight: An All-in-One LM Safety Toolkit</b> â­ 34</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12092) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12092) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12092)

**ğŸ’» Code:** [â­ Code](https://github.com/AI45Lab/DeepSafe) â€¢ [â­ Code](https://github.com/AI45Lab/DeepScan/)

> We propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm.

</details>

<details>
<summary><b>18. Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.05548) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.05548) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.05548)

**ğŸ’» Code:** [â­ Code](https://github.com/HKU-HealthAI/A-GRAE)

> See https://github.com/HKU-HealthAI/A-GRAE for the code base, and https://yu7-code.github.io/A-GRAE-web/ for the project page

</details>

<details>
<summary><b>19. Adapting Vision-Language Models for E-commerce Understanding at Scale</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11733) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11733) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11733)

> Figure 1: Output of our E-commerce Adapted VLMs compared against same size LLaVA-OneVision . We show our models ability to more faithfully extract attributes from e-commerce items. In red, we highlight wrong model predictions that are neither tied...

</details>

<details>
<summary><b>20. Voxtral Realtime</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11298) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11298) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11298)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Streaming Speech Recognition with Decoder-Only Large Language Models and La...

</details>

<details>
<summary><b>21. PISCO: Precise Video Instance Insertion with Sparse Control</b> â­ 27</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08277) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08277) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08277)

**ğŸ’» Code:** [â­ Code](https://github.com/taco-group/PISCO)

> PISCO: Precise Video Instance Insertion with Sparse Control

</details>

<details>
<summary><b>22. T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</b> â­ 13</summary>

<br/>

**ğŸ‘¥ Authors:** Xiaoxiao He, Haizhou Shi, Ligong Han, Xinxi Zhang, Tyrion279

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12262) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12262) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12262)

**ğŸ’» Code:** [â­ Code](https://github.com/Tyrion58/T3D)

> Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggress...

</details>

<details>
<summary><b>23. Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11964) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11964) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11964)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IDRBench: Interactive Deep Research Benchmark (2026) Agent World Model: Inf...

</details>

<details>
<summary><b>24. MemFly: On-the-Fly Memory Optimization via Information Bottleneck</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Wei Xue, Zhenbo Song, Zhiqin Yang, Xianzhang Jia, Zhenyuan Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07885) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07885) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07885)

> No abstract available.

</details>

<details>
<summary><b>25. Single-minus gluon tree amplitudes are nonzero</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12176) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12176) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12176)

> A group of theoretical physicists derive a new result in quantum field theory using GPT-5.2 Pro.

</details>

<details>
<summary><b>26. ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Julian McAuley, Haoliang Wang, Xiang Chen, Tong Yu, XinXuNLPer

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11683) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11683) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11683)

> This paper proposes ThinkRouter, a confidence-aware routing mechanism to improve reasoning performance for large reasoning models (LRMs), which routes LRMs thinking between latent and discrete token spaces based on model confidence at inference time.

</details>

<details>
<summary><b>27. Dreaming in Code for Curriculum Learning in Open-Ended Worlds</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08194) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08194) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08194)

**ğŸ’» Code:** [â­ Code](https://github.com/konstantinosmitsides/dreaming-in-code)

> Large Language Models that "dream" and materialize executable environment code to scaffold learning in open-ended worlds.

</details>

<details>
<summary><b>28. MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11761) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11761) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11761)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Hybrid Linear Attention Done Right: Efficient Distillation and Effective Ar...

</details>

<details>
<summary><b>29. MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation</b> â­ 92</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11337) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11337) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11337)

**ğŸ’» Code:** [â­ Code](https://github.com/allenai/molmospaces)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes (2026) Gen...

</details>

<details>
<summary><b>30. Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm</b> â­ 17</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11543) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11543) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11543)

**ğŸ’» Code:** [â­ Code](https://github.com/zjr2000/SPES)

> We propose SPES, a decentralized framework for pretraining MoE LLMs. SPES supports sparse training on weakly connected nodes, reducing memory and communication costs and enabling efficient pretraining on resource-constrained devices.

</details>

<details>
<summary><b>31. MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning</b> â­ 4</summary>

<br/>

**ğŸ‘¥ Authors:** Hongsheng Li, Yazhe Niu, Chenhao Zhang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10575) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10575) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10575)

**ğŸ’» Code:** [â­ Code](https://github.com/MING-ZCH/MetaphorStar)

> Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emo...

</details>

<details>
<summary><b>32. ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12203) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12203) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12203)

> We introduce ExStrucTiny, a new benchmark for structured information extraction from document images that unifies in one task (1) key entity extraction, (2) relation extraction, and (3) visual question answering across diverse input schemas and do...

</details>

<details>
<summary><b>33. Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12164) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12164) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12164)

**ğŸ’» Code:** [â­ Code](https://github.com/InternScience/Sci-CoE)

> Weâ€™re excited to share our new work, Sci-CoE ! ğŸ‰ In this project, we tackle a fundamental challenge: ğŸ§© How can we train LLMs with RL when there is no explicit final answer and no way to compute outcome rewards via unit tests or exact matching? Thi...

</details>

<details>
<summary><b>34. P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling</b> â­ 12</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.12116) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.12116) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.12116)

**ğŸ’» Code:** [â­ Code](https://github.com/Tongyi-ConvAI/Qwen-Character)

> Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing pe...

</details>

<details>
<summary><b>35. Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11541) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11541) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11541)

> Don't Let Your Agent Max Out Your Credit Card! ğŸ’³ Most current research pushes the boundaries of agent performance but often overlooks the actual economic cost. Can agents still make rational decisions when every tool call comes with a price tag? W...

</details>

<details>
<summary><b>36. Multimodal Fact-Level Attribution for Verifiable Reasoning</b> â­ 3</summary>

<br/>

**ğŸ‘¥ Authors:** Hyunji Lee, Elias Stengel-Eskin, Ziyang Wang, David Wan, HanNight

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11509) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11509) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11509)

**ğŸ’» Code:** [â­ Code](https://github.com/meetdavidwan/murgat)

> No abstract available.

</details>

<details>
<summary><b>37. ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11636) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11636) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11636)

**ğŸ’» Code:** [â­ Code](https://github.com/ChangtiWu/ScalSelect)

> ScalSelect: Training-Free and Scalable Data Selection for Visual Instruction Tuning Large-scale visual instruction tuning datasets are highly redundant, yet full-data training remains the default â€” leading to substantial computational waste. This ...

</details>

<details>
<summary><b>38. ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation</b> â­ 37</summary>

<br/>

**ğŸ‘¥ Authors:** Minghua Luo, Yanfen Shen, Xiaolong Wu, Shichao Xie, Zedong Chu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11598) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11598) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11598)

**ğŸ’» Code:** [â­ Code](https://github.com/amap-cvlab/ABot-Navigation)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aeria...

</details>

<details>
<summary><b>39. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** Aidong Zhang, Sanchit Sinha, Guangzhi Xiong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10585) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10585) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10585)

**ğŸ’» Code:** [â­ Code](https://github.com/Teddy-XiongGZ/NAE)

> TL;DR: We introduce Neural Additive Experts (NAEs), a context-gated mixture-of-experts extension of generalized additive models that preserves per-feature explanations while capturing interactions when needed, using a single tunable regularizer to...

</details>

<details>
<summary><b>40. Stemphonic: All-at-once Flexible Multi-stem Music Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09891) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09891) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09891)

> Check out intro & demo video here! https://youtu.be/IrGD3CHaPYU?si=nutPyU5sz5iHfES5 More sound examples: https://stemphonic-demo.vercel.app

</details>

<details>
<summary><b>41. Detecting RLVR Training Data via Structural Convergence of Reasoning</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.11792) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.11792) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.11792)

**ğŸ’» Code:** [â­ Code](https://github.com/StevenZHB/Detect_RLVR_Data)

> Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-le...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 41 |
| ğŸ“… Today | [`2026-02-15.json`](data/daily/2026-02-15.json) | 41 |
| ğŸ“† This Week | [`2026-W06.json`](data/weekly/2026-W06.json) | 293 |
| ğŸ—“ï¸ This Month | [`2026-02.json`](data/monthly/2026-02.json) | 695 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-02-15 | 41 | [View JSON](data/daily/2026-02-15.json) |
| ğŸ“„ 2026-02-14 | 41 | [View JSON](data/daily/2026-02-14.json) |
| ğŸ“„ 2026-02-13 | 47 | [View JSON](data/daily/2026-02-13.json) |
| ğŸ“„ 2026-02-12 | 57 | [View JSON](data/daily/2026-02-12.json) |
| ğŸ“„ 2026-02-11 | 58 | [View JSON](data/daily/2026-02-11.json) |
| ğŸ“„ 2026-02-10 | 2 | [View JSON](data/daily/2026-02-10.json) |
| ğŸ“„ 2026-02-09 | 47 | [View JSON](data/daily/2026-02-09.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W06 | 293 | [View JSON](data/weekly/2026-W06.json) |
| ğŸ“… 2026-W05 | 357 | [View JSON](data/weekly/2026-W05.json) |
| ğŸ“… 2026-W04 | 214 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-02 | 695 | [View JSON](data/monthly/2026-02.json) |
| ğŸ—“ï¸ 2026-01 | 781 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
