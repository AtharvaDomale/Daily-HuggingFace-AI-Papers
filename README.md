<div align="center">

# ğŸ¤– Daily AI Papers

### ğŸ“Š Trending AI Research Papers from HuggingFace

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/yourusername/daily-ai-papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-25-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-49+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](#)

</div>

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">25</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">73</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">73</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">49+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 05, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. Qwen3-VL Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2511.21631) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.21631) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2511.21631)

> Qwen3-VL Technical Report

</details>

<details>
<summary><b>2. Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</b> â­ 7</summary>

<br/>

**ğŸ‘¥ Authors:** Xiu Li, Ling Pan, Siyuan Yang, haoranhe, breezeyoung

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.02834) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.02834) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.02834)

**ğŸ’» Code:** [â­ Code](https://github.com/breez3young/TACO)

> ğŸš€ Excited to share our latest work: TACO ğŸŒ®: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! ğŸ˜± Inspi...

</details>

<details>
<summary><b>3. PretrainZero: Reinforcement Active Pretraining</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Guoqi Li, Jie Lou, debingzhang, Zhiyuan-Fan, xrxing

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03442) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03442) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03442)

> Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abi...

</details>

<details>
<summary><b>4. ViDiC: Video Difference Captioning</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** jiakaiW, heyween, wrz123, LongoXC, Leexeo

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03405) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03405) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03405)

> Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Dif...

</details>

<details>
<summary><b>5. SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04069) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04069) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04069)

**ğŸ’» Code:** [â­ Code](https://github.com/spacetools/SpaceTools)

> TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks...

</details>

<details>
<summary><b>6. OneThinker: All-in-one Reasoning Model for Image and Video</b> â­ 43</summary>

<br/>

**ğŸ‘¥ Authors:** Kaixuan Fan, Hongyu Li, Manyuan Zhang, Kaituo Feng, zhengli1013

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03043) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03043) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03043)

**ğŸ’» Code:** [â­ Code](https://github.com/tulerfeng/OneThinker)

> Project page: https://github.com/tulerfeng/OneThinker

</details>

<details>
<summary><b>7. Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Difan Liu, Yiran Xu, Mamshad Nayeem Rizve, Sangwoo Mo, Subin Kim

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03534) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03534) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03534)

> Scaling visuals with prompts redesigned for the scaled outputs â†’ break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute ...

</details>

<details>
<summary><b>8. RELIC: Interactive Video World Model with Long-Horizon Memory</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Chongjian Ge, Yiqun Mei, kalyanks, saibi, YicongHong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04040) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04040) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04040)

> A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving a...

</details>

<details>
<summary><b>9. Thinking with Programming Vision: Towards a Unified View for Thinking with Images</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Tao Jin, Kai Jia, Feng Zhang, Minjie Hong, Zirun Guo

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03746) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03746) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03746)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy...

</details>

<details>
<summary><b>10. Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</b> â­ 6</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2511.22345) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.22345) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2511.22345)

**ğŸ’» Code:** [â­ Code](https://github.com/MCG-NJU/FlowBack)

> No abstract available.

</details>

<details>
<summary><b>11. CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</b> â­ 3</summary>

<br/>

**ğŸ‘¥ Authors:** Yi Yao, Hongxia Xie, Bin Wen, Ruoxuan Zhang, twbear2024

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03540) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03540) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03540)

**ğŸ’» Code:** [â­ Code](https://github.com/zhangdaxia22/CookAnything)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DCText: Scheduled Attention Masking for Visual Text Generation via Divide-a...

</details>

<details>
<summary><b>12. AutoNeural: Co-Designing Vision-Language Models for NPU Inference</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.02924) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.02924) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.02924)

> AutoNeural-VL-1.5B â€” the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a softwareâ€“hardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines...

</details>

<details>
<summary><b>13. SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yi Yang, yixuantt

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.02807) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.02807) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.02807)

> ğŸ¤¯ We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output...

</details>

<details>
<summary><b>14. Jina-VLM: Small Multilingual Vision Language Model</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04032) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04032) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04032)

> our latest multilingual vlm model at 2b size, about to release soon

</details>

<details>
<summary><b>15. Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03073) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03073) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03073)

> We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model prope...

</details>

<details>
<summary><b>16. UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs</b> â­ 4</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03383) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03383) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03383)

**ğŸ’» Code:** [â­ Code](https://github.com/enyac-group/UniQL)

> ğŸ“š  support Transformers, State Space Models (SSMs), and hybrid architectures âœ‚ï¸ Efficient, quantization-friendly pruning algorithms ğŸ”— One-pass framework for quantization + structured low-rank pruning ğŸ“± On-device adaptive pruning driven by real-tim...

</details>

<details>
<summary><b>17. AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Tosho Hirasawa, Shohei Tanaka, Kuniaki Saito, yushiku, risashinoda

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2511.20515) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.20515) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2511.20515)

> Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment....

</details>

<details>
<summary><b>18. SkillFactory: Self-Distillation For Learning Cognitive Behaviors</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Manya Wadhwa, Jack Lu, gregdurrett, sedrickkeh, Zaynes

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04072) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04072) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04072)

> SkillFactory: Self-Distillation For Learning Cognitive Behaviors

</details>

<details>
<summary><b>19. BlurDM: A Blur Diffusion Model for Image Deblurring</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03979) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03979) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03979)

> We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process ...

</details>

<details>
<summary><b>20. In-Context Representation Hijacking</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** yossig, MichaelKar, aimir, tux

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03771) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03771) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03771)

> We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple i...

</details>

<details>
<summary><b>21. PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</b> â­ 4</summary>

<br/>

**ğŸ‘¥ Authors:** Bohan Zhuang, Weijie Wang, Xi Lin, Youping Gu, Xiaolong Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04025) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04025) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04025)

**ğŸ’» Code:** [â­ Code](https://github.com/ziplab/Pyramid-Sparse-Attention)

> Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant p...

</details>

<details>
<summary><b>22. Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04000) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04000) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04000)

**ğŸ’» Code:** [â­ Code](https://github.com/Jialuo-Li/DIG)

> The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on qu...

</details>

<details>
<summary><b>23. AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.03794) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.03794) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.03794)

**ğŸ’» Code:** [â­ Code](https://github.com/AdaptVision/AdaptVision)

> AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Mo...

</details>

<details>
<summary><b>24. PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design</b> â­ 17</summary>

<br/>

**ğŸ‘¥ Authors:** Tianyu Lao, Ken Li, Jiazhe Wei, ChenyangSi, wanghaofan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.04082) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.04082) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.04082)

**ğŸ’» Code:** [â­ Code](https://github.com/JiazheWei/PosterCopilot)

> No abstract available.

</details>

<details>
<summary><b>25. Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Artur Janicki, j-hoscilowic

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2511.20494) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2511.20494) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2511.20494)

> We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these â€œAdversarial CAPTCHAsâ€ into web content pushes models into systemic decoding failures, from confident hallucinati...

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 25 |
| ğŸ“… Today | [`2025-12-05.json`](data/daily/2025-12-05.json) | 25 |
| ğŸ“† This Week | [`2025-W48.json`](data/weekly/2025-W48.json) | 73 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 73 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-05 | 25 | [View JSON](data/daily/2025-12-05.json) |
| ğŸ“„ 2025-12-04 | 24 | [View JSON](data/daily/2025-12-04.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W48 | 73 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 73 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/yourusername/daily-ai-papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/yourusername/daily-ai-papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/yourusername/daily-ai-papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/yourusername/daily-ai-papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/daily-ai-papers&type=Date)](https://star-history.com/#yourusername/daily-ai-papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/yourusername/daily-ai-papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/yourusername/daily-ai-papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ by the AI Community**

[â¬† Back to Top](#-daily-ai-papers)

</div>
