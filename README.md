# ü§ñ Daily AI Papers

Automatically updated list of trending AI research papers from HuggingFace.

**Last Updated:** 2025-12-04

## üìä Statistics

- **Today's Papers:** 24
- **This Week:** 24 papers
- **This Month:** 24 papers

## üìÅ Archives

- **Daily:** [`data/daily/2025-12-04.json`](data/daily/2025-12-04.json)
- **Weekly:** [`data/weekly/2025-W48.json`](data/weekly/2025-W48.json)
- **Monthly:** [`data/monthly/2025-12.json`](data/monthly/2025-12.json)
- **Latest:** [`data/latest.json`](data/latest.json)

---

## üìö Today's Papers (2025-12-04)

### 1. Qwen3-VL Technical Report

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2511.21631) | [arXiv](https://arxiv.org/abs/2511.21631) | [PDF](https://arxiv.org/pdf/2511.21631)

**Abstract:** Qwen3-VL Technical Report

---

### 2. Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach

**Authors:** Xiu Li, Ling Pan, Siyuan Yang, haoranhe, breezeyoung

**‚≠ê Stars:** 7

**Links:** [HuggingFace](https://huggingface.co/papers/2512.02834) | [arXiv](https://arxiv.org/abs/2512.02834) | [PDF](https://arxiv.org/pdf/2512.02834)

**GitHub:** [Repo](https://github.com/breez3young/TACO)

**Abstract:** üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle t...

---

### 3. PretrainZero: Reinforcement Active Pretraining

**Authors:** Guoqi Li, Jie Lou, debingzhang, Zhiyuan-Fan, xrxing

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03442) | [arXiv](https://arxiv.org/abs/2512.03442) | [PDF](https://arxiv.org/pdf/2512.03442)

**Abstract:** Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely he...

---

### 4. ViDiC: Video Difference Captioning

**Authors:** jiakaiW, heyween, wrz123, LongoXC, Leexeo

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03405) | [arXiv](https://arxiv.org/abs/2512.03405) | [PDF](https://arxiv.org/pdf/2512.03405)

**Abstract:** Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to des...

---

### 5. OneThinker: All-in-one Reasoning Model for Image and Video

**Authors:** Kaixuan Fan, Hongyu Li, Manyuan Zhang, Kaituo Feng, zhengli1013

**‚≠ê Stars:** 43

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03043) | [arXiv](https://arxiv.org/abs/2512.03043) | [PDF](https://arxiv.org/pdf/2512.03043)

**GitHub:** [Repo](https://github.com/tulerfeng/OneThinker)

**Abstract:** Project page: https://github.com/tulerfeng/OneThinker

---

### 6. SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL

**‚≠ê Stars:** 1

**Links:** [HuggingFace](https://huggingface.co/papers/2512.04069) | [arXiv](https://arxiv.org/abs/2512.04069) | [PDF](https://arxiv.org/pdf/2512.04069)

**GitHub:** [Repo](https://github.com/spacetools/SpaceTools)

**Abstract:** TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation...

---

### 7. Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation

**Authors:** Difan Liu, Yiran Xu, Mamshad Nayeem Rizve, Sangwoo Mo, Subin Kim

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03534) | [arXiv](https://arxiv.org/abs/2512.03534) | [PDF](https://arxiv.org/pdf/2512.03534)

**Abstract:** Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly addre...

---

### 8. RELIC: Interactive Video World Model with Long-Horizon Memory

**Authors:** Chongjian Ge, Yiqun Mei, kalyanks, saibi, YicongHong

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.04040) | [arXiv](https://arxiv.org/abs/2512.04040) | [PDF](https://arxiv.org/pdf/2512.04040)

**Abstract:** A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor ...

---

### 9. Thinking with Programming Vision: Towards a Unified View for Thinking with Images

**Authors:** Tao Jin, Kai Jia, Feng Zhang, Minjie Hong, Zirun Guo

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03746) | [arXiv](https://arxiv.org/abs/2512.03746) | [PDF](https://arxiv.org/pdf/2512.03746)

**Abstract:** No abstract available.

---

### 10. Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment

**‚≠ê Stars:** 6

**Links:** [HuggingFace](https://huggingface.co/papers/2511.22345) | [arXiv](https://arxiv.org/abs/2511.22345) | [PDF](https://arxiv.org/pdf/2511.22345)

**GitHub:** [Repo](https://github.com/MCG-NJU/FlowBack)

**Abstract:** No abstract available.

---

### 11. CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation

**Authors:** Yi Yao, Hongxia Xie, Bin Wen, Ruoxuan Zhang, twbear2024

**‚≠ê Stars:** 3

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03540) | [arXiv](https://arxiv.org/abs/2512.03540) | [PDF](https://arxiv.org/pdf/2512.03540)

**GitHub:** [Repo](https://github.com/zhangdaxia22/CookAnything)

**Abstract:** No abstract available.

---

### 12. AutoNeural: Co-Designing Vision-Language Models for NPU Inference

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.02924) | [arXiv](https://arxiv.org/abs/2512.02924) | [PDF](https://arxiv.org/pdf/2512.02924)

**Abstract:** AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful yo...

---

### 13. SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment

**Authors:** Yi Yang, yixuantt

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.02807) | [arXiv](https://arxiv.org/abs/2512.02807) | [PDF](https://arxiv.org/pdf/2512.02807)

**Abstract:** ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hi...

---

### 14. Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03073) | [arXiv](https://arxiv.org/abs/2512.03073) | [PDF](https://arxiv.org/pdf/2512.03073)

**Abstract:** We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transp...

---

### 15. Jina-VLM: Small Multilingual Vision Language Model

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.04032) | [arXiv](https://arxiv.org/abs/2512.04032) | [PDF](https://arxiv.org/pdf/2512.04032)

**Abstract:** our latest multilingual vlm model at 2b size, about to release soon

---

### 16. AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs

**Authors:** Tosho Hirasawa, Shohei Tanaka, Kuniaki Saito, yushiku, risashinoda

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2511.20515) | [arXiv](https://arxiv.org/abs/2511.20515) | [PDF](https://arxiv.org/pdf/2511.20515)

**Abstract:** Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provide...

---

### 17. SkillFactory: Self-Distillation For Learning Cognitive Behaviors

**Authors:** Manya Wadhwa, Jack Lu, gregdurrett, sedrickkeh, Zaynes

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.04072) | [arXiv](https://arxiv.org/abs/2512.04072) | [PDF](https://arxiv.org/pdf/2512.04072)

**Abstract:** SkillFactory: Self-Distillation For Learning Cognitive Behaviors

---

### 18. In-Context Representation Hijacking

**Authors:** yossig, MichaelKar, aimir, tux

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03771) | [arXiv](https://arxiv.org/abs/2512.03771) | [PDF](https://arxiv.org/pdf/2512.03771)

**Abstract:** We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful...

---

### 19. UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs

**‚≠ê Stars:** 4

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03383) | [arXiv](https://arxiv.org/abs/2512.03383) | [PDF](https://arxiv.org/pdf/2512.03383)

**GitHub:** [Repo](https://github.com/enyac-group/UniQL)

**Abstract:** üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups...

---

### 20. Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding

**‚≠ê Stars:** 1

**Links:** [HuggingFace](https://huggingface.co/papers/2512.04000) | [arXiv](https://arxiv.org/abs/2512.04000) | [PDF](https://arxiv.org/pdf/2512.04000)

**GitHub:** [Repo](https://github.com/Jialuo-Li/DIG)

**Abstract:** The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incu...

---

### 21. BlurDM: A Blur Diffusion Model for Image Deblurring

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03979) | [arXiv](https://arxiv.org/abs/2512.03979) | [PDF](https://arxiv.org/pdf/2512.03979)

**Abstract:** We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing...

---

### 22. AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition

**‚≠ê Stars:** 2

**Links:** [HuggingFace](https://huggingface.co/papers/2512.03794) | [arXiv](https://arxiv.org/abs/2512.03794) | [PDF](https://arxiv.org/pdf/2512.03794)

**GitHub:** [Repo](https://github.com/AdaptVision/AdaptVision)

**Abstract:** AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVisio...

---

### 23. PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design

**Authors:** Tianyu Lao, Ken Li, Jiazhe Wei, ChenyangSi, wanghaofan

**‚≠ê Stars:** 17

**Links:** [HuggingFace](https://huggingface.co/papers/2512.04082) | [arXiv](https://arxiv.org/abs/2512.04082) | [PDF](https://arxiv.org/pdf/2512.04082)

**GitHub:** [Repo](https://github.com/JiazheWei/PosterCopilot)

**Abstract:** No abstract available.

---

### 24. Adversarial Confusion Attack: Disrupting Multimodal Large Language Models

**Authors:** Artur Janicki, j-hoscilowic

**‚≠ê Stars:** 0

**Links:** [HuggingFace](https://huggingface.co/papers/2511.20494) | [arXiv](https://arxiv.org/abs/2511.20494) | [PDF](https://arxiv.org/pdf/2511.20494)

**Abstract:** We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt...

---


## üìÖ Historical Data

### Recent Days
- **2025-12-04**: [24 papers](data/daily/2025-12-04.json)

### Weekly Archives
- **2025-W48**: [24 papers](data/weekly/2025-W48.json)

### Monthly Archives
- **2025-12**: [24 papers](data/monthly/2025-12.json)


---

## üîÑ Update Schedule

This repository automatically updates daily at 00:00 UTC with the latest AI papers from HuggingFace.

## üõ†Ô∏è How It Works

This repository uses:
- **Crawl4AI** for web scraping
- **GitHub Actions** for daily automation
- **Python** for data processing
- **Organized archives** by day, week, and month

## üìä Data Structure
```
data/
‚îú‚îÄ‚îÄ daily/          # Individual day snapshots
‚îÇ   ‚îú‚îÄ‚îÄ 2024-12-04.json
‚îÇ   ‚îú‚îÄ‚îÄ 2024-12-05.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ weekly/         # Cumulative weekly papers
‚îÇ   ‚îú‚îÄ‚îÄ 2024-W48.json
‚îÇ   ‚îú‚îÄ‚îÄ 2024-W49.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ monthly/        # Cumulative monthly papers
‚îÇ   ‚îú‚îÄ‚îÄ 2024-12.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ latest.json     # Always the most recent scrape
```

## üìù License

MIT License - feel free to use this data for your own projects!

---

*Generated by [Daily AI Papers Bot](https://github.com/yourusername/daily-ai-papers)*
