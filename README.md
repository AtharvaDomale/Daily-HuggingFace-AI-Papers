<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-57-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-2085+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">57</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">164</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">566</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">2085+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** February 12, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.05400) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.05400) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.05400)

> In this paper, we argue that LLM pre-training is entering a â€œdata-wallâ€ regime where readily available high-quality public text is approaching exhaustion, so progress must shift from more tokens to better tokens chosen at the right time. While mos...

</details>

<details>
<summary><b>2. Code2World: A GUI World Model via Renderable Code Generation</b> â­ 131</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09856) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09856) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09856)

**ğŸ’» Code:** [â­ Code](https://github.com/AMAP-ML/Code2World)

> Project Page: https://amap-ml.github.io/Code2World/ Github: https://github.com/AMAP-ML/Code2World

</details>

<details>
<summary><b>3. UI-Venus-1.5 Technical Report</b> â­ 708</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09082) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09082) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09082)

**ğŸ’» Code:** [â­ Code](https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5) â€¢ [â­ Code](https://github.com/inclusionAI/UI-Venus)

> Is your GUI Agent ready for real work? ğŸ”¥ Weâ€™ve seen many great previous GUI Agents, but making a "stable assistant" for phones and websites is still hard. There are three main problems: 1ï¸âƒ£ Knowledge Gap: AI often misses less common icons and does...

</details>

<details>
<summary><b>4. Chain of Mindset: Reasoning with Adaptive Cognitive Modes</b> â­ 18</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10063) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10063) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10063)

**ğŸ’» Code:** [â­ Code](https://github.com/QuantaAlpha/chain-of-mindset)

> CoM is a training-free agentic framework that dynamically orchestrates four step-level mindsets (Spatial, Convergent, Divergent, Algorithmic) via a Meta-Agent and a Context Gate, avoiding one-size-fits-all reasoning and improving accuracy and effi...

</details>

<details>
<summary><b>5. SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</b> â­ 140</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08234) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08234) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08234)

**ğŸ’» Code:** [â­ Code](https://github.com/aiming-lab/SkillRL)

> Skill accumulation is the new paradigm for AI agents. Weâ€™re moving from static models to recursive evolution ğŸ§¬. SkillRL proves skills > scale, enabling a 7B model to beat GPT-4o ğŸš€. Evolving > Scaling. ğŸ’¡ Paper: https://arxiv.org/abs/2602.08234 Code...

</details>

<details>
<summary><b>6. P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</b> â­ 13</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09443) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09443) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09443)

**ğŸ’» Code:** [â­ Code](https://github.com/PRIME-RL/P1-VL)

> Project: https://prime-rl.github.io/P1-VL GitHub: https://github.com/PRIME-RL/P1-VL

</details>

<details>
<summary><b>7. Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</b> â­ 44</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10090) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10090) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10090)

**ğŸ’» Code:** [â­ Code](https://github.com/Snowflake-Labs/agent-world-model)

> Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning ğŸš€ Introducing Agent World Model (AWM) â€” we synthesized 1,000 code-driven environments with 35K tools and 10K tasks for large-scale agentic reinforcement learning...

</details>

<details>
<summary><b>8. Prism: Spectral-Aware Block-Sparse Attention</b> â­ 19</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08426) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08426) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08426)

**ğŸ’» Code:** [â­ Code](https://github.com/xinghaow99/prism)

> TL;DR Prism is a training-free method to accelerate long-context LLM pre-filling. It addresses the "blind spot" in standard mean pooling caused by Rotary Positional Embeddings (RoPE) by disentangling attention into high-frequency and low-frequency...

</details>

<details>
<summary><b>9. DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents</b> â­ 10</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07035) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07035) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07035)

**ğŸ’» Code:** [â­ Code](https://github.com/bubble65/DLLM-Searcher)

> ğŸ§ ğŸ” DLLM-Searcher: Adapting Diffusion Large Language Models for Search Agents Diffusion Large Language Models (dLLMs) offer flexible generation but struggle as search agents due to latency and weak tool-use capabilities.  This paper introduces DLLM...

</details>

<details>
<summary><b>10. Olaf-World: Orienting Latent Actions for Video World Modeling</b> â­ 33</summary>

<br/>

**ğŸ‘¥ Authors:** Mike Zheng Shou, Ivor W. Tsang, Yuchao Gu, YuxinJ

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10104) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10104) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10104)

**ğŸ’» Code:** [â­ Code](https://github.com/showlab/Olaf-World)

> No abstract available.

</details>

<details>
<summary><b>11. Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling</b> â­ 24</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09084) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09084) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09084)

**ğŸ’» Code:** [â­ Code](https://github.com/taco-group/agent-banana)

> Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling

</details>

<details>
<summary><b>12. Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07022) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07022) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07022)

> This study presents a theoretical analysis of autoregressive image generation with diffusion loss, demonstrating that patch denoising optimization effectively mitigates condition errors and leads to a stable condition distribution. To further addr...

</details>

<details>
<summary><b>13. TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation</b> â­ 10</summary>

<br/>

**ğŸ‘¥ Authors:** Lior Wolf, Amit Edenzon, Eitan Shaar, shaulov

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.00268) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.00268) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.00268)

**ğŸ’» Code:** [â­ Code](https://github.com/arielshaulov/TokenTrim)

> Project page: https://arielshaulov.github.io/TokenTrim/ Open source code ğŸ¥³: https://github.com/arielshaulov/TokenTrim

</details>

<details>
<summary><b>14. SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04208) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04208) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04208)

> We tackle test-time robustness of VLA models without additional training or multiple forward passes, by proposing SCALE: jointly modulate visual attention and action decoding based on self-uncertainty.

</details>

<details>
<summary><b>15. LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.00462) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.00462) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.00462)

**ğŸ’» Code:** [â­ Code](https://github.com/McGill-NLP/latentlens)

> In this paper we propose a new interpretability method LatentLens. With this we can finally show that visual tokens are actually interpretable across all layers in an LLM, something that past methods like logit lens and or using the LLM's embeddin...

</details>

<details>
<summary><b>16. BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Xiaoyu Chen, Yanjiang Guo, Yuanfei Luo, Jianke Zhang, Yucheng Hu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09849) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09849) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09849)

> BagelVLA is a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework for long-horizon manipulation tasks. ğŸ§  Model Architecture BagelVLA utilizes a Mixture-of-Transformers (MoT) archit...

</details>

<details>
<summary><b>17. VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model</b> â­ 12</summary>

<br/>

**ğŸ‘¥ Authors:** Zezhi Liu, Shaojie Ren, Zekun Qi, Wenyao Zhang, Jingwen Sun

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10098) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10098) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10098)

**ğŸ’» Code:** [â­ Code](https://github.com/ginwind/VLA-JEPA)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Ac...

</details>

<details>
<summary><b>18. ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.06820) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.06820) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.06820)

> We introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on u...

</details>

<details>
<summary><b>19. Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09439) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09439) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09439)

> Dataset: https://huggingface.co/datasets/ma-xu/fine-t2i Space: https://huggingface.co/spaces/ma-xu/fine-t2i-explore

</details>

<details>
<summary><b>20. Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models</b> â­ 9</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09017) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09017) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09017)

**ğŸ’» Code:** [â­ Code](https://github.com/jeffacce/cap-policy)

> The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical...

</details>

<details>
<summary><b>21. Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems</b> â­ 53</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08847) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08847) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08847)

**ğŸ’» Code:** [â­ Code](https://github.com/langfengQ/DrMAS)

> Dr. MAS is designed for stable end-to-end RL post-training ğŸ”¥ of multi-agent LLM systems. It enables agents to collaborate on complex reasoning tasks with: âœ¨ Flexible agent registry & multi-agent orchestration âœ¨ Heterogeneous LLMs (shared/non-share...

</details>

<details>
<summary><b>22. Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments</b> â­ 7</summary>

<br/>

**ğŸ‘¥ Authors:** Yang Wang, Wei Zhang, Yuyang Song, Yizhi Li, Siwei Wu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.01244) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.01244) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.01244)

**ğŸ’» Code:** [â­ Code](https://github.com/multimodal-art-projection/TerminalTraj)

> This is a repo for paper "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments"

</details>

<details>
<summary><b>23. VideoWorld 2: Learning Transferable Knowledge from Real-world Videos</b> â­ 685</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10102) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10102) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10102)

**ğŸ’» Code:** [â­ Code](https://github.com/ByteDance-Seed/VideoWorld/tree/main/VideoWorld2)

> ğŸ¤–Text is not enough, Visual is the key to AGIï¼Can Al learn transferable knowledge for complex tasks directly from videos? Just like a child learns to fold a paper airplane or build a LEGO from video tutorialsğŸ‘¶ ğŸ˜Thrilled to introduce VideoWorld 2, ...

</details>

<details>
<summary><b>24. Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07276) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07276) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07276)

> Activation steering has emerged as a promising method for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering approaches identify and steer the model from a single static direction for each ta...

</details>

<details>
<summary><b>25. Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08382) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08382) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08382)

> Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning. We introduce LycheeMemory, a cognitively inspired framework that enables efficient long-context inference via chunk-wise compression and selective memory ...

</details>

<details>
<summary><b>26. Rethinking Global Text Conditioning in Diffusion Transformers</b> â­ 13</summary>

<br/>

**ğŸ‘¥ Authors:** Yuchen Liu, Ilya Drobyshevskiy, Zongze Wu, Daniil Pakhomov, Nikita Starodubcev

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09268) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09268) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09268)

**ğŸ’» Code:** [â­ Code](https://github.com/quickjkee/modulation-guidance)

> GitHub: https://github.com/quickjkee/modulation-guidance

</details>

<details>
<summary><b>27. iGRPO: Self-Feedback-Driven LLM Reasoning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09000) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09000) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09000)

> Let's discuss Self-Feedback for RL Reasoning (iGRPO) Motivation. Current RL methods for reasoning (GRPO, DAPO, etc.) treat each generation as a one-shot attempt. The model samples, gets a reward, updates, and moves on. But humans almost never solv...

</details>

<details>
<summary><b>28. Covo-Audio Technical Report</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09823) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09823) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09823)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Fun-Audio-Chat Technical Report (2025) FlashLabs Chroma 1.0: A Real-Time En...

</details>

<details>
<summary><b>29. Effective Reasoning Chains Reduce Intrinsic Dimensionality</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09276) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09276) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09276)

> No abstract available.

</details>

<details>
<summary><b>30. TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Liming Zheng, Lei Chen, Xuanle Zhao, Jing Huang, Deyang Jiang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09662) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09662) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09662)

**ğŸ’» Code:** [â­ Code](https://github.com/UITron-hub/TreeCUA)

> TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution

</details>

<details>
<summary><b>31. ANCHOR: Branch-Point Data Generation for GUI Agents</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07153) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07153) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07153)

> End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-d...

</details>

<details>
<summary><b>32. SAGE: Scalable Agentic 3D Scene Generation for Embodied AI</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10116) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10116) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10116)

> No abstract available.

</details>

<details>
<summary><b>33. Autoregressive Image Generation with Masked Bit Modeling</b> â­ 23</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09024) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09024) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09024)

**ğŸ’» Code:** [â­ Code](https://github.com/amazon-far/BAR)

> SOTA discrete visual generation defeats diffusion models with 0.99 FID score, project page is available at https://bar-gen.github.io/

</details>

<details>
<summary><b>34. OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jianfei Zhang, Xiangyu Xi, Jianing Wang, Qi Guo, DeyangKong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08344) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08344) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08344)

> Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational re...

</details>

<details>
<summary><b>35. TodoEvolve: Learning to Architect Agent Planning Systems</b> â­ 6</summary>

<br/>

**ğŸ‘¥ Authors:** Heng Chang, Zihan Zhang, Guibin Zhang, Yanzuo Jiang, Jiaxi Liu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07839) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07839) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07839)

**ğŸ’» Code:** [â­ Code](https://github.com/EcthelionLiu/TodoEvolve)

> Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the stru...

</details>

<details>
<summary><b>36. Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model</b> â­ 1</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07422) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07422) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07422)

**ğŸ’» Code:** [â­ Code](https://github.com/AndrewWTY/SecCoderX)

> Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionalityâ€“sec...

</details>

<details>
<summary><b>37. Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.06161) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.06161) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.06161)

> We found a silly failure mode in Parallel Revocable Diffusion Decoding: flip-flop . A token gets ReMaskâ€™edâ€¦ then comes back unchanged. In the existing approach, <1% of ReMasks actually change the token (â‰ˆ99% wasted). We propose COVER which verifie...

</details>

<details>
<summary><b>38. Stable Velocity: A Variance Perspective on Flow Matching</b> â­ 14</summary>

<br/>

**ğŸ‘¥ Authors:** Xin Tao, Liang Hou, Xin Yu, Yongxing Zhang, Donglin Yang

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.05435) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.05435) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.05435)

**ğŸ’» Code:** [â­ Code](https://github.com/linYDTHU/StableVelocity)

> While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-vari...

</details>

<details>
<summary><b>39. From Directions to Regions: Decomposing Activations in Language Models via Local Geometry</b> â­ 12</summary>

<br/>

**ğŸ‘¥ Authors:** Atticus Geiger, Shauli Ravfogel, Omri Fahn, Shaked Ronen, Or Shafran

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.02464) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.02464) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.02464)

**ğŸ’» Code:** [â­ Code](https://github.com/ordavid-s/decomposing-activations-local-geometry)

> Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability...

</details>

<details>
<summary><b>40. On the Optimal Reasoning Length for RL-Trained Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Rio Yokota, Taishi-N324, neodymium6

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09591) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09591) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09591)

> RL-trained reasoning models often produce longer CoT, increasing test-time cost. We compare several length-control methods on Qwen3-1.7B-Base and DeepSeek-R1-Distill-Qwen-1.5B, and characterize when length penalties hurt reasoning acquisition vs w...

</details>

<details>
<summary><b>41. Learning Self-Correction in Vision-Language Models via Rollout Augmentation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ruqi Zhang, Bolian Li, Ziliang Qiu, Yi Ding

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08503) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08503) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08503)

> Learning self-correction in Vision-language models via rollout augmentation

</details>

<details>
<summary><b>42. Learning to Continually Learn via Meta-learning Agentic Memory Designs</b> â­ 39</summary>

<br/>

**ğŸ‘¥ Authors:** Jeff Clune, Shengran Hu, Yiming Xiong

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07755) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07755) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07755)

**ğŸ’» Code:** [â­ Code](https://github.com/zksha/alma)

> Can AI agents design better memory mechanisms for themselves? Introducing Learning to Continually Learn via Meta-learning Memory Designs. A meta agent automatically designs memory mechanisms, including what info to store, how to retrieve it, and h...

</details>

<details>
<summary><b>43. ContextBench: A Benchmark for Context Retrieval in Coding Agents</b> â­ 4</summary>

<br/>

**ğŸ‘¥ Authors:** Jiaming Wang, Rili Feng, Bohan Zhang, Letian Zhu, Han Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.05892) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.05892) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.05892)

**ğŸ’» Code:** [â­ Code](https://github.com/EuniAI/ContextBench)

> Most repo-level benchmarks measure Pass@k âœ… But fixing a bug does not mean the agent understood the code ğŸ‘€ We built ContextBench ğŸ‰ A benchmark to measure whether coding agents actually retrieve and use the right context ğŸ”ğŸ“‚ ğŸ“Š Whatâ€™s inside ğŸ§© 1,136 ...

</details>

<details>
<summary><b>44. Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.05085) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.05085) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.05085)

> We introduce Locas, a parametric memory for parameter-efficient Test-Time Training (TTT) and continual learning. Unlike previous methods that only introduce in-place low-rank model updates (such as LoRA) that do not provide expanded capacity or re...

</details>

<details>
<summary><b>45. Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders</b> â­ 3</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.10099) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.10099) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.10099)

**ğŸ’» Code:** [â­ Code](https://github.com/amandpkr/RJF)

> Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a ca...

</details>

<details>
<summary><b>46. LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Chris Russell, William Bankes, Thomas Foster, William Lugoloobi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09924) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09924) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09924)

**ğŸ’» Code:** [â­ Code](https://github.com/KabakaWilliam/llms_know_difficulty)

> We show that LLMs maintain a linearly accessible internal representation of difficulty that differs from human assessments and varies across decoding settings. We apply this to route queries between models with different reasoning capabilities. Gi...

</details>

<details>
<summary><b>47. Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering</b> â­ 21</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08519) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08519) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08519)

**ğŸ’» Code:** [â­ Code](https://github.com/Cloudy1225/PyAGC)

> PyAGC is a production-ready, modular library and comprehensive benchmark for Attributed Graph Clustering (AGC), built on PyTorch and PyTorch Geometric. It unifies 20+ state-of-the-art algorithms under a principled Encode-Cluster-Optimize (ECO) fra...

</details>

<details>
<summary><b>48. MIND: Benchmarking Memory Consistency and Action Control in World Models</b> â­ 22</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.08025) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.08025) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.08025)

**ğŸ’» Code:** [â­ Code](https://github.com/CSU-JPG/MIND)

> TL;DR: The first open-domain closed-loop revisited benchmark for evaluating memory consistency and action control in world models

</details>

<details>
<summary><b>49. CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07918) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07918) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07918)

> I'm excited to share our latest work to defend Prompt Injection: "CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution". CausalArmor, a selective defense: ğŸ§  Causal attribution at privileged actions: measure whether th...

</details>

<details>
<summary><b>50. Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jarrodbarnes

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07670) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07670) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07670)

**ğŸ’» Code:** [â­ Code](https://github.com/jbarnes850/test-time-training)

> Standard practice selects the most confident model output. I tested the opposite on GPU kernel optimization and found that selecting by surprisal (the model's least confident correct solution) achieves 80% success vs 50% for confidence-guided, wit...

</details>

<details>
<summary><b>51. AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management</b> â­ 2</summary>

<br/>

**ğŸ‘¥ Authors:** Ning Zhang, Chaowei Xiao, Hao Li, Ruoyao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.07398) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.07398) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.07398)

**ğŸ’» Code:** [â­ Code](https://github.com/ruoyaow/agentsys-memory)

> AgentSys defends against indirect prompt injection through explicit hierarchical memory management, reducing attack surface and preserving agent decision-making by preventing malicious instructions from persisting in the context window.

</details>

<details>
<summary><b>52. VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?</b> â­ 11</summary>

<br/>

**ğŸ‘¥ Authors:** Yujie Cheng, Xinzhe Han, Yuhao Wang, Juntong Feng, liuqa

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04802) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04802) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04802)

**ğŸ’» Code:** [â­ Code](https://github.com/QingAnLiu/VISTA-Bench)

> Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently a...

</details>

<details>
<summary><b>53. C-Î”Î˜: Circuit-Restricted Weight Arithmetic for Selective Refusal</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04521) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04521) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04521)

> C-Î”Î˜ (Circuit-Restricted Weight Arithmetic) shifts selective refusal from inference-time steering to an offline, checkpoint-level edit. It first identifies the refusal-causal circuit via EAP-IG, then applies a circuit-restricted weight update that...

</details>

<details>
<summary><b>54. Temporal Pair Consistency for Variance-Reduced Flow Matching</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Jindong Wang, Chikap421

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.04908) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.04908) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.04908)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Stable Velocity: A Variance Perspective on Flow Matching (2026) Rethinking ...

</details>

<details>
<summary><b>55. SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</b> â­ 5</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.01725) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.01725) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.01725)

**ğŸ’» Code:** [â­ Code](https://github.com/YurunChen/SafePred)

> With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constrain...

</details>

<details>
<summary><b>56. SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Lisa Erickson, Tushar Bandopadhyay, alokabhishek

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.21235) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.21235) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.21235)

> Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar score...

</details>

<details>
<summary><b>57. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes</b> â­ 46</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2602.09153) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2602.09153) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2602.09153)

**ğŸ’» Code:** [â­ Code](https://github.com/nepfaff/scenesmith)

> Meet SceneSmith: An agentic system that generates entire simulation-ready environments from a single text prompt. VLM agents collaborate to build scenes with dozens of objects per room, articulated furniture, and full physics properties.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 57 |
| ğŸ“… Today | [`2026-02-12.json`](data/daily/2026-02-12.json) | 57 |
| ğŸ“† This Week | [`2026-W06.json`](data/weekly/2026-W06.json) | 164 |
| ğŸ—“ï¸ This Month | [`2026-02.json`](data/monthly/2026-02.json) | 566 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-02-12 | 57 | [View JSON](data/daily/2026-02-12.json) |
| ğŸ“„ 2026-02-11 | 58 | [View JSON](data/daily/2026-02-11.json) |
| ğŸ“„ 2026-02-10 | 2 | [View JSON](data/daily/2026-02-10.json) |
| ğŸ“„ 2026-02-09 | 47 | [View JSON](data/daily/2026-02-09.json) |
| ğŸ“„ 2026-02-08 | 47 | [View JSON](data/daily/2026-02-08.json) |
| ğŸ“„ 2026-02-07 | 47 | [View JSON](data/daily/2026-02-07.json) |
| ğŸ“„ 2026-02-06 | 52 | [View JSON](data/daily/2026-02-06.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W06 | 164 | [View JSON](data/weekly/2026-W06.json) |
| ğŸ“… 2026-W05 | 357 | [View JSON](data/weekly/2026-W05.json) |
| ğŸ“… 2026-W04 | 214 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-02 | 566 | [View JSON](data/monthly/2026-02.json) |
| ğŸ—“ï¸ 2026-01 | 781 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
