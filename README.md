<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-23-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-637+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">23</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">83</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">686</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">637+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** December 24, 2025

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16676) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16676) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16676)

**ğŸ’» Code:** [â­ Code](https://github.com/OpenDCAI/DataFlow)

> code link: https://github.com/OpenDCAI/DataFlow

</details>

<details>
<summary><b>2. The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</b> â­ 56</summary>

<br/>

**ğŸ‘¥ Authors:** Ziwei Liu, Dahua Lin, Quan Wang, Haiwen Diao, Weichen Fan

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19693) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19693) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19693)

**ğŸ’» Code:** [â­ Code](https://github.com/WeichenFan/UAE)

> Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explor...

</details>

<details>
<summary><b>3. Region-Constraint In-Context Generation for Instructional Video Editing</b> â­ 32</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17650) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17650) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17650)

**ğŸ’» Code:** [â­ Code](https://github.com/HiDream-ai/ReCo)

> Region-Constraint In-Context Generation for Instructional Video Editing Paper: https://arxiv.org/abs/2512.17650 Project Page: https://zhw-zhang.github.io/ReCo-page/ Github: https://github.com/HiDream-ai/ReCo ReCo-Data: https://huggingface.co/datas...

</details>

<details>
<summary><b>4. Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</b> â­ 21</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17040) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17040) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17040)

**ğŸ’» Code:** [â­ Code](https://github.com/emjay73/InfCam)

> No abstract available.

</details>

<details>
<summary><b>5. QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</b> â­ 8</summary>

<br/>

**ğŸ‘¥ Authors:** Lu Cheng, Tongtong Wu, Kailin Zhang, Dehai Min

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19134) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19134) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19134)

**ğŸ’» Code:** [â­ Code](https://github.com/ZhishanQ/QuCo-RAG)

> A new framework for dynamic retrieval-augmented generation.

</details>

<details>
<summary><b>6. Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Hong Jiao, Jian Chen, Yunze Xiao, Han Chen, Ming Li

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.18880) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.18880) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.18880)

**ğŸ’» Code:** [â­ Code](https://github.com/MingLiiii/Difficulty_Alignment)

> Key Findings of our Human-LLM difficulty alignment study: Systematic Misalignment : Contrary to standard capability metrics, scaling does not reliably translate into alignment. Increasing model scale does not improve difficulty predictions; instea...

</details>

<details>
<summary><b>7. WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</b> â­ 31</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19678) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19678) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19678)

**ğŸ’» Code:** [â­ Code](https://github.com/HyoKong/WorldWarp)

> Long-range camera-conditioned scene generation from a single image. Project page and code: https://hyokong.github.io/worldwarp-page/ .

</details>

<details>
<summary><b>8. LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yuan Shen, Tai Wang, Yuqiang Yang, Wenzhe Cai, Jiaqi Peng

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19629) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19629) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19629)

> LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry

</details>

<details>
<summary><b>9. UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yuqing Ma, Lin Jing, Wei Zhang, Jian Yang, Jiajun Wu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17385) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17385) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17385)

> This paper introduces UCoder, an unsupervised framework for training code-generating large language models without requiring any external datasets, including unlabeled code snippets. The approach, called IPC (Internal Probing of LLMs for Code gene...

</details>

<details>
<summary><b>10. GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</b> â­ 8</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19682) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19682) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19682)

**ğŸ’» Code:** [â­ Code](https://github.com/Gen-Verse/GenEnv)

> Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutiona...

</details>

<details>
<summary><b>11. StoryMem: Multi-shot Long Video Storytelling with Memory</b> â­ 7</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19539) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19539) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19539)

**ğŸ’» Code:** [â­ Code](https://github.com/Kevin-thu/StoryMem)

> Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis condi...

</details>

<details>
<summary><b>12. LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.16229) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.16229) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.16229)

**ğŸ’» Code:** [â­ Code](https://github.com/zhijie-group/LoPA)

> ğŸ”—Paperï¼š https://arxiv.org/abs/2512.16229 ğŸ”—GitHubï¼š https://github.com/zhijie-group/LoPA ğŸ”—blog: https://zhijie-group.github.io/blogs/lopa

</details>

<details>
<summary><b>13. Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.17206) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.17206) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.17206)

> Reasoning Palette addresses the challenge of controlling LLM generation style and enabling effective exploration in RL by introducing a stochastic latent variable that encodes diverse reasoning strategies. This latent, inferred via a VAE from ques...

</details>

<details>
<summary><b>14. MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19432) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19432) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19432)

> Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and m...

</details>

<details>
<summary><b>15. Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.18658) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.18658) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.18658)

> Most LLMs today are powerful at language but weak at worlds: they generate fluent outputs without maintaining a consistent, verifiable model of reality. As a result, many AI applications plateau at demos or copilots and fail in complex, high-stake...

</details>

<details>
<summary><b>16. Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Liliang Chen, Shengcong Chen, Di Chen, Hongwei Fan, Yujie Zhao

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19402) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19402) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19402)

> Paper: https://arxiv.org/abs/2512.19402 Project Page: https://real2edit2real.github.io/

</details>

<details>
<summary><b>17. CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</b> â­ 6</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19535) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19535) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19535)

**ğŸ’» Code:** [â­ Code](https://github.com/kyutai-labs/casa)

> Code: https://github.com/kyutai-labs/casa

</details>

<details>
<summary><b>18. Name That Part: 3D Part Segmentation and Naming</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Alan Yuille, Anand Bhattad, Ankit Vaidya, Prakhar Kaushik, Soumava Paul

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.18003) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.18003) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.18003)

> We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unla...

</details>

<details>
<summary><b>19. MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</b> â­ 15</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.18314) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.18314) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.18314)

**ğŸ’» Code:** [â­ Code](https://github.com/cgtuebingen/MatSpray)

> ğŸŒ https://matspray.jdihlmann.com/ ğŸ“ƒ https://arxiv.org/abs/2512.18314 ğŸ’¾ https://github.com/cgtuebingen/MatSpray

</details>

<details>
<summary><b>20. Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Sujata Ghosh, Saptarshi Sahoo, Aheli Poddar

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.12620) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.12620) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.12620)

**ğŸ’» Code:** [â­ Code](https://github.com/XAheli/Logic-in-LLMs)

> arXiv lens breakdown of this paper ğŸ‘‰ https://arxivlens.com/PaperView/Details/understanding-syllogistic-reasoning-in-llms-from-formal-and-natural-language-perspectives-822-84433a31 Executive Summary Detailed Breakdown Practical Applications

</details>

<details>
<summary><b>21. Over++: Generative Video Compositing for Layer Interaction Effects</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Roni Sengupta, Cary Phillips, Jun Myeong Choi, Jiaye Wu, Luchao Qi

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19661) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19661) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19661)

> No abstract available.

</details>

<details>
<summary><b>22. Brain-Grounded Axes for Reading and Steering LLM States</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Sandro Andric

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.19399) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.19399) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.19399)

**ğŸ’» Code:** [â­ Code](https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States)

> These research supports a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.

</details>

<details>
<summary><b>23. SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Scott Thornton

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2512.18542) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2512.18542) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2512.18542)

**ğŸ’» Code:** [â­ Code](https://github.com/scthornton/securecode-v2)

> No abstract available.

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 23 |
| ğŸ“… Today | [`2025-12-24.json`](data/daily/2025-12-24.json) | 23 |
| ğŸ“† This Week | [`2025-W51.json`](data/weekly/2025-W51.json) | 83 |
| ğŸ—“ï¸ This Month | [`2025-12.json`](data/monthly/2025-12.json) | 686 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2025-12-24 | 23 | [View JSON](data/daily/2025-12-24.json) |
| ğŸ“„ 2025-12-23 | 22 | [View JSON](data/daily/2025-12-23.json) |
| ğŸ“„ 2025-12-22 | 38 | [View JSON](data/daily/2025-12-22.json) |
| ğŸ“„ 2025-12-21 | 38 | [View JSON](data/daily/2025-12-21.json) |
| ğŸ“„ 2025-12-20 | 37 | [View JSON](data/daily/2025-12-20.json) |
| ğŸ“„ 2025-12-19 | 30 | [View JSON](data/daily/2025-12-19.json) |
| ğŸ“„ 2025-12-18 | 38 | [View JSON](data/daily/2025-12-18.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2025-W51 | 83 | [View JSON](data/weekly/2025-W51.json) |
| ğŸ“… 2025-W50 | 230 | [View JSON](data/weekly/2025-W50.json) |
| ğŸ“… 2025-W49 | 186 | [View JSON](data/weekly/2025-W49.json) |
| ğŸ“… 2025-W48 | 187 | [View JSON](data/weekly/2025-W48.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2025-12 | 686 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
