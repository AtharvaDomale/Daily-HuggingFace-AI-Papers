<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-27-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-1377+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {len(df)}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {len(trending)}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${papers.length} papers today!`);
  papers.forEach(paper => {
    console.log(`\nğŸ“„ ${paper.title}`);
    console.log(`â­ ${paper.stars} stars`);
    console.log(`ğŸ”— ${paper.details.arxiv_page_url}`);
  });
}

getTodaysPapers();
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">27</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">27</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">639</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">1377+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** January 26, 2026

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

<details>
<summary><b>1. EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience</b> â­ 154</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15876) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15876) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15876)

**ğŸ’» Code:** [â­ Code](https://github.com/meituan/EvoCUA)

> EvoCUA: Evolving Computer Use Agent ğŸ¥‡ #1 Open-Source Model on OSWorld | A General-Purpose Multimodal Model Excelling at Computer Use ğŸ”— Paper: https://arxiv.org/abs/2601.14724 ğŸ’» Code: https://github.com/meituan/EvoCUA ğŸŒŸ Highlights ğŸ¥‡ #1 Open-Source ...

</details>

<details>
<summary><b>2. HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</b> â­ 38</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14724) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14724) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14724)

**ğŸ’» Code:** [â­ Code](https://github.com/haowei-freesky/HERMES)

> ğŸš€ Introducing HERMES: The Future of Real-Time Streaming Video Understanding! While today's Multimodal Large Language Models (MLLMs) perform impressively at offline video comprehension, they often face a "painful trade-off" when it comes to real-ti...

</details>

<details>
<summary><b>3. LLM-in-Sandbox Elicits General Agentic Intelligence</b> â­ 81</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16206) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16206) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16206)

**ğŸ’» Code:** [â­ Code](https://github.com/llm-in-sandbox/llm-in-sandbox)

> Introducing LLM-in-Sandbox â€” put your LLM in a virtual computer to unlock general agentic intelligence for non-code tasks! Significant gains for chemistry, long-context QA, instruction following, and more. No extra training needed. ğŸŒ Demo: https:/...

</details>

<details>
<summary><b>4. The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</b> â­ 71</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15165) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15165) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15165)

**ğŸ’» Code:** [â­ Code](https://github.com/LeapLabTHU/JustGRPO)

> Links ğŸ“„ paper: https://arxiv.org/abs/2601.15165 ğŸ  project page: https://nzl-thu.github.io/the-flexibility-trap ğŸ’» code: https://github.com/LeapLabTHU/JustGRPO ğŸ¤— model: https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO

</details>

<details>
<summary><b>5. BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</b> â­ 15</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15197) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15197) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15197)

**ğŸ’» Code:** [â­ Code](https://github.com/ZGC-EmbodyAI/BayesianVLA)

> ğŸ—ï¸ Architecture BayesianVLA is a novel framework designed to solve the Vision Shortcut problem in Vision-Language-Action (VLA) models. In current VLA training, goal-driven datasets often make language instructions highly predictable from visual ob...

</details>

<details>
<summary><b>6. Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</b> â­ 129</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16208) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16208) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16208)

**ğŸ’» Code:** [â­ Code](https://github.com/ZitengWangNYU/Scale-RAE)

> We scale RAE to text-to-image, and its advantage over VAEs still holds!

</details>

<details>
<summary><b>7. Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</b> â­ 28</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15892) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15892) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15892)

**ğŸ’» Code:** [â­ Code](https://github.com/ByteDance-Seed/Stable-DiffCoder)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion L...

</details>

<details>
<summary><b>8. SAMTok: Representing Any Mask with Two Words</b> â­ 1.51k</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16093) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16093) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16093)

**ğŸ’» Code:** [â­ Code](https://github.com/bytedance/Sa2VA/tree/main/projects/samtok)

> Project page: https://zhouyiks.github.io/projects/SAMTok/ Training Code: https://github.com/bytedance/Sa2VA/tree/main/projects/samtok Short Bio:   We present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens a...

</details>

<details>
<summary><b>9. Learning to Discover at Test Time</b> â­ 163</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16175) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16175) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16175)

**ğŸ’» Code:** [â­ Code](https://github.com/test-time-training/discover)

> New paper on scientific discovery with test time training. New discoveries on several open scientific problems.

</details>

<details>
<summary><b>10. Qwen3-TTS Technical Report</b> â­ 4.27k</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15621) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15621) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15621)

**ğŸ’» Code:** [â­ Code](https://github.com/QwenLM/Qwen3-TTS)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IndexTTS 2.5 Technical Report (2026) FlashLabs Chroma 1.0: A Real-Time End-...

</details>

<details>
<summary><b>11. Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</b> â­ 1.41k</summary>

<br/>

**ğŸ‘¥ Authors:** Boxuan Li, Nicholas Carlini, Alexander G. Shaw, Mike A. Merrill, menorf

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.11868) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.11868) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.11868)

**ğŸ’» Code:** [â­ Code](https://github.com/laude-institute/terminal-bench)

> This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Re...

</details>

<details>
<summary><b>12. Towards Automated Kernel Generation in the Era of LLMs</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Yixin Shen, Haiming Wu, Chi Hsu Tsai, Peiyu Zang, Yang Yu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15727) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15727) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15727)

**ğŸ’» Code:** [â­ Code](https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation)

> Summary of Key Points Kernel quality is a fundamental bottleneck for modern AI system performance, yet high-quality kernel engineering is expert-intensive, time-consuming, and difficult to scale. Recent advances in large language models (LLMs) and...

</details>

<details>
<summary><b>13. OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15369) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15369) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15369)

> Project Page: https://ucsc-vlaa.github.io/OpenVision3/

</details>

<details>
<summary><b>14. Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</b> â­ 1</summary>

<br/>

**ğŸ‘¥ Authors:** Dingkun Long, Zhuoning Guo, Mingxin Li, Yanzhao Zhang, songtingyu

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16125) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16125) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16125)

**ğŸ’» Code:** [â­ Code](https://github.com/SighingSnow/edir)

> A new benchmark for Composed Image Retrieval.

</details>

<details>
<summary><b>15. Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16163) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16163) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16163)

> Cosmos Policy fine-tunes a pretrained video model in one stage for visuomotor control, enabling action latent frames, future state prediction, and planning, achieving state-of-the-art robotic benchmarks.

</details>

<details>
<summary><b>16. ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</b> â­ 91</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16148) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16148) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16148)

**ğŸ’» Code:** [â­ Code](https://github.com/facebookresearch/actionmesh)

> ğŸ¤—Try it out: https://huggingface.co/spaces/facebook/ActionMesh ğŸŒProject Page: https://remysabathier.github.io/actionmesh/ ğŸ“„Paper: https://remysabathier.github.io/actionmesh/actionmesh_2026.pdf ğŸ’»Code: https://github.com/facebookresearch/actionmesh

</details>

<details>
<summary><b>17. VideoMaMa: Mask-Guided Video Matting via Generative Prior</b> â­ 108</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.14255) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.14255) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.14255)

**ğŸ’» Code:** [â­ Code](https://github.com/cvlab-kaist/VideoMaMa)

> Demo: https://huggingface.co/spaces/SammyLim/VideoMaMa Git: https://github.com/cvlab-kaist/VideoMaMa Project Page: https://cvlab-kaist.github.io/VideoMaMa/

</details>

<details>
<summary><b>18. PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</b> â­ 76</summary>

<br/>

**ğŸ‘¥ Authors:** Dingcheng Wang, Haoran Lu, Haosen Sun, Jianshu Zhang, Raymond-Qiancx

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15224) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15224) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15224)

**ğŸ’» Code:** [â­ Code](https://github.com/ProgressLM/ProgressLM)

> Towards General Progress Understanding for Embodied Agents

</details>

<details>
<summary><b>19. Agentic Uncertainty Quantification</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15703) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15703) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15703)

> ğŸ›‘ Stop the "Spiral of Hallucination" in Autonomous Agents! Long-horizon agents often fail because minor early errors snowball into irreversible failures. We introduce Agentic Uncertainty Quantification (AUQ) , a training-free Dual-Process framewor...

</details>

<details>
<summary><b>20. 360Anything: Geometry-Free Lifting of Images and Videos to 360Â°</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16192) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16192) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16192)

> 360Anything lifts arbitrary perspective images and videos to seamless, gravity-aligned 360Â° panoramas, without using any camera or 3D information. Project page: https://360anything.github.io/

</details>

<details>
<summary><b>21. Agentic Confidence Calibration</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15778) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15778) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15778)

> ğŸ¯ Don't let your Agents be "Confidently Wrong"! Traditional calibration works for static text, but Autonomous Agents fail differentlyâ€”errors compound over long trajectories. We introduce Holistic Trajectory Calibration (HTC) , a new paradigm to di...

</details>

<details>
<summary><b>22. From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15690) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15690) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15690)

> ğŸ—ºï¸ The 2026 Roadmap for Reliable AI: Making Uncertainty Actionable We are witnessing a paradigm shift in LLMs: Uncertainty is no longer just a passive score for diagnosisâ€”it is evolving into an Active Control Signal for real-time decision-making. ...

</details>

<details>
<summary><b>23. VIOLA: Towards Video In-Context Learning with Minimal Annotations</b> â­ 0</summary>

<br/>

**ğŸ‘¥ Authors:** Ryo Hachiuma, Hideo Saito, Ryo Fujii

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15549) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15549) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15549)

> Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adapt...

</details>

<details>
<summary><b>24. LLM Prompt Evaluation for Educational Applications</b> â­ 0</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16134) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16134) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16134)

> Some of the observations founded are :- -- Prompt design matters as much as the model : The study shows that different prompt templates using the same LLM produce significantly different educational outcomes, proving prompt engineering is a critic...

</details>

<details>
<summary><b>25. Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware</b> â­ 5</summary>

<br/>

**ğŸ‘¥ Authors:** Cohaerence

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.16004) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.16004) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.16004)

**ğŸ’» Code:** [â­ Code](https://github.com/christopher-altman/ibm-qml-kernel)

> We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-frie...

</details>

<details>
<summary><b>26. Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization</b> â­ 2</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.15440) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.15440) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.15440)

**ğŸ’» Code:** [â­ Code](https://github.com/sandyherho/dla-ideal-solver)

> In this work, we address the performance limitations often encountered in Python-based DLA simulations. By utilizing Numba for just-in-time compilation, we developed an implementation that achieves computational speeds comparable to legacy Fortran...

</details>

<details>
<summary><b>27. MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness</b> â­ 10</summary>

<br/>

**ğŸ”— Links:** [ğŸ¤— HuggingFace](https://huggingface.co/papers/2601.08118) â€¢ [ğŸ“„ arXiv](https://arxiv.org/abs/2601.08118) â€¢ [ğŸ“¥ PDF](https://arxiv.org/pdf/2601.08118)

**ğŸ’» Code:** [â­ Code](https://github.com/SAP/mirrorbench)

> The framework is open-sourced at https://github.com/SAP/mirrorbench

</details>

---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | 27 |
| ğŸ“… Today | [`2026-01-26.json`](data/daily/2026-01-26.json) | 27 |
| ğŸ“† This Week | [`2026-W04.json`](data/weekly/2026-W04.json) | 27 |
| ğŸ—“ï¸ This Month | [`2026-01.json`](data/monthly/2026-01.json) | 639 |

### ğŸ“œ Recent Days

| Date | Papers | Link |
|------|--------|------|
| ğŸ“Œ 2026-01-26 | 27 | [View JSON](data/daily/2026-01-26.json) |
| ğŸ“„ 2026-01-25 | 27 | [View JSON](data/daily/2026-01-25.json) |
| ğŸ“„ 2026-01-24 | 27 | [View JSON](data/daily/2026-01-24.json) |
| ğŸ“„ 2026-01-23 | 26 | [View JSON](data/daily/2026-01-23.json) |
| ğŸ“„ 2026-01-22 | 32 | [View JSON](data/daily/2026-01-22.json) |
| ğŸ“„ 2026-01-21 | 11 | [View JSON](data/daily/2026-01-21.json) |
| ğŸ“„ 2026-01-20 | 22 | [View JSON](data/daily/2026-01-20.json) |

### ğŸ“š Weekly Archives

| Week | Papers | Link |
|------|--------|------|
| ğŸ“… 2026-W04 | 27 | [View JSON](data/weekly/2026-W04.json) |
| ğŸ“… 2026-W03 | 183 | [View JSON](data/weekly/2026-W03.json) |
| ğŸ“… 2026-W02 | 232 | [View JSON](data/weekly/2026-W02.json) |
| ğŸ“… 2026-W01 | 156 | [View JSON](data/weekly/2026-W01.json) |

### ğŸ—‚ï¸ Monthly Archives

| Month | Papers | Link |
|------|--------|------|
| ğŸ—“ï¸ 2026-01 | 639 | [View JSON](data/monthly/2026-01.json) |
| ğŸ—“ï¸ 2025-12 | 787 | [View JSON](data/monthly/2025-12.json) |

---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC
- ğŸ“Š **Comprehensive Data** - Abstracts, authors, links, and metadata
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots
- ğŸ”— **Direct Links** - arXiv, PDF, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics
- ğŸ’¾ **JSON Format** - Easy to parse and integrate into your projects
- ğŸ¨ **Clean Interface** - Beautiful, organized README

---

## ğŸš€ Usage

### View Papers

- **Latest Papers**: Check this README (updated daily)
- **JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **Historical Data**: Browse the [`data/`](data/) directory

### Integrate Into Your Project

```python
import requests

# Get latest papers
response = requests.get('https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json')
papers = response.json()

for paper in papers:
    print(f"Title: {paper['title']}")
    print(f"arXiv: {paper['details']['arxiv_page_url']}")
    print(f"PDF: {paper['details']['pdf_url']}")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ Star this repository
- ğŸ‘€ Watch for notifications
- ğŸ”” Enable "All Activity" for daily updates

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape
```

### JSON Schema

```json
{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {
    "abstract": "Paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {}
  }
}
```

---

## ğŸ› ï¸ How It Works

This repository uses:

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Modern web scraping framework
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing
- **[GitHub Actions](https://github.com/features/actions)** - Automated daily runs
- **Python 3.11+** - Data processing and generation

### Workflow

1. ğŸ• GitHub Actions triggers at 00:00 UTC daily
2. ğŸ” Scrapes HuggingFace Papers page
3. ğŸ“¥ Downloads detailed info for each paper
4. ğŸ’¾ Saves to daily/weekly/monthly archives
5. ğŸ“ Generates this beautiful README
6. âœ… Commits and pushes updates

---

## ğŸ¤ Contributing

Found a bug or have a feature request? 

- ğŸ› [Report Issues](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- ğŸ’¡ [Submit Ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ”§ [Pull Requests Welcome](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls)

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

See [LICENSE](LICENSE) for more details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ“¬ Contact & Support

- ğŸ’¬ [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions)
- ğŸ› [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)
- â­ Don't forget to star this repo!

---

<div align="center">

**Made with â¤ï¸ for the AI Community**

[â¬† Back to Top](#-daily-huggingface-ai-papers)

</div>
