from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import asyncio
import json
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

BASE_URL = "https://huggingface.co"


async def scrape_paper_details(crawler, paper_url):
    """Scrape individual paper page for detailed information"""
    try:
        result = await crawler.arun(url=paper_url)
        soup = BeautifulSoup(result.html, "html.parser")

        # Title
        title = soup.select_one("h1")
        title = title.get_text(strip=True) if title else None

        # Abstract
        abstract_block = soup.select_one("div.prose")
        abstract = abstract_block.get_text(" ", strip=True) if abstract_block else None

        # arXiv and PDF Links
        arxiv_page_url = None
        pdf_url = None
        
        for a in soup.select("a[href*='arxiv.org']"):
            href = a.get("href", "")
            if "/abs/" in href:
                arxiv_page_url = href
            elif "/pdf/" in href or href.endswith(".pdf"):
                pdf_url = href

        # GitHub Repos (deduplicated)
        github_links = list(set([
            a.get("href") for a in soup.select("a[href*='github.com']")
            if a.get("href")
        ]))

        # Metadata
        metadata = {}
        for dl in soup.select("dl"):
            dt = dl.select_one("dt")
            dd = dl.select_one("dd")
            if dt and dd:
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                metadata[key] = value

        return {
            "title": title,
            "abstract": abstract,
            "arxiv_page_url": arxiv_page_url,
            "pdf_url": pdf_url,
            "github_links": github_links,
            "metadata": metadata,
            "page_url": paper_url,
            "scraped_at": datetime.now().isoformat()
        }
    
    except Exception as e:
        print(f"Error scraping {paper_url}: {e}")
        return {
            "error": str(e),
            "page_url": paper_url
        }


async def scrape_hf_papers():
    """Scrape all papers from HuggingFace daily papers page"""
    async with AsyncWebCrawler() as crawler:
        print("Fetching papers listing...")
        listing = await crawler.arun(url=f"{BASE_URL}/papers")
        soup = BeautifulSoup(listing.html, "html.parser")

        cards = soup.select("div.from-gray-50-to-white")
        total_found = len(cards)
        print(f"Found {total_found} papers on listing page")
        print(f"Scraping ALL {total_found} papers...")

        all_papers = []

        for idx, card in enumerate(cards, 1):
            print(f"\n[{idx}/{len(cards)}] Processing paper...")
            
            # Title + link
            title_tag = card.select_one("h3 a")
            title = title_tag.get_text(strip=True) if title_tag else None
            paper_url = BASE_URL + title_tag.get("href") if title_tag else None
            
            if not paper_url:
                print("  âš ï¸  No URL found, skipping")
                continue

            print(f"  Title: {title}")

            # Authors
            authors = [a.get("title") for a in card.select("ul li[title]") if a.get("title")]

            # Star count
            star_tag = card.select_one("a.flex span")
            stars = star_tag.get_text(strip=True) if star_tag else "0"

            # Deep scraping
            print(f"  Fetching details from {paper_url}...")
            details = await scrape_paper_details(crawler, paper_url)

            # Merge
            paper_data = {
                "title": title,
                "paper_url": paper_url,
                "authors": authors,
                "stars": stars,
                "details": details
            }
            
            all_papers.append(paper_data)
            
            # Be respectful - add delay
            if idx < len(cards):
                await asyncio.sleep(1.0)

        return all_papers


def save_to_json(data):
    """Save scraped data with daily, weekly, and monthly archives"""
    today = datetime.now()
    today_str = today.strftime("%Y-%m-%d")
    week_str = today.strftime("%Y-W%W")  # Week number
    month_str = today.strftime("%Y-%m")
    
    # Create archive directories
    Path("data/daily").mkdir(parents=True, exist_ok=True)
    Path("data/weekly").mkdir(parents=True, exist_ok=True)
    Path("data/monthly").mkdir(parents=True, exist_ok=True)
    
    # Add scrape date to each paper
    for paper in data:
        paper["scraped_date"] = today_str
    
    # Save daily snapshot
    daily_file = f"data/daily/{today_str}.json"
    with open(daily_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… Saved daily snapshot: {daily_file}")
    
    # Save/update weekly archive
    weekly_file = f"data/weekly/{week_str}.json"
    weekly_data = []
    if Path(weekly_file).exists():
        with open(weekly_file, "r", encoding="utf-8") as f:
            weekly_data = json.load(f)
    weekly_data.extend(data)
    with open(weekly_file, "w", encoding="utf-8") as f:
        json.dump(weekly_data, f, indent=2, ensure_ascii=False)
    print(f"âœ… Updated weekly archive: {weekly_file} (Total: {len(weekly_data)} papers)")
    
    # Save/update monthly archive
    monthly_file = f"data/monthly/{month_str}.json"
    monthly_data = []
    if Path(monthly_file).exists():
        with open(monthly_file, "r", encoding="utf-8") as f:
            monthly_data = json.load(f)
    monthly_data.extend(data)
    with open(monthly_file, "w", encoding="utf-8") as f:
        json.dump(monthly_data, f, indent=2, ensure_ascii=False)
    print(f"âœ… Updated monthly archive: {monthly_file} (Total: {len(monthly_data)} papers)")
    
    # Save latest for easy access
    with open("data/latest.json", "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… Saved latest papers: data/latest.json")


def load_recent_papers(days=7):
    """Load papers from the last N days"""
    papers = []
    today = datetime.now()
    
    for i in range(days):
        date = (today - timedelta(days=i)).strftime("%Y-%m-%d")
        daily_file = f"data/daily/{date}.json"
        
        if Path(daily_file).exists():
            with open(daily_file, "r", encoding="utf-8") as f:
                daily_papers = json.load(f)
                papers.extend(daily_papers)
    
    return papers


def generate_readme(papers):
    """Generate an enhanced README.md with promotional content"""
    today = datetime.now()
    today_str = today.strftime("%Y-%m-%d")
    today_display = today.strftime("%B %d, %Y")
    week_str = today.strftime("%Y-W%W")
    month_str = today.strftime("%Y-%m")
    
    # Load week and month totals
    weekly_file = f"data/weekly/{week_str}.json"
    monthly_file = f"data/monthly/{month_str}.json"
    
    weekly_count = 0
    if Path(weekly_file).exists():
        with open(weekly_file, "r", encoding="utf-8") as f:
            weekly_count = len(json.load(f))
    
    monthly_count = 0
    if Path(monthly_file).exists():
        with open(monthly_file, "r", encoding="utf-8") as f:
            monthly_count = len(json.load(f))
    
    # Count total papers in all archives
    total_papers = 0
    if Path("data/daily").exists():
        for daily_file in Path("data/daily").glob("*.json"):
            with open(daily_file, "r", encoding="utf-8") as f:
                total_papers += len(json.load(f))
    
    readme = f"""<div align="center">

# ğŸ¤– Daily HuggingFace AI Papers

### ğŸ“Š Your Automated AI Research Companion

> **Never miss groundbreaking AI research again!** Get daily updates on the hottest papers from HuggingFace, automatically curated and archived. Perfect for researchers, ML engineers, and AI enthusiasts. ğŸ”¥

[![Update Daily](https://img.shields.io/badge/Update-Daily-brightgreen?style=for-the-badge&logo=github-actions)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/actions)
[![Papers Today](https://img.shields.io/badge/Papers%20Today-{len(papers)}-blue?style=for-the-badge&logo=arxiv)](data/latest.json)
[![Total Papers](https://img.shields.io/badge/Total%20Papers-{total_papers}+-orange?style=for-the-badge&logo=academia)](data/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/AtharvaDomale/Daily-HuggingFace-AI-Papers?style=social)](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/stargazers)

**Automatically updated every day at 00:00 UTC** â°

[ğŸ“Š View Data](data/) | [ğŸ” Latest Papers](data/latest.json) | [ğŸ“… Archives](#-historical-archives) | [â­ Star This Repo](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers)

</div>

---

## ğŸ¯ Why This Repo?

- âœ… **Saves 30+ minutes** of daily paper hunting
- âœ… **Organized archives** - daily, weekly, and monthly snapshots
- âœ… **Direct links** to arXiv, PDFs, and GitHub repositories
- âœ… **Machine-readable JSON** format for easy integration
- âœ… **Zero maintenance** - fully automated via GitHub Actions
- âœ… **Historical data** - track AI research trends over time

---

## ğŸš€ Who Is This For?

<table>
<tr>
<td align="center">ğŸ”¬<br/><b>Researchers</b><br/>Stay current with latest developments</td>
<td align="center">ğŸ’¼<br/><b>ML Engineers</b><br/>Discover SOTA techniques</td>
<td align="center">ğŸ“š<br/><b>Students</b><br/>Learn from cutting-edge research</td>
</tr>
<tr>
<td align="center">ğŸ¢<br/><b>Companies</b><br/>Track AI trends & competition</td>
<td align="center">ğŸ“°<br/><b>Content Creators</b><br/>Find topics for blogs & videos</td>
<td align="center">ğŸ¤–<br/><b>AI Enthusiasts</b><br/>Explore the latest in AI</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Get Today's Papers (cURL)

```bash
curl https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json
```

### 2ï¸âƒ£ Python Integration

```python
import requests
import pandas as pd

# Load latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Convert to DataFrame for analysis
df = pd.DataFrame(papers)
print(f"ğŸ“š Today's papers: {{len(df)}}")

# Filter by stars
trending = df[df['stars'].astype(int) > 10]
print(f"ğŸ”¥ Trending papers: {{len(trending)}}")
```

### 3ï¸âƒ£ JavaScript/Node.js

```javascript
const fetch = require('node-fetch');

async function getTodaysPapers() {{
  const response = await fetch(
    'https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json'
  );
  const papers = await response.json();
  
  console.log(`ğŸ“š Found ${{papers.length}} papers today!`);
  papers.forEach(paper => {{
    console.log(`\\nğŸ“„ ${{paper.title}}`);
    console.log(`â­ ${{paper.stars}} stars`);
    console.log(`ğŸ”— ${{paper.details.arxiv_page_url}}`);
  }});
}}

getTodaysPapers();
```

### 4ï¸âƒ£ Build Your Own Newsletter

```python
import requests
from datetime import datetime

def generate_weekly_digest():
    # Load this week's papers
    papers = requests.get(
        "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/weekly/{week_str}.json"
    ).json()
    
    # Sort by stars
    papers.sort(key=lambda x: int(x['stars']), reverse=True)
    
    # Generate email content
    email = f"# Top AI Papers This Week ({{datetime.now().strftime('%Y-%m-%d')}})\\n\\n"
    
    for i, paper in enumerate(papers[:10], 1):
        email += f"{{i}}. **{{paper['title']}}** â­ {{paper['stars']}}\\n"
        email += f"   {{paper['details']['arxiv_page_url']}}\\n\\n"
    
    return email

print(generate_weekly_digest())
```

---

## ğŸ“ˆ Statistics

<table>
<tr>
<td align="center"><b>ğŸ“„ Today</b><br/><font size="5">{len(papers)}</font><br/>papers</td>
<td align="center"><b>ğŸ“… This Week</b><br/><font size="5">{weekly_count}</font><br/>papers</td>
<td align="center"><b>ğŸ“† This Month</b><br/><font size="5">{monthly_count}</font><br/>papers</td>
<td align="center"><b>ğŸ—„ï¸ Total Archive</b><br/><font size="5">{total_papers}+</font><br/>papers</td>
</tr>
</table>

**Last Updated:** {today_display}

---

## ğŸ”¥ Today's Trending Papers

> Latest AI research papers from HuggingFace Papers, updated daily

"""
    
    for i, paper in enumerate(papers, 1):
        title = paper.get('title', 'N/A')
        paper_url = paper.get('paper_url', '#')
        authors = paper.get('authors', [])
        stars = paper.get('stars', '0')
        details = paper.get('details', {})
        
        arxiv_url = details.get('arxiv_page_url', '')
        pdf_url = details.get('pdf_url', '')
        github_links = details.get('github_links', [])
        abstract = details.get('abstract', '')
        
        # Handle None or empty abstract
        if not abstract:
            abstract = 'No abstract available.'
        elif len(abstract) > 250:
            abstract = abstract[:247] + "..."
        
        readme += f"""<details>
<summary><b>{i}. {title}</b> â­ {stars}</summary>

<br/>

"""
        
        if authors:
            authors_str = ", ".join(authors[:5])
            if len(authors) > 5:
                authors_str += f" _+{len(authors) - 5} more_"
            readme += f"**ğŸ‘¥ Authors:** {authors_str}\n\n"
        
        # Links with emojis
        links = []
        if paper_url:
            links.append(f"[ğŸ¤— HuggingFace]({paper_url})")
        if arxiv_url:
            links.append(f"[ğŸ“„ arXiv]({arxiv_url})")
        if pdf_url:
            links.append(f"[ğŸ“¥ PDF]({pdf_url})")
        
        if links:
            readme += f"**ğŸ”— Links:** {' â€¢ '.join(links)}\n\n"
        
        # GitHub repos
        if github_links:
            gh_links = " â€¢ ".join([f"[â­ Code]({link})" for link in github_links[:3]])
            readme += f"**ğŸ’» Code:** {gh_links}\n\n"
        
        # Abstract in blockquote
        readme += f"> {abstract}\n\n"
        readme += "</details>\n\n"
    
    # Archive section
    readme += f"""---

## ğŸ“… Historical Archives

### ğŸ“Š Quick Access

| Type | Link | Papers |
|------|------|--------|
| ğŸ• Latest | [`latest.json`](data/latest.json) | {len(papers)} |
| ğŸ“… Today | [`{today_str}.json`](data/daily/{today_str}.json) | {len(papers)} |
| ğŸ“† This Week | [`{week_str}.json`](data/weekly/{week_str}.json) | {weekly_count} |
| ğŸ—“ï¸ This Month | [`{month_str}.json`](data/monthly/{month_str}.json) | {monthly_count} |

### ğŸ“œ Recent Days

"""
    
    # List last 7 days in a table
    readme += "| Date | Papers | Link |\n"
    readme += "|------|--------|------|\n"
    
    for i in range(7):
        date = (today - timedelta(days=i)).strftime("%Y-%m-%d")
        daily_file = f"data/daily/{date}.json"
        if Path(daily_file).exists():
            with open(daily_file, "r", encoding="utf-8") as f:
                count = len(json.load(f))
            emoji = "ğŸ“Œ" if i == 0 else "ğŸ“„"
            readme += f"| {emoji} {date} | {count} | [View JSON](data/daily/{date}.json) |\n"
    
    readme += "\n### ğŸ“š Weekly Archives\n\n"
    readme += "| Week | Papers | Link |\n"
    readme += "|------|--------|------|\n"
    
    # List available weeks
    if Path("data/weekly").exists():
        weekly_files = sorted(Path("data/weekly").glob("*.json"), reverse=True)
        for week_file in weekly_files[:4]:  # Last 4 weeks
            week_name = week_file.stem
            with open(week_file, "r", encoding="utf-8") as f:
                count = len(json.load(f))
            readme += f"| ğŸ“… {week_name} | {count} | [View JSON](data/weekly/{week_name}.json) |\n"
    
    readme += "\n### ğŸ—‚ï¸ Monthly Archives\n\n"
    readme += "| Month | Papers | Link |\n"
    readme += "|------|--------|------|\n"
    
    # List available months
    if Path("data/monthly").exists():
        monthly_files = sorted(Path("data/monthly").glob("*.json"), reverse=True)
        for month_file in monthly_files[:6]:  # Last 6 months
            month_name = month_file.stem
            with open(month_file, "r", encoding="utf-8") as f:
                count = len(json.load(f))
            readme += f"| ğŸ—“ï¸ {month_name} | {count} | [View JSON](data/monthly/{month_name}.json) |\n"
    
    # Features section with enhanced content
    readme += """
---

## âœ¨ Features

- ğŸ”„ **Automated Daily Updates** - Runs every day at midnight UTC via GitHub Actions
- ğŸ“Š **Comprehensive Data** - Full abstracts, author lists, and metadata for every paper
- ğŸ—„ï¸ **Historical Archives** - Daily, weekly, and monthly snapshots for trend analysis
- ğŸ”— **Direct Links** - Quick access to arXiv, PDFs, GitHub repos, and HuggingFace pages
- ğŸ“ˆ **Trending Papers** - Star counts and popularity metrics to find hot research
- ğŸ’¾ **JSON Format** - Machine-readable format for easy integration into your projects
- ğŸ¨ **Beautiful README** - Clean, organized presentation updated automatically
- ğŸ” **Searchable Archives** - Easy to filter and find papers by date or topic

---

## ğŸš€ Usage Examples

### View Papers

- **ğŸ“– Latest Papers**: Check this README (updated daily at 00:00 UTC)
- **ğŸ“¦ JSON Data**: Download from [`data/latest.json`](data/latest.json)
- **ğŸ“š Historical Data**: Browse the [`data/`](data/) directory for archives

### Integrate Into Your Projects

#### Python Example: Daily Digest Script

```python
import requests
from datetime import datetime

# Fetch latest papers
url = "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
papers = requests.get(url).json()

# Filter papers with code
papers_with_code = [p for p in papers if p['details'].get('github_links')]

print(f"ğŸ“Š Papers with code today: {{len(papers_with_code)}}")

for paper in papers_with_code[:5]:
    print(f"\\nğŸ“„ {{paper['title']}}")
    print(f"â­ {{paper['stars']}} stars")
    print(f"ğŸ”— {{paper['details']['arxiv_page_url']}}")
    for repo in paper['details']['github_links'][:1]:
        print(f"ğŸ’» {{repo}}")
```

#### Build a Research Tracker

```python
import requests
import json

def track_research_keywords(keywords):
    """Find papers matching your research interests"""
    papers = requests.get(
        "https://raw.githubusercontent.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/main/data/latest.json"
    ).json()
    
    matches = []
    for paper in papers:
        title = paper.get('title', '').lower()
        abstract = paper.get('details', {}).get('abstract', '').lower()
        
        if any(keyword.lower() in title or keyword.lower() in abstract 
               for keyword in keywords):
            matches.append(paper)
    
    return matches

# Example usage
llm_papers = track_research_keywords(['llm', 'language model', 'gpt', 'transformer'])
print(f"Found {{len(llm_papers)}} papers about LLMs today!")
```

### Use as RSS Alternative

Monitor this repo for daily AI paper updates:
- â­ **Star this repository** to show your support
- ğŸ‘€ **Watch** â†’ Custom â†’ Check "Releases" for notifications
- ğŸ”” Enable **"All Activity"** to get notified of every daily update
- ğŸ“§ Subscribe to GitHub notifications for commits

---

## ğŸ“Š Data Structure

```
data/
â”œâ”€â”€ daily/              # Individual day snapshots
â”‚   â”œâ”€â”€ 2024-12-04.json
â”‚   â”œâ”€â”€ 2024-12-05.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weekly/             # Cumulative weekly papers
â”‚   â”œâ”€â”€ 2024-W48.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ monthly/            # Cumulative monthly papers
â”‚   â”œâ”€â”€ 2024-12.json
â”‚   â””â”€â”€ ...
â””â”€â”€ latest.json         # Most recent scrape (always current)
```

### JSON Schema

```json
{{
  "title": "Paper Title",
  "paper_url": "https://huggingface.co/papers/...",
  "authors": ["Author 1", "Author 2"],
  "stars": "42",
  "scraped_date": "2024-12-04",
  "details": {{
    "abstract": "Full paper abstract...",
    "arxiv_page_url": "https://arxiv.org/abs/...",
    "pdf_url": "https://arxiv.org/pdf/...",
    "github_links": ["https://github.com/..."],
    "metadata": {{}},
    "scraped_at": "2024-12-04T00:15:30"
  }}
}}
```

---

## ğŸ› ï¸ How It Works

This repository uses modern Python tools to provide reliable, automated paper tracking:

**Technology Stack:**
- ğŸ¤– **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - Advanced web scraping framework
- ğŸœ **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - HTML parsing and extraction
- âš™ï¸ **[GitHub Actions](https://github.com/features/actions)** - Automated daily execution
- ğŸ **Python 3.11+** - Data processing and JSON generation

### Daily Workflow

1. ğŸ• **Trigger**: GitHub Actions runs at 00:00 UTC daily
2. ğŸ” **Scrape**: Fetches HuggingFace Papers trending page
3. ğŸ“¥ **Extract**: Downloads detailed info for each paper (abstracts, links, metadata)
4. ğŸ’¾ **Archive**: Saves to daily/weekly/monthly JSON files
5. ğŸ“ **Generate**: Creates this beautiful, updated README
6. âœ… **Commit**: Automatically commits and pushes changes

### Why This Approach?

- âœ… **Reliable**: No manual updates needed, runs automatically
- âœ… **Complete**: Captures full paper details, not just titles
- âœ… **Organized**: Structured archives make trend analysis easy
- âœ… **Accessible**: JSON format works with any programming language
- âœ… **Transparent**: All code is open source and auditable

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### Ways to Contribute

- ğŸ› **Report Bugs**: [Open an issue](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues) for any problems you find
- ğŸ’¡ **Feature Requests**: [Share your ideas](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions) for new features
- ğŸ”§ **Code Contributions**: [Submit a PR](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/pulls) with improvements
- ğŸ“– **Documentation**: Help improve README or add examples
- â­ **Spread the Word**: Star the repo and share with colleagues!

### Development Setup

```bash
# Clone the repository
git clone https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers.git
cd Daily-HuggingFace-AI-Papers

# Install dependencies
pip install -r requirements.txt

# Run the scraper
python scraper.py
```

### Ideas for Contributions

- Add paper categorization (NLP, CV, RL, etc.)
- Create visualization scripts for trends
- Build a simple search API
- Add RSS feed generation
- Create browser extension
- Add email notification system

---

## ğŸ“œ License

MIT License - feel free to use this data for your own projects!

This means you can:
- âœ… Use commercially
- âœ… Modify and distribute
- âœ… Use privately
- âœ… No warranty provided

See [LICENSE](LICENSE) for full details.

---

## ğŸŒŸ Star History

If you find this useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=AtharvaDomale/Daily-HuggingFace-AI-Papers&type=Date)](https://star-history.com/#AtharvaDomale/Daily-HuggingFace-AI-Papers&Date)

---

## ğŸ’¬ Community & Support

### Get Help

- ğŸ“– **Documentation**: Check this README first
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/discussions) for questions
- ğŸ› **Bug Reports**: [Issue Tracker](https://github.com/AtharvaDomale/Daily-HuggingFace-AI-Papers/issues)

### Stay Updated

- â­ Star this repo to stay in the loop
- ğŸ‘€ Watch for new features and updates
- ğŸ¦ Follow [@AtharvaDomale](https://github.com/AtharvaDomale) for project updates

---

## â“ FAQ

<details>
<summary><b>How often is this updated?</b></summary>
<br/>
Every day at 00:00 UTC via GitHub Actions. You'll always have the latest papers from HuggingFace!
</details>

<details>
<summary><b>Can I use this data in my project?</b></summary>
<br/>
Yes! It's MIT licensed. Use it for research, apps, newsletters, or anything else. Just maintain the license notice.
</details>

<details>
<summary><b>How do I get notified of updates?</b></summary>
<br/>
Star & Watch this repo, or use RSS feeds via GitHub's built-in functionality. You can also write a script to check the JSON daily.
</details>

<details>
<summary><b>Why HuggingFace Papers?</b></summary>
<br/>
HuggingFace Papers curates trending AI research with community engagement (stars). It's a great signal for what's hot in AI research.
</details>

<details>
<summary><b>Can I request specific features?</b></summary>
<br/>
Absolutely! Open a discussion or issue with your ideas. We're always looking to improve!
</details>

---

## ğŸ™ Acknowledgments

- ğŸ¤— **HuggingFace** for providing the excellent Papers platform
- ğŸŒ **Crawl4AI** for the robust scraping framework
- ğŸ‘¥ **Contributors** who help improve this project
- â­ **Everyone** who stars and uses this repository

---

## ğŸ“Š Project Stats

![GitHub stars](https://img.shields.io/github/stars/A
