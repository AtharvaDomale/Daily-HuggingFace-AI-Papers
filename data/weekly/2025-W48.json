[
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-04T20:19:20.961877"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-04T20:19:23.118436"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-04T20:19:25.281652"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-04T20:19:27.476746"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-04T20:19:29.578861"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-04T20:19:31.728245"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-04T20:19:33.755115"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-04T20:19:36.010946"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-04T20:19:38.067484"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-04T20:19:40.618700"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-04T20:19:42.781709"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-04T20:19:44.858222"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-04T20:19:46.947599"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-04T20:19:49.014636"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-04T20:19:51.293128"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-04T20:19:53.370089"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-04T20:19:55.477733"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-04T20:19:57.520691"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-04T20:19:59.592819"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-04T20:20:01.699407"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-04T20:20:03.794116"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-04T20:20:05.813512"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-04T20:20:07.825029"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-04T20:20:09.829762"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-04T21:17:47.069463"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-04T21:17:49.192535"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-04T21:17:51.363234"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-04T21:17:53.492069"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-04T21:17:55.616626"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-04T21:17:57.841621"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-04T21:17:59.919631"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-04T21:18:02.035416"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-04T21:18:04.082998"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-04T21:18:06.180570"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-04T21:18:08.400168"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-04T21:18:10.545247"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-04T21:18:12.576045"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-04T21:18:14.792437"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-04T21:18:16.953344"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-04T21:18:18.990243"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-04T21:18:21.101903"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-04T21:18:23.201891"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-04T21:18:25.299264"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-04T21:18:27.368119"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-04T21:18:29.397458"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-04T21:18:31.533447"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-04T21:18:33.581102"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-04T21:18:35.717215"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-05T01:45:08.800431"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-05T01:45:10.948720"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-05T01:45:13.120150"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-05T01:45:15.330754"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-05T01:45:17.565020"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-05T01:45:19.723255"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-05T01:45:21.795623"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-05T01:45:23.959532"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization (2025) Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning (2025) DeepEyesV2: Toward Agentic Multimodal Model (2025) Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch (2025) From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning (2025) MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning (2025) Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-05T01:45:26.057231"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-05T01:45:28.203868"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation (2025) DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models (2025) CharCom: Composable Identity Control for Multi-Character Story Illustration (2025) The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment (2025) Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-05T01:45:30.339006"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-05T01:45:32.465445"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-05T01:45:34.611634"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-05T01:45:36.734679"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-05T01:45:38.714190"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-05T01:45:41.479727"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-05T01:45:43.560551"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-05T01:45:45.641544"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-05T01:45:47.648471"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-05T01:45:49.740831"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2512.04025",
    "authors": [
      "Bohan Zhuang",
      "Weijie Wang",
      "Xi Lin",
      "Youping Gu",
      "Xiaolong Li"
    ],
    "stars": "4",
    "details": {
      "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
      "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our project page is at: https://ziplab.co/PSA/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04025",
      "pdf_url": "https://arxiv.org/pdf/2512.04025",
      "github_links": [
        "https://github.com/ziplab/Pyramid-Sparse-Attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04025",
      "scraped_at": "2025-12-05T01:45:51.734925"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-05T01:45:53.791038"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-05T01:45:55.865460"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-05T01:45:58.004342"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-05T01:46:00.069008"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-05T21:45:27.136896"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-05T21:45:29.003036"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-05T21:45:30.836133"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "32",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-05T21:45:32.680328"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "87",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-05T21:45:34.526238"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-05T21:45:36.527491"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "230",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-05T21:45:38.358063"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-05T21:45:40.291130"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-05T21:45:42.094547"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-05T21:45:43.969839"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-05T21:45:45.799525"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-05T21:45:47.632081"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "10",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-05T21:45:49.509345"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-05T21:45:51.439704"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-05T21:45:53.263163"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-05T21:45:55.240172"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "742",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-05T21:45:57.093539"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-05T21:45:58.872854"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-05T21:46:00.832911"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-05T21:46:02.669896"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-05T21:46:04.538104"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-05T21:46:06.330856"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "5",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-05T21:46:08.128956"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-05T21:46:09.903559"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "146",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-05T21:46:11.776860"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-05T21:46:13.573649"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-05T21:46:15.445157"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-05T21:46:17.298562"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-05T21:46:19.102566"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-05T21:46:20.916543"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-05T21:46:22.711806"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-05T21:46:24.551712"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-05T21:46:26.346693"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "1",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-05T21:46:28.138846"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-05T21:46:29.948938"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-05T21:46:31.735937"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-05T21:46:33.515128"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-05T21:46:35.309571"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-06T01:39:15.192900"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-06T01:39:17.129412"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-06T01:39:19.022592"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "33",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-06T01:39:20.928064"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "87",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-06T01:39:22.780299"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-06T01:39:24.694509"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "230",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-06T01:39:26.554770"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-06T01:39:28.390259"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-06T01:39:30.642592"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-06T01:39:32.487748"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-06T01:39:34.345229"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-06T01:39:36.204083"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-06T01:39:38.084093"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "10",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-06T01:39:39.989738"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-06T01:39:41.819845"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "742",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-06T01:39:43.747719"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-06T01:39:45.583608"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-06T01:39:47.490249"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-06T01:39:49.355078"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-06T01:39:51.188158"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-06T01:39:54.131792"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-06T01:39:56.410809"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-06T01:39:58.256378"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "5",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-06T01:40:00.105252"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-06T01:40:01.956187"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-06T01:40:03.845611"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "148",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-06T01:40:05.744152"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-06T01:40:07.557068"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-06T01:40:09.385040"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-06T01:40:11.210013"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-06T01:40:13.057458"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-06T01:40:14.900057"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-06T01:40:16.708726"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "1",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-06T01:40:18.512814"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-06T01:40:20.394175"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-06T01:40:22.227829"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-06T01:40:24.083221"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-06T01:40:25.891953"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-07T01:52:46.094380"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-07T01:52:48.210822"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "69",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-07T01:52:50.202899"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-07T01:52:52.202235"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "101",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-07T01:52:54.160261"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "116",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-07T01:52:56.246187"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "350",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-07T01:52:58.305330"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "33",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-07T01:53:00.340224"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-07T01:53:02.544481"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-07T01:53:04.591650"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-07T01:53:06.578999"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-07T01:53:08.537607"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-07T01:53:10.438186"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-07T01:53:12.449058"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-07T01:53:14.374069"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-07T01:53:16.262997"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-07T01:53:18.204429"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "12",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-07T01:53:20.103764"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "747",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-07T01:53:22.017875"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-07T01:53:23.974486"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-07T01:53:25.852135"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "153",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-07T01:53:27.762613"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-07T01:53:29.694599"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-07T01:53:31.626340"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-07T01:53:33.570046"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-07T01:53:35.457087"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-07T01:53:37.370394"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "17",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-07T01:53:39.404271"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-07T01:53:41.262254"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-07T01:53:43.056982"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-07T01:53:44.927975"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-07T01:53:46.922593"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-07T01:53:48.808339"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-07T01:53:50.767147"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "2",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-07T01:53:52.633019"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-07T01:53:54.486599"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-07T01:53:56.300727"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-07T01:53:58.243503"
    },
    "scraped_date": "2025-12-07"
  }
]