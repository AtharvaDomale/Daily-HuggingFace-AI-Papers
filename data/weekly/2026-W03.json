[
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "172",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-19T01:56:41.527757"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "139",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-19T01:56:43.398499"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-19T01:56:45.417046"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-19T01:56:47.195191"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "33",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-19T01:56:49.104995"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-19T01:56:50.951705"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "340",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-19T01:56:52.804505"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "16",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-19T01:56:54.714747"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-19T01:56:56.691194"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "123",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-19T01:56:58.577937"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-19T01:57:00.381000"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-19T01:57:02.250681"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-19T01:57:04.107961"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-19T01:57:05.942403"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-19T01:57:07.809566"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "22",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-19T01:57:09.668057"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-19T01:57:11.540441"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "198",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-19T01:57:13.397394"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "358",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [
        "https://github.com/HeartMuLa/heartlib"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-19T01:57:15.280746"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-19T01:57:17.135623"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-19T01:57:19.016650"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-19T01:57:20.881698"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-19T01:57:22.792635"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-19T01:57:24.623126"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-19T01:57:26.426957"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "5",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-19T01:57:28.241301"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-19T01:57:30.064872"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "68",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-19T01:57:31.858383"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-19T01:57:33.734234"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-19T01:57:35.523261"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-19T01:57:37.320211"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-19T01:57:39.137495"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-19T01:57:40.957007"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-19T01:57:42.760780"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-19T01:57:44.669585"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-19T01:57:46.515065"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-19T01:57:48.295647"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "21",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-19T01:57:50.101478"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Your Group-Relative Advantage Is Biased",
    "paper_url": "https://huggingface.co/papers/2601.08521",
    "authors": [
      "Xiaohan Wang",
      "Yikunb",
      "PandaChai",
      "chenzherui007",
      "ShortCatisLong"
    ],
    "stars": "0",
    "details": {
      "title": "Your Group-Relative Advantage Is Biased",
      "abstract": "This paper fundamentally shows that: \"The commonly used group-relative advantage estimator is inherently biased except at p_t = 0.5: it systematically underestimates true advantage on hard prompts and overestimates true advantag on easy prompts\". This bias is not just random‚Äîit becomes deterministic in extreme difficulty regimes, meaning the estimator must underestimate for very hard prompts and must overestimate for very easy prompts. This analysis highlights this as a core limitation in group-relative methods and motivates corrections that better align estimated and true advantage.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08521",
      "pdf_url": "https://arxiv.org/pdf/2601.08521",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08521",
      "scraped_at": "2026-01-20T01:51:45.604021"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
    "paper_url": "https://huggingface.co/papers/2601.11496",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
      "abstract": "Imagine a company introducing a shiny new technology üçé. Not to use it, but to force a regulator to rewrite the rules. Once the rules change? The apple is discarded. The technology is never used. But the strategic shift is complete: the manipulator secures a higher payoff, while other players are left worse off. In our new paper, \"The Poisoned Apple Effect\", we identify a strategic vulnerability in AI-mediated markets. We show how agents can expand the technological space purely to manipulate the mediator's design. The result? The manipulator profits from the new rules, while competitors (and social welfare) pay the price.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11496",
      "pdf_url": "https://arxiv.org/pdf/2601.11496",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11496",
      "scraped_at": "2026-01-20T01:51:47.504722"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
    "paper_url": "https://huggingface.co/papers/2601.10355",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
      "abstract": "We propose a novel \"Text to Trajectory\" paradigm to address the scarcity of multi-turn tool usage trajectory data needed to train agents. Traditional methods rely on predefined API sets to synthesize data, but this approach is limited by the scope of tools and is costly. We observe that text corpora naturally contain rich multi-step problem-solving experiences, which can be extracted and transformed into realistic, scalable, and high-quality multi-turn tool usage data. Based on this insight, we develop a pipeline called GEM to enable automatic generation and extraction of multi-turn tool-use trajectory to validate this paradigm.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10355",
      "pdf_url": "https://arxiv.org/pdf/2601.10355",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10355",
      "scraped_at": "2026-01-20T01:51:49.467451"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
    "paper_url": "https://huggingface.co/papers/2601.08430",
    "authors": [
      "Jiale Zhao",
      "Sunzhu Li",
      "kaikezhang",
      "liushunyu",
      "renhuimin"
    ],
    "stars": "25",
    "details": {
      "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
      "abstract": "We introduce RubricHub, a large-scale (~110k) and multi-domain rubric dataset constructed via an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces highly discriminative criteria capable of capturing subtle nuances in model responses. dataset: https://huggingface.co/datasets/sojuL/RubricHub_v1 github: https://github.com/teqkilla/RubricHub arxiv: https://arxiv.org/abs/2601.08430 alphaXiv: https://www.alphaxiv.org/zh/overview/2601.08430v1 Training  code is coming soon!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08430",
      "pdf_url": "https://arxiv.org/pdf/2601.08430",
      "github_links": [
        "https://github.com/teqkilla/RubricHub"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08430",
      "scraped_at": "2026-01-20T01:51:51.409998"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
    "paper_url": "https://huggingface.co/papers/2601.11000",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
      "abstract": "üí° Overview Personalization is increasingly adopted in modern LLM systems, but we find it can systematically distort factual reasoning. We identify personalization-induced hallucinations, where models generate answers aligned with user history rather than objective truth. To mitigate this, we also propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that detects harmful personalization and adaptively steers internal representations to recover factual correctness while keeping useful personalization. üî•Key Insights Problem discovery: We provide the first systematic study of personalization-induced hallucinations and show risks to factual reliability, downstream knowledge acquisition, and long-term user trust. Mitigation method: We propose FPPS, a lightweight inference-time framework that selectively restores factuality under personalization. Evaluation dataset: We develop PFQABench to jointly evaluate factual QA and personalized QA under aligned user sessions, enabling controlled assessment of factuality failures and mitigation. Results: Extensive experiments across multiple LLM backbones and personalization methods show FPPS substantially improves factual accuracy without sacrificing personalization performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11000",
      "pdf_url": "https://arxiv.org/pdf/2601.11000",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11000",
      "scraped_at": "2026-01-20T01:51:53.340876"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2601.11404",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
      "abstract": "abs: Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11404",
      "pdf_url": "https://arxiv.org/pdf/2601.11404",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11404",
      "scraped_at": "2026-01-20T01:51:55.298099"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
    "paper_url": "https://huggingface.co/papers/2601.11037",
    "authors": [
      "Yunbo Tang",
      "bitwjg",
      "Elliott",
      "yongjing",
      "ShiyuLiu"
    ],
    "stars": "16",
    "details": {
      "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
      "abstract": "Boundary-Aware Policy OptimizationÔºàBAPOÔºâ is a novel reinforcement learning-based framework for training reliable agentic search models. Beyond correctness rewards, BAPO incorporates boundary-aware rewards to encourage appropriate \"I Don't Know\" (IDK) responses. To tackle the tradeoff between exploration and exploitation during RL training, we introduce an adaptive reward modulator to prevent the model from being over-encouraged to admit ignorance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11037",
      "pdf_url": "https://arxiv.org/pdf/2601.11037",
      "github_links": [
        "https://github.com/Liushiyu-0709/BAPO-Reliable-Search"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11037",
      "scraped_at": "2026-01-20T01:51:57.225931"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
    "paper_url": "https://huggingface.co/papers/2601.10909",
    "authors": [
      "Gerard Pons-Moll",
      "andreas-geiger",
      "Yongcao",
      "xianghuix",
      "coralli"
    ],
    "stars": "37",
    "details": {
      "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
      "abstract": "TL;DR: We introduce the first framework for atomic, part-level motion control, powered by our new hierarchical Frankenstein dataset (39h) constructed via LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10909",
      "pdf_url": "https://arxiv.org/pdf/2601.10909",
      "github_links": [
        "https://github.com/Coral79/FrankenMotion-Code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10909",
      "scraped_at": "2026-01-20T01:51:59.256617"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM",
    "paper_url": "https://huggingface.co/papers/2601.09001",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM",
      "abstract": "A first exploration of a lightweight, inference-time method for monitoring LLM accuracy under domain drift using output-entropy traces derived from next-token probabilities. This approach demonstrates promising results for slice-level accuracy estimation across STEM reasoning benchmarks, suggesting that entropy-based signals could serve as a practical tool for real-time model monitoring in production. It offers potential utility for both continuous performance tracking and prioritizing data acquisition in dynamic environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09001",
      "pdf_url": "https://arxiv.org/pdf/2601.09001",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09001",
      "scraped_at": "2026-01-20T01:52:01.228592"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
    "paper_url": "https://huggingface.co/papers/2601.09195",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
      "abstract": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks. Codes are available at https://github.com/Utaotao/ProFit",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09195",
      "pdf_url": "https://arxiv.org/pdf/2601.09195",
      "github_links": [
        "https://github.com/Utaotao/ProFit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09195",
      "scraped_at": "2026-01-20T01:52:03.229359"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10781",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
      "abstract": "We introduce FOFPred, a language-driven future optical flow prediction framework that enables improved robot control and video generation. Instead of reacting to motion, FOFPred predicts how motion will evolve, conditioned on natural language. üåê Project: fofpred.github.io üìÑ Paper: arxiv.org/abs/2601.10781 üíª Code: github.com/SalesforceAIResearch/FOFPred ü§ó Model: huggingface.co/Salesforce/FOFPred üïπÔ∏è Demo: fofpred.salesforceresearch.ai",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10781",
      "pdf_url": "https://arxiv.org/pdf/2601.10781",
      "github_links": [
        "https://github.com/SalesforceAIResearch/FOFPred"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10781",
      "scraped_at": "2026-01-20T01:52:05.129097"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
    "paper_url": "https://huggingface.co/papers/2601.11514",
    "authors": [],
    "stars": "175",
    "details": {
      "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
      "abstract": "Project Page | Paper | Video | HF-Model | HF Evaluation Dataset",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11514",
      "pdf_url": "https://arxiv.org/pdf/2601.11514",
      "github_links": [
        "https://github.com/facebookresearch/ShapeR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11514",
      "scraped_at": "2026-01-20T01:52:07.039726"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Reasoning Models Generate Societies of Thought",
    "paper_url": "https://huggingface.co/papers/2601.10825",
    "authors": [
      "James Evans",
      "Blaise Ag√ºera y Arcas",
      "ninoscherrer",
      "ShiYangLAI",
      "junsol"
    ],
    "stars": "0",
    "details": {
      "title": "Reasoning Models Generate Societies of Thought",
      "abstract": "Reasoning models gain accuracy via internal multi-agent-like debates among diverse perspectives, enabling broader exploration of solutions and improved reasoning than single-agent baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10825",
      "pdf_url": "https://arxiv.org/pdf/2601.10825",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10825",
      "scraped_at": "2026-01-20T01:52:08.924713"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
    "paper_url": "https://huggingface.co/papers/2601.11087",
    "authors": [
      "Zheng Zhang",
      "Qiyuan Zhang",
      "shen12313",
      "Shuaishuai0219",
      "BiaoGong"
    ],
    "stars": "0",
    "details": {
      "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
      "abstract": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11087",
      "pdf_url": "https://arxiv.org/pdf/2601.11087",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11087",
      "scraped_at": "2026-01-20T01:52:10.837296"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
    "paper_url": "https://huggingface.co/papers/2601.09636",
    "authors": [
      "Liqiang Nie",
      "Weili Guan",
      "Rui Shao",
      "cgwfeel",
      "user0102"
    ],
    "stars": "0",
    "details": {
      "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
      "abstract": "good",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09636",
      "pdf_url": "https://arxiv.org/pdf/2601.09636",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09636",
      "scraped_at": "2026-01-20T01:52:12.769316"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Building Production-Ready Probes For Gemini",
    "paper_url": "https://huggingface.co/papers/2601.11516",
    "authors": [
      "Rohin Shah",
      "Zheng Wang",
      "Joshua Engels",
      "bilalchughtai",
      "jkramar"
    ],
    "stars": "0",
    "details": {
      "title": "Building Production-Ready Probes For Gemini",
      "abstract": "Proposes long-context robust probes for Gemini misuse mitigation, showing architecture and diverse-training distribution requirements for generalization, and demonstrates efficient pairing with prompted classifiers and automated probe architecture search.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11516",
      "pdf_url": "https://arxiv.org/pdf/2601.11516",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11516",
      "scraped_at": "2026-01-20T01:52:14.665843"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
    "paper_url": "https://huggingface.co/papers/2601.11044",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
      "abstract": "Some of the observations founded are :- i. Long-horizon tasks remain challenging : Even frontier models struggle with sustained reasoning over real world tasks that require 1M tokens and 90 tool calls, indicating limits in long context autonomy. ii. Proprietary models outperform open source models: Closed source models achieve a higher average score (48.4%) than open source counterparts (32.1%), revealing a persistent performance gap on complex agentic tasks. iii. Feedback driven self correction varies widely: Models like GPT 5.2 and Claude show strong gains from iterative feedback, while others (e.g., DeepSeek-V3.2) exhibit minimal or no improvement after feedback. iv. Efficiency trade offs are significant: High performing models often consume far more tokens and time, some models (e.g. Grok-4.1 Fast) are more token efficient despite lower absolute scores. v. Agentic scaffolds strongly influence performance: Models tend to perform best within their native or optimized ecosystems, highlighting that agent performance depends on tight coupling between the model and its scaffold not the model alone.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11044",
      "pdf_url": "https://arxiv.org/pdf/2601.11044",
      "github_links": [
        "https://github.com/GAIR-NLP/AgencyBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11044",
      "scraped_at": "2026-01-20T01:52:16.689123"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
    "paper_url": "https://huggingface.co/papers/2601.07812",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
      "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07812",
      "pdf_url": "https://arxiv.org/pdf/2601.07812",
      "github_links": [
        "https://github.com/anurag-198/MIMIC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07812",
      "scraped_at": "2026-01-20T01:52:18.607248"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "paper_url": "https://huggingface.co/papers/2601.11354",
    "authors": [
      "Jingjing Gong",
      "Weiyi Wang",
      "xpqiu",
      "xjhuang",
      "dalstonchen"
    ],
    "stars": "4",
    "details": {
      "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
      "abstract": "Introduces AstroReason-Bench, a benchmark for evaluating unified agentic planning in space planning problems with physics constraints, heterogeneous objectives, and long-horizon decisions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11354",
      "pdf_url": "https://arxiv.org/pdf/2601.11354",
      "github_links": [
        "https://github.com/Mtrya/astro-reason"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11354",
      "scraped_at": "2026-01-20T01:52:20.488459"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Language of Thought Shapes Output Diversity in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.11227",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Language of Thought Shapes Output Diversity in Large Language Models",
      "abstract": "This paper reveals that controlling the language used during model thinking‚Äîthe language of thought‚Äîprovides a novel and structural source of output diversity.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11227",
      "pdf_url": "https://arxiv.org/pdf/2601.11227",
      "github_links": [
        "https://github.com/iNLP-Lab/Multilingual-LoT-Diversity"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11227",
      "scraped_at": "2026-01-20T01:52:22.355536"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge",
    "paper_url": "https://huggingface.co/papers/2601.10922",
    "authors": [
      "Vikas Kumar",
      "Pavel Bushuyeu",
      "Boris Sobolev",
      "Michael Buriek",
      "Yosub Shin"
    ],
    "stars": "0",
    "details": {
      "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge",
      "abstract": "Some of the observations founded are : i. Difficulty based example selection is the dominant driver of performance: Selecting challenging but learnable examples yields the largest gains in multimodal reasoning accuracy, outperforming other curation strategies. ii. Increasing dataset size does not reliably improve mean accuracy: Once a well aligned base dataset is chosen, larger datasets mainly reduce run to run variance rather than boosting average performance. iii. Data curation operates in a saturation regime: Most performance improvements come from a relatively small number of carefully curated examples, with diminishing returns from adding more data. iv. Common diversity heuristics provide little or no benefit: Techniques such as clustering based diversity, category balancing, and synthetic augmentation often fail to improve performance and can even degrade accuracy. v. Alignment between dataset, benchmark, and base model is crucial: Strong alignment amplifies the effectiveness of difficulty filtering and explains why compact, well aligned datasets can outperform larger but less aligned ones.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10922",
      "pdf_url": "https://arxiv.org/pdf/2601.10922",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10922",
      "scraped_at": "2026-01-20T01:52:24.215009"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09255",
    "authors": [
      "Boxi Wu",
      "Xiaofei He",
      "Hengjia Li",
      "zjuyb"
    ],
    "stars": "0",
    "details": {
      "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
      "abstract": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline, PhyRPR: PhyReason‚ÄìPhyPlan‚ÄìPhyRefine, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09255",
      "pdf_url": "https://arxiv.org/pdf/2601.09255",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09255",
      "scraped_at": "2026-01-20T01:52:26.096444"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
    "paper_url": "https://huggingface.co/papers/2601.11077",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
      "abstract": "Hi everyone,  I'm one of the authors of ABC-Bench . (arXiv:2601.11077). While building Code Agents, we realized that current benchmarks often stop at \"generating correct code snippets.\" But as developers, we know that real-world backend engineering is much more than that‚Äîit's about exploring unfamiliar repos, configuring environments, writing Dockerfiles, and actually deploying a live service. That's why we created ABC-Bench. ‚ú® Key Features: Full Lifecycle: We evaluate everything from code editing to Containerization and Service Launch. Real Integration Testing: We validate agents by sending actual HTTP requests to the service they deploy. Diverse Stack: 224 tasks from real-world repos (8 languages, 19 frameworks). ü§ó Open Source on HF: We've released the full dataset and fine-tuned models: üìö Dataset: OpenMOSS-Team/ABC-Bench ü§ñ Models: OpenMOSS-Team/Qwen3-8B-ABC & OpenMOSS-Team/Qwen3-32B-ABC Hope this serves as a useful testbed for the community! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11077",
      "pdf_url": "https://arxiv.org/pdf/2601.11077",
      "github_links": [
        "https://github.com/OpenMOSS/ABC-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11077",
      "scraped_at": "2026-01-21T01:54:38.294314"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
    "paper_url": "https://huggingface.co/papers/2601.08808",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
      "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08808",
      "pdf_url": "https://arxiv.org/pdf/2601.08808",
      "github_links": [
        "https://github.com/GMLR-Penn/Multiplex-Thinking"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08808",
      "scraped_at": "2026-01-21T01:54:40.218487"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
    "paper_url": "https://huggingface.co/papers/2601.11004",
    "authors": [
      "Tianshi Zheng",
      "Qingcheng Zeng",
      "Qing Zong",
      "Rui Wang",
      "Jiayu Liu"
    ],
    "stars": "6",
    "details": {
      "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
      "abstract": "This paper addresses the often-overlooked problem of confidence calibration for large language models (LLMs) in retrieval-augmented generation (RAG) settings, where noisy retrieved contexts can severely inflate model overconfidence. The authors systematically study calibration performance across multiple benchmarks and propose Noise-AwAre Confidence CaLibration Rules (NAACL Rules) along with a noise-aware supervised fine-tuning framework (NAACL) that leverages guided supervision to imbue models with intrinsic noise awareness. Empirical results demonstrate consistent reductions in expected calibration error both in-domain and out-of-domain, highlighting the method‚Äôs potential to improve epistemic reliability of LLM outputs in factual applications. This work is timely and relevant for enhancing trustworthiness of deployed RAG systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11004",
      "pdf_url": "https://arxiv.org/pdf/2601.11004",
      "github_links": [
        "https://github.com/HKUST-KnowComp/NAACL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11004",
      "scraped_at": "2026-01-21T01:54:42.116241"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10880",
    "authors": [
      "Ziyang Yan",
      "Jiachen Tu",
      "Chuhan Song",
      "Tianxingjian Ding",
      "ChongCong"
    ],
    "stars": "18",
    "details": {
      "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
      "abstract": "üè• Medical SAM3: Bridging the Gap in Text-Guided Medical Image Segmentation Existing foundation models often face challenges when applying \"segment anything\" paradigms to medical imaging, particularly in the absence of spatial prompts (bounding boxes). Medical SAM3 aims to address this by enhancing the model's semantic understanding through full-parameter fine-tuning. üí° Key Contributions: üó®Ô∏è Reduced Reliance on Spatial Cues: The model is trained to perform segmentation using solely text prompts (e.g., \"Polyp\", \"Tumor\"), aiming for a more automated workflow. üìà Improved Generalization: Experiments on 7 unseen external datasets suggest a significant performance improvement in zero-shot settings (Dice score: 11.9% vs 73.9%). ü©ª Diverse Training Data: Developed on a corpus of 33 datasets across 10 imaging modalities to capture a wide range of medical semantics. We hope this work contributes to the development of more robust, prompt-driven medical AI assistants.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10880",
      "pdf_url": "https://arxiv.org/pdf/2601.10880",
      "github_links": [
        "https://github.com/AIM-Research-Lab/Medical-SAM3.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10880",
      "scraped_at": "2026-01-21T01:54:43.975841"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
    "paper_url": "https://huggingface.co/papers/2601.10387",
    "authors": [
      "Jack Lindsey",
      "Kyle Fish",
      "Jonathan Michala",
      "Jack Gallagher",
      "Christina Lu"
    ],
    "stars": "0",
    "details": {
      "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
      "abstract": "arXivlens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/the-assistant-axis-situating-and-stabilizing-the-default-persona-of-language-models-6264-f01123de Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10387",
      "pdf_url": "https://arxiv.org/pdf/2601.10387",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10387",
      "scraped_at": "2026-01-21T01:54:45.824669"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
    "paper_url": "https://huggingface.co/papers/2601.11096",
    "authors": [
      "Hengshuang",
      "shen12313",
      "DonJoey",
      "fengyutong",
      "kema"
    ],
    "stars": "0",
    "details": {
      "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
      "abstract": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11096",
      "pdf_url": "https://arxiv.org/pdf/2601.11096",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11096",
      "scraped_at": "2026-01-21T01:54:47.749022"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.11061",
    "authors": [
      "Lecheng Yan",
      "ChrisLee",
      "kksinn",
      "JiahuiGengNLP",
      "rzdiversity"
    ],
    "stars": "6",
    "details": {
      "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
      "abstract": "RLVR is the secret sauce for reasoning models, but it has a dark side. The Spurious Rewards Paradox reveals how models exploit latent contamination to achieve SOTA benchmark results without genuine reasoning. By identifying the specific Anchor-Adapter circuit, our paper shows we can now causally steer a model's reliance on shortcuts. Check out the code ( https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts ) and the circuit analysis in our paper to see how reasoning might just be hidden memorization in disguise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11061",
      "pdf_url": "https://arxiv.org/pdf/2601.11061",
      "github_links": [
        "https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11061",
      "scraped_at": "2026-01-21T01:54:49.633759"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.08441",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
      "abstract": "Dense steering vectors often fail due to feature entanglement. YaPO solves this by learning sparse steering vectors directly in a Sparse Autoencoder's latent space using preference data in a DPO-fashion optimization loss. Highlights: Precision & Stability: Converges significantly faster and is more stable than dense baselines like BiPO. Cultural Alignment: Superior performance on a new 15-culture benchmark, specifically closing the \"implicit-explicit\" gap where models usually struggle. Generalization: Works on hallucination and jailbreaks without degrading general knowledge (MMLU).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08441",
      "pdf_url": "https://arxiv.org/pdf/2601.08441",
      "github_links": [
        "https://github.com/MBZUAI-Paris/YaPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08441",
      "scraped_at": "2026-01-21T01:54:51.510472"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "PubMed-OCR: PMC Open Access OCR Annotations",
    "paper_url": "https://huggingface.co/papers/2601.11425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PubMed-OCR: PMC Open Access OCR Annotations",
      "abstract": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11425",
      "pdf_url": "https://arxiv.org/pdf/2601.11425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11425",
      "scraped_at": "2026-01-21T01:54:53.335576"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
    "paper_url": "https://huggingface.co/papers/2601.10108",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
      "abstract": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10108",
      "pdf_url": "https://arxiv.org/pdf/2601.10108",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10108",
      "scraped_at": "2026-01-21T01:54:55.279403"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "paper_url": "https://huggingface.co/papers/2601.09512",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
      "abstract": "TL;DR ü§ñ CLARE enables Vision-Language-Action models to learn new robot tasks without forgetting previous ones ‚Äî no replay buffers, no task IDs at inference. üîå Plug-and-play adapters : Extends PEFT with a new CLARE adapter type üß† Smart expansion : Automatically adds new adapter modules only when needed (based on feature similarity) üéØ Task-free inference : Autoencoder-based routing selects the right adapters without knowing the task üìà SOTA on LIBERO : Outperforms exemplar-based continual learning methods on long task sequences Built on ü§ó LeRobot + PEFT .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09512",
      "pdf_url": "https://arxiv.org/pdf/2601.09512",
      "github_links": [
        "https://github.com/huggingface/lerobot",
        "https://github.com/utiasDSL/clare",
        "https://github.com/huggingface/peft"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09512",
      "scraped_at": "2026-01-21T01:54:57.181509"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
    "paper_url": "https://huggingface.co/papers/2601.12993",
    "authors": [],
    "stars": "265",
    "details": {
      "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
      "abstract": "We scale human-centric robot learning with Being-H0.5 toward cross-embodiment generalization. Building on over 35,000 hours data, we unify human hand motion and diverse robot embodiments with a Unified Action Space, and train all heterogeneous supervision through unified sequence modeling under a single framework. This yields a single foundation model that can perceive, describe, and act within one framework, enabling robust cross-embodiment generalization and real-world deployment across diverse robots and tasks. Blog: https://research.beingbeyond.com/being-h05 arXiv: https://arxiv.org/pdf/2601.12993 GitHub: https://github.com/BeingBeyond/Being-H HuggingFace: https://huggingface.co/collections/BeingBeyond/being-h05",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12993",
      "pdf_url": "https://arxiv.org/pdf/2601.12993",
      "github_links": [
        "https://github.com/BeingBeyond/Being-H"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12993",
      "scraped_at": "2026-01-22T01:55:03.772388"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
    "paper_url": "https://huggingface.co/papers/2601.11655",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
      "abstract": "üöÄ Awesome issue resolution: a comprehensive survey! This paper surveyed 175+ works to construct the first unified taxonomy serving as the comprehensive roadmap for issue resolution.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11655",
      "pdf_url": "https://arxiv.org/pdf/2601.11655",
      "github_links": [
        "https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11655",
      "scraped_at": "2026-01-22T01:55:05.630833"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Think3D: Thinking with Space for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.13029",
    "authors": [
      "Yuhan Wu",
      "JeremyYin",
      "sunz525",
      "luciasnowblack",
      "MrBean2024"
    ],
    "stars": "32",
    "details": {
      "title": "Think3D: Thinking with Space for Spatial Reasoning",
      "abstract": "We introduce Think3D, a framework that enables VLM agents to think in 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13029",
      "pdf_url": "https://arxiv.org/pdf/2601.13029",
      "github_links": [
        "https://github.com/zhangzaibin/spagent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13029",
      "scraped_at": "2026-01-22T01:55:07.494034"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "paper_url": "https://huggingface.co/papers/2601.14250",
    "authors": [],
    "stars": "54",
    "details": {
      "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
      "abstract": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer , a unified framework for spatio-temporal video transfer . It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance ( ID and style ) and temporal transfer ( camera movement and video effects ), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14250",
      "pdf_url": "https://arxiv.org/pdf/2601.14250",
      "github_links": [
        "https://github.com/PangzeCheung/OmniTransfer"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14250",
      "scraped_at": "2026-01-22T01:55:09.432481"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
    "paper_url": "https://huggingface.co/papers/2601.14192",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
      "abstract": "This paper surveys efficiency-oriented methods for agentic systems across memory, tool learning, and planning, distills shared design principles, and summarizes how recent methods and benchmarks measure efficiency, which hopes to guide the development of efficient agents. github link: https://github.com/yxf203/Awesome-Efficient-Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14192",
      "pdf_url": "https://arxiv.org/pdf/2601.14192",
      "github_links": [
        "https://github.com/yxf203/Awesome-Efficient-Agents"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14192",
      "scraped_at": "2026-01-22T01:55:11.370876"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
    "paper_url": "https://huggingface.co/papers/2601.13836",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
      "abstract": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs üîó Paper: https://arxiv.org/pdf/2601.13836 üíª Code: https://github.com/OpenMOSS/FutureOmni üåê Project: https://openmoss.github.io/FutureOmni üé¨ Datasets: https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13836",
      "pdf_url": "https://arxiv.org/pdf/2601.13836",
      "github_links": [
        "https://github.com/OpenMOSS/FutureOmni"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13836",
      "scraped_at": "2026-01-22T01:55:13.353686"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.11969",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
      "abstract": "Check our code: https://github.com/LCM-Lab/MemRewardBench and Benchmark: https://huggingface.co/datasets/LCM-Lab/MemRewardBench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11969",
      "pdf_url": "https://arxiv.org/pdf/2601.11969",
      "github_links": [
        "https://github.com/LCM-Lab/MemRewardBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11969",
      "scraped_at": "2026-01-22T01:55:15.293524"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.14004",
    "authors": [
      "qiaw99",
      "WANGYIWEI",
      "zunhai",
      "mingyang26",
      "hengyuanya"
    ],
    "stars": "0",
    "details": {
      "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
      "abstract": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14004",
      "pdf_url": "https://arxiv.org/pdf/2601.14004",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14004",
      "scraped_at": "2026-01-22T01:55:17.331702"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.11522",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
      "abstract": "We introduce UniX, a unified foundation model for Chest X-Ray that combines Autoregression (for understanding) and Diffusion (for generation) within a decoupled dual-branch architecture! üè•‚ú® Why UniX? Current unified models often face a conflict between semantic abstraction and pixel-level reconstruction. UniX solves this via structural decoupling and Cross-Modal Self-Attention. üî• Key Results: Compared to previous works (like LLM-CXR), UniX achieves: üìà +46.1% improvement in Understanding. üé® +24.2% improvement in Generation Quality. ‚ö° Only 25% of the parameters! Resources: Code: https://github.com/ZrH42/UniX Weights: https://huggingface.co/ZrH42/UniX Paper: arXiv:2601.11522",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11522",
      "pdf_url": "https://arxiv.org/pdf/2601.11522",
      "github_links": [
        "https://github.com/ZrH42/UniX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11522",
      "scraped_at": "2026-01-22T01:55:19.364836"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
    "paper_url": "https://huggingface.co/papers/2601.12294",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "abstract": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12294",
      "pdf_url": "https://arxiv.org/pdf/2601.12294",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12294",
      "scraped_at": "2026-01-22T01:55:21.277931"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "paper_url": "https://huggingface.co/papers/2601.13247",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
      "abstract": "WorldMind helps language models stop making physically impossible plans by learning real-world rules from feedback and successful experiences, rather than retraining the model itself.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13247",
      "pdf_url": "https://arxiv.org/pdf/2601.13247",
      "github_links": [
        "https://github.com/zjunlp/WorldMind"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13247",
      "scraped_at": "2026-01-22T01:55:23.093451"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Agentic-R: Learning to Retrieve for Agentic Search",
    "paper_url": "https://huggingface.co/papers/2601.11888",
    "authors": [
      "Daiting Shi",
      "Yuchen Li",
      "Yutao Zhu",
      "Xinyu Ma",
      "Wenhan Liu"
    ],
    "stars": "0",
    "details": {
      "title": "Agentic-R: Learning to Retrieve for Agentic Search",
      "abstract": "Agentic-R: Learning to Retrieve for Agentic Search",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11888",
      "pdf_url": "https://arxiv.org/pdf/2601.11888",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11888",
      "scraped_at": "2026-01-22T01:55:24.928034"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
    "paper_url": "https://huggingface.co/papers/2601.13288",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
      "abstract": "Rather than adding another model to the stack, this work reuses computation already paid for in the serving LLM‚Äôs forward pass by training compact probes on hidden states. It frames the problem as principled selection across tokens and layers (not just ‚Äúfinal layer‚Äù or ‚Äúfirst token‚Äù), implemented with a two-stage aggregation template and lightweight variants that stay close to serving-time cost.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13288",
      "pdf_url": "https://arxiv.org/pdf/2601.13288",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13288",
      "scraped_at": "2026-01-22T01:55:26.778477"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2601.14232",
    "authors": [
      "Aleksandr I. Panov",
      "Alexey K. Kovalev",
      "Daniil Zelezetsky",
      "Egor Cherepanov"
    ],
    "stars": "7",
    "details": {
      "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
      "abstract": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14232",
      "pdf_url": "https://arxiv.org/pdf/2601.14232",
      "github_links": [
        "https://github.com/CognitiveAISystems/kage-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14232",
      "scraped_at": "2026-01-22T01:55:28.677125"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
    "paper_url": "https://huggingface.co/papers/2601.14251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
      "abstract": "We present LightOnOCR-2-1B , a 1B-parameter end-to-end multilingual vision-language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9√ó smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14251",
      "pdf_url": "https://arxiv.org/pdf/2601.14251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14251",
      "scraped_at": "2026-01-22T01:55:30.586443"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.14046",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
      "abstract": "Main take-aways PRiSM is the first fully-open benchmark that evaluates Phone-Recognition systems on both intrinsic (phone-transcription) and extrinsic (down-stream) tasks across 12 datasets covering clinical, L2-learning and multilingual settings.  We find that Large Audio-Language Models still lag behind specialized PR models on such tasks. Since intrinsic phone recognition capability is not fully indicative of performance in extrinsic settings, we design transcript and representation based probes that allow an exhaustive analysis, interpretability, and fair comparison. Language exposure > data size: multilingual training with broad, diverse data matters more for cross lingual generalization. Code, prompts and data are released under permissive licences.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14046",
      "pdf_url": "https://arxiv.org/pdf/2601.14046",
      "github_links": [
        "https://github.com/changelinglab/prism"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14046",
      "scraped_at": "2026-01-22T01:55:32.480062"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "paper_url": "https://huggingface.co/papers/2601.13976",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "abstract": "FantasyVLN is a unified multimodal Chain-of-Thought (CoT) reasoning framework that enables efficient and precise navigation based on natural language instructions and visual observations. FantasyVLN combines the benefits of textual, visual, and multimodal CoT reasoning by constructing a unified representation space across these reasoning modes. To enable efficient reasoning, we align these CoT reasoning modes with non-CoT reasoning during training, while using only non-CoT reasoning at test time. Notably, we perform visual CoT in the latent space of a VAR model, where only low-scale latent representations are predicted. Compared to traditional pixel-level visual CoT methods, our approach significantly improves both training and inference efficiency. See our project page for more detail: https://fantasy-amap.github.io/fantasy-vln/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13976",
      "pdf_url": "https://arxiv.org/pdf/2601.13976",
      "github_links": [
        "https://github.com/Fantasy-AMAP/fantasy-vln",
        "https://github.com/FoundationVision/VAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13976",
      "scraped_at": "2026-01-22T01:55:34.333254"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
    "paper_url": "https://huggingface.co/papers/2601.13761",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
      "abstract": "In this work, we introduce the DARC framework, which adopts decoupled training and asymmetric self-distillation to stabilize self-evolving. We hope this work provides useful insights for LLM self-evolution. avXiv: https://arxiv.org/abs/2601.13761 Github: https://github.com/RUCBM/DARC HuggingFace: https://huggingface.co/papers/2601.13761",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13761",
      "pdf_url": "https://arxiv.org/pdf/2601.13761",
      "github_links": [
        "https://github.com/RUCBM/DARC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13761",
      "scraped_at": "2026-01-22T01:55:36.147165"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "paper_url": "https://huggingface.co/papers/2601.14249",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
      "abstract": "Code: https://github.com/UmeanNever/RankSurprisalRatio",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14249",
      "pdf_url": "https://arxiv.org/pdf/2601.14249",
      "github_links": [
        "https://github.com/UmeanNever/RankSurprisalRatio"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14249",
      "scraped_at": "2026-01-22T01:55:37.991407"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.14209",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
      "abstract": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14209",
      "pdf_url": "https://arxiv.org/pdf/2601.14209",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14209",
      "scraped_at": "2026-01-22T01:55:39.795059"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
    "paper_url": "https://huggingface.co/papers/2601.13697",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
      "abstract": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13697",
      "pdf_url": "https://arxiv.org/pdf/2601.13697",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13697",
      "scraped_at": "2026-01-22T01:55:41.659816"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing",
    "paper_url": "https://huggingface.co/papers/2601.12937",
    "authors": [
      "Marten van Dijk",
      "Kaleel Mahmood",
      "Min Chen",
      "emirhanboge",
      "bilgehanertan"
    ],
    "stars": "0",
    "details": {
      "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing",
      "abstract": "üßë‚Äç‚öñÔ∏èüìÑ This paper shows that membership inference attacks are not reliable technical evidence for copyright infringement in court. Even with strong MIAs, semantics-preserving paraphrasing breaks the signal while keeping utility, making them brittle in adversarial legal settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12937",
      "pdf_url": "https://arxiv.org/pdf/2601.12937",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12937",
      "scraped_at": "2026-01-22T01:55:43.453742"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "paper_url": "https://huggingface.co/papers/2601.10237",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
      "abstract": "This paper quantifies a fundamental lower bound on the noise required for differentially private stochastic gradient descent (DP-SGD) to maintain strong privacy, revealing that even with massive datasets and both shuffled and Poisson subsampling, the utility degradation due to necessary noise is substantial and persistent. https://www.alphaxiv.org/overview/2601.10237",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10237",
      "pdf_url": "https://arxiv.org/pdf/2601.10237",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10237",
      "scraped_at": "2026-01-22T01:55:45.232902"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
    "paper_url": "https://huggingface.co/papers/2601.13591",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
      "abstract": "This paper introduce the DSAEval, evaluating LLM based Data Agent in a wide-range of real world problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13591",
      "pdf_url": "https://arxiv.org/pdf/2601.13591",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13591",
      "scraped_at": "2026-01-22T01:55:47.016763"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
    "paper_url": "https://huggingface.co/papers/2601.13253",
    "authors": [
      "√ñzay Ezerceli",
      "Mehmet Emin Buldur",
      "MElHuseyni",
      "etosun"
    ],
    "stars": "0",
    "details": {
      "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
      "abstract": "Addressing data scarcity in low-resource languages, this paper introduces a cost-effective ($65) pipeline for generating large-scale semantic datasets. By integrating FastText clustering, Gemini 2.5-Flash labeling, and dictionary curation, the authors release a Turkish corpus of 843,000 pairs (synonyms, antonyms, co-hyponyms), achieving 90% F1-macro on classification benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13253",
      "pdf_url": "https://arxiv.org/pdf/2601.13253",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13253",
      "scraped_at": "2026-01-22T01:55:48.814077"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
    "paper_url": "https://huggingface.co/papers/2601.13251",
    "authors": [
      "√ñzay Ezerceli",
      "Mehmet Emin Buldur",
      "MElHuseyni",
      "etosun"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
      "abstract": "This paper addresses the inability of neural embeddings to distinguish synonyms from antonyms. The authors introduce a soft-to-hard clustering algorithm that prevents semantic drift and a 3-way relation discriminator (90% F1). Validated against a new dataset of 843k pairs generated via Gemini 2.5, the pipeline yields 2.9M semantic clusters, significantly enhancing precision for RAG and semantic search.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13251",
      "pdf_url": "https://arxiv.org/pdf/2601.13251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13251",
      "scraped_at": "2026-01-22T01:55:50.632933"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions",
    "paper_url": "https://huggingface.co/papers/2601.13075",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions",
      "abstract": "Students have immense research potential, but enough mentors for them. What if we could design an AI system to mentor them? We introduce METIS (Mentoring Engine for Thoughtful Inquiry & Solutions), a stage-aware research mentor.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13075",
      "pdf_url": "https://arxiv.org/pdf/2601.13075",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13075",
      "scraped_at": "2026-01-22T01:55:52.565769"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
    "paper_url": "https://huggingface.co/papers/2601.12910",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
      "abstract": "We introduce the SciCoQA dataset for evaluating models on detecting discrepancies between paper and code. Find all resources here: Paper: arXiv Data: Hugging Face Dataset Code: GitHub Demo: Hugging Face Space Project Page : UKPLab/scicoqa",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12910",
      "pdf_url": "https://arxiv.org/pdf/2601.12910",
      "github_links": [
        "https://github.com/UKPLab/scicoqa",
        "https://github.com/ukplab/scicoqa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12910",
      "scraped_at": "2026-01-22T01:55:54.385925"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "paper_url": "https://huggingface.co/papers/2601.10700",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
      "abstract": "The paper addresses the lack of reliable ground-truth benchmarks for evaluating concept-based explainability in Large Language Models. The authors introduce LIBERTy, a framework that generates \"structural counterfactuals\" by explicitly defining Structured Causal Models (SCMs) where the LLM acts as a component to generate text. By intervening on high-level concepts (e.g., gender, disease symptoms) within the SCM and propagating these changes to the LLM's output, the framework creates synthetic yet causally grounded datasets without relying on costly human annotation. The study introduces three such datasets (covering disease detection, CV screening, and workplace violence) and a new metric called \"order-faithfulness.\" Experiments using LIBERTy reveal that while fine-tuned matching methods currently offer the best explanations, there is significant room for improvement, and some proprietary models like GPT-4o exhibit notably low sensitivity to demographic interventions due to safety alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10700",
      "pdf_url": "https://arxiv.org/pdf/2601.10700",
      "github_links": [
        "https://github.com/GilatToker/Liberty-benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10700",
      "scraped_at": "2026-01-22T01:55:56.208957"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging",
    "paper_url": "https://huggingface.co/papers/2601.13677",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging",
      "abstract": "üöÄ Building on nnActive , an evaluation framework for active learning in 3D biomedical imaging, this paper proposes a simple and effective method that consistently outperforms strong random baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.19183",
      "pdf_url": "https://arxiv.org/pdf/2601.13677",
      "github_links": [
        "https://github.com/MIC-DKFZ/nnActive/tree/nnActive_v2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13677",
      "scraped_at": "2026-01-22T01:55:58.016384"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
    "paper_url": "https://huggingface.co/papers/2601.13481",
    "authors": [
      "Yu He",
      "Weiping Fu",
      "Zhiyuan Wang",
      "Zhangqi Wang",
      "Jian Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
      "abstract": "We propose APOLO (Automated Prompt Optimization for Linguistic emOtion diagnosis), a framework that systematically explores a broader and finer-grained prompt space to enhance diagnostic efficiency and robustness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13481",
      "pdf_url": "https://arxiv.org/pdf/2601.13481",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13481",
      "scraped_at": "2026-01-22T01:55:59.798013"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection",
    "paper_url": "https://huggingface.co/papers/2601.11898",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection",
      "abstract": "https://github.com/yilmazkorkmaz1/RemoteVAR",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11898",
      "pdf_url": "https://arxiv.org/pdf/2601.11898",
      "github_links": [
        "https://github.com/yilmazkorkmaz1/RemoteVAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11898",
      "scraped_at": "2026-01-22T01:56:01.626876"
    },
    "scraped_date": "2026-01-22"
  }
]