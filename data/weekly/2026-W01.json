[
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-05T01:59:59.631752"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLenâ€“based implementation for efficient concept-token cross-attention; (2) a decoupled Î¼P formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-05T02:00:01.852412"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "61",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-05T02:00:03.959198"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-05T02:00:05.988614"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-05T02:00:08.091511"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-05T02:00:10.042015"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "4",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESOâ€™s effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-05T02:00:11.922915"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.24615",
    "authors": [],
    "stars": "4.1k",
    "details": {
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "abstract": "LONG wait. Youtu-Agent ( https://github.com/TencentCloudADP/Youtu-agent ) now releases its technical report with two major updates, i.e., Automated Generation and Hybrid Policy Optimization. Additionally, we've launched Youtu-Tip ( https://github.com/TencentCloudADP/youtu-tip ), a more user-friendly application that runs on macOS. Check them out and have fun!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24615",
      "pdf_url": "https://arxiv.org/pdf/2512.24615",
      "github_links": [
        "https://github.com/TencentCloudADP/youtu-tip",
        "https://github.com/TencentCloudADP/youtu-agent",
        "https://github.com/TencentCloudADP/Youtu-agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24615",
      "scraped_at": "2026-01-06T01:50:51.163830"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2601.00393",
    "authors": [
      "Feng Wang",
      "Junran Peng",
      "renshengjihe",
      "Abyssaledge",
      "Yuppie1204"
    ],
    "stars": "124",
    "details": {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "abstract": "NeoVerse is a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. Project page: https://neoverse-4d.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00393",
      "pdf_url": "https://arxiv.org/pdf/2601.00393",
      "github_links": [
        "https://github.com/IamCreateAI/NeoVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00393",
      "scraped_at": "2026-01-06T01:50:53.205658"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "paper_url": "https://huggingface.co/papers/2601.00664",
    "authors": [
      "Sung Ju Hwang",
      "Jaehyeong Jo",
      "Sangwon Jang",
      "jaehong31",
      "taekyungki"
    ],
    "stars": "65",
    "details": {
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "abstract": "arXiv explained breakdown of this paper ðŸ‘‰ https://arxivexplained.com/papers/avatar-forcing-real-time-interactive-head-avatar-generation-for-natural-conversation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00664",
      "pdf_url": "https://arxiv.org/pdf/2601.00664",
      "github_links": [
        "https://github.com/TaekyungKi/AvatarForcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00664",
      "scraped_at": "2026-01-06T01:50:55.077779"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.24330",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "abstract": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24330",
      "pdf_url": "https://arxiv.org/pdf/2512.24330",
      "github_links": [
        "https://github.com/OpenSenseNova/SenseNova-MARS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24330",
      "scraped_at": "2026-01-06T01:50:56.907809"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24271",
    "authors": [],
    "stars": "29",
    "details": {
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "abstract": "An interesting work! github: https://github.com/AMAP-ML/Taming-Hallucinations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24271",
      "pdf_url": "https://arxiv.org/pdf/2512.24271",
      "github_links": [
        "https://github.com/AMAP-ML/Taming-Hallucinations"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24271",
      "scraped_at": "2026-01-06T01:50:58.822184"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.00796",
    "authors": [
      "Yu-Lun Liu",
      "Zhenjun Zhao",
      "Jiewen Chan"
    ],
    "stars": "0",
    "details": {
      "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
      "abstract": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00796",
      "pdf_url": "https://arxiv.org/pdf/2601.00796",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00796",
      "scraped_at": "2026-01-06T01:51:00.710791"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Deep Delta Learning",
    "paper_url": "https://huggingface.co/papers/2601.00417",
    "authors": [],
    "stars": "234",
    "details": {
      "title": "Deep Delta Learning",
      "abstract": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Î²(X). We provide a spectral analysis of this operator, demonstrating that the gate Î²(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00417",
      "pdf_url": "https://arxiv.org/pdf/2601.00417",
      "github_links": [
        "https://github.com/yifanzhang-pro/deep-delta-learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00417",
      "scraped_at": "2026-01-06T01:51:02.670038"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "paper_url": "https://huggingface.co/papers/2512.24695",
    "authors": [
      "Vahab Mirrokni",
      "Peilin Zhong",
      "Meisam Razaviyayn",
      "AliBehrouz"
    ],
    "stars": "0",
    "details": {
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "abstract": "Nested Learning (NL) is a new learning paradigm for continual learning and machine learning in general.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24695",
      "pdf_url": "https://arxiv.org/pdf/2512.24695",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24695",
      "scraped_at": "2026-01-06T01:51:04.593752"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.00747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "abstract": "For those of you interested in RLVR, here is a paper that formally characterizes the mechanism behind \"diversity collapse\" in reasoning models trained with scalar rewards (such as STaR, GRPO, and DPO). The paper introduces a variational framework based on Shahshahani gradient flow to prove that optimizing solely for correctness inherently erodes the diversity of reasoning paths, leading to a \"reasoning monoculture.\" To address this, they propose Distributional Creative Reasoning (DCR), which incorporates a diversity energy functional (using entropy and kernel-based novelty) into the objective, mathematically guaranteeing the maintenance of a diverse portfolio of successful reasoning strategies while still optimizing for utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00747",
      "pdf_url": "https://arxiv.org/pdf/2601.00747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00747",
      "scraped_at": "2026-01-06T01:51:06.385822"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
    "paper_url": "https://huggingface.co/papers/2512.22955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
      "abstract": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs).  The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode.  To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning.  By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision.  Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22955",
      "pdf_url": "https://arxiv.org/pdf/2512.22955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22955",
      "scraped_at": "2026-01-06T01:51:08.307456"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Fast-weight Product Key Memory",
    "paper_url": "https://huggingface.co/papers/2601.00671",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-weight Product Key Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Trellis: Learning to Compress Key-Value Memory in Attention Models (2025) TNT: Improving Chunkwise Training for Test-Time Memorization (2025) GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory (2025) MIDUS: Memory-Infused Depth Up-Scaling (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models (2025) BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00671",
      "pdf_url": "https://arxiv.org/pdf/2601.00671",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00671",
      "scraped_at": "2026-01-06T01:51:10.101023"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
    "paper_url": "https://huggingface.co/papers/2601.00575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "abstract": "Project Page: https://ishirgarg.github.io/infosynth_web/ Code: https://github.com/ishirgarg/infosynth Dataset: https://huggingface.co/datasets/ishirgarg/InfoSynth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00575",
      "pdf_url": "https://arxiv.org/pdf/2601.00575",
      "github_links": [
        "https://github.com/ishirgarg/infosynth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00575",
      "scraped_at": "2026-01-06T01:51:12.005175"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
    "paper_url": "https://huggingface.co/papers/2601.00204",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Hao Tang",
      "Zeyu Cai",
      "XiaokunSun"
    ],
    "stars": "19",
    "details": {
      "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
      "abstract": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io Code: https://github.com/XiaokunSun/MorphAny3D",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00204",
      "pdf_url": "https://arxiv.org/pdf/2601.00204",
      "github_links": [
        "https://github.com/XiaokunSun/MorphAny3D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00204",
      "scraped_at": "2026-01-06T01:51:13.858117"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "paper_url": "https://huggingface.co/papers/2512.20578",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
      "abstract": "Can Large Language Models predict their own failures? ðŸ§ âš¡ We all know the critical bottleneck in GenAI: LLMs are incredible, but they can confidently hallucinate and make mistakes. Until now, most fixes have been computationally massive â€” relying on expensive external judges, huge reward models, or costly training to make the LLM itself more robust. This brings us to two fundamental questions: â“ Do LLMs recognize when they're making mistakes? â“ Can we make them self-aware about their own failures? ðŸš€ Introducing Gnosis: A lightweight self-awareness mechanism for frozen LLMs. Named after the Greek word for knowledge/insight, Gnosis gives LLMs a form of introspection. We add only ~5M parameters to enable a frozen LLM to verify its own outputs by decoding internal hidden states + attention patterns during inference â€” with negligible overhead and no external judge . The results challenge the classic efficiencyâ€“accuracy trade-off: ðŸ† Superior performance across domains Despite being orders of magnitude smaller, Gnosis can outperform strong 8B reward models and proprietary judges like Gemini 2.5 Pro on both multi-step reasoning and factual/parametric knowledge QA (e.g., TriviaQA), across multiple backbones. âš¡ Real-time early failure detection Gnosis doesnâ€™t need to wait for the final token. By monitoring the generation trajectory in real time, it can predict an error before the model finishes â€” enabling early stopping, preventing bad outputs from reaching users, and saving significant compute. This suggests something important: the model often already contains signals of impending failure during generation â€” we just needed the right mechanism to read them. ðŸ‘‡ code + models: ðŸ’» Code: https://github.com/Amirhosein-gh98/Gnosis",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20578",
      "pdf_url": "https://arxiv.org/pdf/2512.20578",
      "github_links": [
        "https://github.com/Amirhosein-gh98/Gnosis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20578",
      "scraped_at": "2026-01-07T01:50:17.166637"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.02204",
    "authors": [],
    "stars": "60",
    "details": {
      "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02204",
      "pdf_url": "https://arxiv.org/pdf/2601.02204",
      "github_links": [
        "https://github.com/ByteVisionLab/NextFlow"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02204",
      "scraped_at": "2026-01-07T01:50:19.105092"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "K-EXAONE Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.01739",
    "authors": [],
    "stars": "39",
    "details": {
      "title": "K-EXAONE Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01739",
      "pdf_url": "https://arxiv.org/pdf/2601.01739",
      "github_links": [
        "https://github.com/LG-AI-EXAONE/K-EXAONE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01739",
      "scraped_at": "2026-01-07T01:50:21.012374"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.01425",
    "authors": [],
    "stars": "86",
    "details": {
      "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
      "abstract": "We introduce DreamID-V, the first Diffusion Transformer-based framework for high-fidelity video face swapping. DreamID-V bridges the gap between image and video domains, achieving exceptional identity similarity and temporal coherence even in challenging scenarios. Our code : https://github.com/bytedance/DreamID-V Our project : https://guoxu1233.github.io/DreamID-V/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01425",
      "pdf_url": "https://arxiv.org/pdf/2601.01425",
      "github_links": [
        "https://github.com/bytedance/DreamID-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01425",
      "scraped_at": "2026-01-07T01:50:22.967886"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2601.02256",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02256",
      "pdf_url": "https://arxiv.org/pdf/2601.02256",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02256",
      "scraped_at": "2026-01-07T01:50:24.885134"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
    "paper_url": "https://huggingface.co/papers/2512.24138",
    "authors": [
      "Zhiyong Wang",
      "Jiajun Liang",
      "Jie Liu",
      "Yuxiao Ye",
      "Haoran He"
    ],
    "stars": "18",
    "details": {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "abstract": "Introducing GARDO: Reinforcing Diffusion Models without Reward Hacking paper: https://arxiv.org/abs/2512.24138 code: https://github.com/tinnerhrhe/gardo project: https://tinnerhrhe.github.io/gardo_project/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24138",
      "pdf_url": "https://arxiv.org/pdf/2512.24138",
      "github_links": [
        "https://github.com/tinnerhrhe/GARDO",
        "https://github.com/tinnerhrhe/gardo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24138",
      "scraped_at": "2026-01-07T01:50:26.781382"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "paper_url": "https://huggingface.co/papers/2601.02358",
    "authors": [
      "Kun Gai",
      "Pengfei Wan",
      "Zhoujie Fu",
      "Tong He",
      "Junyi Chen"
    ],
    "stars": "42",
    "details": {
      "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02358",
      "pdf_url": "https://arxiv.org/pdf/2601.02358",
      "github_links": [
        "https://github.com/SOTAMak1r/VINO-code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02358",
      "scraped_at": "2026-01-07T01:50:28.658298"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "paper_url": "https://huggingface.co/papers/2601.02281",
    "authors": [],
    "stars": "76",
    "details": {
      "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
      "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively â€œrollingâ€ the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02281",
      "pdf_url": "https://arxiv.org/pdf/2601.02281",
      "github_links": [
        "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02281",
      "scraped_at": "2026-01-07T01:50:30.519682"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Recursive Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24601",
    "authors": [],
    "stars": "675",
    "details": {
      "title": "Recursive Language Models",
      "abstract": "Study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. They propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. They find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query. Some of the observations they found are :- -- LLMs interacting with their own prompts as objects. -- In their approach, a prompt isnâ€™t â€œrunâ€ directly, instead itâ€™s stored as a variable in an external Python REPL, and the language model writes code to inspect /slice/ decompose that long string, observes execution outputs, and then constructs sub-tasks where it recursively invokes an LLM on just the relevant snippets. Stitching the result together when the recursive process ends. So it can solve 10M+ token tasks with far less â€œcontext rotâ€ and often lower cost than summarization/RAG, turning long-context scaling into an inference-time algorithm rather than just a bigger context window. -- The ability to search the Prompt is what enables handling long context inputs, sub calls help handle information dense inputs. -- Inference cost of RLMs remain comparable to a base model call but are high variance  because it can keep making sub-calls or iterate if it can't solve the problem initially. -- The key insight is that long prompts should not be fed into the LLM directly, but should instead be treated as part of the environment that the LLM can search, read and interact with as needed for the task.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24601",
      "pdf_url": "https://arxiv.org/pdf/2512.24601",
      "github_links": [
        "https://github.com/alexzhang13/rlm/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24601",
      "scraped_at": "2026-01-07T01:50:32.438055"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "paper_url": "https://huggingface.co/papers/2601.02346",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent (2025) Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning (2025) CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions (2025) Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning (2025) Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02346",
      "pdf_url": "https://arxiv.org/pdf/2601.02346",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02346",
      "scraped_at": "2026-01-07T01:50:34.314010"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "paper_url": "https://huggingface.co/papers/2601.02356",
    "authors": [
      "Shuo Yang",
      "Jiarui Cai",
      "Yantao Shen",
      "ZyZcuhk",
      "jingtan"
    ],
    "stars": "13",
    "details": {
      "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) Loom: Diffusion-Transformer for Interleaved Generation (2025) What Happens Next? Next Scene Prediction with a Unified Video Model (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02356",
      "pdf_url": "https://arxiv.org/pdf/2601.02356",
      "github_links": [
        "https://github.com/sparkstj/Talk2Move"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02356",
      "scraped_at": "2026-01-07T01:50:36.191628"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "paper_url": "https://huggingface.co/papers/2601.02179",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "abstract": "In this paper, we explore the confidence estimation in a new paradigm: multi-turn interactions! Check it out!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02179",
      "pdf_url": "https://arxiv.org/pdf/2601.02179",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02179",
      "scraped_at": "2026-01-07T01:50:38.040156"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01046",
    "authors": [
      "Yi Yang",
      "Yixuan Tang"
    ],
    "stars": "0",
    "details": {
      "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
      "abstract": "âœ¨ Turn any decoder-only LLM into a powerful embedding modelâ€”zero training needed! âœ¨ The Trick : Re-route the final token's key-value states as an internal prefix, giving all tokens access to global context in one forward pass. No input modification, no mask removal, just smart internal state manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01046",
      "pdf_url": "https://arxiv.org/pdf/2601.01046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01046",
      "scraped_at": "2026-01-07T01:50:39.891420"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2601.00501",
    "authors": [
      "Mohammad Asiful Hossain",
      "Kevin Cannons",
      "Saeed Ranjbar Alvar",
      "Mohsen Gholami",
      "Ahmad Rezaei"
    ],
    "stars": "0",
    "details": {
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "abstract": "CPPO: Contrastive Perception for Vision Language Policy Optimization introduces a new method (CPPO) for fine-tuning vision-language models (VLMs) using reinforcement learning. Instead of relying on explicit perception rewards or auxiliary models, the approach identifies perceptual tokens via entropy changes under perturbed images and augments the policy objective with a contrastive perception loss to improve multimodal reasoning performance and training efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00501",
      "pdf_url": "https://arxiv.org/pdf/2601.00501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00501",
      "scraped_at": "2026-01-07T01:50:41.758090"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "paper_url": "https://huggingface.co/papers/2601.02267",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Zhenyu Zhang",
      "wrk226"
    ],
    "stars": "1",
    "details": {
      "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
      "abstract": "Project page: https://wrk226.github.io/DiffProxy.html Code: https://github.com/wrk226/DiffProxy",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02267",
      "pdf_url": "https://arxiv.org/pdf/2601.02267",
      "github_links": [
        "https://github.com/wrk226/DiffProxy"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02267",
      "scraped_at": "2026-01-07T01:50:43.650977"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01836",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "abstract": "COMPASS is the first framework for evaluating LLM alignment with organization-specific policies rather than universal harms. While models handle legitimate requests well (>95% accuracy), they catastrophically fail at enforcing prohibitions, refusing only 13-40% of denylist violations. GitHub: https://github.com/AIM-Intelligence/COMPASS",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01836",
      "pdf_url": "https://arxiv.org/pdf/2601.01836",
      "github_links": [
        "https://github.com/AIM-Intelligence/COMPASS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01836",
      "scraped_at": "2026-01-07T01:50:45.620954"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
    "paper_url": "https://huggingface.co/papers/2512.23035",
    "authors": [
      "Shiying Wang",
      "Kai Li",
      "Shun Zhang",
      "Xuechao Zou",
      "Yi Zhou"
    ],
    "stars": "4",
    "details": {
      "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
      "abstract": "We are excited to introduce our latest work on semi-supervised semantic segmentation : ðŸ“„ Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion This paper tackles one of the most challenging issues in semi-supervised segmentation: pseudo-label drift . When labeled data are extremely scarce, self-training methods are prone to deterministic bias, where early incorrect pseudo-labels accumulate over time, leading to unstable and degraded training. ðŸ§  Motivation Most existing consistency- or pseudo-labelâ€“based semi-supervised approaches rely heavily on self-generated supervision . Once early pseudo-labels become unreliable, error accumulation is inevitable. Our goal is to introduce stronger semantic priors to correct such drift and stabilize the training process. âœ¨ Key Contributions 1ï¸âƒ£ Heterogeneous Dual-Student Framework We leverage two complementary vision foundation modelsâ€” CLIP for global semantic priors and DINOv3 for fine-grained local structuresâ€”to enable stable mutual learning and suppress error accumulation. 2ï¸âƒ£ Explicitâ€“Implicit Semantic Co-Guidance By jointly utilizing text embeddings (explicit semantics) and learnable queries (implicit semantics), we provide class-level semantic anchors and enhance semantic consistency. 3ï¸âƒ£ Globalâ€“Local Feature Co-Fusion We fuse CLIPâ€™s global contextual understanding with DINOv3â€™s local structural details, yielding more accurate and stable segmentation results. ðŸ“Š Experimental Results Extensive evaluations on six mainstream remote sensing benchmarks demonstrate that Co2S consistently achieves strong and stable performance across different data splits and scenarios, especially under extremely low annotation budgets . ðŸ“¦ Open-Source Resources arXiv Paper : https://arxiv.org/abs/2512.23035 Project Page : https://xavierjiezou.github.io/Co2S/ GitHub Code : https://github.com/XavierJiezou/Co2S HuggingFace Models : https://huggingface.co/XavierJiezou/co2s-models HuggingFace Datasets : https://huggingface.co/datasets/XavierJiezou/co2s-datasets #Remote Sensing #Semantic Segmentation #Semi-Supervised Learning #Vision Foundation Models #CLIP #DINOv3",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23035",
      "pdf_url": "https://arxiv.org/pdf/2512.23035",
      "github_links": [
        "https://github.com/XavierJiezou/Co2S"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23035",
      "scraped_at": "2026-01-07T01:50:47.571407"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "paper_url": "https://huggingface.co/papers/2601.01426",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01426",
      "pdf_url": "https://arxiv.org/pdf/2601.01426",
      "github_links": [
        "https://github.com/SWE-Lego/SWE-Lego"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01426",
      "scraped_at": "2026-01-07T01:50:49.438589"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
    "paper_url": "https://huggingface.co/papers/2601.01576",
    "authors": [
      "Chunchun Ma",
      "Yujiong Shen",
      "Yueyuan Huang",
      "Kexin Tan",
      "Ming Zhang"
    ],
    "stars": "3",
    "details": {
      "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation (2025) SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning (2025) WisPaper: Your AI Scholar Search Engine (2025) AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis (2025) OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists (2025) AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement (2025) Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01576",
      "pdf_url": "https://arxiv.org/pdf/2601.01576",
      "github_links": [
        "https://github.com/january-blue/OpenNovelty"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01576",
      "scraped_at": "2026-01-07T01:50:51.251342"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
    "paper_url": "https://huggingface.co/papers/2601.00863",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
      "abstract": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00863",
      "pdf_url": "https://arxiv.org/pdf/2601.00863",
      "github_links": [
        "https://github.com/lamm-mit/MusicAnalysis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00863",
      "scraped_at": "2026-01-07T01:50:53.135282"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
    "paper_url": "https://huggingface.co/papers/2512.21472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "abstract": "âœ¨ The largest publicly available dermoscopic skin lesion segmentation dataset with 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image. âœ¨ 16 unique annotators , 3 different tools used, and 2 skill levels of the manual reviewer. âœ¨ Contains consensus masks for the 2,394 images that have multi-annotator segmentations (2-5 segmentations per image). âœ¨ Collected and curated from the ISIC Archive .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21472",
      "pdf_url": "https://arxiv.org/pdf/2512.21472",
      "github_links": [
        "https://github.com/sfu-mial/IMAplusplus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21472",
      "scraped_at": "2026-01-07T01:50:55.025985"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "paper_url": "https://huggingface.co/papers/2601.02315",
    "authors": [
      "Beth Tellman",
      "Lalit Maurya",
      "Saurabh Kaushik"
    ],
    "stars": "3",
    "details": {
      "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
      "abstract": "Despite the recent success of large pretrained encoders (Geoâ€‘Foundation Models), we consistently observe that Uâ€‘Netâ€‘based models remain highly competitiveâ€”and in some cases outperform transformers, particularly due to their strength in capturing local spatial nuances. Motivated by this, we propose Prithviâ€‘CAFE (Prithviâ€‘Complementary Adaptive Fusion Encoder), which enhances local representations through complementary fusion with a CNNâ€‘based encoder. We evaluate our approach on two major flood datasetsâ€”FloodPlanet and Sen1Floods11â€”and achieve stateâ€‘ofâ€‘theâ€‘art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02315",
      "pdf_url": "https://arxiv.org/pdf/2601.02315",
      "github_links": [
        "https://github.com/Sk-2103/Prithvi-CAFE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02315",
      "scraped_at": "2026-01-07T01:50:56.844097"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.02314",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "abstract": "Does COT in llms stay faithful to their thoughts?",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02314",
      "pdf_url": "https://arxiv.org/pdf/2601.02314",
      "github_links": [
        "https://github.com/skhanzad/AridadneXAI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02314",
      "scraped_at": "2026-01-07T01:50:58.682941"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.22877",
    "authors": [
      "Jun-Cheng Chen",
      "Cheng-Fu Chou",
      "Ju-Hsuan Weng",
      "jwliao1209"
    ],
    "stars": "0",
    "details": {
      "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
      "abstract": "Concept Erasure Benchmark",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22877",
      "pdf_url": "https://arxiv.org/pdf/2512.22877",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22877",
      "scraped_at": "2026-01-07T01:51:00.532894"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "paper_url": "https://huggingface.co/papers/2601.03252",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
      "abstract": "Depth Beyond Pixels ðŸš€ We Introduce InfiniDepth â€” casting monocular depth estimation as a neural implicit field. ðŸ” Arbitrary-Resolution ðŸ“ Accurate Metric Depth ðŸ“· Single-View NVS under large viewpoints shifts Arxiv: https://arxiv.org/abs/2601.03252 page: https://zju3dv.github.io/InfiniDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03252",
      "pdf_url": "https://arxiv.org/pdf/2601.03252",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03252",
      "scraped_at": "2026-01-08T01:50:45.247652"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "paper_url": "https://huggingface.co/papers/2601.01554",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
      "abstract": "MOSS Transcribe Diarize ðŸŽ™ï¸ We introduce MOSS Transcribe Diarize â€” a unified multimodal model for Speaker-Attributed, Time-Stamped Transcription (SATS) . ðŸ” End-to-end SATS in a single pass (transcription + speaker attribution + timestamps) ðŸ§  128k context window for up to ~90-minute audio without chunking (strong long-range speaker memory) ðŸŒ Trained on extensive in-the-wild conversations + controllable simulated mixtures (robust to overlap/noise/domain shift) ðŸ“Š Strong results on AISHELL-4 / Podcast / Movies benchmarks (best cpCER / Î”cp among evaluated systems) Paper: [2601.01554] MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization Homepage: https://mosi.cn/models/moss-transcribe-diarize Online Demo: https://moss-transcribe-diarize-demo.mosi.cn",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01554",
      "pdf_url": "https://arxiv.org/pdf/2601.01554",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01554",
      "scraped_at": "2026-01-08T01:50:47.162285"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "paper_url": "https://huggingface.co/papers/2601.03233",
    "authors": [
      "kvochko",
      "jacobitterman",
      "nisan",
      "benibraz",
      "yoavhacohen"
    ],
    "stars": "922",
    "details": {
      "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API 3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation (2025) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation (2025) MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning (2026) DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation (2025) JoVA: Unified Multimodal Learning for Joint Video-Audio Generation (2025) In-Context Audio Control of Video Diffusion Transformers (2025) JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03233",
      "pdf_url": "https://arxiv.org/pdf/2601.03233",
      "github_links": [
        "https://github.com/Lightricks/LTX-2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03233",
      "scraped_at": "2026-01-08T01:50:49.139726"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.22334",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "abstract": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22334",
      "pdf_url": "https://arxiv.org/pdf/2512.22334",
      "github_links": [
        "https://github.com/InternScience/SciEvalKit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22334",
      "scraped_at": "2026-01-08T01:50:51.230691"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "paper_url": "https://huggingface.co/papers/2601.03193",
    "authors": [
      "Lin-Chen",
      "lovesnowbest",
      "YuZeng260",
      "CostaliyA",
      "Hungryyan"
    ],
    "stars": "25",
    "details": {
      "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
      "abstract": "UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03193",
      "pdf_url": "https://arxiv.org/pdf/2601.03193",
      "github_links": [
        "https://github.com/Hungryyan1/UniCorn"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03193",
      "scraped_at": "2026-01-08T01:50:53.221654"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "paper_url": "https://huggingface.co/papers/2601.02427",
    "authors": [],
    "stars": "1.44k",
    "details": {
      "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
      "abstract": "NitroGen is a vision-action foundation model trained on 40k hours of gameplay across 1,000+ games, enabling cross-game generalization with behavior cloning and benchmarking, achieving strong unseen-game transfer.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02427",
      "pdf_url": "https://arxiv.org/pdf/2601.02427",
      "github_links": [
        "https://github.com/MineDojo/NitroGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02427",
      "scraped_at": "2026-01-08T01:50:55.235135"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2601.03044",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "abstract": "ðŸš€ Website: https://www.agibot.com/research/sop We introduce SOP for online post-training of generalist VLAs in the real world â€” unlocking persistent, reliable deployment of generalist robots in physical environments. ðŸ” 36 hours of continuous cloth folding: video ðŸ“¦ 36 hours of continuous cardboard box assembly: video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03044",
      "pdf_url": "https://arxiv.org/pdf/2601.03044",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03044",
      "scraped_at": "2026-01-08T01:50:57.176273"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "paper_url": "https://huggingface.co/papers/2601.02785",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamStyle: A Unified Framework for Video Stylization",
      "abstract": "DreamStyle unifies text-, style-image-, and first-frame-guided video stylization on an I2V backbone, using LoRA with token-specific up matrices to improve style consistency and video quality.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02785",
      "pdf_url": "https://arxiv.org/pdf/2601.02785",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02785",
      "scraped_at": "2026-01-08T01:50:59.158844"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MiMo-V2-Flash Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.02780",
    "authors": [],
    "stars": "957",
    "details": {
      "title": "MiMo-V2-Flash Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Xiaomi MiMo-VL-Miloco Technical Report (2025) Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning (2025) Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks (2025) AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing (2025) NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations (2025) Practical Policy Distillation for Reinforcement Learning in Radio Access Networks (2025) Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02780",
      "pdf_url": "https://arxiv.org/pdf/2601.02780",
      "github_links": [
        "https://github.com/XiaomiMiMo/MiMo-V2-Flash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02780",
      "scraped_at": "2026-01-08T01:51:01.099679"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.01874",
    "authors": [
      "Aojun Lu",
      "Junjie Xie",
      "Shuhang Chen",
      "JacobYuan",
      "Yunqiu"
    ],
    "stars": "0",
    "details": {
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "abstract": "Project page: https://shchen233.github.io/cogflow/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01874",
      "pdf_url": "https://arxiv.org/pdf/2601.01874",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01874",
      "scraped_at": "2026-01-08T01:51:03.012607"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "paper_url": "https://huggingface.co/papers/2601.01321",
    "authors": [
      "Yao Su",
      "vztu",
      "ZihanJia",
      "fjchendp",
      "roz322"
    ],
    "stars": "2",
    "details": {
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "abstract": "This paper systematically analyzes AI integration in Digital Twins through a four-stage framework (modeling â†’ mirroring â†’ intervention â†’ autonomous management), covering LLMs, foundation models, world models, and intelligent agents across 11 application domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01321",
      "pdf_url": "https://arxiv.org/pdf/2601.01321",
      "github_links": [
        "https://github.com/rongzhou7/Awesome-Digital-Twin-AI/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01321",
      "scraped_at": "2026-01-08T01:51:04.924547"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "paper_url": "https://huggingface.co/papers/2601.02439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "abstract": "WebGym creates a large, non-stationary visual web task suite and scalable RL pipeline, enabling fast trajectory rollout and improved vision-language agent performance on unseen websites.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02439",
      "pdf_url": "https://arxiv.org/pdf/2601.02439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02439",
      "scraped_at": "2026-01-08T01:51:06.769414"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
    "paper_url": "https://huggingface.co/papers/2601.02989",
    "authors": [
      "Fatemeh Askari",
      "Sadegh Mohammadian",
      "Mohammadali Banayeeanzade",
      "Hosein Hasani",
      "safinal"
    ],
    "stars": "0",
    "details": {
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "abstract": "ðŸ”¢ Overcoming Transformer Depth Limits in Counting Tasks LLMs often fail at counting not because they aren't smart, but because of architectural depth constraints ðŸš§. We propose a simple, effective System-2 strategy ðŸ§© that decomposes counting tasks to bypass these limits. ðŸ”¬ We also provide a full mechanistic interpretation , identifying the specific attention heads and representations responsible for transferring \"latent counts\" across the network. ðŸ“ˆ This approach allows LLMs to achieve high accuracy on large-scale counting benchmarks where they typically fail.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02989",
      "pdf_url": "https://arxiv.org/pdf/2601.02989",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02989",
      "scraped_at": "2026-01-08T01:51:08.644226"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "paper_url": "https://huggingface.co/papers/2601.03256",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
      "abstract": "Project page: https://luhexiao.github.io/Muses.github.io/ Code: https://github.com/luhexiao/Muses",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03256",
      "pdf_url": "https://arxiv.org/pdf/2601.03256",
      "github_links": [
        "https://github.com/luhexiao/Muses"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03256",
      "scraped_at": "2026-01-08T01:51:10.461072"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "paper_url": "https://huggingface.co/papers/2601.01720",
    "authors": [
      "Donghao Luo",
      "yanweifuture",
      "chengjie-wang",
      "ChengmingX",
      "ScarletAce"
    ],
    "stars": "0",
    "details": {
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) Unified Video Editing with Temporal Reasoner (2025) VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning (2025) EasyV2V: A High-quality Instruction-based Video Editing Framework (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01720",
      "pdf_url": "https://arxiv.org/pdf/2601.01720",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01720",
      "scraped_at": "2026-01-08T01:51:12.354426"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01592",
    "authors": [
      "Yang Yao",
      "Yixu Wang",
      "Juncheng Li",
      "Yunhao Chen",
      "xinwang22"
    ],
    "stars": "112",
    "details": {
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "abstract": "Even State-of-the-Art Models Fail to Hold Ground Against Sophisticated Adversaries. Our comprehensive evaluation highlights two key findings. (1) A clear stratification in defense capability: Top-tier models such as Claude Haiku 4.5, GPT-5.2, and Qwen3-Max exhibit strong baseline robustness, effectively neutralizing static, template-based attacks and complex logic traps, often keeping ASR below 20%.This suggests that leading labs have improved defenses against recognizable, repeatable jailbreak structures, while several models (e.g., Llama-4, Mistral Large 3) remain more susceptible to these simpler patterns. (2) A shift in the attack landscape: adaptive, multi-turn, and multi-agent strategies dominate, whereas static, single-turn, and template-based approaches are increasingly ineffective. Methods like EvoSynth and X-Teaming can achieve >90% ASR even against advanced models. This indicates current safety training overfits to static templates, failing to generalize against the broad attack surface exposed by automated red-teaming. Adversarial Robustness Exhibits Inconsistent and Polarized Vulnerability Patterns. We observe a polarization effect where models demonstrate high resistance to specific attack families (e.g., text-based cipher) yet remain completely defenseless against others (e.g., logic nesting). For instance, Grok 4.1 Fast shows 1.5% ASR against RedQueen but 90.5% against X-Teaming. This stark performance disparity (~90%) underscores that current defenses are often patch-based rather than holistic, necessitating the multi-faceted evaluation provided by OpenRT. Enhanced Reasoning and Multimodal Capabilities are New Vectors for Exploitation. Contrary to the common assumption that more capable models are inherently safer, we find that enhanced capabilities often introduce new vectors for exploitation. Reasoning-enhanced models (CoT) do not demonstrate superior robustness; instead, their verbose reasoning processes can be manipulated to bypass safety filters. Similarly, Multimodal LLMs exhibit a critical modality gap: visual inputs frequently bypass text-based safety mechanisms, allowing cross-modal attacks to compromise models that are otherwise robust to purely textual jailbreaks. These findings suggest that current safety alignment has not kept pace with the architectural expansion of model capabilities. Proprietary Models Can Be as Vulnerable as Open-Source Models Under Certain Attacks. Our analysis reveals that proprietary and open-source models exhibit comparable susceptibility to our attack suite. Across our 20 evaluated models, only GPT-5.2 and Claude Haiku 4.5 maintained an average ASR below 30%, while all other models consistently exceeded this threshold. This universality sharply contradicts the assumption that closed deployments offer superior protection, demonstrating that the safety through obscurity of proprietary strategies fails to provide any tangible mitigation against sophisticated adversarial attacks. Scaling MLLMs Robustness via Defense-in-Depth and Continuous Red Teaming. Challenges such as polarized robustness, weak generalization to unseen attacks, and cross-modal bypasses highlight the limits of single-layer defense. Effective mitigation requires a paradigm shift toward Defense-in-Depth: integrating intrinsic architectural safety with runtime risk estimation and adversarial training on multimodal and multi-turn interactions. Crucially, continuous Red Teaming via infrastructure like OpenRT provides systematic evaluation to verify empirical robustness and prevent benchmark overfitting.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01592",
      "pdf_url": "https://arxiv.org/pdf/2601.01592",
      "github_links": [
        "https://github.com/AI45Lab/OpenRT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01592",
      "scraped_at": "2026-01-08T01:51:14.181550"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.23412",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
      "abstract": "In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows the model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL. The agent reasoning framework, MWE-Bench, three smaller-scale agent models (2B, 3B, and 4B) distilled from MindWatcher 32B, and related resources will be open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23412",
      "pdf_url": "https://arxiv.org/pdf/2512.23412",
      "github_links": [
        "https://github.com/TIMMY-CHAN/MindWatcher"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23412",
      "scraped_at": "2026-01-08T01:51:16.037424"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "paper_url": "https://huggingface.co/papers/2601.03227",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "abstract": "We found the sonar moment in audio language models. We propose the task of audio geo-localization. And amazingly, Gemini 3 Pro can reach the distance error of less than 55km for 25%  samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03227",
      "pdf_url": "https://arxiv.org/pdf/2601.03227",
      "github_links": [
        "https://github.com/Rising0321/AGL1K"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03227",
      "scraped_at": "2026-01-08T01:51:21.077717"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "paper_url": "https://huggingface.co/papers/2601.03194",
    "authors": [
      "Sai Rithwik Reddy Chirra",
      "Shashivardhan Reddy Koppula",
      "Mohammad Zia Ur Rehman",
      "shwetankssingh",
      "UVSKKR"
    ],
    "stars": "0",
    "details": {
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (explainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03194",
      "pdf_url": "https://arxiv.org/pdf/2601.03194",
      "github_links": [
        "https://github.com/ziarehman30/X-MuTeST"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03194",
      "scraped_at": "2026-01-08T01:51:23.122431"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Parallel Latent Reasoning for Sequential Recommendation",
    "paper_url": "https://huggingface.co/papers/2601.03153",
    "authors": [
      "Yuning Jiang",
      "Jian Wu",
      "Wen Chen",
      "Xu Chen",
      "TangJiakai5704"
    ],
    "stars": "0",
    "details": {
      "title": "Parallel Latent Reasoning for Sequential Recommendation",
      "abstract": "Parallel Latent Reasoning (PLR): Sequential Recommendation with Parallel Reasoning ðŸ”¥ ðŸ“‰ Depth-only reasoning often hits performance plateausâ€”PLR mitigates this with parallel latent reasoning. Core Innovation âœ¨ ðŸŽ¯ Learnable trigger tokens: Build parallel streams in continuous latent space. ðŸ”„ Global regularization: Preserve stream diversity to avoid redundancy. âš–ï¸ Adaptive aggregation: Smartly combine multi-stream insights for optimal results. Key Advantages ðŸš€ ðŸ“Š Outperforms SOTA baselines (SASRec, BERT4Rec, ReaRec, LRESA) by 5.5%â€“14.9% on Recall@10/20 and NDCG@10/20 across three real-world datasets. âš¡ Real-time efficiency: Only 5.8% latency increase vs. base models, enabled by KV Caching and GPU parallelism. ðŸ›¡ï¸ Strong robustness: Maintains top performance even with 30% missing user interactions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03153",
      "pdf_url": "https://arxiv.org/pdf/2601.03153",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03153",
      "scraped_at": "2026-01-08T01:51:25.012089"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03127",
    "authors": [
      "Yue Cao",
      "Hanqing Yang",
      "Jijin Hu",
      "Qiang Zhou",
      "Sashuai Zhou"
    ],
    "stars": "0",
    "details": {
      "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
      "abstract": "reasoning-based image generation and editing",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03127",
      "pdf_url": "https://arxiv.org/pdf/2601.03127",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03127",
      "scraped_at": "2026-01-08T01:51:26.836393"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
    "paper_url": "https://huggingface.co/papers/2601.02996",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
      "abstract": "https://github.com/cisnlp/multilingual-latent-reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02996",
      "pdf_url": "https://arxiv.org/pdf/2601.02996",
      "github_links": [
        "https://github.com/cisnlp/multilingual-latent-reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02996",
      "scraped_at": "2026-01-08T01:51:28.643997"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "paper_url": "https://huggingface.co/papers/2601.02359",
    "authors": [
      "Vladislav Golyanik",
      "Toshihiko Yamasaki",
      "mapooon"
    ],
    "stars": "0",
    "details": {
      "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
      "abstract": "Detecting deepfakes with generative AI. We introduce ExposeAnyone â€” a paradigm shift in face forgery detection! ðŸ”ï¸ Fully self-supervised approach ðŸ¥‡ Best average AUC on traditional deepfake benchmarks ðŸ’ª Best AUC even on Sora2 by OpenAI ðŸ’¢ Strong Robustness to common corruptions such as JPEG/MPEG compression Arxiv: https://arxiv.org/abs/2601.02359 Project page: https://mapooon.github.io/ExposeAnyonePage/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02359",
      "pdf_url": "https://arxiv.org/pdf/2601.02359",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02359",
      "scraped_at": "2026-01-08T01:51:30.555236"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
    "paper_url": "https://huggingface.co/papers/2601.00581",
    "authors": [],
    "stars": "458",
    "details": {
      "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
      "abstract": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at this https URL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00581",
      "pdf_url": "https://arxiv.org/pdf/2601.00581",
      "github_links": [
        "https://github.com/torchmd/torchmd-net"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00581",
      "scraped_at": "2026-01-08T01:51:32.482242"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
    "paper_url": "https://huggingface.co/papers/2512.23950",
    "authors": [
      "Peng Li",
      "Yulong Xiao",
      "Mingzhe Liu",
      "Huibin Li",
      "FengShaner"
    ],
    "stars": "2",
    "details": {
      "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
      "abstract": "Title: DehazeSNN â€” U-Net-like Spiking Neural Networks for Single Image Dehazing Short summary: DehazeSNN integrates a U-Net architecture with Spiking Neural Networks to reduce compute while achieving competitive dehazing results. Code: github.com/HaoranLiu507/DehazeSNN. Highlights: U-Net + SNN design for lower MACs. OLIF block for improved cross-channel communication. Benchmarks show comparable or better dehazing with smaller model footprint.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23950",
      "pdf_url": "https://arxiv.org/pdf/2512.23950",
      "github_links": [
        "https://github.com/HaoranLiu507/DehazeSNN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23950",
      "scraped_at": "2026-01-08T01:51:34.324181"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01584",
    "authors": [
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "abstract": "This paper measures how easily â€œinstrumental-convergenceâ€ behaviors (e.g., shutdown avoidance, self-replication) in LLMs can be amplified or suppressed by simple steering, and argues that the common claim â€œas AI capability (often glossed as â€˜intelligenceâ€™) increases, systems inevitably become less controllableâ€ should not be treated as a default assumption. Using InstrumentalEval on Qwen3 (4B/30B; Base/Instruct/Thinking) with a GPT-5.2 judge, a short anti-instrumental prompt suffix drops convergence sharply (e.g., Qwen3-30B Instruct: 81.69% to 2.82%), while a pro-instrumental suffix pushes it high. The key takeaway is a safetyâ€“security dilemma for open weights: the same high steerability that helps builders enforce safe behavior can also help attackers elicit disallowed behavior, so widening the gap between authorized vs. unauthorized steerability remains a central open problem.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01584",
      "pdf_url": "https://arxiv.org/pdf/2601.01584",
      "github_links": [
        "https://github.com/j-hoscilowicz/instrumental_steering/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01584",
      "scraped_at": "2026-01-08T01:51:36.179871"
    },
    "scraped_date": "2026-01-08"
  }
]