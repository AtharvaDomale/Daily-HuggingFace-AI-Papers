[
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-05T01:59:59.631752"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLenâ€“based implementation for efficient concept-token cross-attention; (2) a decoupled Î¼P formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-05T02:00:01.852412"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "61",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-05T02:00:03.959198"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-05T02:00:05.988614"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-05T02:00:08.091511"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-05T02:00:10.042015"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "4",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESOâ€™s effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-05T02:00:11.922915"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.24615",
    "authors": [],
    "stars": "4.1k",
    "details": {
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "abstract": "LONG wait. Youtu-Agent ( https://github.com/TencentCloudADP/Youtu-agent ) now releases its technical report with two major updates, i.e., Automated Generation and Hybrid Policy Optimization. Additionally, we've launched Youtu-Tip ( https://github.com/TencentCloudADP/youtu-tip ), a more user-friendly application that runs on macOS. Check them out and have fun!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24615",
      "pdf_url": "https://arxiv.org/pdf/2512.24615",
      "github_links": [
        "https://github.com/TencentCloudADP/youtu-tip",
        "https://github.com/TencentCloudADP/youtu-agent",
        "https://github.com/TencentCloudADP/Youtu-agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24615",
      "scraped_at": "2026-01-06T01:50:51.163830"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2601.00393",
    "authors": [
      "Feng Wang",
      "Junran Peng",
      "renshengjihe",
      "Abyssaledge",
      "Yuppie1204"
    ],
    "stars": "124",
    "details": {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "abstract": "NeoVerse is a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. Project page: https://neoverse-4d.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00393",
      "pdf_url": "https://arxiv.org/pdf/2601.00393",
      "github_links": [
        "https://github.com/IamCreateAI/NeoVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00393",
      "scraped_at": "2026-01-06T01:50:53.205658"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "paper_url": "https://huggingface.co/papers/2601.00664",
    "authors": [
      "Sung Ju Hwang",
      "Jaehyeong Jo",
      "Sangwon Jang",
      "jaehong31",
      "taekyungki"
    ],
    "stars": "65",
    "details": {
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "abstract": "arXiv explained breakdown of this paper ðŸ‘‰ https://arxivexplained.com/papers/avatar-forcing-real-time-interactive-head-avatar-generation-for-natural-conversation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00664",
      "pdf_url": "https://arxiv.org/pdf/2601.00664",
      "github_links": [
        "https://github.com/TaekyungKi/AvatarForcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00664",
      "scraped_at": "2026-01-06T01:50:55.077779"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.24330",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "abstract": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24330",
      "pdf_url": "https://arxiv.org/pdf/2512.24330",
      "github_links": [
        "https://github.com/OpenSenseNova/SenseNova-MARS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24330",
      "scraped_at": "2026-01-06T01:50:56.907809"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24271",
    "authors": [],
    "stars": "29",
    "details": {
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "abstract": "An interesting work! github: https://github.com/AMAP-ML/Taming-Hallucinations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24271",
      "pdf_url": "https://arxiv.org/pdf/2512.24271",
      "github_links": [
        "https://github.com/AMAP-ML/Taming-Hallucinations"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24271",
      "scraped_at": "2026-01-06T01:50:58.822184"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.00796",
    "authors": [
      "Yu-Lun Liu",
      "Zhenjun Zhao",
      "Jiewen Chan"
    ],
    "stars": "0",
    "details": {
      "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
      "abstract": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00796",
      "pdf_url": "https://arxiv.org/pdf/2601.00796",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00796",
      "scraped_at": "2026-01-06T01:51:00.710791"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Deep Delta Learning",
    "paper_url": "https://huggingface.co/papers/2601.00417",
    "authors": [],
    "stars": "234",
    "details": {
      "title": "Deep Delta Learning",
      "abstract": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Î²(X). We provide a spectral analysis of this operator, demonstrating that the gate Î²(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00417",
      "pdf_url": "https://arxiv.org/pdf/2601.00417",
      "github_links": [
        "https://github.com/yifanzhang-pro/deep-delta-learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00417",
      "scraped_at": "2026-01-06T01:51:02.670038"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "paper_url": "https://huggingface.co/papers/2512.24695",
    "authors": [
      "Vahab Mirrokni",
      "Peilin Zhong",
      "Meisam Razaviyayn",
      "AliBehrouz"
    ],
    "stars": "0",
    "details": {
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "abstract": "Nested Learning (NL) is a new learning paradigm for continual learning and machine learning in general.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24695",
      "pdf_url": "https://arxiv.org/pdf/2512.24695",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24695",
      "scraped_at": "2026-01-06T01:51:04.593752"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.00747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "abstract": "For those of you interested in RLVR, here is a paper that formally characterizes the mechanism behind \"diversity collapse\" in reasoning models trained with scalar rewards (such as STaR, GRPO, and DPO). The paper introduces a variational framework based on Shahshahani gradient flow to prove that optimizing solely for correctness inherently erodes the diversity of reasoning paths, leading to a \"reasoning monoculture.\" To address this, they propose Distributional Creative Reasoning (DCR), which incorporates a diversity energy functional (using entropy and kernel-based novelty) into the objective, mathematically guaranteeing the maintenance of a diverse portfolio of successful reasoning strategies while still optimizing for utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00747",
      "pdf_url": "https://arxiv.org/pdf/2601.00747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00747",
      "scraped_at": "2026-01-06T01:51:06.385822"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
    "paper_url": "https://huggingface.co/papers/2512.22955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
      "abstract": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs).  The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode.  To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning.  By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision.  Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22955",
      "pdf_url": "https://arxiv.org/pdf/2512.22955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22955",
      "scraped_at": "2026-01-06T01:51:08.307456"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Fast-weight Product Key Memory",
    "paper_url": "https://huggingface.co/papers/2601.00671",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-weight Product Key Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Trellis: Learning to Compress Key-Value Memory in Attention Models (2025) TNT: Improving Chunkwise Training for Test-Time Memorization (2025) GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory (2025) MIDUS: Memory-Infused Depth Up-Scaling (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models (2025) BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00671",
      "pdf_url": "https://arxiv.org/pdf/2601.00671",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00671",
      "scraped_at": "2026-01-06T01:51:10.101023"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
    "paper_url": "https://huggingface.co/papers/2601.00575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "abstract": "Project Page: https://ishirgarg.github.io/infosynth_web/ Code: https://github.com/ishirgarg/infosynth Dataset: https://huggingface.co/datasets/ishirgarg/InfoSynth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00575",
      "pdf_url": "https://arxiv.org/pdf/2601.00575",
      "github_links": [
        "https://github.com/ishirgarg/infosynth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00575",
      "scraped_at": "2026-01-06T01:51:12.005175"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
    "paper_url": "https://huggingface.co/papers/2601.00204",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Hao Tang",
      "Zeyu Cai",
      "XiaokunSun"
    ],
    "stars": "19",
    "details": {
      "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
      "abstract": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io Code: https://github.com/XiaokunSun/MorphAny3D",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00204",
      "pdf_url": "https://arxiv.org/pdf/2601.00204",
      "github_links": [
        "https://github.com/XiaokunSun/MorphAny3D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00204",
      "scraped_at": "2026-01-06T01:51:13.858117"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "paper_url": "https://huggingface.co/papers/2512.20578",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
      "abstract": "Can Large Language Models predict their own failures? ðŸ§ âš¡ We all know the critical bottleneck in GenAI: LLMs are incredible, but they can confidently hallucinate and make mistakes. Until now, most fixes have been computationally massive â€” relying on expensive external judges, huge reward models, or costly training to make the LLM itself more robust. This brings us to two fundamental questions: â“ Do LLMs recognize when they're making mistakes? â“ Can we make them self-aware about their own failures? ðŸš€ Introducing Gnosis: A lightweight self-awareness mechanism for frozen LLMs. Named after the Greek word for knowledge/insight, Gnosis gives LLMs a form of introspection. We add only ~5M parameters to enable a frozen LLM to verify its own outputs by decoding internal hidden states + attention patterns during inference â€” with negligible overhead and no external judge . The results challenge the classic efficiencyâ€“accuracy trade-off: ðŸ† Superior performance across domains Despite being orders of magnitude smaller, Gnosis can outperform strong 8B reward models and proprietary judges like Gemini 2.5 Pro on both multi-step reasoning and factual/parametric knowledge QA (e.g., TriviaQA), across multiple backbones. âš¡ Real-time early failure detection Gnosis doesnâ€™t need to wait for the final token. By monitoring the generation trajectory in real time, it can predict an error before the model finishes â€” enabling early stopping, preventing bad outputs from reaching users, and saving significant compute. This suggests something important: the model often already contains signals of impending failure during generation â€” we just needed the right mechanism to read them. ðŸ‘‡ code + models: ðŸ’» Code: https://github.com/Amirhosein-gh98/Gnosis",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20578",
      "pdf_url": "https://arxiv.org/pdf/2512.20578",
      "github_links": [
        "https://github.com/Amirhosein-gh98/Gnosis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20578",
      "scraped_at": "2026-01-07T01:50:17.166637"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.02204",
    "authors": [],
    "stars": "60",
    "details": {
      "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02204",
      "pdf_url": "https://arxiv.org/pdf/2601.02204",
      "github_links": [
        "https://github.com/ByteVisionLab/NextFlow"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02204",
      "scraped_at": "2026-01-07T01:50:19.105092"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "K-EXAONE Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.01739",
    "authors": [],
    "stars": "39",
    "details": {
      "title": "K-EXAONE Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01739",
      "pdf_url": "https://arxiv.org/pdf/2601.01739",
      "github_links": [
        "https://github.com/LG-AI-EXAONE/K-EXAONE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01739",
      "scraped_at": "2026-01-07T01:50:21.012374"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.01425",
    "authors": [],
    "stars": "86",
    "details": {
      "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
      "abstract": "We introduce DreamID-V, the first Diffusion Transformer-based framework for high-fidelity video face swapping. DreamID-V bridges the gap between image and video domains, achieving exceptional identity similarity and temporal coherence even in challenging scenarios. Our code : https://github.com/bytedance/DreamID-V Our project : https://guoxu1233.github.io/DreamID-V/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01425",
      "pdf_url": "https://arxiv.org/pdf/2601.01425",
      "github_links": [
        "https://github.com/bytedance/DreamID-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01425",
      "scraped_at": "2026-01-07T01:50:22.967886"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2601.02256",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02256",
      "pdf_url": "https://arxiv.org/pdf/2601.02256",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02256",
      "scraped_at": "2026-01-07T01:50:24.885134"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
    "paper_url": "https://huggingface.co/papers/2512.24138",
    "authors": [
      "Zhiyong Wang",
      "Jiajun Liang",
      "Jie Liu",
      "Yuxiao Ye",
      "Haoran He"
    ],
    "stars": "18",
    "details": {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "abstract": "Introducing GARDO: Reinforcing Diffusion Models without Reward Hacking paper: https://arxiv.org/abs/2512.24138 code: https://github.com/tinnerhrhe/gardo project: https://tinnerhrhe.github.io/gardo_project/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24138",
      "pdf_url": "https://arxiv.org/pdf/2512.24138",
      "github_links": [
        "https://github.com/tinnerhrhe/GARDO",
        "https://github.com/tinnerhrhe/gardo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24138",
      "scraped_at": "2026-01-07T01:50:26.781382"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "paper_url": "https://huggingface.co/papers/2601.02358",
    "authors": [
      "Kun Gai",
      "Pengfei Wan",
      "Zhoujie Fu",
      "Tong He",
      "Junyi Chen"
    ],
    "stars": "42",
    "details": {
      "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02358",
      "pdf_url": "https://arxiv.org/pdf/2601.02358",
      "github_links": [
        "https://github.com/SOTAMak1r/VINO-code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02358",
      "scraped_at": "2026-01-07T01:50:28.658298"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "paper_url": "https://huggingface.co/papers/2601.02281",
    "authors": [],
    "stars": "76",
    "details": {
      "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
      "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively â€œrollingâ€ the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02281",
      "pdf_url": "https://arxiv.org/pdf/2601.02281",
      "github_links": [
        "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02281",
      "scraped_at": "2026-01-07T01:50:30.519682"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Recursive Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24601",
    "authors": [],
    "stars": "675",
    "details": {
      "title": "Recursive Language Models",
      "abstract": "Study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. They propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. They find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query. Some of the observations they found are :- -- LLMs interacting with their own prompts as objects. -- In their approach, a prompt isnâ€™t â€œrunâ€ directly, instead itâ€™s stored as a variable in an external Python REPL, and the language model writes code to inspect /slice/ decompose that long string, observes execution outputs, and then constructs sub-tasks where it recursively invokes an LLM on just the relevant snippets. Stitching the result together when the recursive process ends. So it can solve 10M+ token tasks with far less â€œcontext rotâ€ and often lower cost than summarization/RAG, turning long-context scaling into an inference-time algorithm rather than just a bigger context window. -- The ability to search the Prompt is what enables handling long context inputs, sub calls help handle information dense inputs. -- Inference cost of RLMs remain comparable to a base model call but are high variance  because it can keep making sub-calls or iterate if it can't solve the problem initially. -- The key insight is that long prompts should not be fed into the LLM directly, but should instead be treated as part of the environment that the LLM can search, read and interact with as needed for the task.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24601",
      "pdf_url": "https://arxiv.org/pdf/2512.24601",
      "github_links": [
        "https://github.com/alexzhang13/rlm/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24601",
      "scraped_at": "2026-01-07T01:50:32.438055"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "paper_url": "https://huggingface.co/papers/2601.02346",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent (2025) Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning (2025) CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions (2025) Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning (2025) Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02346",
      "pdf_url": "https://arxiv.org/pdf/2601.02346",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02346",
      "scraped_at": "2026-01-07T01:50:34.314010"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "paper_url": "https://huggingface.co/papers/2601.02356",
    "authors": [
      "Shuo Yang",
      "Jiarui Cai",
      "Yantao Shen",
      "ZyZcuhk",
      "jingtan"
    ],
    "stars": "13",
    "details": {
      "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) Loom: Diffusion-Transformer for Interleaved Generation (2025) What Happens Next? Next Scene Prediction with a Unified Video Model (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02356",
      "pdf_url": "https://arxiv.org/pdf/2601.02356",
      "github_links": [
        "https://github.com/sparkstj/Talk2Move"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02356",
      "scraped_at": "2026-01-07T01:50:36.191628"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "paper_url": "https://huggingface.co/papers/2601.02179",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "abstract": "In this paper, we explore the confidence estimation in a new paradigm: multi-turn interactions! Check it out!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02179",
      "pdf_url": "https://arxiv.org/pdf/2601.02179",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02179",
      "scraped_at": "2026-01-07T01:50:38.040156"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01046",
    "authors": [
      "Yi Yang",
      "Yixuan Tang"
    ],
    "stars": "0",
    "details": {
      "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
      "abstract": "âœ¨ Turn any decoder-only LLM into a powerful embedding modelâ€”zero training needed! âœ¨ The Trick : Re-route the final token's key-value states as an internal prefix, giving all tokens access to global context in one forward pass. No input modification, no mask removal, just smart internal state manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01046",
      "pdf_url": "https://arxiv.org/pdf/2601.01046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01046",
      "scraped_at": "2026-01-07T01:50:39.891420"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2601.00501",
    "authors": [
      "Mohammad Asiful Hossain",
      "Kevin Cannons",
      "Saeed Ranjbar Alvar",
      "Mohsen Gholami",
      "Ahmad Rezaei"
    ],
    "stars": "0",
    "details": {
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "abstract": "CPPO: Contrastive Perception for Vision Language Policy Optimization introduces a new method (CPPO) for fine-tuning vision-language models (VLMs) using reinforcement learning. Instead of relying on explicit perception rewards or auxiliary models, the approach identifies perceptual tokens via entropy changes under perturbed images and augments the policy objective with a contrastive perception loss to improve multimodal reasoning performance and training efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00501",
      "pdf_url": "https://arxiv.org/pdf/2601.00501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00501",
      "scraped_at": "2026-01-07T01:50:41.758090"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "paper_url": "https://huggingface.co/papers/2601.02267",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Zhenyu Zhang",
      "wrk226"
    ],
    "stars": "1",
    "details": {
      "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
      "abstract": "Project page: https://wrk226.github.io/DiffProxy.html Code: https://github.com/wrk226/DiffProxy",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02267",
      "pdf_url": "https://arxiv.org/pdf/2601.02267",
      "github_links": [
        "https://github.com/wrk226/DiffProxy"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02267",
      "scraped_at": "2026-01-07T01:50:43.650977"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01836",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "abstract": "COMPASS is the first framework for evaluating LLM alignment with organization-specific policies rather than universal harms. While models handle legitimate requests well (>95% accuracy), they catastrophically fail at enforcing prohibitions, refusing only 13-40% of denylist violations. GitHub: https://github.com/AIM-Intelligence/COMPASS",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01836",
      "pdf_url": "https://arxiv.org/pdf/2601.01836",
      "github_links": [
        "https://github.com/AIM-Intelligence/COMPASS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01836",
      "scraped_at": "2026-01-07T01:50:45.620954"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
    "paper_url": "https://huggingface.co/papers/2512.23035",
    "authors": [
      "Shiying Wang",
      "Kai Li",
      "Shun Zhang",
      "Xuechao Zou",
      "Yi Zhou"
    ],
    "stars": "4",
    "details": {
      "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
      "abstract": "We are excited to introduce our latest work on semi-supervised semantic segmentation : ðŸ“„ Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion This paper tackles one of the most challenging issues in semi-supervised segmentation: pseudo-label drift . When labeled data are extremely scarce, self-training methods are prone to deterministic bias, where early incorrect pseudo-labels accumulate over time, leading to unstable and degraded training. ðŸ§  Motivation Most existing consistency- or pseudo-labelâ€“based semi-supervised approaches rely heavily on self-generated supervision . Once early pseudo-labels become unreliable, error accumulation is inevitable. Our goal is to introduce stronger semantic priors to correct such drift and stabilize the training process. âœ¨ Key Contributions 1ï¸âƒ£ Heterogeneous Dual-Student Framework We leverage two complementary vision foundation modelsâ€” CLIP for global semantic priors and DINOv3 for fine-grained local structuresâ€”to enable stable mutual learning and suppress error accumulation. 2ï¸âƒ£ Explicitâ€“Implicit Semantic Co-Guidance By jointly utilizing text embeddings (explicit semantics) and learnable queries (implicit semantics), we provide class-level semantic anchors and enhance semantic consistency. 3ï¸âƒ£ Globalâ€“Local Feature Co-Fusion We fuse CLIPâ€™s global contextual understanding with DINOv3â€™s local structural details, yielding more accurate and stable segmentation results. ðŸ“Š Experimental Results Extensive evaluations on six mainstream remote sensing benchmarks demonstrate that Co2S consistently achieves strong and stable performance across different data splits and scenarios, especially under extremely low annotation budgets . ðŸ“¦ Open-Source Resources arXiv Paper : https://arxiv.org/abs/2512.23035 Project Page : https://xavierjiezou.github.io/Co2S/ GitHub Code : https://github.com/XavierJiezou/Co2S HuggingFace Models : https://huggingface.co/XavierJiezou/co2s-models HuggingFace Datasets : https://huggingface.co/datasets/XavierJiezou/co2s-datasets #Remote Sensing #Semantic Segmentation #Semi-Supervised Learning #Vision Foundation Models #CLIP #DINOv3",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23035",
      "pdf_url": "https://arxiv.org/pdf/2512.23035",
      "github_links": [
        "https://github.com/XavierJiezou/Co2S"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23035",
      "scraped_at": "2026-01-07T01:50:47.571407"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "paper_url": "https://huggingface.co/papers/2601.01426",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01426",
      "pdf_url": "https://arxiv.org/pdf/2601.01426",
      "github_links": [
        "https://github.com/SWE-Lego/SWE-Lego"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01426",
      "scraped_at": "2026-01-07T01:50:49.438589"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
    "paper_url": "https://huggingface.co/papers/2601.01576",
    "authors": [
      "Chunchun Ma",
      "Yujiong Shen",
      "Yueyuan Huang",
      "Kexin Tan",
      "Ming Zhang"
    ],
    "stars": "3",
    "details": {
      "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation (2025) SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning (2025) WisPaper: Your AI Scholar Search Engine (2025) AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis (2025) OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists (2025) AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement (2025) Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01576",
      "pdf_url": "https://arxiv.org/pdf/2601.01576",
      "github_links": [
        "https://github.com/january-blue/OpenNovelty"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01576",
      "scraped_at": "2026-01-07T01:50:51.251342"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
    "paper_url": "https://huggingface.co/papers/2601.00863",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
      "abstract": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00863",
      "pdf_url": "https://arxiv.org/pdf/2601.00863",
      "github_links": [
        "https://github.com/lamm-mit/MusicAnalysis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00863",
      "scraped_at": "2026-01-07T01:50:53.135282"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
    "paper_url": "https://huggingface.co/papers/2512.21472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "abstract": "âœ¨ The largest publicly available dermoscopic skin lesion segmentation dataset with 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image. âœ¨ 16 unique annotators , 3 different tools used, and 2 skill levels of the manual reviewer. âœ¨ Contains consensus masks for the 2,394 images that have multi-annotator segmentations (2-5 segmentations per image). âœ¨ Collected and curated from the ISIC Archive .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21472",
      "pdf_url": "https://arxiv.org/pdf/2512.21472",
      "github_links": [
        "https://github.com/sfu-mial/IMAplusplus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21472",
      "scraped_at": "2026-01-07T01:50:55.025985"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "paper_url": "https://huggingface.co/papers/2601.02315",
    "authors": [
      "Beth Tellman",
      "Lalit Maurya",
      "Saurabh Kaushik"
    ],
    "stars": "3",
    "details": {
      "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
      "abstract": "Despite the recent success of large pretrained encoders (Geoâ€‘Foundation Models), we consistently observe that Uâ€‘Netâ€‘based models remain highly competitiveâ€”and in some cases outperform transformers, particularly due to their strength in capturing local spatial nuances. Motivated by this, we propose Prithviâ€‘CAFE (Prithviâ€‘Complementary Adaptive Fusion Encoder), which enhances local representations through complementary fusion with a CNNâ€‘based encoder. We evaluate our approach on two major flood datasetsâ€”FloodPlanet and Sen1Floods11â€”and achieve stateâ€‘ofâ€‘theâ€‘art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02315",
      "pdf_url": "https://arxiv.org/pdf/2601.02315",
      "github_links": [
        "https://github.com/Sk-2103/Prithvi-CAFE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02315",
      "scraped_at": "2026-01-07T01:50:56.844097"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.02314",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "abstract": "Does COT in llms stay faithful to their thoughts?",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02314",
      "pdf_url": "https://arxiv.org/pdf/2601.02314",
      "github_links": [
        "https://github.com/skhanzad/AridadneXAI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02314",
      "scraped_at": "2026-01-07T01:50:58.682941"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.22877",
    "authors": [
      "Jun-Cheng Chen",
      "Cheng-Fu Chou",
      "Ju-Hsuan Weng",
      "jwliao1209"
    ],
    "stars": "0",
    "details": {
      "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
      "abstract": "Concept Erasure Benchmark",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22877",
      "pdf_url": "https://arxiv.org/pdf/2512.22877",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22877",
      "scraped_at": "2026-01-07T01:51:00.532894"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "paper_url": "https://huggingface.co/papers/2601.03252",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
      "abstract": "Depth Beyond Pixels ðŸš€ We Introduce InfiniDepth â€” casting monocular depth estimation as a neural implicit field. ðŸ” Arbitrary-Resolution ðŸ“ Accurate Metric Depth ðŸ“· Single-View NVS under large viewpoints shifts Arxiv: https://arxiv.org/abs/2601.03252 page: https://zju3dv.github.io/InfiniDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03252",
      "pdf_url": "https://arxiv.org/pdf/2601.03252",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03252",
      "scraped_at": "2026-01-08T01:50:45.247652"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "paper_url": "https://huggingface.co/papers/2601.01554",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
      "abstract": "MOSS Transcribe Diarize ðŸŽ™ï¸ We introduce MOSS Transcribe Diarize â€” a unified multimodal model for Speaker-Attributed, Time-Stamped Transcription (SATS) . ðŸ” End-to-end SATS in a single pass (transcription + speaker attribution + timestamps) ðŸ§  128k context window for up to ~90-minute audio without chunking (strong long-range speaker memory) ðŸŒ Trained on extensive in-the-wild conversations + controllable simulated mixtures (robust to overlap/noise/domain shift) ðŸ“Š Strong results on AISHELL-4 / Podcast / Movies benchmarks (best cpCER / Î”cp among evaluated systems) Paper: [2601.01554] MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization Homepage: https://mosi.cn/models/moss-transcribe-diarize Online Demo: https://moss-transcribe-diarize-demo.mosi.cn",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01554",
      "pdf_url": "https://arxiv.org/pdf/2601.01554",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01554",
      "scraped_at": "2026-01-08T01:50:47.162285"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "paper_url": "https://huggingface.co/papers/2601.03233",
    "authors": [
      "kvochko",
      "jacobitterman",
      "nisan",
      "benibraz",
      "yoavhacohen"
    ],
    "stars": "922",
    "details": {
      "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API 3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation (2025) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation (2025) MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning (2026) DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation (2025) JoVA: Unified Multimodal Learning for Joint Video-Audio Generation (2025) In-Context Audio Control of Video Diffusion Transformers (2025) JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03233",
      "pdf_url": "https://arxiv.org/pdf/2601.03233",
      "github_links": [
        "https://github.com/Lightricks/LTX-2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03233",
      "scraped_at": "2026-01-08T01:50:49.139726"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.22334",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "abstract": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22334",
      "pdf_url": "https://arxiv.org/pdf/2512.22334",
      "github_links": [
        "https://github.com/InternScience/SciEvalKit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22334",
      "scraped_at": "2026-01-08T01:50:51.230691"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "paper_url": "https://huggingface.co/papers/2601.03193",
    "authors": [
      "Lin-Chen",
      "lovesnowbest",
      "YuZeng260",
      "CostaliyA",
      "Hungryyan"
    ],
    "stars": "25",
    "details": {
      "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
      "abstract": "UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03193",
      "pdf_url": "https://arxiv.org/pdf/2601.03193",
      "github_links": [
        "https://github.com/Hungryyan1/UniCorn"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03193",
      "scraped_at": "2026-01-08T01:50:53.221654"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "paper_url": "https://huggingface.co/papers/2601.02427",
    "authors": [],
    "stars": "1.44k",
    "details": {
      "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
      "abstract": "NitroGen is a vision-action foundation model trained on 40k hours of gameplay across 1,000+ games, enabling cross-game generalization with behavior cloning and benchmarking, achieving strong unseen-game transfer.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02427",
      "pdf_url": "https://arxiv.org/pdf/2601.02427",
      "github_links": [
        "https://github.com/MineDojo/NitroGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02427",
      "scraped_at": "2026-01-08T01:50:55.235135"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2601.03044",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "abstract": "ðŸš€ Website: https://www.agibot.com/research/sop We introduce SOP for online post-training of generalist VLAs in the real world â€” unlocking persistent, reliable deployment of generalist robots in physical environments. ðŸ” 36 hours of continuous cloth folding: video ðŸ“¦ 36 hours of continuous cardboard box assembly: video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03044",
      "pdf_url": "https://arxiv.org/pdf/2601.03044",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03044",
      "scraped_at": "2026-01-08T01:50:57.176273"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "paper_url": "https://huggingface.co/papers/2601.02785",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamStyle: A Unified Framework for Video Stylization",
      "abstract": "DreamStyle unifies text-, style-image-, and first-frame-guided video stylization on an I2V backbone, using LoRA with token-specific up matrices to improve style consistency and video quality.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02785",
      "pdf_url": "https://arxiv.org/pdf/2601.02785",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02785",
      "scraped_at": "2026-01-08T01:50:59.158844"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MiMo-V2-Flash Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.02780",
    "authors": [],
    "stars": "957",
    "details": {
      "title": "MiMo-V2-Flash Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Xiaomi MiMo-VL-Miloco Technical Report (2025) Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning (2025) Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks (2025) AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing (2025) NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations (2025) Practical Policy Distillation for Reinforcement Learning in Radio Access Networks (2025) Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02780",
      "pdf_url": "https://arxiv.org/pdf/2601.02780",
      "github_links": [
        "https://github.com/XiaomiMiMo/MiMo-V2-Flash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02780",
      "scraped_at": "2026-01-08T01:51:01.099679"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.01874",
    "authors": [
      "Aojun Lu",
      "Junjie Xie",
      "Shuhang Chen",
      "JacobYuan",
      "Yunqiu"
    ],
    "stars": "0",
    "details": {
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "abstract": "Project page: https://shchen233.github.io/cogflow/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01874",
      "pdf_url": "https://arxiv.org/pdf/2601.01874",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01874",
      "scraped_at": "2026-01-08T01:51:03.012607"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "paper_url": "https://huggingface.co/papers/2601.01321",
    "authors": [
      "Yao Su",
      "vztu",
      "ZihanJia",
      "fjchendp",
      "roz322"
    ],
    "stars": "2",
    "details": {
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "abstract": "This paper systematically analyzes AI integration in Digital Twins through a four-stage framework (modeling â†’ mirroring â†’ intervention â†’ autonomous management), covering LLMs, foundation models, world models, and intelligent agents across 11 application domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01321",
      "pdf_url": "https://arxiv.org/pdf/2601.01321",
      "github_links": [
        "https://github.com/rongzhou7/Awesome-Digital-Twin-AI/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01321",
      "scraped_at": "2026-01-08T01:51:04.924547"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "paper_url": "https://huggingface.co/papers/2601.02439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "abstract": "WebGym creates a large, non-stationary visual web task suite and scalable RL pipeline, enabling fast trajectory rollout and improved vision-language agent performance on unseen websites.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02439",
      "pdf_url": "https://arxiv.org/pdf/2601.02439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02439",
      "scraped_at": "2026-01-08T01:51:06.769414"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
    "paper_url": "https://huggingface.co/papers/2601.02989",
    "authors": [
      "Fatemeh Askari",
      "Sadegh Mohammadian",
      "Mohammadali Banayeeanzade",
      "Hosein Hasani",
      "safinal"
    ],
    "stars": "0",
    "details": {
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "abstract": "ðŸ”¢ Overcoming Transformer Depth Limits in Counting Tasks LLMs often fail at counting not because they aren't smart, but because of architectural depth constraints ðŸš§. We propose a simple, effective System-2 strategy ðŸ§© that decomposes counting tasks to bypass these limits. ðŸ”¬ We also provide a full mechanistic interpretation , identifying the specific attention heads and representations responsible for transferring \"latent counts\" across the network. ðŸ“ˆ This approach allows LLMs to achieve high accuracy on large-scale counting benchmarks where they typically fail.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02989",
      "pdf_url": "https://arxiv.org/pdf/2601.02989",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02989",
      "scraped_at": "2026-01-08T01:51:08.644226"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "paper_url": "https://huggingface.co/papers/2601.03256",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
      "abstract": "Project page: https://luhexiao.github.io/Muses.github.io/ Code: https://github.com/luhexiao/Muses",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03256",
      "pdf_url": "https://arxiv.org/pdf/2601.03256",
      "github_links": [
        "https://github.com/luhexiao/Muses"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03256",
      "scraped_at": "2026-01-08T01:51:10.461072"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "paper_url": "https://huggingface.co/papers/2601.01720",
    "authors": [
      "Donghao Luo",
      "yanweifuture",
      "chengjie-wang",
      "ChengmingX",
      "ScarletAce"
    ],
    "stars": "0",
    "details": {
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) Unified Video Editing with Temporal Reasoner (2025) VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning (2025) EasyV2V: A High-quality Instruction-based Video Editing Framework (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01720",
      "pdf_url": "https://arxiv.org/pdf/2601.01720",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01720",
      "scraped_at": "2026-01-08T01:51:12.354426"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01592",
    "authors": [
      "Yang Yao",
      "Yixu Wang",
      "Juncheng Li",
      "Yunhao Chen",
      "xinwang22"
    ],
    "stars": "112",
    "details": {
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "abstract": "Even State-of-the-Art Models Fail to Hold Ground Against Sophisticated Adversaries. Our comprehensive evaluation highlights two key findings. (1) A clear stratification in defense capability: Top-tier models such as Claude Haiku 4.5, GPT-5.2, and Qwen3-Max exhibit strong baseline robustness, effectively neutralizing static, template-based attacks and complex logic traps, often keeping ASR below 20%.This suggests that leading labs have improved defenses against recognizable, repeatable jailbreak structures, while several models (e.g., Llama-4, Mistral Large 3) remain more susceptible to these simpler patterns. (2) A shift in the attack landscape: adaptive, multi-turn, and multi-agent strategies dominate, whereas static, single-turn, and template-based approaches are increasingly ineffective. Methods like EvoSynth and X-Teaming can achieve >90% ASR even against advanced models. This indicates current safety training overfits to static templates, failing to generalize against the broad attack surface exposed by automated red-teaming. Adversarial Robustness Exhibits Inconsistent and Polarized Vulnerability Patterns. We observe a polarization effect where models demonstrate high resistance to specific attack families (e.g., text-based cipher) yet remain completely defenseless against others (e.g., logic nesting). For instance, Grok 4.1 Fast shows 1.5% ASR against RedQueen but 90.5% against X-Teaming. This stark performance disparity (~90%) underscores that current defenses are often patch-based rather than holistic, necessitating the multi-faceted evaluation provided by OpenRT. Enhanced Reasoning and Multimodal Capabilities are New Vectors for Exploitation. Contrary to the common assumption that more capable models are inherently safer, we find that enhanced capabilities often introduce new vectors for exploitation. Reasoning-enhanced models (CoT) do not demonstrate superior robustness; instead, their verbose reasoning processes can be manipulated to bypass safety filters. Similarly, Multimodal LLMs exhibit a critical modality gap: visual inputs frequently bypass text-based safety mechanisms, allowing cross-modal attacks to compromise models that are otherwise robust to purely textual jailbreaks. These findings suggest that current safety alignment has not kept pace with the architectural expansion of model capabilities. Proprietary Models Can Be as Vulnerable as Open-Source Models Under Certain Attacks. Our analysis reveals that proprietary and open-source models exhibit comparable susceptibility to our attack suite. Across our 20 evaluated models, only GPT-5.2 and Claude Haiku 4.5 maintained an average ASR below 30%, while all other models consistently exceeded this threshold. This universality sharply contradicts the assumption that closed deployments offer superior protection, demonstrating that the safety through obscurity of proprietary strategies fails to provide any tangible mitigation against sophisticated adversarial attacks. Scaling MLLMs Robustness via Defense-in-Depth and Continuous Red Teaming. Challenges such as polarized robustness, weak generalization to unseen attacks, and cross-modal bypasses highlight the limits of single-layer defense. Effective mitigation requires a paradigm shift toward Defense-in-Depth: integrating intrinsic architectural safety with runtime risk estimation and adversarial training on multimodal and multi-turn interactions. Crucially, continuous Red Teaming via infrastructure like OpenRT provides systematic evaluation to verify empirical robustness and prevent benchmark overfitting.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01592",
      "pdf_url": "https://arxiv.org/pdf/2601.01592",
      "github_links": [
        "https://github.com/AI45Lab/OpenRT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01592",
      "scraped_at": "2026-01-08T01:51:14.181550"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.23412",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
      "abstract": "In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows the model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL. The agent reasoning framework, MWE-Bench, three smaller-scale agent models (2B, 3B, and 4B) distilled from MindWatcher 32B, and related resources will be open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23412",
      "pdf_url": "https://arxiv.org/pdf/2512.23412",
      "github_links": [
        "https://github.com/TIMMY-CHAN/MindWatcher"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23412",
      "scraped_at": "2026-01-08T01:51:16.037424"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "paper_url": "https://huggingface.co/papers/2601.03227",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "abstract": "We found the sonar moment in audio language models. We propose the task of audio geo-localization. And amazingly, Gemini 3 Pro can reach the distance error of less than 55km for 25%  samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03227",
      "pdf_url": "https://arxiv.org/pdf/2601.03227",
      "github_links": [
        "https://github.com/Rising0321/AGL1K"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03227",
      "scraped_at": "2026-01-08T01:51:21.077717"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "paper_url": "https://huggingface.co/papers/2601.03194",
    "authors": [
      "Sai Rithwik Reddy Chirra",
      "Shashivardhan Reddy Koppula",
      "Mohammad Zia Ur Rehman",
      "shwetankssingh",
      "UVSKKR"
    ],
    "stars": "0",
    "details": {
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (explainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03194",
      "pdf_url": "https://arxiv.org/pdf/2601.03194",
      "github_links": [
        "https://github.com/ziarehman30/X-MuTeST"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03194",
      "scraped_at": "2026-01-08T01:51:23.122431"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Parallel Latent Reasoning for Sequential Recommendation",
    "paper_url": "https://huggingface.co/papers/2601.03153",
    "authors": [
      "Yuning Jiang",
      "Jian Wu",
      "Wen Chen",
      "Xu Chen",
      "TangJiakai5704"
    ],
    "stars": "0",
    "details": {
      "title": "Parallel Latent Reasoning for Sequential Recommendation",
      "abstract": "Parallel Latent Reasoning (PLR): Sequential Recommendation with Parallel Reasoning ðŸ”¥ ðŸ“‰ Depth-only reasoning often hits performance plateausâ€”PLR mitigates this with parallel latent reasoning. Core Innovation âœ¨ ðŸŽ¯ Learnable trigger tokens: Build parallel streams in continuous latent space. ðŸ”„ Global regularization: Preserve stream diversity to avoid redundancy. âš–ï¸ Adaptive aggregation: Smartly combine multi-stream insights for optimal results. Key Advantages ðŸš€ ðŸ“Š Outperforms SOTA baselines (SASRec, BERT4Rec, ReaRec, LRESA) by 5.5%â€“14.9% on Recall@10/20 and NDCG@10/20 across three real-world datasets. âš¡ Real-time efficiency: Only 5.8% latency increase vs. base models, enabled by KV Caching and GPU parallelism. ðŸ›¡ï¸ Strong robustness: Maintains top performance even with 30% missing user interactions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03153",
      "pdf_url": "https://arxiv.org/pdf/2601.03153",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03153",
      "scraped_at": "2026-01-08T01:51:25.012089"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03127",
    "authors": [
      "Yue Cao",
      "Hanqing Yang",
      "Jijin Hu",
      "Qiang Zhou",
      "Sashuai Zhou"
    ],
    "stars": "0",
    "details": {
      "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
      "abstract": "reasoning-based image generation and editing",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03127",
      "pdf_url": "https://arxiv.org/pdf/2601.03127",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03127",
      "scraped_at": "2026-01-08T01:51:26.836393"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
    "paper_url": "https://huggingface.co/papers/2601.02996",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
      "abstract": "https://github.com/cisnlp/multilingual-latent-reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02996",
      "pdf_url": "https://arxiv.org/pdf/2601.02996",
      "github_links": [
        "https://github.com/cisnlp/multilingual-latent-reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02996",
      "scraped_at": "2026-01-08T01:51:28.643997"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "paper_url": "https://huggingface.co/papers/2601.02359",
    "authors": [
      "Vladislav Golyanik",
      "Toshihiko Yamasaki",
      "mapooon"
    ],
    "stars": "0",
    "details": {
      "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
      "abstract": "Detecting deepfakes with generative AI. We introduce ExposeAnyone â€” a paradigm shift in face forgery detection! ðŸ”ï¸ Fully self-supervised approach ðŸ¥‡ Best average AUC on traditional deepfake benchmarks ðŸ’ª Best AUC even on Sora2 by OpenAI ðŸ’¢ Strong Robustness to common corruptions such as JPEG/MPEG compression Arxiv: https://arxiv.org/abs/2601.02359 Project page: https://mapooon.github.io/ExposeAnyonePage/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02359",
      "pdf_url": "https://arxiv.org/pdf/2601.02359",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02359",
      "scraped_at": "2026-01-08T01:51:30.555236"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
    "paper_url": "https://huggingface.co/papers/2601.00581",
    "authors": [],
    "stars": "458",
    "details": {
      "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
      "abstract": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at this https URL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00581",
      "pdf_url": "https://arxiv.org/pdf/2601.00581",
      "github_links": [
        "https://github.com/torchmd/torchmd-net"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00581",
      "scraped_at": "2026-01-08T01:51:32.482242"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
    "paper_url": "https://huggingface.co/papers/2512.23950",
    "authors": [
      "Peng Li",
      "Yulong Xiao",
      "Mingzhe Liu",
      "Huibin Li",
      "FengShaner"
    ],
    "stars": "2",
    "details": {
      "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
      "abstract": "Title: DehazeSNN â€” U-Net-like Spiking Neural Networks for Single Image Dehazing Short summary: DehazeSNN integrates a U-Net architecture with Spiking Neural Networks to reduce compute while achieving competitive dehazing results. Code: github.com/HaoranLiu507/DehazeSNN. Highlights: U-Net + SNN design for lower MACs. OLIF block for improved cross-channel communication. Benchmarks show comparable or better dehazing with smaller model footprint.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23950",
      "pdf_url": "https://arxiv.org/pdf/2512.23950",
      "github_links": [
        "https://github.com/HaoranLiu507/DehazeSNN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23950",
      "scraped_at": "2026-01-08T01:51:34.324181"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01584",
    "authors": [
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "abstract": "This paper measures how easily â€œinstrumental-convergenceâ€ behaviors (e.g., shutdown avoidance, self-replication) in LLMs can be amplified or suppressed by simple steering, and argues that the common claim â€œas AI capability (often glossed as â€˜intelligenceâ€™) increases, systems inevitably become less controllableâ€ should not be treated as a default assumption. Using InstrumentalEval on Qwen3 (4B/30B; Base/Instruct/Thinking) with a GPT-5.2 judge, a short anti-instrumental prompt suffix drops convergence sharply (e.g., Qwen3-30B Instruct: 81.69% to 2.82%), while a pro-instrumental suffix pushes it high. The key takeaway is a safetyâ€“security dilemma for open weights: the same high steerability that helps builders enforce safe behavior can also help attackers elicit disallowed behavior, so widening the gap between authorized vs. unauthorized steerability remains a central open problem.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01584",
      "pdf_url": "https://arxiv.org/pdf/2601.01584",
      "github_links": [
        "https://github.com/j-hoscilowicz/instrumental_steering/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01584",
      "scraped_at": "2026-01-08T01:51:36.179871"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
    "paper_url": "https://huggingface.co/papers/2601.02151",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
      "abstract": "ðŸ’» Code: https://github.com/PRIS-CV/EAFT âœ¨ Project Page: https://ymxyll.github.io/EAFT/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02151",
      "pdf_url": "https://arxiv.org/pdf/2601.02151",
      "github_links": [
        "https://github.com/hiyouga/LLaMA-Factory",
        "https://github.com/PRIS-CV/EAFT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02151",
      "scraped_at": "2026-01-09T01:51:09.218763"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Evolving Programmatic Skill Networks",
    "paper_url": "https://huggingface.co/papers/2601.03509",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evolving Programmatic Skill Networks",
      "abstract": "We study continual skill acquisition in openended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1) REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSNâ€™s learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03509",
      "pdf_url": "https://arxiv.org/pdf/2601.03509",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03509",
      "scraped_at": "2026-01-09T01:51:11.028376"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.03872",
    "authors": [
      "Yuhao Shen",
      "Jiahao Yuan",
      "Ruihan Jin",
      "Guocheng Zhai",
      "Jinyang23"
    ],
    "stars": "0",
    "details": {
      "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
      "abstract": "ðŸš€ [New Paper] Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning The growing diversity of LLMs and external tools presents a significant challenge: how to select the optimal model-tool combination for complex reasoning tasks. Existing methods often fall short by relying on single models or fixed tool-calling logic. ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation) addresses this by introducing a dual-path framework for dynamic model-tool alignment and invocation across multiple domains. âœ¨ The Core Intuition: ATLAS employs a dual-path approach to achieve dynamic model-tool alignment and invocation: 1ï¸âƒ£ Training-Free Cluster-Based Routing: This path leverages empirical priors for domain-specific alignment, efficiently guiding the model-tool selection process. 2ï¸âƒ£ RL-Based Multi-Step Routing: This path explores autonomous trajectories to achieve strong generalization, particularly for out-of-distribution tasks. ðŸ“ˆ Highlights: Superior Performance: ATLAS significantly outperforms closed-source models like GPT-4o and existing routing methods, achieving +10.1% on in-distribution tasks and +13.1% on out-of-distribution tasks across 15 benchmarks. Enhanced Visual Reasoning: The framework demonstrates substantial improvements in visual reasoning by effectively orchestrating specialized multi-modal tools. Adaptive Orchestration: ATLAS learns to assess its internal state and dynamically invoke external resources, internalizing the alignment between domains and tool utilization. Robust and Generalizable: The design ensures that the routing policy effectively captures expertise distribution, making it robust and generalizable even as tools and models evolve.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03872",
      "pdf_url": "https://arxiv.org/pdf/2601.03872",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03872",
      "scraped_at": "2026-01-09T01:51:12.875928"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
    "paper_url": "https://huggingface.co/papers/2601.03986",
    "authors": [
      "Muling Wu",
      "Changze Lv",
      "Jingwen Xu",
      "Qi Qian",
      "ChengsongHuang"
    ],
    "stars": "0",
    "details": {
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03986",
      "pdf_url": "https://arxiv.org/pdf/2601.03986",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03986",
      "scraped_at": "2026-01-09T01:51:14.732608"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
    "paper_url": "https://huggingface.co/papers/2601.03822",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
      "abstract": "ROI-Reasoning introduces a principled framework for budget-aware inference-time reasoning in large language models. Instead of blindly scaling computation, the authors formulate multi-task reasoning under a global token constraint as an Ordered Stochastic Multiple-Choice Knapsack Problem, explicitly modeling the trade-off between reasoning cost and expected utility. The proposed two-stage approach combines Meta-Cognitive Fine-Tuning, which enables models to anticipate difficulty and make solve-or-skip decisions before reasoning, with Rationality-Aware Reinforcement Learning, which optimizes long-horizon computation allocation under strict budgets. Across challenging mathematical reasoning benchmarks, ROI-Reasoning consistently improves total score and substantially reduces regretâ€”demonstrating that meta-cognitive planning, not just stronger reasoning, is key to efficient test-time scaling of LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03822",
      "pdf_url": "https://arxiv.org/pdf/2601.03822",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03822",
      "scraped_at": "2026-01-09T01:51:16.648836"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "paper_url": "https://huggingface.co/papers/2601.04151",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
      "abstract": "Klear: 26B model for joint audio-video generation Single-tower DiT with \"Omni-Full Attention\" across video, audio, and text Progressive multi-task training (T2V, T2A, T2AV, I2V all in one model) 81M sample dataset with dense captions Claims Veo 3-level performance on lip-sync & AV consistency",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04151",
      "pdf_url": "https://arxiv.org/pdf/2601.04151",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04151",
      "scraped_at": "2026-01-09T01:51:18.670540"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Choreographing a World of Dynamic Objects",
    "paper_url": "https://huggingface.co/papers/2601.04194",
    "authors": [
      "Hadi Alzayer",
      "Yunzhi Zhang",
      "Karthik Dharmarajan",
      "Chen Geng",
      "Yanzhe Lyu"
    ],
    "stars": "0",
    "details": {
      "title": "Choreographing a World of Dynamic Objects",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Animus3D: Text-driven 3D Animation via Motion Score Distillation (2025) AnimaMimic: Imitating 3D Animation from Video Priors (2025) Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions (2025) DIMO: Diverse 3D Motion Generation for Arbitrary Objects (2025) Inferring Compositional 4D Scenes without Ever Seeing One (2025) SS4D: Native 4D Generative Model via Structured Spacetime Latents (2025) WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04194",
      "pdf_url": "https://arxiv.org/pdf/2601.04194",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04194",
      "scraped_at": "2026-01-09T01:51:20.596669"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "paper_url": "https://huggingface.co/papers/2601.04171",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
      "abstract": "Agentic Rubrics for verifying SWE agent patches WITHOUT running tests! An agent explores the codebase to generate context-grounded checklists, then scores patches execution-free. Rubrics provide dense, interpretable reward signals that could scale RL training for coding agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04171",
      "pdf_url": "https://arxiv.org/pdf/2601.04171",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04171",
      "scraped_at": "2026-01-09T01:51:22.502781"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
    "paper_url": "https://huggingface.co/papers/2601.02075",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "abstract": "project: https://github.com/FredericVAN/PKU_MDAgent2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02075",
      "pdf_url": "https://arxiv.org/pdf/2601.02075",
      "github_links": [
        "https://github.com/FredericVAN/PKU_MDAgent2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02075",
      "scraped_at": "2026-01-09T01:51:24.349527"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
    "paper_url": "https://huggingface.co/papers/2601.00423",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
      "abstract": "We propose an entropy aware Group Relative Policy Optimization (E-GRPO) to increase the entropy of SDE sampling steps. We have integrated a variety of current GRPO-based reinforcement learning methods as well as different image reward models. Code: https://github.com/shengjun-zhang/VisualGRPO Model: https://huggingface.co/studyOverflow/E-GRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00423",
      "pdf_url": "https://arxiv.org/pdf/2601.00423",
      "github_links": [
        "https://github.com/shengjun-zhang/VisualGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00423",
      "scraped_at": "2026-01-09T01:51:26.319389"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.03471",
    "authors": [
      "Guanchen Wu",
      "Yuzhang Xie",
      "Zewen Liu",
      "Dehai Min",
      "Mingyang Wei"
    ],
    "stars": "0",
    "details": {
      "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
      "abstract": "EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03471",
      "pdf_url": "https://arxiv.org/pdf/2601.03471",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03471",
      "scraped_at": "2026-01-09T01:51:28.218342"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.03699",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "abstract": "RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03699",
      "pdf_url": "https://arxiv.org/pdf/2601.03699",
      "github_links": [
        "https://github.com/knoveleng/redeval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03699",
      "scraped_at": "2026-01-09T01:51:30.046104"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
    "paper_url": "https://huggingface.co/papers/2601.03315",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
      "abstract": "We find that LLMs aren't scientists yet.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03315",
      "pdf_url": "https://arxiv.org/pdf/2601.03315",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03315",
      "scraped_at": "2026-01-09T01:51:31.842661"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.03467",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) Unified Thinker: A General Reasoning Modular Core for Image Generation (2026) ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning (2025) ThinkGen: Generalized Thinking for Visual Generation (2025) MIRA: Multimodal Iterative Reasoning Agent for Image Editing (2025) EditThinker: Unlocking Iterative Reasoning for Any Image Editor (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03467",
      "pdf_url": "https://arxiv.org/pdf/2601.03467",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03467",
      "scraped_at": "2026-01-09T01:51:33.683864"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
    "paper_url": "https://huggingface.co/papers/2601.03448",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
      "abstract": "We propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. L2T establishes the structural scaffolding required for linguistic competence, complementing world knowledge acquired through standard CLM. The code is available on GitHub: https://github.com/gucci-j/l2t",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03448",
      "pdf_url": "https://arxiv.org/pdf/2601.03448",
      "github_links": [
        "https://github.com/gucci-j/l2t"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03448",
      "scraped_at": "2026-01-09T01:51:35.563270"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Pearmut: Human Evaluation of Translation Made Trivial",
    "paper_url": "https://huggingface.co/papers/2601.02933",
    "authors": [
      "Tom Kocmi",
      "VilÃ©m Zouhar"
    ],
    "stars": "7",
    "details": {
      "title": "Pearmut: Human Evaluation of Translation Made Trivial",
      "abstract": "Happy to discuss how people human-evaluate multilingual tasks! ðŸ™‚",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02933",
      "pdf_url": "https://arxiv.org/pdf/2601.02933",
      "github_links": [
        "https://github.com/zouharvi/pearmut"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02933",
      "scraped_at": "2026-01-09T01:51:37.483381"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03955",
    "authors": [
      "Ming Lu",
      "Kun Gai",
      "Huan Yang",
      "Cheng Da",
      "Xu Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03955",
      "pdf_url": "https://arxiv.org/pdf/2601.03955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03955",
      "scraped_at": "2026-01-09T01:51:39.413532"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
    "paper_url": "https://huggingface.co/papers/2601.03236",
    "authors": [
      "Bingzhe Li",
      "Guanpeng Li",
      "Yi Li",
      "Dongming Jiang"
    ],
    "stars": "0",
    "details": {
      "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
      "abstract": "This ia giid paper",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03236",
      "pdf_url": "https://arxiv.org/pdf/2601.03236",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03236",
      "scraped_at": "2026-01-09T01:51:41.306990"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.04090",
    "authors": [
      "Yuewen Ma",
      "Lin Ma",
      "Bangbang Yang",
      "Yuanbo Yang",
      "Jiaxin Huang"
    ],
    "stars": "34",
    "details": {
      "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
      "abstract": "We Introduce Gen3R â€” create multi-quantity geometry with RGB from images. ðŸ“· Photorealistic Video ðŸš€ Accurate 3D Scene Geometry Arxiv: https://arxiv.org/abs/2601.04090 Project page: https://xdimlab.github.io/Gen3R/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04090",
      "pdf_url": "https://arxiv.org/pdf/2601.04090",
      "github_links": [
        "https://github.com/JaceyHuang/Gen3R"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04090",
      "scraped_at": "2026-01-09T01:51:43.101762"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "paper_url": "https://huggingface.co/papers/2601.00705",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
      "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00705",
      "pdf_url": "https://arxiv.org/pdf/2601.00705",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00705",
      "scraped_at": "2026-01-09T01:51:44.961750"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "64",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL â€” only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. â±ï¸ Each run can be completed in ~1 hour on 8Ã—A100s, or ~2.5 hours on a single A100. ðŸ”„ Switching from GRPO to GDPO is easy. ðŸ‘‰ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-10T01:47:38.996866"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "98",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the Î¼P multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes Î¼P, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-10T01:47:40.904445"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "12",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-10T01:47:42.783069"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-10T01:47:44.648079"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "8",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-10T01:47:46.604457"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "6",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-10T01:47:48.511757"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-10T01:47:50.402121"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-10T01:47:52.299582"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-10T01:47:54.355619"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-10T01:47:56.251754"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-10T01:47:58.153470"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-10T01:48:00.071087"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [
      "Meng Liu",
      "Qiancheng Xu",
      "Caiqi Zhang",
      "HongruCai",
      "dd101bb"
    ],
    "stars": "9",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-10T01:48:01.910045"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-10T01:48:03.769181"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-10T01:48:05.666518"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in imageâ€“text interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-10T01:48:07.549671"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [
      "Jing Ma",
      "Yuxuan Gu",
      "Shidong Cao",
      "Ziyang",
      "danielhzlin"
    ],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-10T01:48:09.387444"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-10T01:48:11.254706"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-10T01:48:13.102178"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-10T01:48:14.948337"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-10T01:48:16.843733"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification ðŸ” Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: âœ… Outperforms strong baselines like EAGLE-2 and LANTERN. âœ… Consistently delivers greater speedups across 512p and 1024p resolutions. âœ… Integrates seamlessly with unified Multimodal LLMs. ðŸ”— https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-10T01:48:18.728989"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-10T01:48:20.557489"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-10T01:48:22.412221"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-10T01:48:24.291348"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "ðŸš€ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-10T01:48:26.242615"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-10T01:48:28.093828"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "0",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-10T01:48:29.955705"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-10T01:48:31.814190"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-TÃ¼r",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-10T01:48:33.663145"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-10T01:48:35.486191"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-10T01:48:37.365709"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "1",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-10T01:48:39.220990"
    },
    "scraped_date": "2026-01-10"
  }
]