[
  {
    "title": "Kling-Omni Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kling-Omni Technical Report",
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16776",
      "pdf_url": "https://arxiv.org/pdf/2512.16776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16776",
      "scraped_at": "2025-12-22T01:52:28.433456"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Adaptation of Agentic AI",
    "paper_url": "https://huggingface.co/papers/2512.16301",
    "authors": [
      "XueqiangXu",
      "p-song1",
      "Gabshi",
      "linjc16",
      "pat-jj"
    ],
    "stars": "293",
    "details": {
      "title": "Adaptation of Agentic AI",
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16301",
      "pdf_url": "https://arxiv.org/pdf/2512.16301",
      "github_links": [
        "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16301",
      "scraped_at": "2025-12-22T01:52:30.476204"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "paper_url": "https://huggingface.co/papers/2512.16922",
    "authors": [],
    "stars": "87",
    "details": {
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "abstract": "Make SSL great again.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16922",
      "pdf_url": "https://arxiv.org/pdf/2512.16922",
      "github_links": [
        "https://github.com/SihanXU/nepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16922",
      "scraped_at": "2025-12-22T01:52:32.563303"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "paper_url": "https://huggingface.co/papers/2512.15745",
    "authors": [],
    "stars": "172",
    "details": {
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15745",
      "pdf_url": "https://arxiv.org/pdf/2512.15745",
      "github_links": [
        "https://github.com/inclusionAI/LLaDA2.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15745",
      "scraped_at": "2025-12-22T01:52:34.922051"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "paper_url": "https://huggingface.co/papers/2512.16915",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "abstract": "StereoPilot replaces the fragile \"Depth-Warp-Inpaint\" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo dataset with a learnable domain switcher, it provides a unified and efficient solution for both parallel and converged 3D video formats.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16915",
      "pdf_url": "https://arxiv.org/pdf/2512.16915",
      "github_links": [
        "https://github.com/KlingTeam/StereoPilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16915",
      "scraped_at": "2025-12-22T01:52:37.000210"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "paper_url": "https://huggingface.co/papers/2512.13507",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
      "abstract": "Seedance 1.5 pro Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13507",
      "pdf_url": "https://arxiv.org/pdf/2512.13507",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13507",
      "scraped_at": "2025-12-22T01:52:38.879192"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.16923",
    "authors": [
      "Yu-Lun Liu",
      "Jia-Bin Huang",
      "rayray9999"
    ],
    "stars": "64",
    "details": {
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16923",
      "pdf_url": "https://arxiv.org/pdf/2512.16923",
      "github_links": [
        "https://github.com/rayray9999/Genfocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16923",
      "scraped_at": "2025-12-22T01:52:40.769733"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "paper_url": "https://huggingface.co/papers/2512.16913",
    "authors": [
      "Wenxuan Lu",
      "Dizhe Zhang",
      "Meixi Song",
      "Xin Lin",
      "haodongli"
    ],
    "stars": "77",
    "details": {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP website/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16913",
      "pdf_url": "https://arxiv.org/pdf/2512.16913",
      "github_links": [
        "https://github.com/Insta360-Research-Team/DAP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16913",
      "scraped_at": "2025-12-22T01:52:42.690376"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "paper_url": "https://huggingface.co/papers/2512.16905",
    "authors": [
      "Jiarong Ou",
      "Miao Yang",
      "Xi Chen",
      "Yang Zhou",
      "Kaixin Ding"
    ],
    "stars": "0",
    "details": {
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "abstract": "data selection",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16905",
      "pdf_url": "https://arxiv.org/pdf/2512.16905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16905",
      "scraped_at": "2025-12-22T01:52:44.630408"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16625",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
      "abstract": "‚ú® Image editing is awesome; but it can leak user information! üõ°Ô∏è Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. üìÑ Paper: https://arxiv.org/abs/2512.16625 üíª Code: https://github.com/LinghuiiShen/DeContext üåê Project Page: https://linghuiishen.github.io/decontext_project_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16625",
      "pdf_url": "https://arxiv.org/pdf/2512.16625",
      "github_links": [
        "https://github.com/LinghuiiShen/DeContext"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16625",
      "scraped_at": "2025-12-22T01:52:46.565496"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.16636",
    "authors": [
      "Giorgos Sfikas",
      "Theodoros Giannakopoulos",
      "Bill Psomas",
      "Christos Sgouropoulos",
      "Giorgos Petsangourakis"
    ],
    "stars": "1",
    "details": {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16636",
      "pdf_url": "https://arxiv.org/pdf/2512.16636",
      "github_links": [
        "https://github.com/giorgospets/reglue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16636",
      "scraped_at": "2025-12-22T01:52:48.434250"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "paper_url": "https://huggingface.co/papers/2512.16924",
    "authors": [],
    "stars": "74",
    "details": {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "abstract": "Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16924",
      "pdf_url": "https://arxiv.org/pdf/2512.16924",
      "github_links": [
        "https://github.com/pPetrichor/WorldCanvas"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16924",
      "scraped_at": "2025-12-22T01:52:50.342955"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "paper_url": "https://huggingface.co/papers/2512.16649",
    "authors": [],
    "stars": "77",
    "details": {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "abstract": "‚ú®What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2√ó less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16649",
      "pdf_url": "https://arxiv.org/pdf/2512.16649",
      "github_links": [
        "https://github.com/thunlp/JustRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16649",
      "scraped_at": "2025-12-22T01:52:52.263342"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.16561",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "abstract": "Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16561",
      "pdf_url": "https://arxiv.org/pdf/2512.16561",
      "github_links": [
        "https://github.com/W-Ted/N3D-VLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16561",
      "scraped_at": "2025-12-22T01:52:54.191233"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "paper_url": "https://huggingface.co/papers/2512.16920",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16920",
      "pdf_url": "https://arxiv.org/pdf/2512.16920",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16920",
      "scraped_at": "2025-12-22T01:52:56.070423"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "paper_url": "https://huggingface.co/papers/2512.16918",
    "authors": [
      "Zhixun Li",
      "Zhongyu Wang",
      "Dongyang Chen",
      "Kaituo Feng",
      "Chaoyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "abstract": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16918",
      "pdf_url": "https://arxiv.org/pdf/2512.16918",
      "github_links": [
        "https://github.com/CYWang735/AdaTooler-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16918",
      "scraped_at": "2025-12-22T01:52:57.930176"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "paper_url": "https://huggingface.co/papers/2512.16912",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16912",
      "pdf_url": "https://arxiv.org/pdf/2512.16912",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16912",
      "scraped_at": "2025-12-22T01:52:59.800293"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "paper_url": "https://huggingface.co/papers/2512.16899",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "abstract": "Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta Superintelligence Labs released their reward benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16899",
      "pdf_url": "https://arxiv.org/pdf/2512.16899",
      "github_links": [
        "https://github.com/facebookresearch/MMRB2/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16899",
      "scraped_at": "2025-12-22T01:53:01.679382"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "paper_url": "https://huggingface.co/papers/2512.16864",
    "authors": [
      "Yuqi Liu",
      "Longxiang Tang",
      "Xiaohang Zhan",
      "Lei Ke",
      "TainU"
    ],
    "stars": "17",
    "details": {
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "abstract": "üöß The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). üìâ The Gap: Global Semantic Guidance Existing methods, including unified VLM-diffusion architectures, predominantly rely on Global Semantic Guidance . They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. üöÄ Our Solution: Region-Aligned Guidance RePlan introduces a Plan-then-Execute framework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performs Chain-of-Thought (CoT) reasoning to decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enables precise, multi-region parallel edits in a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities using Group Relative Policy Optimization (GRPO) . Remarkably, we achieve strong planning performance using only ~1k instruction-only samples , bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance is fully editable , enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supports regional negative prompts to prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establish IV-Edit , the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16864",
      "pdf_url": "https://arxiv.org/pdf/2512.16864",
      "github_links": [
        "https://github.com/dvlab-research/RePlan"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16864",
      "scraped_at": "2025-12-22T01:53:03.602686"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "paper_url": "https://huggingface.co/papers/2512.16900",
    "authors": [],
    "stars": "92",
    "details": {
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16900",
      "pdf_url": "https://arxiv.org/pdf/2512.16900",
      "github_links": [
        "https://github.com/Francis-Rings/FlashPortrait"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16900",
      "scraped_at": "2025-12-22T01:53:05.498824"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "paper_url": "https://huggingface.co/papers/2512.16501",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16501",
      "pdf_url": "https://arxiv.org/pdf/2512.16501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16501",
      "scraped_at": "2025-12-22T01:53:07.369391"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "paper_url": "https://huggingface.co/papers/2512.16106",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "ModelTables: A Corpus of Tables about Models",
      "abstract": "ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and configuration tables. The benchmark defines model and table relatedness using multiple signals, including paper citations, model-card links and inheritance, and shared training datasets, and supports downstream applications such as table discovery and semantic retrieval from both structural and semantic perspectives.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16106",
      "pdf_url": "https://arxiv.org/pdf/2512.16106",
      "github_links": [
        "https://github.com/RJMillerLab/ModelTables"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16106",
      "scraped_at": "2025-12-22T01:53:09.188933"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_url": "https://huggingface.co/papers/2512.16378",
    "authors": [
      "Carlos Escolano",
      "Vil√©m Zouhar",
      "zhopto3",
      "javi8979",
      "spapi"
    ],
    "stars": "13",
    "details": {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "abstract": "Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challenging conditions (noise, accents, disfluencies, long-form). Results show that cascades remain the most reliable overall, while SpeechLLMs close the gap in specific settings (notably noise and code-switching).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16378",
      "pdf_url": "https://arxiv.org/pdf/2512.16378",
      "github_links": [
        "https://github.com/sarapapi/hearing2translate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16378",
      "scraped_at": "2025-12-22T01:53:11.027318"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "paper_url": "https://huggingface.co/papers/2512.16921",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. Project Page: https://auditdm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16921",
      "pdf_url": "https://arxiv.org/pdf/2512.16921",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16921",
      "scraped_at": "2025-12-22T01:53:12.963299"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "paper_url": "https://huggingface.co/papers/2512.11251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
      "abstract": "Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptions.  Inspired by LLaVA, we showed instruction-tuning on TS-Insights can enable LLMs to understand time series as a native input modality and generate textual descriptions. This work was originally done in Summer 2023.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11251",
      "pdf_url": "https://arxiv.org/pdf/2512.11251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11251",
      "scraped_at": "2025-12-22T01:53:14.895310"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16615",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
      "abstract": "Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16615",
      "pdf_url": "https://arxiv.org/pdf/2512.16615",
      "github_links": [
        "https://github.com/SingleZombie/LLSA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16615",
      "scraped_at": "2025-12-22T01:53:16.820184"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "paper_url": "https://huggingface.co/papers/2512.15489",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "abstract": "Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization‚Äîespecially on HLE-Math‚Äîwhile maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15489",
      "pdf_url": "https://arxiv.org/pdf/2512.15489",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15489",
      "scraped_at": "2025-12-22T01:53:18.671559"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "paper_url": "https://huggingface.co/papers/2512.16767",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16767",
      "pdf_url": "https://arxiv.org/pdf/2512.16767",
      "github_links": [
        "https://github.com/jasongzy/Make-It-Poseable"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16767",
      "scraped_at": "2025-12-22T01:53:20.517544"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "paper_url": "https://huggingface.co/papers/2512.16670",
    "authors": [
      "Hendrik P. A. Lensch",
      "Ole Beisswenger",
      "JDihlmann"
    ],
    "stars": "0",
    "details": {
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "abstract": "FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination from a stream of G-buffer data. We overfit SD on single scenes and show that it learns the illumination setting for the scene and can transfer it to OOD views of the scene.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16670",
      "pdf_url": "https://arxiv.org/pdf/2512.16670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16670",
      "scraped_at": "2025-12-22T01:53:22.394603"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "paper_url": "https://huggingface.co/papers/2512.10953",
    "authors": [
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Qiao Sun",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "abstract": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10953",
      "pdf_url": "https://arxiv.org/pdf/2512.10953",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10953",
      "scraped_at": "2025-12-22T01:53:24.387244"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "paper_url": "https://huggingface.co/papers/2512.12880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Improving Recursive Transformers with Mixture of LoRAs",
      "abstract": "Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected shared FFNs with a small set of token-routed LoRA experts (sparse routing), allowing conditional computation while keeping the backbone compact. We pretrain ModernALBERT (50M to 120M) with RoPE, GeGLU, FlashAttention, and distillation-based initialisation, and report state-of-the-art results among compact models on GLUE, SQuAD-v2, and BEIR, often surpassing larger fully parameterised baselines. For deployment, we introduce expert merging (including an EMA-based strategy) that compresses MoL into a single adapter at inference, removing routing overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12880",
      "pdf_url": "https://arxiv.org/pdf/2512.12880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12880",
      "scraped_at": "2025-12-22T01:53:26.216029"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
    "paper_url": "https://huggingface.co/papers/2512.12623",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
      "abstract": "üåê Website: https://mllm-dmlr.github.io üìÑ Paper: https://arxiv.org/abs/2512.12623",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12623",
      "pdf_url": "https://arxiv.org/pdf/2512.12623",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12623",
      "scraped_at": "2025-12-22T01:53:28.064538"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.12576",
    "authors": [
      "Ben He",
      "Hongyu Lin",
      "Yanjiang Liu",
      "Jie Lou",
      "Aunderline"
    ],
    "stars": "0",
    "details": {
      "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
      "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12576",
      "pdf_url": "https://arxiv.org/pdf/2512.12576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12576",
      "scraped_at": "2025-12-22T01:53:29.883782"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "paper_url": "https://huggingface.co/papers/2512.16909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "abstract": "Project Page: https://hybridrobotics.github.io/MomaGraph/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16909",
      "pdf_url": "https://arxiv.org/pdf/2512.16909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16909",
      "scraped_at": "2025-12-22T01:53:31.722427"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.15907",
    "authors": [
      "Vivek Gupta",
      "Aparna Garimella",
      "Juhna Park",
      "Tejas Anvekar"
    ],
    "stars": "1",
    "details": {
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15907",
      "pdf_url": "https://arxiv.org/pdf/2512.15907",
      "github_links": [
        "https://github.com/CoRAL-ASU/TabReX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15907",
      "scraped_at": "2025-12-22T01:53:33.535002"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "paper_url": "https://huggingface.co/papers/2512.14884",
    "authors": [
      "Yutong Bai",
      "Michael D. Grossberg",
      "Andrew Lu",
      "Katherine Xu",
      "Huzheng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
      "abstract": "what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14884",
      "pdf_url": "https://arxiv.org/pdf/2512.14884",
      "github_links": [
        "https://github.com/huzeyann/VibeSpace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14884",
      "scraped_at": "2025-12-22T01:53:35.652691"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "paper_url": "https://huggingface.co/papers/2512.15528",
    "authors": [
      "Can Ma. Yu Zhou",
      "Dongbao Yang",
      "Daiqing Wu"
    ],
    "stars": "1",
    "details": {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "abstract": "Update the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15528",
      "pdf_url": "https://arxiv.org/pdf/2512.15528",
      "github_links": [
        "https://github.com/wdqqdw/EmoCaliber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15528",
      "scraped_at": "2025-12-22T01:53:37.441726"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "paper_url": "https://huggingface.co/papers/2512.14805",
    "authors": [
      "Michael Carbin",
      "Tian Jin",
      "Logan Weber",
      "ellieyhc"
    ],
    "stars": "3",
    "details": {
      "title": "Sharing State Between Prompts and Programs",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14805",
      "pdf_url": "https://arxiv.org/pdf/2512.14805",
      "github_links": [
        "https://github.com/psg-mit/nightjarpy/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14805",
      "scraped_at": "2025-12-22T01:53:39.288175"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "paper_url": "https://huggingface.co/papers/2512.16969",
    "authors": [
      "Yuhao Zhou",
      "SciYu",
      "VitaCoco",
      "BoKelvin",
      "CoCoOne"
    ],
    "stars": "56",
    "details": {
      "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
      "abstract": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16969",
      "pdf_url": "https://arxiv.org/pdf/2512.16969",
      "github_links": [
        "https://github.com/InternScience/SGI-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16969",
      "scraped_at": "2025-12-23T01:48:12.405586"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.16793",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16793",
      "pdf_url": "https://arxiv.org/pdf/2512.16793",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16793",
      "scraped_at": "2025-12-23T01:48:14.425346"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "When Reasoning Meets Its Laws",
    "paper_url": "https://huggingface.co/papers/2512.17901",
    "authors": [
      "Liu Ziyin",
      "Jingyan Shen",
      "Tianang Leng",
      "Yifan Sun",
      "jyzhang1208"
    ],
    "stars": "15",
    "details": {
      "title": "When Reasoning Meets Its Laws",
      "abstract": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17901",
      "pdf_url": "https://arxiv.org/pdf/2512.17901",
      "github_links": [
        "https://github.com/ASTRAL-Group/LoRe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17901",
      "scraped_at": "2025-12-23T01:48:16.357933"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "paper_url": "https://huggingface.co/papers/2512.17260",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
      "abstract": "Github: https://github.com/ByteDance-Seed/Seed-Prover",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17260",
      "pdf_url": "https://arxiv.org/pdf/2512.17260",
      "github_links": [
        "https://github.com/ByteDance-Seed/Seed-Prover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17260",
      "scraped_at": "2025-12-23T01:48:18.269306"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2512.17909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
      "abstract": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components. Project Page: https://jshilong.github.io/PS-VAE-PAGE/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17909",
      "pdf_url": "https://arxiv.org/pdf/2512.17909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17909",
      "scraped_at": "2025-12-23T01:48:20.166848"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "paper_url": "https://huggingface.co/papers/2512.17012",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
      "abstract": "Project page: https://ca-joe-yang.github.io/resource/projects/4D_RGPT We propose 4D-RGPT , a specialized MLLM that perceives 4D information for enhanced video understanding. We propose the P erceptual 4 D D istillation ( P4D ) training framework to distill 4D perceptual knowledge into 4D-RGPT without introducing additional inference cost. We introduce R4D-Bench , a region-based 4D VQA benchmark that requires region-level 4D understanding. Our 4D-RGPT improves over the baseline on both non-region-based 3D/4D benchmarks ( +5.3% on average across 6 benchmarks ) and our region-based R4D-Bench benchmark ( +4.3% ), while effectively capturing explicit 4D signals.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17012",
      "pdf_url": "https://arxiv.org/pdf/2512.17012",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17012",
      "scraped_at": "2025-12-23T01:48:22.025726"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
    "paper_url": "https://huggingface.co/papers/2512.16041",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
      "abstract": "We argue that evaluating LLM-as-a-Judge is biased by human-annotated ground truth, rethink the evaluation of LLM-as-a-Judge, and design metrics that do not need human annotations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16041",
      "pdf_url": "https://arxiv.org/pdf/2512.16041",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16041",
      "scraped_at": "2025-12-23T01:48:23.866902"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
    "paper_url": "https://huggingface.co/papers/2512.11362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
      "abstract": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our https://suyuz1.github.io/VLA-Survey-Anatomy/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11362",
      "pdf_url": "https://arxiv.org/pdf/2512.11362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11362",
      "scraped_at": "2025-12-23T01:48:25.863004"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
    "paper_url": "https://huggingface.co/papers/2512.17897",
    "authors": [
      "Or Litany",
      "Shengyu Huang",
      "Sanja Fidler",
      "Fangqiang Ding",
      "TomerBo"
    ],
    "stars": "6",
    "details": {
      "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
      "abstract": "Check out radargen.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17897",
      "pdf_url": "https://arxiv.org/pdf/2512.17897",
      "github_links": [
        "https://github.com/tomerborreda/RadarGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17897",
      "scraped_at": "2025-12-23T01:48:27.747323"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.17495",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
      "abstract": "Our new benchmark for evaluating the grounding capabilities of frontier MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17495",
      "pdf_url": "https://arxiv.org/pdf/2512.17495",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17495",
      "scraped_at": "2025-12-23T01:48:29.641583"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "paper_url": "https://huggingface.co/papers/2512.17351",
    "authors": [],
    "stars": "278",
    "details": {
      "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
      "abstract": "https://x.com/ZeyuanAllenZhu/status/2000892470306152701 https://physics.allen-zhu.com/part-4-architecture-design/part-4-1",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17351",
      "pdf_url": "https://arxiv.org/pdf/2512.17351",
      "github_links": [
        "https://github.com/facebookresearch/PhysicsLM4"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17351",
      "scraped_at": "2025-12-23T01:48:31.515651"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
    "paper_url": "https://huggingface.co/papers/2512.17008",
    "authors": [
      "Lihong Li",
      "Meet P. Vadera",
      "Rui Meng",
      "Peng Zhou",
      "ljb121002"
    ],
    "stars": "0",
    "details": {
      "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
      "abstract": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17008",
      "pdf_url": "https://arxiv.org/pdf/2512.17008",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17008",
      "scraped_at": "2025-12-23T01:48:33.437469"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
    "paper_url": "https://huggingface.co/papers/2512.14870",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
      "abstract": "üîó Project page: https://herbench.github.io/ üìÑ  arXiv: https://arxiv.org/abs/2512.14870 ü§ó  HF dataset card: https://huggingface.co/datasets/DanBenAmi/HERBench üñ•  Code (GitHub): https://github.com/DanBenAmi/HERBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14870",
      "pdf_url": "https://arxiv.org/pdf/2512.14870",
      "github_links": [
        "https://github.com/DanBenAmi/HERBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14870",
      "scraped_at": "2025-12-23T01:48:35.301291"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Animate Any Character in Any World",
    "paper_url": "https://huggingface.co/papers/2512.17796",
    "authors": [
      "Yan Lu",
      "Bo Dai",
      "Hongyang Zhang",
      "Fangyun Wei",
      "Yitong Wang"
    ],
    "stars": "28",
    "details": {
      "title": "Animate Any Character in Any World",
      "abstract": "Introducing AniX, a system enables users to provide 3DGS scene along with a 3D or multi-view character, enabling interactive control of the character's behaviors and active exploration of the environment through natural language commands. The system features:",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17796",
      "pdf_url": "https://arxiv.org/pdf/2512.17796",
      "github_links": [
        "https://github.com/snowflakewang/AniX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17796",
      "scraped_at": "2025-12-23T01:48:37.149428"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "paper_url": "https://huggingface.co/papers/2512.17419",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
      "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17419",
      "pdf_url": "https://arxiv.org/pdf/2512.17419",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17419",
      "scraped_at": "2025-12-23T01:48:38.957080"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
    "paper_url": "https://huggingface.co/papers/2512.16483",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
      "abstract": "github: https://github.com/sen-mao/StageVAR",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16483",
      "pdf_url": "https://arxiv.org/pdf/2512.16483",
      "github_links": [
        "https://github.com/sen-mao/StageVAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16483",
      "scraped_at": "2025-12-23T01:48:40.740165"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Bolmo: Byteifying the Next Generation of Language Models",
    "paper_url": "https://huggingface.co/papers/2512.15586",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Bolmo: Byteifying the Next Generation of Language Models",
      "abstract": "So cool idea to make use of mLSTM and developing this byteifying approach üòç",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15586",
      "pdf_url": "https://arxiv.org/pdf/2512.15586",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15586",
      "scraped_at": "2025-12-23T01:48:42.629718"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Meta-RL Induces Exploration in Language Agents",
    "paper_url": "https://huggingface.co/papers/2512.16848",
    "authors": [
      "Maria Brbic",
      "Michael Moor",
      "Damien Teney",
      "Liangze Jiang",
      "Yulun Jiang"
    ],
    "stars": "0",
    "details": {
      "title": "Meta-RL Induces Exploration in Language Agents",
      "abstract": "üåäLaMer, a general Meta-RL framework that enables LLM agents to explore and learn from the environment feedback at test time.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16848",
      "pdf_url": "https://arxiv.org/pdf/2512.16848",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16848",
      "scraped_at": "2025-12-23T01:48:44.450669"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "paper_url": "https://huggingface.co/papers/2512.17532",
    "authors": [
      "Runtao Liu",
      "Xiaogang Xu",
      "Wei Wei",
      "Jianmin Chen",
      "Jiaqi-hkust"
    ],
    "stars": "0",
    "details": {
      "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
      "abstract": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17532",
      "pdf_url": "https://arxiv.org/pdf/2512.17532",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17532",
      "scraped_at": "2025-12-23T01:48:46.210116"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
    "paper_url": "https://huggingface.co/papers/2512.17459",
    "authors": [
      "Hendrik P. A. Lensch",
      "Tobias Sautter",
      "JDihlmann"
    ],
    "stars": "33",
    "details": {
      "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
      "abstract": "üåê https://3dregen.jdihlmann.com/ üìÉ https://arxiv.org/abs/2512.17459 üíæ https://github.com/cgtuebingen/3D-RE-GEN",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17459",
      "pdf_url": "https://arxiv.org/pdf/2512.17459",
      "github_links": [
        "https://github.com/cgtuebingen/3D-RE-GEN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17459",
      "scraped_at": "2025-12-23T01:48:48.020152"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
    "paper_url": "https://huggingface.co/papers/2512.16978",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
      "abstract": "üåê Website: https://mbzuai-oryx.github.io/LongShOT/ üíª Github: https://github.com/mbzuai-oryx/longshot ü§ó HuggingFace: https://huggingface.co/datasets/MBZUAI/longshot-bench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16978",
      "pdf_url": "https://arxiv.org/pdf/2512.16978",
      "github_links": [
        "https://github.com/mbzuai-oryx/longshot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16978",
      "scraped_at": "2025-12-23T01:48:49.866724"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.13427",
    "authors": [
      "Tomer Michaeli",
      "Inbar Huberman-Spiegelglas",
      "Nurit Spingarn-Eliezer",
      "Noa Cohen"
    ],
    "stars": "0",
    "details": {
      "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
      "abstract": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13427",
      "pdf_url": "https://arxiv.org/pdf/2512.13427",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13427",
      "scraped_at": "2025-12-23T01:48:51.620855"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "paper_url": "https://huggingface.co/papers/2512.16676",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
      "abstract": "code link: https://github.com/OpenDCAI/DataFlow",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16676",
      "pdf_url": "https://arxiv.org/pdf/2512.16676",
      "github_links": [
        "https://github.com/OpenDCAI/DataFlow"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16676",
      "scraped_at": "2025-12-24T01:46:29.753237"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
    "paper_url": "https://huggingface.co/papers/2512.19693",
    "authors": [
      "Ziwei Liu",
      "Dahua Lin",
      "Quan Wang",
      "Haiwen Diao",
      "Weichen Fan"
    ],
    "stars": "56",
    "details": {
      "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
      "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19693",
      "pdf_url": "https://arxiv.org/pdf/2512.19693",
      "github_links": [
        "https://github.com/WeichenFan/UAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19693",
      "scraped_at": "2025-12-24T01:46:31.698181"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
    "paper_url": "https://huggingface.co/papers/2512.17650",
    "authors": [],
    "stars": "32",
    "details": {
      "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
      "abstract": "Region-Constraint In-Context Generation for Instructional Video Editing Paper: https://arxiv.org/abs/2512.17650 Project Page: https://zhw-zhang.github.io/ReCo-page/ Github: https://github.com/HiDream-ai/ReCo ReCo-Data: https://huggingface.co/datasets/HiDream-ai/ReCo-Data ReCo-Bench: https://huggingface.co/datasets/HiDream-ai/ReCo-Bench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17650",
      "pdf_url": "https://arxiv.org/pdf/2512.17650",
      "github_links": [
        "https://github.com/HiDream-ai/ReCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17650",
      "scraped_at": "2025-12-24T01:46:33.664644"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.17040",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17040",
      "pdf_url": "https://arxiv.org/pdf/2512.17040",
      "github_links": [
        "https://github.com/emjay73/InfCam"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17040",
      "scraped_at": "2025-12-24T01:46:35.583698"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
    "paper_url": "https://huggingface.co/papers/2512.19134",
    "authors": [
      "Lu Cheng",
      "Tongtong Wu",
      "Kailin Zhang",
      "Dehai Min"
    ],
    "stars": "8",
    "details": {
      "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
      "abstract": "A new framework for dynamic retrieval-augmented generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19134",
      "pdf_url": "https://arxiv.org/pdf/2512.19134",
      "github_links": [
        "https://github.com/ZhishanQ/QuCo-RAG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19134",
      "scraped_at": "2025-12-24T01:46:37.568351"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
    "paper_url": "https://huggingface.co/papers/2512.18880",
    "authors": [
      "Hong Jiao",
      "Jian Chen",
      "Yunze Xiao",
      "Han Chen",
      "Ming Li"
    ],
    "stars": "0",
    "details": {
      "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "abstract": "Key Findings of our Human-LLM difficulty alignment study: Systematic Misalignment : Contrary to standard capability metrics, scaling does not reliably translate into alignment. Increasing model scale does not improve difficulty predictions; instead, models form a cohesive Machine Consensus, aligning significantly stronger with each other than with human reality. Limits of Simulation : Neither extrinsic ensembling nor proficiency simulation serves as a reliable fix for the misalignment. Ensemble performance is strictly bounded by weaker models, while proficiency simulation proves highly inconsistent as models struggle to authentically mimic different proficiency levels. The Curse of Knowledge : Our IRT-based analysis reveals a fundamental mechanistic divergence: the difficulty derived from models' actual correctness correlates even worse with humans than their explicit perceptions. Items that are difficult for humans are frequently trivial for models, and this capability exhibits significant inertia even under weak student prompts. Metacognitive Blindness : We identify a critical lack of introspection. With AUROC scores hovering near random guessing, models fail to predict their own limitations, indicating that explicit difficulty estimates are effectively decoupled from the model's actual correctness, lacking the internal signal to ground their predictions.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18880",
      "pdf_url": "https://arxiv.org/pdf/2512.18880",
      "github_links": [
        "https://github.com/MingLiiii/Difficulty_Alignment"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18880",
      "scraped_at": "2025-12-24T01:46:39.452943"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.19678",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "abstract": "Long-range camera-conditioned scene generation from a single image. Project page and code: https://hyokong.github.io/worldwarp-page/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19678",
      "pdf_url": "https://arxiv.org/pdf/2512.19678",
      "github_links": [
        "https://github.com/HyoKong/WorldWarp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19678",
      "scraped_at": "2025-12-24T01:46:41.458062"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
    "paper_url": "https://huggingface.co/papers/2512.19629",
    "authors": [
      "Yuan Shen",
      "Tai Wang",
      "Yuqiang Yang",
      "Wenzhe Cai",
      "Jiaqi Peng"
    ],
    "stars": "0",
    "details": {
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "abstract": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19629",
      "pdf_url": "https://arxiv.org/pdf/2512.19629",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19629",
      "scraped_at": "2025-12-24T01:46:43.484735"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.17385",
    "authors": [
      "Yuqing Ma",
      "Lin Jing",
      "Wei Zhang",
      "Jian Yang",
      "Jiajun Wu"
    ],
    "stars": "0",
    "details": {
      "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
      "abstract": "This paper introduces UCoder, an unsupervised framework for training code-generating large language models without requiring any external datasets, including unlabeled code snippets. The approach, called IPC (Internal Probing of LLMs for Code generation), leverages latent programming knowledge already present in pre-trained models through a six-stage self-bootstrapping process: (1-3) problem space probing that generates diverse algorithmic problems with specifications, (4) test understanding probing to create comprehensive test suites, (5) solution space probing using dense sampling (128 candidates per problem), and (6) knowledge consolidation through supervised fine-tuning on high-quality solutions. The key innovation is execution-driven consensus clustering, which identifies correct implementations by finding clusters of behaviorally identical solutions‚Äîcorrect code naturally clusters together while incorrect solutions fail heterogeneously. Experiments on UCoder models (7B, 14B, 32B parameters) demonstrate competitive performance with supervised baselines across multiple benchmarks (HumanEval, MBPP, BigCodeBench, LiveCodeBench, FullStackBench), with smaller models showing greater improvement gains (inverse scaling). The work proves that self-generated data maintains lexical, semantic, and structural diversity sufficient for effective learning, opening possibilities for resource-efficient LLM training without human annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17385",
      "pdf_url": "https://arxiv.org/pdf/2512.17385",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17385",
      "scraped_at": "2025-12-24T01:46:45.428926"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "paper_url": "https://huggingface.co/papers/2512.19682",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19682",
      "pdf_url": "https://arxiv.org/pdf/2512.19682",
      "github_links": [
        "https://github.com/Gen-Verse/GenEnv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19682",
      "scraped_at": "2025-12-24T01:46:47.338413"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
    "paper_url": "https://huggingface.co/papers/2512.19539",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "abstract": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19539",
      "pdf_url": "https://arxiv.org/pdf/2512.19539",
      "github_links": [
        "https://github.com/Kevin-thu/StoryMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19539",
      "scraped_at": "2025-12-24T01:46:49.226679"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
    "paper_url": "https://huggingface.co/papers/2512.16229",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
      "abstract": "üîóPaperÔºö https://arxiv.org/abs/2512.16229 üîóGitHubÔºö https://github.com/zhijie-group/LoPA üîóblog: https://zhijie-group.github.io/blogs/lopa",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16229",
      "pdf_url": "https://arxiv.org/pdf/2512.16229",
      "github_links": [
        "https://github.com/zhijie-group/LoPA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16229",
      "scraped_at": "2025-12-24T01:46:51.047673"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
    "paper_url": "https://huggingface.co/papers/2512.17206",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
      "abstract": "Reasoning Palette addresses the challenge of controlling LLM generation style and enabling effective exploration in RL by introducing a stochastic latent variable that encodes diverse reasoning strategies. This latent, inferred via a VAE from question-answer pairs, is decoded into token prefixes that modulate the model's internal reasoning before generation. A brief SFT phase adapts the model to this conditioning, and during RL, it enables structured, on-demand exploration, boosting both efficiency and performance across reasoning benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17206",
      "pdf_url": "https://arxiv.org/pdf/2512.17206",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17206",
      "scraped_at": "2025-12-24T01:46:52.865009"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
    "paper_url": "https://huggingface.co/papers/2512.19432",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
      "abstract": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19432",
      "pdf_url": "https://arxiv.org/pdf/2512.19432",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19432",
      "scraped_at": "2025-12-24T01:46:54.727159"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "paper_url": "https://huggingface.co/papers/2512.18658",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
      "abstract": "Most LLMs today are powerful at language but weak at worlds: they generate fluent outputs without maintaining a consistent, verifiable model of reality. As a result, many AI applications plateau at demos or copilots and fail in complex, high-stakes workflows. This paper shows that progress requires shifting from ad-hoc reasoning to explicit, evidence-grounded world models. Cap table tie-out exposes this gap‚Äîand demonstrates how closing it enables genuinely autonomous systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18658",
      "pdf_url": "https://arxiv.org/pdf/2512.18658",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18658",
      "scraped_at": "2025-12-24T01:46:56.999396"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
    "paper_url": "https://huggingface.co/papers/2512.19402",
    "authors": [
      "Liliang Chen",
      "Shengcong Chen",
      "Di Chen",
      "Hongwei Fan",
      "Yujie Zhao"
    ],
    "stars": "0",
    "details": {
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "abstract": "Paper: https://arxiv.org/abs/2512.19402 Project Page: https://real2edit2real.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19402",
      "pdf_url": "https://arxiv.org/pdf/2512.19402",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19402",
      "scraped_at": "2025-12-24T01:46:59.224433"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "paper_url": "https://huggingface.co/papers/2512.19535",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "abstract": "Code: https://github.com/kyutai-labs/casa",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19535",
      "pdf_url": "https://arxiv.org/pdf/2512.19535",
      "github_links": [
        "https://github.com/kyutai-labs/casa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19535",
      "scraped_at": "2025-12-24T01:47:01.059156"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Name That Part: 3D Part Segmentation and Naming",
    "paper_url": "https://huggingface.co/papers/2512.18003",
    "authors": [
      "Alan Yuille",
      "Anand Bhattad",
      "Ankit Vaidya",
      "Prakhar Kaushik",
      "Soumava Paul"
    ],
    "stars": "0",
    "details": {
      "title": "Name That Part: 3D Part Segmentation and Naming",
      "abstract": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18003",
      "pdf_url": "https://arxiv.org/pdf/2512.18003",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18003",
      "scraped_at": "2025-12-24T01:47:02.926590"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "paper_url": "https://huggingface.co/papers/2512.18314",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
      "abstract": "üåê https://matspray.jdihlmann.com/ üìÉ https://arxiv.org/abs/2512.18314 üíæ https://github.com/cgtuebingen/MatSpray",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18314",
      "pdf_url": "https://arxiv.org/pdf/2512.18314",
      "github_links": [
        "https://github.com/cgtuebingen/MatSpray"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18314",
      "scraped_at": "2025-12-24T01:47:04.779920"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
    "paper_url": "https://huggingface.co/papers/2512.12620",
    "authors": [
      "Sujata Ghosh",
      "Saptarshi Sahoo",
      "Aheli Poddar"
    ],
    "stars": "0",
    "details": {
      "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
      "abstract": "arXiv lens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/understanding-syllogistic-reasoning-in-llms-from-formal-and-natural-language-perspectives-822-84433a31 Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12620",
      "pdf_url": "https://arxiv.org/pdf/2512.12620",
      "github_links": [
        "https://github.com/XAheli/Logic-in-LLMs"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12620",
      "scraped_at": "2025-12-24T01:47:06.655524"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
    "paper_url": "https://huggingface.co/papers/2512.19661",
    "authors": [
      "Roni Sengupta",
      "Cary Phillips",
      "Jun Myeong Choi",
      "Jiaye Wu",
      "Luchao Qi"
    ],
    "stars": "0",
    "details": {
      "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19661",
      "pdf_url": "https://arxiv.org/pdf/2512.19661",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19661",
      "scraped_at": "2025-12-24T01:47:08.509366"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Brain-Grounded Axes for Reading and Steering LLM States",
    "paper_url": "https://huggingface.co/papers/2512.19399",
    "authors": [
      "Sandro Andric"
    ],
    "stars": "0",
    "details": {
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "abstract": "These research supports a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19399",
      "pdf_url": "https://arxiv.org/pdf/2512.19399",
      "github_links": [
        "https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19399",
      "scraped_at": "2025-12-24T01:47:10.322980"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
    "paper_url": "https://huggingface.co/papers/2512.18542",
    "authors": [
      "Scott Thornton"
    ],
    "stars": "1",
    "details": {
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18542",
      "pdf_url": "https://arxiv.org/pdf/2512.18542",
      "github_links": [
        "https://github.com/scthornton/securecode-v2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18542",
      "scraped_at": "2025-12-24T01:47:12.113503"
    },
    "scraped_date": "2025-12-24"
  }
]