[
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "348",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-08T01:45:50.074934"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-08T01:45:52.315362"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-08T01:45:54.258639"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "50",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/open-compass/VLMEvalKit/pull/1334",
        "https://github.com/InternLM/ARM-Thinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-08T01:45:56.554505"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "670",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-08T01:45:58.852851"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "111",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-08T01:46:01.038360"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "180",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-08T01:46:03.070805"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-08T01:46:05.376820"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-08T01:46:07.678018"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-08T01:46:09.658067"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-08T01:46:11.943491"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-08T01:46:15.287360"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-08T01:46:17.530303"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-08T01:46:19.898642"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-08T01:46:22.128948"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-08T01:46:24.087696"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-08T01:46:26.031383"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "749",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-08T01:46:28.371364"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "13",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-08T01:46:30.273623"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-08T01:46:32.517669"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-08T01:46:34.496564"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-08T01:46:36.498721"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "157",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-08T01:46:38.851452"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-08T01:46:41.174546"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-08T01:46:43.171361"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-08T01:46:45.529468"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "22",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-08T01:46:47.448319"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-08T01:46:49.455630"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-08T01:46:52.138190"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-08T01:46:54.052433"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-08T01:46:56.366922"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-08T01:46:58.622222"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-08T01:47:00.882199"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-08T01:47:03.075799"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "2",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-08T01:47:05.025805"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-08T01:47:07.334262"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-08T01:47:09.230588"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-08T01:47:11.482221"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "paper_url": "https://huggingface.co/papers/2512.05150",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
      "abstract": "Taming 20B full-parameter few-step training with self-adversarial flows! üëèüèª One-model Simplicity: We eliminate the need for auxiliary networks (discriminators, teachers, fake score estimators...), everything in one model! Scalability on Large Models: We transform Qwen-Image-20B into high-quality few-step generators by full-parameter training (Optimized for human figure generation!). Checkout our 2-NFE images generated by our TwinFlow-Qwen-Image! üëá We are also working on Z-Image-Turbo , stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05150",
      "pdf_url": "https://arxiv.org/pdf/2512.05150",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05150",
      "scraped_at": "2025-12-09T01:44:30.991822"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
    "paper_url": "https://huggingface.co/papers/2512.05965",
    "authors": [
      "Ziyu Guo",
      "Manyuan Zhang",
      "Longin-Yu",
      "zhengli1013",
      "appletea2333"
    ],
    "stars": "25",
    "details": {
      "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
      "abstract": "Instruction-based image editing has emerged as a prominent research area. Benefiting from image generation foundation models, it has achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to \"think\" while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions, followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produces the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05965",
      "pdf_url": "https://arxiv.org/pdf/2512.05965",
      "github_links": [
        "https://github.com/appletea233/EditThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05965",
      "scraped_at": "2025-12-09T01:44:32.917190"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "paper_url": "https://huggingface.co/papers/2512.02580",
    "authors": [
      "Yang Li",
      "Shuai Zhang",
      "Yuchen Liu",
      "Jinyang Wu",
      "Changpeng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
      "abstract": "üöÄ [New Paper] CAPO: From Imitation to Discrimination ‚Äì Rethinking Advantage in RL Early RL training often suffers from instability due to \"mixed signals\" (simultaneous positive & negative feedback). Inspired by child cognitive development, we propose CAPO (Curriculum Advantage Policy Optimization) . ‚ú® The Core Intuition: Instead of a static curriculum, we leverage Advantage values to create a dynamic, two-phase process: 1Ô∏è‚É£ Imitation Phase: Train on Positive Advantage only. This reduces variance and establishes a stable behavioral foundation (Imitate to learn). 2Ô∏è‚É£ Discrimination Phase: Introduce Negative Signals later. [cite_start]This restores unbiased estimation and refines decision boundaries (Discriminate to generalize). üìà Highlights: Plug-and-Play: Delivers consistent gains when compatible with GRPO, PPO, RLOO, & Reinforce++ . Cross-Domain: Not just for Math! CAPO demonstrates impressive generalization on Multimodal GUI Agent tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02580",
      "pdf_url": "https://arxiv.org/pdf/2512.02580",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02580",
      "scraped_at": "2025-12-09T01:44:34.806051"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
    "paper_url": "https://huggingface.co/papers/2512.04810",
    "authors": [
      "Qi Tian",
      "Lingxi Xie",
      "Jianbo Ouyang",
      "Longhui Wei",
      "Xin He"
    ],
    "stars": "13",
    "details": {
      "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
      "abstract": "The project page https://emma-umm.github.io/emma/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04810",
      "pdf_url": "https://arxiv.org/pdf/2512.04810",
      "github_links": [
        "https://github.com/umm-emma/emma"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04810",
      "scraped_at": "2025-12-09T01:44:36.824643"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
    "paper_url": "https://huggingface.co/papers/2512.04784",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
      "abstract": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04784",
      "pdf_url": "https://arxiv.org/pdf/2512.04784",
      "github_links": [
        "https://github.com/X-GenGroup/PaCo-RL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04784",
      "scraped_at": "2025-12-09T01:44:38.814591"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
    "paper_url": "https://huggingface.co/papers/2512.05905",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
      "abstract": "SCAIL is a new framework for studio-grade character animation that uses a novel 3D pose representation and full-sequence context injection to deliver more stable, realistic motion transfer under complex and cross-identity scenarios.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05905",
      "pdf_url": "https://arxiv.org/pdf/2512.05905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05905",
      "scraped_at": "2025-12-09T01:44:40.683227"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.05591",
    "authors": [
      "Zijia Lin",
      "Tiehua Mei",
      "Minxuan Lv",
      "Leiyu Pan",
      "Suu"
    ],
    "stars": "0",
    "details": {
      "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
      "abstract": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05591",
      "pdf_url": "https://arxiv.org/pdf/2512.05591",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05591",
      "scraped_at": "2025-12-09T01:44:42.544097"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.05044",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
      "abstract": "Project Page: https://ivg-yanranzhang.github.io/MoRe4D/ Github Repo: https://github.com/Zhangyr2022/MoRe4D The dataset is coming soon. Stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05044",
      "pdf_url": "https://arxiv.org/pdf/2512.05044",
      "github_links": [
        "https://github.com/Zhangyr2022/MoRe4D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05044",
      "scraped_at": "2025-12-09T01:44:44.580407"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04563",
    "authors": [
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Hengzhu Tang",
      "CUDAOUTOFMEMORY",
      "Starrrrrry"
    ],
    "stars": "5",
    "details": {
      "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
      "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04563",
      "pdf_url": "https://arxiv.org/pdf/2512.04563",
      "github_links": [
        "https://github.com/zhangzef/COOPER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04563",
      "scraped_at": "2025-12-09T01:44:46.516932"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
    "paper_url": "https://huggingface.co/papers/2512.00473",
    "authors": [
      "Zilong Huang",
      "Dongzhi Jiang",
      "Yuncheng Guo",
      "Leiqi Zhu",
      "Junyan Ye"
    ],
    "stars": "65",
    "details": {
      "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
      "abstract": "arXiv: https://arxiv.org/abs/2512.00473 code: https://github.com/yejy53/RealGen project page: https://yejy53.github.io/RealGen/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.00473",
      "pdf_url": "https://arxiv.org/pdf/2512.00473",
      "github_links": [
        "https://github.com/yejy53/RealGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.00473",
      "scraped_at": "2025-12-09T01:44:48.413384"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Self-Improving VLM Judges Without Human Annotations",
    "paper_url": "https://huggingface.co/papers/2512.05145",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving VLM Judges Without Human Annotations",
      "abstract": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05145",
      "pdf_url": "https://arxiv.org/pdf/2512.05145",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05145",
      "scraped_at": "2025-12-09T01:44:50.242883"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "paper_url": "https://huggingface.co/papers/2512.05927",
    "authors": [
      "Anirudha Majumdar",
      "Ola Shorinwa",
      "Micah Baker",
      "Tenny Yin",
      "Zhiting Mei"
    ],
    "stars": "0",
    "details": {
      "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
      "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05927",
      "pdf_url": "https://arxiv.org/pdf/2512.05927",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05927",
      "scraped_at": "2025-12-09T01:44:52.149809"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "paper_url": "https://huggingface.co/papers/2512.05343",
    "authors": [
      "Marc Pollefeys",
      "Or Litany",
      "Ian Huang",
      "Francis Engelmann",
      "efedele"
    ],
    "stars": "0",
    "details": {
      "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
      "abstract": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at this https URL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05343",
      "pdf_url": "https://arxiv.org/pdf/2512.05343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05343",
      "scraped_at": "2025-12-09T01:44:54.015630"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.02835",
    "authors": [
      "Shengju Qian",
      "Weikai Chen",
      "Lingting Zhu",
      "Yingda Yin",
      "Tangerine24"
    ],
    "stars": "5",
    "details": {
      "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
      "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02835",
      "pdf_url": "https://arxiv.org/pdf/2512.02835",
      "github_links": [
        "https://github.com/Clementine24/ReVSeg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02835",
      "scraped_at": "2025-12-09T01:44:55.832519"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
    "paper_url": "https://huggingface.co/papers/2512.05356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
      "abstract": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05356",
      "pdf_url": "https://arxiv.org/pdf/2512.05356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05356",
      "scraped_at": "2025-12-09T01:44:57.637941"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
    "paper_url": "https://huggingface.co/papers/2512.03514",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
      "abstract": "Can we build universal document retrievers that maintain strong results across typologically diverse languages without losing English performance. This question led us to design synthetic training data and multilingual benchmarks to teach a model to match documents across scripts and formats. We are excited to launch NetraEmbed our SoTA model for multimodal multilingual document retrieval along with the M3DR: Towards Universal Multilingual Multimodal Document Retrieval paper. The release includes the NetraEmbed model which produces a single dense embedding with matryoshka support at 768,1536 and 2560 dimensions and the ColNetraEmbed model which produces patch level multivector embeddings. Both models are finetuned on Gemma3-4B-it and gained ~150% improvement over baselines. To measure progress we also built the NayanaIR Benchmark with 22 multilingual and 1 cross lingual dataset and documented the full framework in the M3DR paper . Links Blog: https://www.cognitivelab.in/blog/introducing-netraembed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03514",
      "pdf_url": "https://arxiv.org/pdf/2512.03514",
      "github_links": [
        "https://github.com/adithya-s-k/colpali"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03514",
      "scraped_at": "2025-12-09T01:44:59.738369"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
    "paper_url": "https://huggingface.co/papers/2512.05277",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
      "abstract": "This work introduces TAD, the first benchmark targeting temporal understanding in ego-centric autonomous-driving videos, evaluates SoTA VLMs, and boosts their performance with two training-free motion-reasoning methods (Scene-CoT and TCogMap).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05277",
      "pdf_url": "https://arxiv.org/pdf/2512.05277",
      "github_links": [
        "https://github.com/vbdi/tad_bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05277",
      "scraped_at": "2025-12-09T01:45:01.601687"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "paper_url": "https://huggingface.co/papers/2512.05564",
    "authors": [
      "Yuhao Cheng",
      "Terry Jingchen Zhang",
      "Jing Wang",
      "Panwen Hu",
      "Zijun Wang"
    ],
    "stars": "0",
    "details": {
      "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
      "abstract": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05564",
      "pdf_url": "https://arxiv.org/pdf/2512.05564",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05564",
      "scraped_at": "2025-12-09T01:45:04.335515"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.05409",
    "authors": [
      "Minghui Yu",
      "Jinyuan Shi",
      "Hantao Huang",
      "Hao Zeng",
      "Ruixuan Huang"
    ],
    "stars": "0",
    "details": {
      "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
      "abstract": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05409",
      "pdf_url": "https://arxiv.org/pdf/2512.05409",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05409",
      "scraped_at": "2025-12-09T01:45:06.133971"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
    "paper_url": "https://huggingface.co/papers/2512.04694",
    "authors": [
      "Salih Tileylioglu",
      "Erdem Akag√ºnd√ºz",
      "Bevan Deniz Cilgin",
      "Barisylmz"
    ],
    "stars": "0",
    "details": {
      "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
      "abstract": "This work presents a transformer-based generative model for complex time-series signals, with experiments on seismic accelerometer data. Key idea: treat seismic waveforms as structured high-dimensional sequences and learn a latent trajectory that captures both physical dynamics and long-range temporal dependencies.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04694",
      "pdf_url": "https://arxiv.org/pdf/2512.04694",
      "github_links": [
        "https://github.com/brsylmz23/TimesNet-Gen/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04694",
      "scraped_at": "2025-12-09T01:45:07.983185"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.03667",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
      "abstract": "Colonoscopy saves lives ‚Äî but AI for colonoscopy is still far from intelligent. We are excited to launch the Colon-X project, an open initiative aimed at advancing multimodal intelligence in colonoscopy and beyond. Beyond serving as a community-wide data foundation, we're focused on a critical yet under-explored transition ‚Äì evolving from multimodal understanding to clinical reasoning. Keywords: Multimodal Colonoscopy Analysis, Multimodal Understanding, Clinical Reasoning, Reinforcement Learning, Multimodal Benchmark, AI Healthcare, and Abdomen",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03667",
      "pdf_url": "https://arxiv.org/pdf/2512.03667",
      "github_links": [
        "https://github.com/ai4colonoscopy/Colon-X"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03667",
      "scraped_at": "2025-12-09T01:45:09.862566"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.05774",
    "authors": [
      "Caiming Xiong",
      "Junnan Li",
      "Shijie Wang",
      "Honglu Zhou",
      "Ziyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05774",
      "pdf_url": "https://arxiv.org/pdf/2512.05774",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05774",
      "scraped_at": "2025-12-09T01:45:11.724493"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04142",
    "authors": [
      "Aimee van Wynsberghe",
      "Lisa Biber-Freudenberger",
      "Sasha Luccioni",
      "nicholasKluge",
      "sophia-falk"
    ],
    "stars": "0",
    "details": {
      "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
      "abstract": "The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4 üå±üçÉ",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04142",
      "pdf_url": "https://arxiv.org/pdf/2512.04142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04142",
      "scraped_at": "2025-12-09T01:45:13.587428"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.05339",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
      "abstract": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05339",
      "pdf_url": "https://arxiv.org/pdf/2512.05339",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05339",
      "scraped_at": "2025-12-09T01:45:15.478241"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.07461",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
      "abstract": "Paper: https://arxiv.org/abs/2512.07461 Code: https://github.com/bigai-nlco/Native-Parallel-Reasoner Model & Data: https://huggingface.co/bigai-NPR Website: https://bigai-nlco.github.io/Native-Parallel-Reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07461",
      "pdf_url": "https://arxiv.org/pdf/2512.07461",
      "github_links": [
        "https://github.com/bigai-nlco/Native-Parallel-Reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07461",
      "scraped_at": "2025-12-10T01:46:58.731334"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
    "paper_url": "https://huggingface.co/papers/2512.07525",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
      "abstract": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07525",
      "pdf_url": "https://arxiv.org/pdf/2512.07525",
      "github_links": [
        "https://github.com/OpenMOSS/rope_pp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07525",
      "scraped_at": "2025-12-10T01:47:00.619948"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Unified Video Editing with Temporal Reasoner",
    "paper_url": "https://huggingface.co/papers/2512.07469",
    "authors": [],
    "stars": "25",
    "details": {
      "title": "Unified Video Editing with Temporal Reasoner",
      "abstract": "A Chain of Frames video editing method enbale temporal reasoning and 4x video length extrapolation with just 50k training pairs! üè† Page: videocof.github.io/ üìÑ Paper: arxiv.org/abs/2512.07469 üíª Code: github.com/knightyxp/VideoCoF",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07469",
      "pdf_url": "https://arxiv.org/pdf/2512.07469",
      "github_links": [
        "https://github.com/knightyxp/VideoCoF"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07469",
      "scraped_at": "2025-12-10T01:47:02.532632"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "paper_url": "https://huggingface.co/papers/2512.07834",
    "authors": [
      "Yu-Lun Liu",
      "chien90190",
      "JiewenChan",
      "YiChuanH"
    ],
    "stars": "0",
    "details": {
      "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
      "abstract": "Stylized voxel art is widely used in games and digital media, but turning 3D meshes into visually appealing voxel forms remains challenging and often requires manual effort. Existing methods struggle to preserve semantic structure and offer limited control over stylization, particularly in discrete color and abstraction. We present Voxify3D, a differentiable two-stage framework for generating stylized voxel art from 3D meshes. In the first stage, we initialize a coarse voxel grid via neural volume rendering. In the second stage, we refine the grid under six-view orthographic pixel art supervision, guided by a discrete color palette derived from clustering strategies (e.g., K-means, Max-Min, Median Cut). To support differentiable palette-based quantization, we design a rendering mechanism based on Gumbel-Softmax and incorporate a CLIP-based perceptual loss to enforce semantic alignment between voxel renderings and the original mesh.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07834",
      "pdf_url": "https://arxiv.org/pdf/2512.07834",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07834",
      "scraped_at": "2025-12-10T01:47:04.380187"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Scaling Zero-Shot Reference-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.06905",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Scaling Zero-Shot Reference-to-Video Generation",
      "abstract": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06905",
      "pdf_url": "https://arxiv.org/pdf/2512.06905",
      "github_links": [
        "https://github.com/franciszzj/Saber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06905",
      "scraped_at": "2025-12-10T01:47:06.279606"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "paper_url": "https://huggingface.co/papers/2512.06749",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
      "abstract": "Project website with an intro video is available at: https://aka.ms/DoVer .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06749",
      "pdf_url": "https://arxiv.org/pdf/2512.06749",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06749",
      "scraped_at": "2025-12-10T01:47:08.156914"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Distribution Matching Variational AutoEncoder",
    "paper_url": "https://huggingface.co/papers/2512.07778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Distribution Matching Variational AutoEncoder",
      "abstract": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder‚Äôs latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at \\url{ https://github.com/sen-ye/dmvae}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07778",
      "pdf_url": "https://arxiv.org/pdf/2512.07778",
      "github_links": [
        "https://github.com/sen-ye/dmvae%7D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07778",
      "scraped_at": "2025-12-10T01:47:10.004095"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "paper_url": "https://huggingface.co/papers/2512.06065",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
      "abstract": "We propose a framework for real-time egocentric video editing. Our system is composed of: EgoEditData, a manually curated dataset of 100k video editing pairs focusing on the egocentric case and featuring object substitution and removal under challenging hand occlusions, interactions, and large egomotion; EgoEdit the first real-time autoregressive model for egocentric video editing running in real time on a single H100 with 855ms first-frame latency and enabling live augmented reality (AR) interactions; EgoEditBench, a comprehensive benchmark for evaluation of egocentric video editing systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06065",
      "pdf_url": "https://arxiv.org/pdf/2512.06065",
      "github_links": [
        "https://github.com/snap-research/EgoEdit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06065",
      "scraped_at": "2025-12-10T01:47:11.841255"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Relational Visual Similarity",
    "paper_url": "https://huggingface.co/papers/2512.07833",
    "authors": [
      "Jing Shi",
      "Yilin Wang",
      "Krishna Kumar Singh",
      "Sicheng Mo",
      "thaoshibe"
    ],
    "stars": "14",
    "details": {
      "title": "Relational Visual Similarity",
      "abstract": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07833",
      "pdf_url": "https://arxiv.org/pdf/2512.07833",
      "github_links": [
        "https://github.com/thaoshibe/relsim"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07833",
      "scraped_at": "2025-12-10T01:47:13.783773"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "paper_url": "https://huggingface.co/papers/2512.07806",
    "authors": [
      "Jungwoo Kim",
      "Younggeun Lee",
      "Seungtae Nam",
      "Seungkwon Yang",
      "Gynjn"
    ],
    "stars": "56",
    "details": {
      "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
      "abstract": "We are excited to share our recent work \"Multi-view Pyramid Transformer: Look Coarser to See Broader\" Paper: https://arxiv.org/abs/2512.07806 Project page: https://gynjn.github.io/MVP/ Code: https://github.com/Gynjn/MVP",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07806",
      "pdf_url": "https://arxiv.org/pdf/2512.07806",
      "github_links": [
        "https://github.com/Gynjn/MVP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07806",
      "scraped_at": "2025-12-10T01:47:15.632640"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "LongCat-Image Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.07584",
    "authors": [],
    "stars": "307",
    "details": {
      "title": "LongCat-Image Technical Report",
      "abstract": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07584",
      "pdf_url": "https://arxiv.org/pdf/2512.07584",
      "github_links": [
        "https://github.com/meituan-longcat/LongCat-Image"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07584",
      "scraped_at": "2025-12-10T01:47:17.470709"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.07831",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
      "abstract": "Project Website https://jackailab.github.io/Projects/UnityVideo/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07831",
      "pdf_url": "https://arxiv.org/pdf/2512.07831",
      "github_links": [
        "https://github.com/dvlab-research/UnityVideo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07831",
      "scraped_at": "2025-12-10T01:47:19.511510"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "paper_url": "https://huggingface.co/papers/2512.07783",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
      "abstract": "We develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model‚Äôs edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07783",
      "pdf_url": "https://arxiv.org/pdf/2512.07783",
      "github_links": [
        "https://github.com/Interplay-LM-Reasoning/Interplay-LM-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07783",
      "scraped_at": "2025-12-10T01:47:21.314074"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.03244",
    "authors": [
      "Nanyun Peng",
      "Swastik Roy",
      "Arpit Gupta",
      "Sruthi Gorantla",
      "Salman Rahman"
    ],
    "stars": "0",
    "details": {
      "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
      "abstract": "Please find our paper on training process reward models without ground truth by leveraging inference-time scaling methods, enabling reinforcement learning in domains where verifiable answers are unavailable.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03244",
      "pdf_url": "https://arxiv.org/pdf/2512.03244",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03244",
      "scraped_at": "2025-12-10T01:47:23.147318"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.03621",
    "authors": [
      "Taojun Ding",
      "Jiehui Huang",
      "Mantang Guo",
      "wangshx",
      "Iron-lyk"
    ],
    "stars": "23",
    "details": {
      "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
      "abstract": "Project page: https://recamdriving.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03621",
      "pdf_url": "https://arxiv.org/pdf/2512.03621",
      "github_links": [
        "https://github.com/Iron-LYK/ReCamDriving"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03621",
      "scraped_at": "2025-12-10T01:47:24.938487"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.06533",
    "authors": [
      "Jiacheng Chen",
      "Ziniu Li",
      "Sheng Tang",
      "Ming Chen",
      "trxcc2002"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06533",
      "pdf_url": "https://arxiv.org/pdf/2512.06533",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06533",
      "scraped_at": "2025-12-10T01:47:26.723851"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.06373",
    "authors": [
      "Yansong Tang",
      "Haoji Zhang",
      "Jingxuan Niu",
      "Wenlong Liu",
      "VoyageWang"
    ],
    "stars": "11",
    "details": {
      "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
      "abstract": "The project page is https://github.com/VoyageWang/VG-Refiner",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06373",
      "pdf_url": "https://arxiv.org/pdf/2512.06373",
      "github_links": [
        "https://github.com/VoyageWang/VG-Refiner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06373",
      "scraped_at": "2025-12-10T01:47:28.523139"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.06589",
    "authors": [
      "Simeng Qin",
      "Teng Ma",
      "Qi Guo",
      "Jie Liao",
      "jiaxiaojunQAQ"
    ],
    "stars": "0",
    "details": {
      "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
      "abstract": "This work presents OmniSafeBench-MM, a unified, open-source benchmark and toolbox designed for comprehensive evaluation of multimodal jailbreak attack and defense methods. It integrates 13 representative attack techniques, 15 defense strategies, and a diverse dataset spanning 9 risk domains and 50 fine-grained categories, covering real-world‚Äìrelevant query types (consultative, imperative, declarative). We also propose a three-dimensional evaluation protocol measuring harmfulness (from low-impact individual harm to societal-level threats), intent alignment, and response detail ‚Äî allowing nuanced safety-utility tradeoff analysis. Our extensive experiments across 10 open-source and 8 closed-source multimodal LLMs reveal widespread vulnerabilities to multimodal jailbreak attacks. By unifying data, methods, and evaluation, OmniSafeBench-MM offers a standardized, reproducible platform ‚Äî which we hope will become a foundational resource for future research on safe, robust multimodal LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06589",
      "pdf_url": "https://arxiv.org/pdf/2512.06589",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06589",
      "scraped_at": "2025-12-10T01:47:30.348829"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.07829",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
      "abstract": "We proposed FAE which adapts pretrained ViT as the latent space for visual generative models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07829",
      "pdf_url": "https://arxiv.org/pdf/2512.07829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07829",
      "scraped_at": "2025-12-10T01:47:32.150504"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Group Representational Position Encoding",
    "paper_url": "https://huggingface.co/papers/2512.07805",
    "authors": [],
    "stars": "30",
    "details": {
      "title": "Group Representational Position Encoding",
      "abstract": "Introducing GRAPE: Group Representational Position Encoding. Embracing General Relative Law of Position Encoding, unifying and improving Multiplicative and Additive Position Encoding, such as RoPE and Alibi! Better performance with a clear theoretical formulation! Project Page: https://model-architectures.github.io/GRAPE/ Paper: https://model-architectures.github.io/GRAPE/GRAPE.pdf Devoted to the frontier of superintelligence, hope you will enjoy it!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07805",
      "pdf_url": "https://arxiv.org/pdf/2512.07805",
      "github_links": [
        "https://github.com/model-architectures/GRAPE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07805",
      "scraped_at": "2025-12-10T01:47:33.925881"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.06835",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
      "abstract": "Experiment Results üìä We evaluate DoGe on 7 benchmarks covering: General visual reasoning & hallucination (MMMU, MMStar, HallBench) Specialized domain reasoning (MathVision, MathVista, ChemBench, MSEarthMCQ) 3B-level Models Performance Method MMMU MMStar HallBench MathVision MathVista ChemBench MSEarthMCQ Avg. InternVL2.5-2B 43.6 53.7 42.6 13.5 51.3 - - - Visionary-3B 40.7 50.5 59.8 17.1 54.7 40.8 38.2 43.1 Qwen2.5VL-3B* (Base) 41.0 49.3 60.6 18.7 48.8 43.4 40.8 43.2 DoGe-3B (Iter1) 46.6 54.5 61.5 21.7 ü•á57.9 45.8 ü•á48.3 48.0 DoGe-3B (Iter2) 48.9 52.5 ü•á62.5 23.1 54.2 ü•á47.7 46.2 47.9 DoGe-3B (Iter3) ü•á50.2 ü•á54.7 61.8 ü•á24.2 57.0 46.9 47.3 ü•á48.9 ‚¨ÜÔ∏è Max Gain (vs. Base) +9.2 +5.4 +1.9 +5.5 +9.1 +4.3 +7.5 +5.7 7B-level Models Performance Method MMMU MMStar HallBench MathVision MathVista ChemBench MSEarthMCQ Avg. InternVL2.5-8B 48.9 62.8 50.1 22.0 64.4 - - - Vision-R1-7B 46.9 60.8 66.7 ü•á29.0 68.5 46.0 44.1 51.7 Qwen2.5VL-7B* (Base) 49.9 60.7 66.3 23.6 64.1 48.6 43.3 50.9 DoGe-7B (Iter1) 53.1 ü•á63.2 54.4 24.3 62.1 48.7 46.4 50.3 DoGe-7B (Iter2) 50.9 60.0 ü•á68.3 25.3 ü•á68.8 ü•á49.0 ü•á46.5 52.7 DoGe-7B (Iter3) ü•á53.6 63.0 68.0 25.2 68.3 48.5 45.8 ü•á53.2 ‚¨ÜÔ∏è Max Gain (vs. Base) +3.7 +2.5 +2.0 +1.7 +4.7 +0.4 +3.2 +2.3 Key Takeaways ‚ú® Stable Self-Evolution : DoGe achieves consistent performance improvement across 3 iterations for both 3B and 7B models Domain Generalization : 3B models: Average +5.7% performance gain across all benchmarks 7B models: Average +2.3% performance gain (maintains superiority over strong baselines) Hallucination Reduction : +2.0% average improvement on HallBench, mitigating visual hallucination Data Efficiency : Excels in data-scarce domains (Chemistry, Earth Science) with limited manual annotations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06835",
      "pdf_url": "https://arxiv.org/pdf/2512.06835",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06835",
      "scraped_at": "2025-12-10T01:47:35.762943"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "paper_url": "https://huggingface.co/papers/2512.06963",
    "authors": [
      "Yaobo Liang",
      "Zhiying Du",
      "Fangyun Wei",
      "godjiaolongge",
      "ys3197"
    ],
    "stars": "0",
    "details": {
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "abstract": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06963",
      "pdf_url": "https://arxiv.org/pdf/2512.06963",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06963",
      "scraped_at": "2025-12-10T01:47:37.689331"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2512.06421",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
      "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06421",
      "pdf_url": "https://arxiv.org/pdf/2512.06421",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06421",
      "scraped_at": "2025-12-10T01:47:39.463814"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
    "paper_url": "https://huggingface.co/papers/2512.06791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
      "abstract": "Gradient methods in games are usually proven to converge only under strong monotonicity in the Euclidean geometry (Rosen-style assumptions). That fails even for simple coupled quadratic games, yet in practice we still often see convergence. This paper takes a structural ‚Äúdesign the geometry‚Äù viewpoint. Small-Gain Nash (SGN) uses per-player curvature and cross-player coupling bounds to build a block-weighted metric where the pseudo-gradient becomes strongly monotone and the joint gradient flow is contracting on a certified region. Once SGN holds on a region, you get: ‚Äì existence + uniqueness of a Nash equilibrium there, ‚Äì exponential contraction of the continuous flow, ‚Äì explicit safe step-size bounds for projected Euler and RK4 via a game-theoretic CFL number, and ‚Äì a finite ‚Äútimescale band‚Äù that plays a TTUR-like role i.e instead of vanishing two-timescale step sizes, it tells you for which relative player weights a single global step size is provably stable. The paper ends with an offline certification pipeline that estimates curvature/couplings on compact regions, optimizes the metric to enlarge the SGN margin, and certifies convergence in non-monotone quadratic and Markov games (including mirror/Fisher geometries for entropy-regularized policy gradient). code: https://github.com/AashVed/SmallGainNash",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06791",
      "pdf_url": "https://arxiv.org/pdf/2512.06791",
      "github_links": [
        "https://github.com/AashVed/SmallGainNash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06791",
      "scraped_at": "2025-12-10T01:47:41.245034"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "paper_url": "https://huggingface.co/papers/2512.06609",
    "authors": [
      "Wendi Zheng",
      "jerytang",
      "Ya-Qin",
      "jmhernandezlobato",
      "xutongda"
    ],
    "stars": "10",
    "details": {
      "title": "Vector Quantization using Gaussian Variational Autoencoder",
      "abstract": "State-of-the-Art VQ-VAE from Gaussian VAE without Training! We train a Gaussian VAE, convert it into VQ-VAE with almost 100% codebook usage, and keeps reconstruction performance! As flexible to setup as VQ-VAE, supporting: codebook size, codebook dimension, codebook number. Pre-trained models can be found in [Huggingface] Paper can be found in [Arxiv] Code can be found in [Github] Quick Start Install dependency dependency in environment.yaml conda env create --file=environment.yaml\nconda activate tokenizer Install this package from source pip install -e . [optional] CUDA kernel for fast run time cd gq_cuda_extension\npip install --no-build-isolation -e . Download pre-trained model Download model \"sd3unet_gq_0.25.ckpt\" from [Huggingface] : mkdir model_256 mv \"sd3unet_gq_0.25.ckpt\" ./model_256 This is a VQ-VAE with codebook_size=2**16=65536 and codebook_dim=16 Infer the model as VQ-VAE Then use the model as follows from PIL import Image from torchvision import transforms from omegaconf import OmegaConf from pit.util import instantiate_from_config import torch\n\ntransform = transforms.Compose([\n    transforms.Resize(( 256 , 256 )),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[ 0.5 , 0.5 , 0.5 ],\n                        std=[ 0.5 , 0.5 , 0.5 ])\n])\n\nimg = transform(Image. open ( \"demo.png\" )).unsqueeze( 0 ).cuda()\nconfig = OmegaConf.load( \"./configs/sd3unet_gq_0.25.yaml\" )\nvae = instantiate_from_config(config.model)\nvae.load_state_dict(\n    torch.load( \"models_256/sd3unet_gq_0.25.ckpt\" ,\n        map_location=torch.device( 'cpu' ))[ \"state_dict\" ],strict= False )\nvae = vae. eval ().cuda()\n\nvae. eval ()\nz, log = vae.encode(img, return_reg_log= True ) \nimg_hat = vae.dequant(log[ \"indices\" ]) # discrete indices img_hat = vae.decode(z) # quantized latent Infer the model as Gaussian VAE Alternatively, the model can be used as a Vanilla Gaussian VAE: from PIL import Image from torchvision import transforms from omegaconf import OmegaConf from pit.util import instantiate_from_config import torch\n\ntransform = transforms.Compose([\n    transforms.Resize(( 256 , 256 )),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[ 0.5 , 0.5 , 0.5 ],\n                        std=[ 0.5 , 0.5 , 0.5 ])\n])\n\nimg = transform(Image. open ( \"demo.png\" )).unsqueeze( 0 ).cuda()\nconfig = OmegaConf.load( \"./configs/sd3unet_gq_0.25.yaml\" )\nvae = instantiate_from_config(config.model)\nvae.load_state_dict(\n    torch.load( \"models_256/sd3unet_gq_0.25.ckpt\" ,\n        map_location=torch.device( 'cpu' ))[ \"state_dict\" ],strict= False )\nvae = vae. eval ().cuda()\n\nvae. eval ()\n\nz = vae.encode(img, return_reg_log= True )[ 1 ][ \"zhat_noquant\" ] # Gaussian VAE latents img_hat = vae.decode(z) Train your own VQ-VAE Determine the VQ-VAE parameters: codebook_size: the codebook size, must be 2**N codebook_dimension: the dimension for each codebook codebook_number: number of sub codebook per spatial dimension Setup \"sd3unet_gq_0.25.yaml\" according to VQ-VAE parameters: n_samples: = codebook_size size, must be 2**N group: = codebook_dimension, dim of each codebook z_channels: = codebook_dimension * codebook_number, total dim of codebook Setup \"sd3unet_gq_0.25.yaml\" according to dataset path root: dataset root image_size: target image size batch_size: batch size Run the training! The default \"sd3unet_gq_0.25.yaml\" is setup for codebook_dimension=16, codebook_number=1, codebook_size=2**16=65536 export WANDB_API_KEY= $YOUR_WANDB_API_KEY python main.py --base configs/sd3unet_gq_0.25.yaml --wandb Run the evaluation! After the training, obtain the ckpt in $CKPT_PATH. Then, evaluate the model as python -m torch.distributed.launch --standalone --use-env \\\n    --nproc-per-node=8 eval.py \\\n    --bs=16 \\\n    --img_size 256 \\\n    --base=/workspace/cogview_dev/xutd/xu/pytorch-image-tokenizer/configs/sd3unet_gq_0.25.yaml \\\n    --ckpt= $CKPT_PATH \\\n    --dataset= $IMAGE_FOLDER_PATH Train with VAVAE Like Alignment See \"configs/sd3unet_gq_0.25_vf.yaml\". Why it Works? The only difference between our Gaussian VAE and vanilla Gaussian VAE is the KL divergence penralization. The key difference is class \"GaussianQuantRegularizer\" in \"./pit/quantization/gaussian.py\". During training, GaussianQuantRegularizer forces each dimension of KL be the same and achieve log(codebook_size). kl2 = 1.4426 * 0.5 * (torch. pow (mu, 2 ) + var - 1.0 - logvar)\nkl2 = kl2.reshape(b,l,self.group,c//self.group)\nkl2 = torch. sum (kl2,dim= 2 ) # sum over group dimension kl2_mean, kl2_min, kl2_max = torch.mean(kl2), torch. min (kl2), torch. max (kl2)\n\nge = (kl2 > self.log_n_samples + self.tolerance). type (kl2.dtype) * self.lam_max\neq = (kl2 <= self.log_n_samples + self.tolerance). type (kl2.dtype) * (\n    kl2 >= self.log_n_samples - self.tolerance\n). type (kl2.dtype)\nle = (kl2 < self.log_n_samples - self.tolerance). type (kl2.dtype) * self.lam_min\nkl_loss = torch. sum ((ge * kl2 + eq * kl2 + le * kl2), dim=[ 1 , 2 ])\nkl_loss = torch. sum (kl_loss) / kl_loss.shape[ 0 ] During inference, GaussianQuantRegularizer create a codebook of iid Gaussian, and find the cloest sample to posterior mean. q_normal_dist = Normal(mu_q[:, None , :], std_q[:, None , :])\nlog_ratios = (\n    q_normal_dist.log_prob(self.prior_samples[ None ])\n    - self.normal_log_prob[ None ] * self.beta\n)\nperturbed = torch. sum (log_ratios, dim= 2 )\nargmax_indices = torch.argmax(perturbed, dim= 1 )\nzhat[i : i + bs] = torch.index_select(self.prior_samples, 0 , argmax_indices)\nindices[i : i + bs] = argmax_indices Basically we limit the KL divergence of Gaussian VAE close to log2 codebook size. Once this constraint is met, the Gaussian VAE can be converted to VQ-VAE without much loss. For more information, see our paper! Contact & Ack Largely from https://github.com/Stability-AI/generative-models Any questions or comments goes to: x.tongda@nyu.edu Or if you have wechat: 18510201763 Reference @ misc {xu2025vectorquantizationusinggaussian,\n      title={Vector Quantization using Gaussian Variational Autoencoder}, \n      author={Tongda Xu and Wendi Zheng and Jiajun He and Jose Miguel Hernandez-Lobato and Yan Wang and Ya-Qin Zhang and Jie Tang},\n      year={2025},\n      eprint={2512.06609},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2512.06609}, \n}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06609",
      "pdf_url": "https://arxiv.org/pdf/2512.06609",
      "github_links": [
        "https://github.com/Stability-AI/generative-models",
        "https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06609",
      "scraped_at": "2025-12-10T01:47:43.136663"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
    "paper_url": "https://huggingface.co/papers/2512.03704",
    "authors": [
      "YijunLiao"
    ],
    "stars": "2",
    "details": {
      "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
      "abstract": "üî• Solving \"State Inertia\" in Long-Context LLMs! We introduce DZ-TDPO, a non-destructive alignment framework. Problem: Standard DPO causes \"Alignment Tax\" (PPL explosion >100) when updating user states in long context. Solution: Dynamic KL Constraints + Dual-Zone Temporal Attention. Result: SOTA 55.4% Win Rate on MSC dataset with Zero PPL degradation (PPL ~26.0). üöÄ Code & SOTA Model (Phi-3.5) are released! Check the Linked Models section.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03704",
      "pdf_url": "https://arxiv.org/pdf/2512.03704",
      "github_links": [
        "https://github.com/lyj20071013/DZ-TDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03704",
      "scraped_at": "2025-12-10T01:47:44.960333"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
    "paper_url": "https://huggingface.co/papers/2512.07168",
    "authors": [
      "Linsey Pang",
      "Aaron Elkins",
      "Aman Chadha",
      "Christos Constantinou",
      "Georgios Ioannides"
    ],
    "stars": "0",
    "details": {
      "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
      "abstract": "This paper introduces JEPA+DAAM , a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Gaussian mixture‚Äìbased Density Adaptive Attention Mechanism (DAAM) to learn semantically rich and highly compressible speech representations, achieving reversible neural tokenization at 47.5 tokens/sec with strong reconstruction quality. ‚û°Ô∏è ùêäùêûùê≤ ùêáùê¢ùê†ùê°ùê•ùê¢ùê†ùê°ùê≠ùê¨ ùê®ùêü ùêâùêÑùêèùêÄ+ùêÉùêÄùêÄùêå: üß† ùë±ùë¨ùë∑ùë® ùë≠ùíêùíì ùë∫ùíÜùíçùíá-ùë∫ùíñùíëùíÜùíìùíóùíäùíîùíÜùíÖ ùë∫ùíëùíÜùíÜùíÑùíâ ùë¨ùíèùíÑùíêùíÖùíäùíèùíà: Decouples representation learning from waveform reconstruction using a masked-prediction JEPA objective in latent space. The encoder learns semantically meaningful embeddings at 2.5 Hz without requiring low-level waveform loss, enabling robust self-supervised pretraining and cross-task adaptability (ASR, TTS, voice conversion). üéØ ùë´ùíÜùíèùíîùíäùíïùíö ùë®ùíÖùíÇùíëùíïùíäùíóùíÜ ùë®ùíïùíïùíÜùíèùíïùíäùíêùíè (ùë´ùë®ùë®ùë¥): Introduces a Gaussian mixture‚Äìbased gating attention mechanism that modulates temporal features based on local statistical salience rather than pairwise dot-products. This allows adaptive feature selection and hierarchical speech-structure discovery with linear complexity, improving JEPA convergence (loss 0.09 vs 0.17 without DAAM). üß© ùë≠ùë∫ùë∏ + ùë¥ùíäùíôùíÜùíÖ-ùëπùíÇùíÖùíäùíô ùëªùíêùíåùíÜùíèùíäùíõùíÇùíïùíäùíêùíè: Implements Finite Scalar Quantization (FSQ) with mixed-radix integer packing for reversible, codebook-free tokenization (47.5 tokens/sec, 16 384-way vocabulary). Combined with a HiFi-GAN decoder, it achieves high-fidelity waveform reconstruction, outperforming or matching neural audio codecs like SoundStream and EnCodec at dramatically lower frame rates.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07168",
      "pdf_url": "https://arxiv.org/pdf/2512.07168",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07168",
      "scraped_at": "2025-12-10T01:47:46.822030"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "paper_url": "https://huggingface.co/papers/2512.06558",
    "authors": [
      "Ganesh Nanduru",
      "amanchadha",
      "Anubis91",
      "alexiglad",
      "mmiakashs"
    ],
    "stars": "0",
    "details": {
      "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
      "abstract": "The paper introduces Refer360 , a comprehensive multimodal dataset for embodied referring expression comprehension in human-robot interaction (HRI), and proposes MuRes , a lightweight guided residual module that selectively reinforces modality-specific features to improve multimodal grounding performance in real-world scenarios. ‚û°Ô∏è ùêäùêûùê≤ ùêáùê¢ùê†ùê°ùê•ùê¢ùê†ùê°ùê≠ùê¨ ùê®ùêü ùê≠ùê°ùêû ùêëùêûùêüùêûùê´ùüëùüîùüé ùêÅùêûùêßùêúùê°ùê¶ùêöùê´ùê§ + ùêåùêÆùêëùêûùê¨ ùêåùê®ùêùùêÆùê•ùêû : üß† ùëπùíÜùíáùíÜùíìùüëùüîùüé: ùë≠ùíäùíìùíîùíï ùë¨ùíéùíÉùíêùíÖùíäùíÜùíÖ ùëπùë¨ ùë´ùíÇùíïùíÇùíîùíÜùíï ùíòùíäùíïùíâ ùë¥ùíñùíçùíïùíä-ùëΩùíäùíÜùíò, ùë¥ùíñùíçùíïùíä-ùë∫ùíÜùíèùíîùíêùíì ùë¥ùíêùíÖùíÇùíçùíäùíïùíäùíÜùíî : Introduces a dataset with synchronized egocentric and exocentric views , RGB, depth, infrared, 3D skeleton , eye gaze , and audio , across indoor and outdoor environments. With 13,990 annotated interactions (3.2M frames), it overcomes biases in existing datasets (e.g., single view, indoor-only, no gesture/gaze integration). üîÅ ùë¥ùíñùëπùíÜùíî: ùëÆùíñùíäùíÖùíÜùíÖ ùëπùíÜùíîùíäùíÖùíñùíÇùíç ùë©ùíêùíïùíïùíçùíÜùíèùíÜùíÑùíå ùíáùíêùíì ùë¥ùíñùíçùíïùíäùíéùíêùíÖùíÇùíç ùë≠ùíñùíîùíäùíêùíè : Proposes a novel residual architecture that uses cross-attention to guide modality-specific signals (visual/language) through an information bottleneck , preventing feature dilution during fusion and outperforming both vanilla residuals and attention-only fusion across 4 datasets. üìà ùë∫ùíäùíàùíèùíäùíáùíäùíÑùíÇùíèùíï ùëÆùíÇùíäùíèùíî ùíÇùíÑùíìùíêùíîùíî ùëØùëπùë∞ ùíÇùíèùíÖ ùëΩùë∏ùë® ùëªùíÇùíîùíåùíî : On Refer360, integrating MuRes into CLIP improved IOU-25 by +3.4% , and on CAESAR-PRO by +4.99% . For broader VQA tasks like ScienceQA and A-OKVQA, MuRes boosted model accuracy by up to +30% , highlighting its generalization ability across task domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06558",
      "pdf_url": "https://arxiv.org/pdf/2512.06558",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06558",
      "scraped_at": "2025-12-10T01:47:48.625230"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.06032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
      "abstract": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3 (also called SAMv2 and SAMv3). We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAMv3. SAMv2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAMv3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAMv2 with multimodal fusion and text-conditioned mask generation of SAMv3; (2) Architectural Divergence, detailing pure vision-temporal design of SAMv2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAMv3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAMv3; (4) Training and Hyperparameter Distinctions, showing why SAMv2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAMv3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06032",
      "pdf_url": "https://arxiv.org/pdf/2512.06032",
      "github_links": [
        "https://github.com/Applied-AI-Research-Lab/The-SAM2-to-SAM3-Gap-in-the-Segment-Anything-Model-Family"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06032",
      "scraped_at": "2025-12-10T01:47:50.436796"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "paper_url": "https://huggingface.co/papers/2512.08765",
    "authors": [],
    "stars": "197",
    "details": {
      "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
      "abstract": "NeurIPS 2025: Wan-Move: Motion-controllable Video Generation viaLatent Trajectory Guidance",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08765",
      "pdf_url": "https://arxiv.org/pdf/2512.08765",
      "github_links": [
        "https://github.com/ali-vilab/Wan-Move"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08765",
      "scraped_at": "2025-12-11T01:47:36.940817"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "paper_url": "https://huggingface.co/papers/2512.08478",
    "authors": [
      "Muyao Niu",
      "Yifan Zhan",
      "Yifei Liu",
      "Yuning Gong",
      "Zuica96"
    ],
    "stars": "162",
    "details": {
      "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
      "abstract": "TL;DR: Visionary is an open, web-native platform built on WebGPU and ONNX Runtime. Enabling real-time rendering of diverse Gaussian Splatting variants (3DGS, MLP-based 3DGS, 4DGS, Neural Avatars and ‚ú®any future algorithms‚ú®), and traditional 3d Mesh, directly in the browser. It also supports post-processing using feed-forward networks. ‚Ä¢ üíª GitHub: https://github.com/Visionary-Laboratory/visionary ‚Ä¢ üåç Project pageÔºö https://visionary-laboratory.github.io/visionary/ ‚Ä¢ üé¨ Video: https://www.youtube.com/watch?v=-K8EjMfk09c ‚Ä¢ üìù Technical report: https://arxiv.org/abs/2512.08478",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08478",
      "pdf_url": "https://arxiv.org/pdf/2512.08478",
      "github_links": [
        "https://github.com/Visionary-Laboratory/visionary"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08478",
      "scraped_at": "2025-12-11T01:47:38.895910"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
    "paper_url": "https://huggingface.co/papers/2512.07951",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
      "abstract": "Project webpage: this https URL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07951",
      "pdf_url": "https://arxiv.org/pdf/2512.07951",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07951",
      "scraped_at": "2025-12-11T01:47:40.786795"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
    "paper_url": "https://huggingface.co/papers/2512.07802",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07802",
      "pdf_url": "https://arxiv.org/pdf/2512.07802",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07802",
      "scraped_at": "2025-12-11T01:47:42.750925"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
    "paper_url": "https://huggingface.co/papers/2512.07843",
    "authors": [
      "Xiuyu Li",
      "Tsu-Jui Fu",
      "Sida Wang",
      "katanaxu",
      "longlian"
    ],
    "stars": "0",
    "details": {
      "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07843",
      "pdf_url": "https://arxiv.org/pdf/2512.07843",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07843",
      "scraped_at": "2025-12-11T01:47:44.703963"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
    "paper_url": "https://huggingface.co/papers/2512.06864",
    "authors": [
      "Dim P. Papadopoulos",
      "Kaixuan Lu",
      "monurcan"
    ],
    "stars": "3",
    "details": {
      "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
      "abstract": "Accepted at WACV'26! Keywords: Video Instance Segmentation; Unsupervised Learning; Segmentation Quality Assessment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06864",
      "pdf_url": "https://arxiv.org/pdf/2512.06864",
      "github_links": [
        "https://github.com/wcbup/AutoQ-VIS/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06864",
      "scraped_at": "2025-12-11T01:47:46.583510"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
    "paper_url": "https://huggingface.co/papers/2512.05033",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
      "abstract": "Modern large language models achieve impressive reasoning capabilities with long chains of thought, but they incur substantial computational cost at inference time. Speculative decoding improves efficiency by using a fast, less accurate draft model to propose tokens that are then verified in parallel by a stronger target model. However, on reasoning tasks, traditional token-level speculative decoding often rejects many semantically valid steps due to superficial token mismatches. Recent step-level semantic verification methods mitigate this by accepting or rejecting entire reasoning steps, but they still waste target compute by regenerating many rejected steps that yield little quality gain. We propose ARBITRAGE , a step-level speculative generation framework that dynamically routes generation based on the relative advantage of the target model over the draft model. Instead of relying on a fixed acceptance threshold, ARBITRAGE uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal ‚Äúarbitrage oracle‚Äù that always selects the higher-quality step, achieving near-optimal efficiency‚Äìaccuracy trade-offs. Across multiple mathematical reasoning benchmarks, ARBITRAGE consistently outperforms prior step-level speculative decoding baselines, reducing inference latency by up to approximately 2√ó at matched accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05033",
      "pdf_url": "https://arxiv.org/pdf/2512.05033",
      "github_links": [
        "https://github.com/SqueezeAILab/Arbitrage"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05033",
      "scraped_at": "2025-12-11T01:47:48.469544"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "paper_url": "https://huggingface.co/papers/2512.06628",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
      "abstract": "We propose MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06628",
      "pdf_url": "https://arxiv.org/pdf/2512.06628",
      "github_links": [
        "https://github.com/Richard-Zhang-AI/MIND-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06628",
      "scraped_at": "2025-12-11T01:47:50.392850"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.02231",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
      "abstract": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02231",
      "pdf_url": "https://arxiv.org/pdf/2512.02231",
      "github_links": [
        "https://github.com/plnguyen2908/AV-SpeakerBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02231",
      "scraped_at": "2025-12-11T01:47:52.347320"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "DeepCode: Open Agentic Coding",
    "paper_url": "https://huggingface.co/papers/2512.07921",
    "authors": [
      "Chao Huang",
      "Xubin Ren",
      "Zirui Guo",
      "Zhonghang Li",
      "Zongwei Li"
    ],
    "stars": "11.8k",
    "details": {
      "title": "DeepCode: Open Agentic Coding",
      "abstract": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07921",
      "pdf_url": "https://arxiv.org/pdf/2512.07921",
      "github_links": [
        "https://github.com/HKUDS/DeepCode"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07921",
      "scraped_at": "2025-12-11T01:47:54.229208"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.08153",
    "authors": [
      "Weirui Ye",
      "Zheng Ding"
    ],
    "stars": "0",
    "details": {
      "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
      "abstract": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4√ó faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08153",
      "pdf_url": "https://arxiv.org/pdf/2512.08153",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08153",
      "scraped_at": "2025-12-11T01:47:56.074458"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.06776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
      "abstract": "NBDiff: A principled path from AR to Diffusion LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06776",
      "pdf_url": "https://arxiv.org/pdf/2512.06776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06776",
      "scraped_at": "2025-12-11T01:47:57.970248"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
    "paper_url": "https://huggingface.co/papers/2512.08924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
      "abstract": "üìç A simple, unified interface for 3D tracking, depth, and pose üåü SOTA results on 4D reconstruction & tracking üöÄ Up to 100x faster pose estimation than prior works",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08924",
      "pdf_url": "https://arxiv.org/pdf/2512.08924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08924",
      "scraped_at": "2025-12-11T01:47:59.945687"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Modular Neural Image Signal Processing",
    "paper_url": "https://huggingface.co/papers/2512.08564",
    "authors": [
      "Michael S. Brown",
      "Ran Zhang",
      "Zhongling Wang",
      "mafifi"
    ],
    "stars": "1",
    "details": {
      "title": "Modular Neural Image Signal Processing",
      "abstract": "Modular Neural Image Signal Processing üé¨ Click to watch the video We present a modular neural image signal processing (ISP) framework that produces high-quality display-referred images while providing a high degree of modularity with explicit control over multiple intermediate stages of the rendering pipeline. Our ISP is fully differentiable and requires no manual tuning, and its modular structure not only improves rendering accuracy but also enhances scalability, debuggability, generalization to unseen cameras, and flexibility to support different user-preference picture styles within a lightweight and efficient design. On top of this modular neural ISP, we developed a user-interactive photo-editing tool that supports diverse editing operations, different picture styles, and enables unlimited post-editable re-rendering and re-styling. The tool accepts DNG raw images from any camera as well as sRGB images from third-party sources. Across multiple test sets, our method consistently delivers competitive qualitative and quantitative performance. Links: paper - code",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08564",
      "pdf_url": "https://arxiv.org/pdf/2512.08564",
      "github_links": [
        "https://github.com/mahmoudnafifi/modular_neural_isp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08564",
      "scraped_at": "2025-12-11T01:48:01.936587"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
    "paper_url": "https://huggingface.co/papers/2512.08186",
    "authors": [],
    "stars": "455",
    "details": {
      "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
      "abstract": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-Language Navigation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08186",
      "pdf_url": "https://arxiv.org/pdf/2512.08186",
      "github_links": [
        "https://github.com/InternRobotics/InternNav"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08186",
      "scraped_at": "2025-12-11T01:48:03.889865"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
    "paper_url": "https://huggingface.co/papers/2512.08868",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
      "abstract": "EcomBench introduces a holistic e-commerce benchmark to evaluate foundation agents on real-world tasks, emphasizing deep retrieval, multi-step reasoning, and cross-source knowledge integration.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08868",
      "pdf_url": "https://arxiv.org/pdf/2512.08868",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08868",
      "scraped_at": "2025-12-11T01:48:05.859496"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
    "paper_url": "https://huggingface.co/papers/2512.08358",
    "authors": [
      "Tianyu Huang",
      "Peng Li",
      "Jiacheng Deng",
      "Jiahao Lu",
      "xwt123"
    ],
    "stars": "0",
    "details": {
      "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
      "abstract": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08358",
      "pdf_url": "https://arxiv.org/pdf/2512.08358",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08358",
      "scraped_at": "2025-12-11T01:48:07.804365"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2512.07197",
    "authors": [
      "Soohyun Lee",
      "Seokhyun Youn",
      "ozbro",
      "shbae84",
      "klavna"
    ],
    "stars": "6",
    "details": {
      "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
      "abstract": "project page: https://cmlab-korea.github.io/Awesome-Efficient-GS/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07197",
      "pdf_url": "https://arxiv.org/pdf/2512.07197",
      "github_links": [
        "https://github.com/CMLab-Korea/Awesome-Efficient-GS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07197",
      "scraped_at": "2025-12-11T01:48:09.673066"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
    "paper_url": "https://huggingface.co/papers/2512.06531",
    "authors": [
      "arghadip2002",
      "Necromancer0912"
    ],
    "stars": "0",
    "details": {
      "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
      "abstract": "We are excited to share our new work tackling the critical challenge of brain tumor detection from MRI scans. Due to high data volume and generalization issues in existing systems, we developed two novel deep learning architectures: SAETCN (Self-Attention Enhancement Tumor Classification Network): A classification model achieving a state-of-the-art 99.38% validation accuracy in classifying four classes (glioma, meningioma, pituitary, and non-tumor). Its self-attention mechanism significantly improves generalization and robustness, overcoming common pitfalls in CAD systems. SAS-Net (Self-Attentive Segmentation Network): For precise tumor localization, achieving 99.23% overall pixel accuracy in segmentation. This paper proposes one of the most accurate and generalized DL architectures for early, automatic brain tumor detection. We hope this work can serve as a strong baseline for future Computer-Aided Diagnosis systems. Check out the paper, and we welcome your feedback and discussions! #MedicalImaging #DeepLearning #SelfAttention #CAD #BrainTumor",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06531",
      "pdf_url": "https://arxiv.org/pdf/2512.06531",
      "github_links": [
        "https://github.com/arghadip2002/SAETCN-and-SASNET-Architectures"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06531",
      "scraped_at": "2025-12-11T01:48:11.542854"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05325",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference (2025) Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning (2025) Temporal Predictors of Outcome in Reasoning Language Models (2025) Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models (2025) C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning (2025) Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads (2025) Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05325",
      "pdf_url": "https://arxiv.org/pdf/2512.05325",
      "github_links": [
        "https://github.com/farukakgul/LYNX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05325",
      "scraped_at": "2025-12-11T01:48:13.451502"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
    "paper_url": "https://huggingface.co/papers/2512.08406",
    "authors": [
      "Jungong Han",
      "Yunqi Miao",
      "gaomingqi"
    ],
    "stars": "15",
    "details": {
      "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
      "abstract": "Code & Gradio Demo : https://github.com/gaomingqi/sam-body4d See our FULL demo and Gradio Demo video below:",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08406",
      "pdf_url": "https://arxiv.org/pdf/2512.08406",
      "github_links": [
        "https://github.com/gaomingqi/sam-body4d"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08406",
      "scraped_at": "2025-12-11T01:48:15.477654"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
    "paper_url": "https://huggingface.co/papers/2512.04763",
    "authors": [
      "Mete Ozay",
      "Zeynep Akata",
      "Umberto Michieli",
      "Ondrej Bohdal",
      "mwbini"
    ],
    "stars": "0",
    "details": {
      "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LightMem: Lightweight and Efficient Memory-Augmented Generation (2025) MemVerse: Multimodal Memory for Lifelong Learning Agents (2025) Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks (2025) BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models (2025) HaluMem: Evaluating Hallucinations in Memory Systems of Agents (2025) ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents (2025) LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04763",
      "pdf_url": "https://arxiv.org/pdf/2512.04763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04763",
      "scraped_at": "2025-12-11T01:48:17.833409"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
    "paper_url": "https://huggingface.co/papers/2512.04434",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
      "abstract": "This paper introduces a new deep learning algorithem to model transient flow around varied complex geometries using the deep operator network (DeepONet)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04434",
      "pdf_url": "https://arxiv.org/pdf/2512.04434",
      "github_links": [
        "https://github.com/baskargroup/TimeDependent-DeepONet"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04434",
      "scraped_at": "2025-12-11T01:48:19.704175"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
    "paper_url": "https://huggingface.co/papers/2512.08923",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
      "abstract": "Paper that evaluates and analyses consistency of MLLMs when providing questions in text vs as rendered-text.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08923",
      "pdf_url": "https://arxiv.org/pdf/2512.08923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08923",
      "scraped_at": "2025-12-11T01:48:21.599664"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
    "paper_url": "https://huggingface.co/papers/2512.08309",
    "authors": [
      "xandergos"
    ],
    "stars": "1",
    "details": {
      "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
      "abstract": "Terrain Diffusion introduces a procedural generation primitive built around InfiniteDiffusion, a sampling method that delivers seamless, seed-consistent, infinite-domain generation with constant-time random access. A multi-scale diffusion hierarchy models planetary structure through a stack of diffusion models that couples planetary context with local detail,. The framework can stream entire worlds and is demonstrated in real time through a full Minecraft integration.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08309",
      "pdf_url": "https://arxiv.org/pdf/2512.08309",
      "github_links": [
        "https://github.com/xandergos/terrain-diffusion"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08309",
      "scraped_at": "2025-12-11T01:48:23.507617"
    },
    "scraped_date": "2025-12-11"
  }
]