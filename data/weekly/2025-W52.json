[
  {
    "title": "Latent Implicit Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.21218",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Latent Implicit Visual Reasoning",
      "abstract": "TL;DR: We introduce a new method that improves visual reasoning by allowing models to implicitly learn latent visual representations, without requiring explicit supervision or additional data for these latents.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21218",
      "pdf_url": "https://arxiv.org/pdf/2512.21218",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21218",
      "scraped_at": "2025-12-29T01:56:48.516527"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "paper_url": "https://huggingface.co/papers/2512.20605",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "abstract": "TLDR: This work reveals that autoregressive models inherently learn linearly controllable, temporally abstract action representations within their residual streams, which can be activated and composed to execute long-horizon behaviors. We leverage these emergent abstractions to introduce Internal RL, a method that reinforces semantically meaningful actions inside the residual stream of a sequence model. This enables solving sparse-reward hierarchical tasks that remain intractable for standard token-level approaches like GRPO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20605",
      "pdf_url": "https://arxiv.org/pdf/2512.20605",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20605",
      "scraped_at": "2025-12-29T01:56:50.390358"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "paper_url": "https://huggingface.co/papers/2512.15716",
    "authors": [],
    "stars": "90",
    "details": {
      "title": "Spatia: Video Generation with Updatable Spatial Memory",
      "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as: Explicit Camera Control 3D-Aware Interactive Editing Long-horizon Scene Exploration",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15716",
      "pdf_url": "https://arxiv.org/pdf/2512.15716",
      "github_links": [
        "https://github.com/ZhaoJingjing713/Spatia"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15716",
      "scraped_at": "2025-12-29T01:56:52.500760"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "paper_url": "https://huggingface.co/papers/2512.19995",
    "authors": [
      "Tianyi Zhou",
      "Soheil Feizi",
      "Yize Cheng",
      "Chenrui Fan",
      "Ming Li"
    ],
    "stars": "14",
    "details": {
      "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "abstract": "We extend a cognitive science-inspired episode annotation framework to an automatic, scalable, sentence-level representation that supports large-scale analysis of reasoning traces and conduct a systematic study of reasoning dynamics across a diverse set of LLMs. Moreover, we demonstrate the practical utility of episode-level representations through downstream case studies on correctness and efficiency, illustrating how reasoning dynamics can be analyzed beyond outcome-based metrics. Key Findings: When reasoning traces are analyzed at the episode level, a functional progression from abstract reasoning to concrete execution, and finally to evaluative control, consistently emerges. Episodes associated with analysis and exploration use more abstract, conceptual language and decrease steadily as reasoning progresses, while execution-oriented episodes dominate the middle of the trace through sustained concrete operations. In contrast, verification-related episodes are characterized by evaluative and meta-level language and increase toward the end of the reasoning process. Comparing reasoning and non-reasoning models, the difference is not merely how many tokens they generate, but how reasoning is structured. Non-reasoning models allocate most of their response trace to execution , with episode transitions largely following a feed-forward pattern toward implementation. In contrast, reasoning models distribute effort across analysis, exploration, execution, and verification, and exhibit frequent iterative Explore-Monitor/Verify loops. Through our correctness-oriented case study, we find that exploration reflects uncertainty and serves as a critical branching point : correct solutions more often route exploration into monitoring or re-analysis, whereas incorrect solutions tend to continue execution or terminate prematurely after exploration. Through our efficiency-oriented case study, we find that different efficient reasoning methods selectively suppress evaluation-oriented episodes and feedback loops, leading to varying degrees of divergence from the reasoning patterns of the base model. Episode-level analysis thus reveals which episodes can be removed to gain efficiency, beyond token-level pruning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19995",
      "pdf_url": "https://arxiv.org/pdf/2512.19995",
      "github_links": [
        "https://github.com/MingLiiii/ThinkARM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19995",
      "scraped_at": "2025-12-29T01:56:54.385195"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "paper_url": "https://huggingface.co/papers/2512.19949",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "How Much 3D Do Video Foundation Models Encode?",
      "abstract": "After training on large 2D videos, will video foundation models naturally encode 3D structure and ego-motion? Our study reveals that state-of-the-art video generators develop strong, generalizable 3D understanding even compared to 3D experts, despite being trained only on 2D video data. Project page: https://vidfm-3d-probe.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19949",
      "pdf_url": "https://arxiv.org/pdf/2512.19949",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19949",
      "scraped_at": "2025-12-29T01:56:56.242485"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "VA-œÄ: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2512.19680",
    "authors": [
      "Yicong Li",
      "Xiaoye Qu",
      "Kai Xu",
      "Qiyuan He",
      "Xinyao Liao"
    ],
    "stars": "4",
    "details": {
      "title": "VA-œÄ: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA- formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA- introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA- enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19680",
      "pdf_url": "https://arxiv.org/pdf/2512.19680",
      "github_links": [
        "https://github.com/Lil-Shake/VA-Pi"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19680",
      "scraped_at": "2025-12-29T01:56:58.061757"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "paper_url": "https://huggingface.co/papers/2512.13043",
    "authors": [
      "Yuanchun Shi",
      "Junliang Xing",
      "Changhao Zhang",
      "Yijun Yang",
      "Tong Wei"
    ],
    "stars": "0",
    "details": {
      "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13043",
      "pdf_url": "https://arxiv.org/pdf/2512.13043",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13043",
      "scraped_at": "2025-12-29T01:56:59.886231"
    },
    "scraped_date": "2025-12-29"
  },
  {
    "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
    "paper_url": "https://huggingface.co/papers/2512.17504",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17504",
      "pdf_url": "https://arxiv.org/pdf/2512.17504",
      "github_links": [
        "https://github.com/myyzzzoooo/InsertAnywhere"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17504",
      "scraped_at": "2025-12-30T01:48:50.975028"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "paper_url": "https://huggingface.co/papers/2512.17220",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
      "abstract": "Our trained models can be downloaded from: https://huggingface.co/MindscapeRAG",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17220",
      "pdf_url": "https://arxiv.org/pdf/2512.17220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17220",
      "scraped_at": "2025-12-30T01:48:53.048565"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
    "paper_url": "https://huggingface.co/papers/2512.22047",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Step-GUI Technical Report (2025) GUI-360\\¬∞: A Comprehensive Dataset and Benchmark for Computer-Using Agents (2025) MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive and MCP-Augmented Environments (2025) OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models (2025) WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation (2025) D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies (2025) AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22047",
      "pdf_url": "https://arxiv.org/pdf/2512.22047",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22047",
      "scraped_at": "2025-12-30T01:48:55.067017"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
    "paper_url": "https://huggingface.co/papers/2512.21675",
    "authors": [
      "Kaiwen Zhu",
      "Xiaohui Li",
      "Jiayang Li",
      "Shuo Cao",
      "Andrew613"
    ],
    "stars": "27",
    "details": {
      "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
      "abstract": "Unipercept",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21675",
      "pdf_url": "https://arxiv.org/pdf/2512.21675",
      "github_links": [
        "https://github.com/thunderbolt215/UniPercept"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21675",
      "scraped_at": "2025-12-30T01:48:57.202905"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "paper_url": "https://huggingface.co/papers/2512.22118",
    "authors": [
      "Kun-Yu Lin",
      "Jian-Jian Jiang",
      "Xiao-Ming Wu",
      "Zhi Ouyang",
      "zhengli1013"
    ],
    "stars": "24",
    "details": {
      "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
      "abstract": "Project page: https://isee-laboratory.github.io/ProEdit",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22118",
      "pdf_url": "https://arxiv.org/pdf/2512.22118",
      "github_links": [
        "https://github.com/iSEE-Laboratory/ProEdit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22118",
      "scraped_at": "2025-12-30T01:48:59.302587"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21859",
    "authors": [
      "Yehan Ma",
      "An Zou",
      "fanqiNO1"
    ],
    "stars": "0",
    "details": {
      "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
      "abstract": "üöÄ Large language models can infer within strict time budgets! üìâ Fixed KV cache eviction or naive speed-up strategies hurt performance under real-time constraints. üéØ TimeBill enables adaptive, time-aware LLM inference by predicting response length and execution time, then dynamically tuning KV cache eviction to meet deadlines without sacrificing quality. üÜï We propose a fine-grained Response Length Predictor (RLP) + workload-guided Execution Time Estimator (ETE) for end-to-end time-budgeted inference with guaranteed completion and high response fidelity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21859",
      "pdf_url": "https://arxiv.org/pdf/2512.21859",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21859",
      "scraped_at": "2025-12-30T01:49:01.499871"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.22120",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
      "abstract": "A framework that leverages programmatically generated paired views to train VLMs to focus on critical visual evidence while rejecting text-only shortcuts.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22120",
      "pdf_url": "https://arxiv.org/pdf/2512.22120",
      "github_links": [
        "https://github.com/zss02/BiPS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22120",
      "scraped_at": "2025-12-30T01:49:03.550021"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
    "paper_url": "https://huggingface.co/papers/2512.21643",
    "authors": [
      "Yixin Chen",
      "Yidi Liu",
      "Xuming He",
      "Zhiwang Zhou",
      "Andrew613"
    ],
    "stars": "0",
    "details": {
      "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
      "abstract": "Submit Omni-Weather",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21643",
      "pdf_url": "https://arxiv.org/pdf/2512.21643",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21643",
      "scraped_at": "2025-12-30T01:49:05.565401"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "paper_url": "https://huggingface.co/papers/2512.18745",
    "authors": [
      "Jierun Chen",
      "Tiezheng Yu",
      "Jiannan Wu",
      "Lewei Yao",
      "m-Just"
    ],
    "stars": "3",
    "details": {
      "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "abstract": "Check out O3-Bench at https://huggingface.co/datasets/m-Just/O3-Bench !",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18745",
      "pdf_url": "https://arxiv.org/pdf/2512.18745",
      "github_links": [
        "https://github.com/m-Just/InSight-o3"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18745",
      "scraped_at": "2025-12-30T01:49:07.649745"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
    "paper_url": "https://huggingface.co/papers/2512.21919",
    "authors": [
      "X. W.",
      "Lei Zhang",
      "Jiawei Chen",
      "Binyuan Hui",
      "KaShun Shum"
    ],
    "stars": "0",
    "details": {
      "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Training Versatile Coding Agents in Synthetic Environments (2025) Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling (2025) Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning (2025) One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents (2025) RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks (2025) SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning (2025) Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21919",
      "pdf_url": "https://arxiv.org/pdf/2512.21919",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21919",
      "scraped_at": "2025-12-30T01:49:09.709108"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.21507",
    "authors": [
      "Xiaojie Xu",
      "Chuanhao Li",
      "Tianmeng Yang",
      "Gongxuan Wang",
      "Wenshuo Peng"
    ],
    "stars": "4",
    "details": {
      "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
      "abstract": "Currently, most of the work focuses on discussing the physical plausibility of the videos; we need more research to examine whether the actions themselves are inherently reasonable. Our project page is available https://github.com/Gloria2tt/SVBench-Evaluation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21507",
      "pdf_url": "https://arxiv.org/pdf/2512.21507",
      "github_links": [
        "https://github.com/Gloria2tt/SVBench-Evaluation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21507",
      "scraped_at": "2025-12-30T01:49:11.751315"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
    "paper_url": "https://huggingface.co/papers/2512.20292",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "abstract": "üîÜ Overview We argue that presentation design is inherently subjective. Users have different preferences in terms of narrative structure, emphasis, conciseness, aesthetic choices, etc. So in this work, we ask: Can we better model such diverse user preferences for personalized paper-to-slides generation? We make the following contributions: Task: We introduce and properly define a new task that conditions paper-to-slide generation on user-specified preferences. System: We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Evaluation: We construct a benchmark dataset that captures diverse user preferences, with meticulously designed interpretable metrics for robust evaluation. Open Source: We release the source code and data to the community. üíª Github: https://github.com/nusnlp/SlideTailor üìú ArXiv: https://arxiv.org/abs/2512.20292 ü§ó HF datasets: https://huggingface.co/datasets/yyyang/SlideTailor-PSP-dataset",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20292",
      "pdf_url": "https://arxiv.org/pdf/2512.20292",
      "github_links": [
        "https://github.com/nusnlp/SlideTailor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20292",
      "scraped_at": "2025-12-30T01:49:13.822291"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
    "paper_url": "https://huggingface.co/papers/2512.21980",
    "authors": [
      "dronperminov"
    ],
    "stars": "0",
    "details": {
      "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
      "abstract": "A 58-addition, rank-23 scheme for exact 3√ó3 matrix multiplication sets a new SOTA. This improves the previous best of 60 additions without basis change. The scheme uses only ternary coefficients {-1,0,1} and was discovered via combinatorial flip-graph search + greedy CSE heuristics.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21980",
      "pdf_url": "https://arxiv.org/pdf/2512.21980",
      "github_links": [
        "https://github.com/dronperminov/ternary_flip_graph"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21980",
      "scraped_at": "2025-12-30T01:49:15.815354"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards",
    "paper_url": "https://huggingface.co/papers/2512.21625",
    "authors": [
      "Zhenduo Zhang",
      "Wayne Xin Zhao",
      "Zhixun Li",
      "Yuliang Zhan",
      "Xinyu Tang"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards",
      "abstract": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21625",
      "pdf_url": "https://arxiv.org/pdf/2512.21625",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21625",
      "scraped_at": "2025-12-30T01:49:18.233273"
    },
    "scraped_date": "2025-12-30"
  },
  {
    "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
    "paper_url": "https://huggingface.co/papers/2512.23447",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
      "abstract": "We propose the Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router‚Äôs decisions with expert capabilities. Unlike prior coupling methods that scale with the number of tokens (often millions per batch), the ERC loss introduces a fixed cost that is independent of batch size. Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, it offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoE models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23447",
      "pdf_url": "https://arxiv.org/pdf/2512.23447",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23447",
      "scraped_at": "2025-12-31T01:49:28.397780"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
    "paper_url": "https://huggingface.co/papers/2512.23576",
    "authors": [
      "Steffi Chern",
      "Jiadi Su",
      "Bohao Tang",
      "Zhulin Hu",
      "Ethan Chern"
    ],
    "stars": "81",
    "details": {
      "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
      "abstract": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23576",
      "pdf_url": "https://arxiv.org/pdf/2512.23576",
      "github_links": [
        "https://github.com/GAIR-NLP/LiveTalk"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23576",
      "scraped_at": "2025-12-31T01:49:30.412534"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
    "paper_url": "https://huggingface.co/papers/2512.22096",
    "authors": [
      "Kaining Ying",
      "Xiaojie Xu",
      "Chuanhao Li",
      "Zhen Li",
      "Xiaofeng Mao"
    ],
    "stars": "426",
    "details": {
      "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
      "abstract": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22096",
      "pdf_url": "https://arxiv.org/pdf/2512.22096",
      "github_links": [
        "https://github.com/stdstu12/YUME"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22096",
      "scraped_at": "2025-12-31T01:49:32.340108"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
    "paper_url": "https://huggingface.co/papers/2512.22322",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
      "abstract": "We introduce SmartSnap , a paradigm shift that transforms GUI agentsüì±üíªü§ñ from passive task executors into proactive self-verifiers. By empowering agents to curate their own evidence of success through the 3C Principles (Completeness, Conciseness, Creativity), we eliminate the bottleneck of expensive post-hoc verification while boosting reliability and performance on complex mobile tasks. SmartSnap redefines the agent's role through a unified policy that handles both task execution and evidence curation . Instead of burdening verifiers with verbose, noisy interaction trajectories, agents learn to select minimal, decisive snapshot evidences from their tool interactions. The framework leverages: Augmented MDP : Agents operate in an extended action space ‚äï consisting of execution actions (click, type, etc.) and curation actions (submit evidence indices) Dual-objective training : GRPO-based RL optimizes for both task completion and evidence quality Dense reward shaping : Multi-component rewards $R_{format}$ + $R_{validity}$ + $R_{complete}$ + $R_{concise}$ guide agents toward becoming effective self-verifiers Creative evidence generation : Agents proactively execute additional actions post-task to capture robust proof when needed The approach achieves up to 26.08% absolute performance gains on AndroidLab across model scales, matching or exceeding much larger models like DeepSeek-V3.1 and Qwen3-235B-A22B.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22322",
      "pdf_url": "https://arxiv.org/pdf/2512.22322",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22322",
      "scraped_at": "2025-12-31T01:49:34.323280"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
    "paper_url": "https://huggingface.co/papers/2512.23705",
    "authors": [],
    "stars": "94",
    "details": {
      "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
      "abstract": "Abstract Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences (1.32M frames) rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines (e.g., Depth-Anything-v2, DepthCrafter), and a normal variant (DKT-Normal) sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame (832√ó480). Integrated into a grasping stack, DKT‚Äôs depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: ‚ÄúDiffusion knows transparency.‚Äù Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation. Code and models are available at https://daniellli.github.io/projects/DKT/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23705",
      "pdf_url": "https://arxiv.org/pdf/2512.23705",
      "github_links": [
        "https://github.com/Daniellli/DKT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23705",
      "scraped_at": "2025-12-31T01:49:36.316397"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.23709",
    "authors": [
      "Po-Fan Yu",
      "Chi-Wei Hsiao",
      "Zhixiang Wang",
      "Chin-Yang Lin",
      "Hau-Shiang Shiu"
    ],
    "stars": "0",
    "details": {
      "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
      "abstract": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23709",
      "pdf_url": "https://arxiv.org/pdf/2512.23709",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23709",
      "scraped_at": "2025-12-31T01:49:38.583727"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
    "paper_url": "https://huggingface.co/papers/2512.22615",
    "authors": [],
    "stars": "41",
    "details": {
      "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
      "abstract": "Building on the success of Dream 7B, we introduce Dream-VL and Dream-VLA, open VL and VLA models that fully unlock discrete diffusion‚Äôs advantages in long-horizon planning, bidirectional reasoning, and parallel action generation for multimodal tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22615",
      "pdf_url": "https://arxiv.org/pdf/2512.22615",
      "github_links": [
        "https://github.com/DreamLM/Dream-VLX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22615",
      "scraped_at": "2025-12-31T01:49:40.576055"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.22323",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
      "abstract": "üéØ SpotEdit: Edit Only What Needs to Be Edited Why regenerate the entire background just to add a scarf to the dog in your photo? This is a frustrating limitation facing many current AI image editing models. Existing methods typically perform a full regeneration of the entire image even for small changes. This not only wastes a massive amount of computing time but often causes distortion or loss of detail in the background‚Äîareas that didn't need to be touched in the first place. SpotEdit is here to solve this problem. We refuse to \"overhaul\" the entire image, sticking instead to a simple yet powerful principle: Edit only what needs to be edited . üöÄ What is SpotEdit? SpotEdit is a training-free universal framework designed specifically for Diffusion Transformer (DiT) models. It automatically identifies which regions require editing and which should remain untouched, eliminating the need for you to manually paint complex masks. ‚ú® The Core \"Magic\" Automated Detection (SpotSelector) Acting like a pair of \"sharp eyes,\" this mechanism uses perceptual similarity to automatically distinguish between stable backgrounds and regions that need to change. It intelligently skips heavy computation for the background, concentrating processing power strictly on the \"cutting edge\" where changes are actually happening. Seamless Fusion (SpotFusion) For the background that doesn't need changing, SpotEdit does not regenerate it; instead, it directly reuses feature information from the original image. Simultaneously, through a dynamic fusion mechanism, it ensures that newly generated objects (like that added scarf) blend perfectly with the original background's lighting and texture, creating a result with no visual inconsistencies. üí° Why Choose SpotEdit? ‚ö°Ô∏è Blazing Fast : By skipping massive amounts of unnecessary background computation, inference speed is boosted by nearly 2√ó . üñºÔ∏è Zero Background Loss : It achieves true \"local editing,\" perfectly preserving every detail of the original background, so you no longer have to worry about the background \"collapsing\" or distorting. üõ†Ô∏è Training-Free : A plug-and-play solution that directly enhances the editing experience of existing models. SpotEdit returns image editing to its essence‚Äîprecise, efficient, and respectful of the original image. HomePage: https://biangbiang0321.github.io/SpotEdit.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22323",
      "pdf_url": "https://arxiv.org/pdf/2512.22323",
      "github_links": [
        "https://github.com/Biangbiang0321/SpotEdit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22323",
      "scraped_at": "2025-12-31T01:49:42.584794"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.15560",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
      "abstract": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about \\textbf{750$\\times$ faster}. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: \\url{ https://anonymous.4open.science/r/GRAN-TED-4FCC/} .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15560",
      "pdf_url": "https://arxiv.org/pdf/2512.15560",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15560",
      "scraped_at": "2025-12-31T01:49:44.443856"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
    "paper_url": "https://huggingface.co/papers/2512.23541",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
      "abstract": "Project page: https://act2goal.github.io/ Abs: Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23541",
      "pdf_url": "https://arxiv.org/pdf/2512.23541",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23541",
      "scraped_at": "2025-12-31T01:49:46.414965"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Web World Models",
    "paper_url": "https://huggingface.co/papers/2512.23676",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "Web World Models",
      "abstract": "In this work, we introduce the Web World Model (WWM), a middle ground where world state and physics are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23676",
      "pdf_url": "https://arxiv.org/pdf/2512.23676",
      "github_links": [
        "https://github.com/Princeton-AI2-Lab/Web-World-Models"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23676",
      "scraped_at": "2025-12-31T01:49:48.340250"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.22234",
    "authors": [],
    "stars": "113",
    "details": {
      "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
      "abstract": "Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks. Code: https://github.com/OpenMOSS/DiRL Model: https://huggingface.co/OpenMOSS-Team/DiRL-8B-Instruct",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22234",
      "pdf_url": "https://arxiv.org/pdf/2512.22234",
      "github_links": [
        "https://github.com/OpenMOSS/DiRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22234",
      "scraped_at": "2025-12-31T01:49:50.346097"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Training AI Co-Scientists Using Rubric Rewards",
    "paper_url": "https://huggingface.co/papers/2512.23707",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Training AI Co-Scientists Using Rubric Rewards",
      "abstract": "How to train language models at generating research plans given diverse open-ended research goals?",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23707",
      "pdf_url": "https://arxiv.org/pdf/2512.23707",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23707",
      "scraped_at": "2025-12-31T01:49:54.597787"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
    "paper_url": "https://huggingface.co/papers/2512.23044",
    "authors": [
      "Kaixin Liang",
      "Minghao Qin",
      "Xiangrui Liu",
      "Yan Shu",
      "Zhengyang Liang"
    ],
    "stars": "0",
    "details": {
      "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
      "abstract": "Introduces Video-BrowseComp, a benchmark of 210 open-web agentic video questions requiring temporal visual evidence to test proactive video reasoning in grounded retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23044",
      "pdf_url": "https://arxiv.org/pdf/2512.23044",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23044",
      "scraped_at": "2025-12-31T01:49:57.027962"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.23646",
    "authors": [
      "Jian Liu",
      "Weiqiang Wang",
      "Bohan Yu",
      "Wenjie Du",
      "Keda Tao"
    ],
    "stars": "0",
    "details": {
      "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
      "abstract": "Website: https://kd-tao.github.io/OmniAgent/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23646",
      "pdf_url": "https://arxiv.org/pdf/2512.23646",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23646",
      "scraped_at": "2025-12-31T01:49:58.935239"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
    "paper_url": "https://huggingface.co/papers/2512.23273",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
      "abstract": "Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23273",
      "pdf_url": "https://arxiv.org/pdf/2512.23273",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23273",
      "scraped_at": "2025-12-31T01:50:00.886043"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
    "paper_url": "https://huggingface.co/papers/2512.22342",
    "authors": [
      "Xihui Liu",
      "Jinming Xu",
      "Meng Wei",
      "Shaohao Zhu",
      "Wensi Huang"
    ],
    "stars": "0",
    "details": {
      "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
      "abstract": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22342",
      "pdf_url": "https://arxiv.org/pdf/2512.22342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22342",
      "scraped_at": "2025-12-31T01:50:02.766646"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Nested Browser-Use Learning for Agentic Information Seeking",
    "paper_url": "https://huggingface.co/papers/2512.23647",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nested Browser-Use Learning for Agentic Information Seeking",
      "abstract": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23647",
      "pdf_url": "https://arxiv.org/pdf/2512.23647",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23647",
      "scraped_at": "2025-12-31T01:50:04.649126"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23162",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
      "abstract": "Proposes SurgWorld world model to learn surgical robot policies from unlabeled videos via synthetic pseudokinematics, enabling data-efficient VLA policies from SATA data.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23162",
      "pdf_url": "https://arxiv.org/pdf/2512.23162",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23162",
      "scraped_at": "2025-12-31T01:50:07.252154"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Monadic Context Engineering",
    "paper_url": "https://huggingface.co/papers/2512.22431",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Monadic Context Engineering",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows (2025) A Layered Protocol Architecture for the Internet of Agents (2025) NormCode: A Semi-Formal Language for Context-Isolated AI Planning (2025) Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation (2025) Agint: Agentic Graph Compilation for Software Engineering Agents (2025) Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases (2025) CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22431",
      "pdf_url": "https://arxiv.org/pdf/2512.22431",
      "github_links": [
        "https://github.com/yifanzhang-pro/monadic-context-engineering"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22431",
      "scraped_at": "2025-12-31T01:50:09.129976"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "An Information Theoretic Perspective on Agentic System Design",
    "paper_url": "https://huggingface.co/papers/2512.21720",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "An Information Theoretic Perspective on Agentic System Design",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference (2025) Towards Efficient Agents: A Co-Design of Inference Architecture and System (2025) Towards a Science of Scaling Agent Systems (2025) Scaling Unverifiable Rewards: A Case Study on Visual Insights (2025) Geometry of Decision Making in Language Models (2025) Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression (2025) FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21720",
      "pdf_url": "https://arxiv.org/pdf/2512.21720",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21720",
      "scraped_at": "2025-12-31T01:50:11.021662"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2512.20927",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
      "abstract": "project page: https://jaesung-choe.github.io/qrender/index.html",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20927",
      "pdf_url": "https://arxiv.org/pdf/2512.20927",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20927",
      "scraped_at": "2025-12-31T01:50:12.850707"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2512.23703",
    "authors": [
      "Yuheng Ji",
      "Zixiao Wang",
      "Yijie Xu",
      "Sixiang Chen",
      "Huajie Tan"
    ],
    "stars": "21",
    "details": {
      "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
      "abstract": "Upload Robo-Dopamine",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23703",
      "pdf_url": "https://arxiv.org/pdf/2512.23703",
      "github_links": [
        "https://github.com/FlagOpen/Robo-Dopamine"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23703",
      "scraped_at": "2025-12-31T01:50:14.748741"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "ProGuard: Towards Proactive Multimodal Safeguard",
    "paper_url": "https://huggingface.co/papers/2512.23573",
    "authors": [
      "Jing Shao",
      "Lu Sheng",
      "Chenyang Si",
      "Lijun Li",
      "Shaohan Yu"
    ],
    "stars": "0",
    "details": {
      "title": "ProGuard: Towards Proactive Multimodal Safeguard",
      "abstract": "The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23573",
      "pdf_url": "https://arxiv.org/pdf/2512.23573",
      "github_links": [
        "https://github.com/yushaohan/ProGuard"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23573",
      "scraped_at": "2025-12-31T01:50:16.591149"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
    "paper_url": "https://huggingface.co/papers/2512.23222",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
      "abstract": "UniMAGE unifies script writing and keyframe generation for long-context video creation using Mixture-of-Transformers and a two-stage interleaving/disentangling training paradigm.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23222",
      "pdf_url": "https://arxiv.org/pdf/2512.23222",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23222",
      "scraped_at": "2025-12-31T01:50:18.465743"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
    "paper_url": "https://huggingface.co/papers/2512.21734",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
      "abstract": "We propose Knot Forcing , a streaming framework for real-time portrait animation that enables high-fidelity, temporally consistent, and interactive video generation from dynamic inputs such as reference images and driving signals. Unlike diffusion-based models that are non-causal and latency-heavy, or autoregressive methods that suffer from error accumulation and motion discontinuities, our approach supports efficient frame-by-frame synthesis while maintaining long-term visual and temporal coherence on consumer-grade hardware. Our method introduces three key innovations: Chunk-wise causal generation with hybrid memory : We preserve global identity by caching KV states of the reference image, while modeling local dynamics using sliding window attention for efficient temporal coherence. Temporal knot module : By overlapping adjacent video chunks and propagating spatio-temporal cues via image-to-video conditioning, we smooth transitions and reduce motion jitter at chunk boundaries. Global context running ahead : During inference, we dynamically update the temporal coordinate of the reference frame to keep its semantic context ahead of the current generation step, enabling stable long-term rollout. Together, these designs enable Knot Forcing to deliver real-time, high-quality portrait animation over infinite sequences with strong visual stability and responsiveness. Project page: this url",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21734",
      "pdf_url": "https://arxiv.org/pdf/2512.21734",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21734",
      "scraped_at": "2025-12-31T01:50:20.482675"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
    "paper_url": "https://huggingface.co/papers/2512.23236",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
      "abstract": "Excited to share our recent work on KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta . We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta‚Äôs latest-generation AI accelerators (MTIA v3) . Writing high-performance GPU kernels is a complex challenge that typically demands years of deep expertise and remains a major focus of industry and academic research. It‚Äôs truly impressive to see KernelEvolve not only achieve state-of-the-art results on open benchmarks, but also deliver 1.25‚Äì17x speedups across Meta production use cases . This milestone was made possible by outstanding collaboration across Meta‚Äîincluding teams from Monetization Infra and Ranking, FAIR, Compiler, MTIA, Serverless Compute, and more . Thank you to everyone for your dedication and teamwork in making this breakthrough happen! You can read the full paper here: üëâ https://lnkd.in/gdPb43EZ This is only ~1% of the journey. There is much more ahead in 2026 as we continue pushing the boundaries. If your background aligns (Agentic, LLM, RL, AI compiler, Kernels, Inference/training optimization etc.) and you‚Äôre interested in joining us on this journey, feel free to DM me. We‚Äôre hiring. ( gangliao@meta.com )",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23236",
      "pdf_url": "https://arxiv.org/pdf/2512.23236",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23236",
      "scraped_at": "2025-12-31T01:50:22.316723"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
    "paper_url": "https://huggingface.co/papers/2512.22100",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
      "abstract": "We proudly present our brand new Turkish NLP benchmarking sets, TrGLUE. Unlike previous work, TrGLUE is not based on translation of original GLUE tasks but tailored for Turkish vocabulary, syntax, semantics and cultural heritage.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22100",
      "pdf_url": "https://arxiv.org/pdf/2512.22100",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22100",
      "scraped_at": "2025-12-31T01:50:24.192023"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.22374",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22374",
      "pdf_url": "https://arxiv.org/pdf/2512.22374",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22374",
      "scraped_at": "2025-12-31T01:50:26.081963"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
    "paper_url": "https://huggingface.co/papers/2512.22255",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
      "abstract": "Training on synthetic CoT traces, even with wrong final answers, improves reasoning due to aligning with the model's distribution and leveraging partial reasoning steps, outperforming human-annotated data. In our paper we explore this interesting observation and provide detailed experimental results and ablation to study the effect of models learning reasoning from unverified noisy and wrong CoTs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22255",
      "pdf_url": "https://arxiv.org/pdf/2512.22255",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22255",
      "scraped_at": "2025-12-31T01:50:27.962488"
    },
    "scraped_date": "2025-12-31"
  },
  {
    "title": "Reverse Personalization",
    "paper_url": "https://huggingface.co/papers/2512.22984",
    "authors": [
      "Nicu Sebe",
      "Tuomas Varanka",
      "Han-Wei Kung"
    ],
    "stars": "0",
    "details": {
      "title": "Reverse Personalization",
      "abstract": "https://github.com/hanweikung/reverse-personalization",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22984",
      "pdf_url": "https://arxiv.org/pdf/2512.22984",
      "github_links": [
        "https://github.com/hanweikung/reverse-personalization"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22984",
      "scraped_at": "2025-12-31T01:50:29.794891"
    },
    "scraped_date": "2025-12-31"
  }
]