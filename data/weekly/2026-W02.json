[
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "126",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-12T01:56:15.369905"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "99",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-12T01:56:17.350077"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "17",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-12T01:56:19.261694"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-12T01:56:21.261190"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "23",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-12T01:56:23.204161"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "16",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-12T01:56:25.164574"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-12T01:56:27.080831"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "13",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-12T01:56:29.109143"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-12T01:56:31.065656"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-12T01:56:33.013335"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-12T01:56:34.977612"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-12T01:56:36.914048"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-12T01:56:38.788826"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-12T01:56:40.809081"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-12T01:56:42.666912"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-12T01:56:44.585372"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-12T01:56:46.426451"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-12T01:56:48.265902"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-12T01:56:50.213336"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-12T01:56:52.201583"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-12T01:56:54.063156"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-12T01:56:55.989473"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-12T01:56:57.907638"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-12T01:56:59.771628"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-12T01:57:01.869712"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "2",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-12T01:57:03.736777"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-12T01:57:05.671962"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-12T01:57:07.412767"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-12T01:57:09.256703"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "4",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [
        "https://github.com/mbar0075/lupi-for-object-detection"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-12T01:57:11.141085"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-12T01:57:13.156228"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-12T01:57:15.060473"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-12T01:57:17.088547"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
    "paper_url": "https://huggingface.co/papers/2601.05432",
    "authors": [],
    "stars": "107",
    "details": {
      "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
      "abstract": "Demo video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05432",
      "pdf_url": "https://arxiv.org/pdf/2601.05432",
      "github_links": [
        "https://github.com/AMAP-ML/Thinking-with-Map"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05432",
      "scraped_at": "2026-01-13T01:48:04.298623"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
    "paper_url": "https://huggingface.co/papers/2601.03017",
    "authors": [
      "Huajian Xin",
      "Hui Shen",
      "Yunta Hsieh",
      "Qi Han",
      "Jing Xiong"
    ],
    "stars": "0",
    "details": {
      "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
      "abstract": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03017",
      "pdf_url": "https://arxiv.org/pdf/2601.03017",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03017",
      "scraped_at": "2026-01-13T01:48:06.184101"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "paper_url": "https://huggingface.co/papers/2601.03319",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
      "abstract": "Project Page: https://c4ricaturegs.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03319",
      "pdf_url": "https://arxiv.org/pdf/2601.03319",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03319",
      "scraped_at": "2026-01-13T01:48:08.069844"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06002",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "abstract": "Glad to share our recent exploratory project: üß™ Title: The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning üåê arXiv: 2601.06002 ‚Äã üßê Why revisit Long CoT? Recent work often focuses on ‚Äúmaking CoT longer,‚Äù but longer traces are more likely to derail‚Äîe.g., drifting off-track, breaking logical continuity, or amplifying hallucinations‚Äîespecially when attempting to cold-start genuine long-horizon reasoning from a standard instruction-tuned model. ‚Äã A key observation is that many trajectories that merely look like long reasoning (e.g., distilling from randomly sampled ICL demonstrations, or using human-written long step-by-step solutions) are not behaviorally stable, and models frequently fail to learn robustly from them. ‚Äã üò≠  Why imitation often fails ‚ÄúLong‚Äù human CoT is not necessarily effective: Fine-tuning on human-written long CoT does not reliably reproduce the gains achieved by distilling from a strong reasoning model. Distill from Weak instruct model + random ICL demonstrations largely fails: Using randomly chosen 1-shot ICL examples to ‚Äúfake‚Äù long reasoning for distillation leads to significant degradation, suggesting that superficial formatting is insufficient. ‚Äã- Keywords are not the driver: Replacing surface tokens (e.g., ‚Äúwait‚Äù) while preserving the underlying reasoning trajectory and behavioral pattern yields similar performance, indicating that SFT primarily learns structure/behavior rather than prompt-specific keywords. ‚Äã üîç  Early evidence: Long CoT has stable ‚Äústructural fingerprints‚Äù We observe a stable behavioral transfer graph: across different strong reasoning models and tasks, the induced distributional characteristics appear highly consistent. ‚Äã- In semantic space, we see ‚Äúlinking‚Äìfolding‚Äù patterns: deep reasoning tends to form locally dense structures; self-reflection tends to create backward links for validation/correction; and exploration tends to form weaker cross-cluster connections. ‚Äã üí° Core hypothesis: effective Long CoT as a ‚Äúmolecular structure‚Äù High-quality Long CoT is not merely a linear chain; it is stabilized by three interaction types‚Äîanalogous to chemical bonds‚Äîthat organize and constrain reasoning trajectories: ‚Äã- Deep Reasoning (covalent-bond-like): forms the main reasoning backbone; if it breaks, the solution collapses. ‚Äã- Self-Reflection (hydrogen-bond-like): folds later steps back to earlier ones to verify assumptions, detect errors, and correct the path. ‚Äã- Self-Exploration (van der Waals-like): weak but important cross-domain probing that broadens coverage and discovers alternative routes. ‚Äã An additional observation is that the Gibbs‚ÄìBoltzmann energy formulation is closely aligned with the attention formulation; hence, the ‚Äúenergy distributions‚Äù of different bonds can be estimated directly from attention, exhibiting a stable ordering reminiscent of real chemical bond energies. ‚Äã üçé ‚ÄúSemantic isomers‚Äù of Long CoT For the same problem, trajectories can be semantically close yet differ in the distribution and transitions of bonds, yielding distinct ‚Äúisomers‚Äù with dramatically different trainability and downstream performance. ‚Äã- Two isomers that appear similar may still be incompatible: mixing them during training can trigger structural conflicts and degrade performance. ‚Äã- ICL is not inherently ineffective; it helps when demonstrations are selected such that their structural distribution aligns with the target high-quality isomer. ‚Äã üîß Solution: MOLE-SYN We propose MOLE-SYN: first estimate a behavioral transfer graph from a strong reasoning model, then use it to guide a pure instruct LLM to synthesize Long CoT trajectories. ‚Äã- Empirically, distilling Qwen-2.5 with MOLE-SYN‚Äìgenerated trajectories can approach the effectiveness of distillation from QwQ. ‚Äã- This initialization also exhibits strong RL potential: it yields more stable RL training curves and sustained improvement headroom. Finally, different behaviors have distinct global effects: deep reasoning makes the core logic more compact, self-reflection increases overall ‚Äúfolding‚Äù tightness, and self-exploration expands the reachable search space. ‚Äã üëÄ A practical implication is that when CoT is heavily summarized or compressed, the ‚Äúmolecular structure‚Äù distribution can be destroyed, and distilled models may underperform even the original teacher. ‚Äã If a prior viewpoint treated CoT behaviors as nodes, this work reframes them as edges that link logical states: the training target may not be ‚Äúlonger answers,‚Äù but a more stable reasoning skeleton controlled by structured reasoning behaviors. ‚Äã",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06002",
      "pdf_url": "https://arxiv.org/pdf/2601.06002",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06002",
      "scraped_at": "2026-01-13T01:48:09.969605"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "paper_url": "https://huggingface.co/papers/2601.06021",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
      "abstract": "Code: https://github.com/THUDM/CaRR Data: https://huggingface.co/datasets/THU-KEG/CaRR-DeepDive",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06021",
      "pdf_url": "https://arxiv.org/pdf/2601.06021",
      "github_links": [
        "https://github.com/THUDM/CaRR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06021",
      "scraped_at": "2026-01-13T01:48:11.869324"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
    "paper_url": "https://huggingface.co/papers/2601.05808",
    "authors": [
      "Zhicheng Dou",
      "Yutao Zhu",
      "Haofei Chang",
      "Xiaoshuai Song",
      "dongguanting"
    ],
    "stars": "0",
    "details": {
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "abstract": "Code: https://github.com/RUC-NLPIR/EnvScaler Data & Model: https://huggingface.co/collections/XXHStudyHard/envscaler",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05808",
      "pdf_url": "https://arxiv.org/pdf/2601.05808",
      "github_links": [
        "https://github.com/RUC-NLPIR/EnvScaler"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05808",
      "scraped_at": "2026-01-13T01:48:13.888446"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
    "paper_url": "https://huggingface.co/papers/2601.04720",
    "authors": [],
    "stars": "620",
    "details": {
      "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
      "abstract": "üöÄ Introducing Qwen3-VL-Embedding and Qwen3-VL-Reranker ‚Äì advancing the state of the art in multimodal retrieval and cross-modal understanding! ‚ú® Highlights: ‚úÖ Built upon the robust Qwen3-VL foundation model ‚úÖ Processes text, images, screenshots, videos, and mixed modality inputs ‚úÖ Supports 30+ languages ‚úÖ Achieves state-of-the-art performance on multimodal retrieval benchmarks ‚úÖ Open source and available on Hugging Face, GitHub, and ModelScope ‚úÖ API deployment on Alibaba Cloud coming soon! üéØ Two-stage retrieval architecture: üìä Embedding Model ‚Äì generates semantically rich vector representations in a unified embedding space üéØ Reranker Model ‚Äì computes fine-grained relevance scores for enhanced retrieval accuracy üîç Key application scenarios: Image-text retrieval, video search, multimodal RAG, visual question answering, multimodal content clustering, multilingual visual search, and more! üåü Developer-friendly capabilities: ‚Ä¢ Configurable embedding dimensions ‚Ä¢ Task-specific instruction customization ‚Ä¢ Embedding quantization support for efficient and cost-effective downstream deployment Hugging FaceÔºö https://huggingface.co/collections/Qwen/qwen3-vl-embedding https://huggingface.co/collections/Qwen/qwen3-vl-reranker Github: https://github.com/QwenLM/Qwen3-VL-Embedding Blog: https://qwen.ai/blog?id=qwen3-vl-embedding Tech Report: https://www.arxiv.org/abs/2601.04720",
      "arxiv_page_url": "https://www.arxiv.org/abs/2601.04720",
      "pdf_url": "https://arxiv.org/pdf/2601.04720",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-VL-Embedding"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04720",
      "scraped_at": "2026-01-13T01:48:15.778962"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "paper_url": "https://huggingface.co/papers/2601.05930",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Can We Predict Before Executing Machine Learning Agents?",
      "abstract": "We replace slow trial-and-error in scientific agents with learned execution prediction, enabling FOREAGENT to think before it runs and achieve 6√ó faster and better scientific discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05930",
      "pdf_url": "https://arxiv.org/pdf/2601.05930",
      "github_links": [
        "https://github.com/zjunlp/predict-before-execute"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05930",
      "scraped_at": "2026-01-13T01:48:17.633216"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
    "paper_url": "https://huggingface.co/papers/2601.05882",
    "authors": [
      "Nikolaos Aletras",
      "Constantinos Karouzos",
      "XingweiT"
    ],
    "stars": "0",
    "details": {
      "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
      "abstract": "Our paper presents a systematic study of preference-optimization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. We found: The adaptation strategy is more influential than the alignment objective. We identify that synthetic supervision is a double-edged sword. While pseudo-labeling yields the highest target-domain win rates, it induces severe mode collapse. This diversity tax results in models that are highly reliable but linguistically monotonous, mirroring the latent templates of the teacher model. Our findings suggest a deployment recommendation: use pseudo-labeling for high-stakes and constrained tasks where reliability is paramount, but favor mixed-domain SFT and online RL for applications requiring creative or varied linguistic expression.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05882",
      "pdf_url": "https://arxiv.org/pdf/2601.05882",
      "github_links": [
        "https://github.com/ckarouzos/prefadap"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05882",
      "scraped_at": "2026-01-13T01:48:19.521997"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "paper_url": "https://huggingface.co/papers/2601.04786",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
      "abstract": "We‚Äôre introducing AgentOCR, a new way to scale LLM agents by reimagining long interaction histories as compact rendered images, leveraging the higher information density of visual tokens to curb exploding context costs. To make long-horizon rollouts practical, we add segment optical caching, splitting history into hashable segments and caching the visuals, so agents avoid redundant re-rendering as trajectories grow.  We go beyond fixed compression with agentic self-compression: the agent actively emits a compression rate and is trained with a compression-aware reward to balance task success against token efficiency. Across ALFWorld and search-based QA, AgentOCR keeps >95% of text-agent performance while cutting token use by >50% average and ~80% in peak, and our analysis shows up to a 20√ó rendering speedup thanks to our segment optical caching",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04786",
      "pdf_url": "https://arxiv.org/pdf/2601.04786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04786",
      "scraped_at": "2026-01-13T01:48:21.395496"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "paper_url": "https://huggingface.co/papers/2601.05966",
    "authors": [
      "Yu Sun",
      "Shuohuan Wang",
      "Xiaoxiong Liu",
      "Longbin Ji",
      "sjy1203"
    ],
    "stars": "0",
    "details": {
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "abstract": "VideoAR presents a scalable autoregressive video-generation framework that combines next-frame scale prediction with a 3D multi-scale tokenizer to improve temporal coherence and efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05966",
      "pdf_url": "https://arxiv.org/pdf/2601.05966",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05966",
      "scraped_at": "2026-01-13T01:48:23.249628"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "paper_url": "https://huggingface.co/papers/2601.05905",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "abstract": "We show that many LLM ‚Äúbeliefs‚Äù that look confident collapse under small context changes, and propose Neighbor-Consistency Belief (NCB) and Structure-Aware Training to measure and train models to keep their knowledge stable and robust under such interference.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05905",
      "pdf_url": "https://arxiv.org/pdf/2601.05905",
      "github_links": [
        "https://github.com/zjunlp/belief"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05905",
      "scraped_at": "2026-01-13T01:48:25.096155"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "paper_url": "https://huggingface.co/papers/2601.05848",
    "authors": [
      "Evan Luo",
      "Zitian Tang",
      "Yinghua Zhou",
      "dakshces",
      "nate-gillman"
    ],
    "stars": "0",
    "details": {
      "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
      "abstract": "Goal Force trains a physics-grounded video model to follow explicit force-directed goals, achieving zero-shot planning in real-world tasks by implicit neural physics simulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05848",
      "pdf_url": "https://arxiv.org/pdf/2601.05848",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05848",
      "scraped_at": "2026-01-13T01:48:27.018745"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05573",
    "authors": [
      "Tianyu Pang",
      "Jialei Wang",
      "Jiayang Xu",
      "Zehan Wang",
      "Viglong"
    ],
    "stars": "82",
    "details": {
      "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
      "abstract": "Code: https://github.com/SpatialVision/Orient-Anything-V2 Demo Space: https://huggingface.co/spaces/Viglong/Orient-Anything-V2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05573",
      "pdf_url": "https://arxiv.org/pdf/2601.05573",
      "github_links": [
        "https://github.com/SpatialVision/Orient-Anything-V2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05573",
      "scraped_at": "2026-01-13T01:48:28.874972"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
    "paper_url": "https://huggingface.co/papers/2601.05403",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
      "abstract": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (MFMD). In this work, we propose MFMD-Scen, a comprehensive benchmark for evaluating behavioral biases of LLMs in MFMD across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, MFMD-Scen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05403",
      "pdf_url": "https://arxiv.org/pdf/2601.05403",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05403",
      "scraped_at": "2026-01-13T01:48:30.700256"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "AnyDepth: Depth Estimation Made Easy",
    "paper_url": "https://huggingface.co/papers/2601.02760",
    "authors": [],
    "stars": "63",
    "details": {
      "title": "AnyDepth: Depth Estimation Made Easy",
      "abstract": "https://aigeeksgroup.github.io/AnyDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02760",
      "pdf_url": "https://arxiv.org/pdf/2601.02760",
      "github_links": [
        "https://github.com/AIGeeksGroup/AnyDepth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02760",
      "scraped_at": "2026-01-13T01:48:32.573488"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "paper_url": "https://huggingface.co/papers/2601.04888",
    "authors": [
      "Guanting Dong",
      "douzc",
      "vvv111222"
    ],
    "stars": "11",
    "details": {
      "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
      "abstract": "Some of the observations founded are :- i. Dual Level Credit Assessment This mechanism provides a comprehensive evaluation of query quality through both rule-based and model-based assessments. It allows for fine-grained supervision, helping to identify not just redundancy but also the usefulness of each query in the context of the search process. ii. Process Reward Mechanism The introduction of process rewards as a guiding signal for training search agents is a novel approach. It shifts the focus from solely final outcomes to the quality of intermediate queries, addressing a significant gap in existing methods that often overlook this aspect. iii. Query Refinement Strategy The framework employs a systematic query refinement process that identifies low quality queries and generates improved versions. This iterative refinement enhances the effectiveness of search trajectories, allowing agents to adaptively improve their queries based on feedback. iv. Three Stage Curriculum Learning Framework SmartSearch introduces a structured curriculum learning approach that progresses from imitation to alignment and finally to generalization. This staged learning process enables search agents to internalize query quality improvement progressively, enhancing their overall performance. v. Empirical Validation Across Diverse Benchmarks The paper presents extensive experimental results demonstrating SmartSearch's superior performance across multiple challenging knowledge-intensive tasks and web exploration scenarios. This empirical validation highlights the framework's robustness and effectiveness in real-world applications, showcasing its potential impact on future research in search agents and information retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04888",
      "pdf_url": "https://arxiv.org/pdf/2601.04888",
      "github_links": [
        "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04888",
      "scraped_at": "2026-01-13T01:48:34.423674"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Over-Searching in Search-Augmented Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.05503",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "abstract": "Systematically analyzes over-search in search-augmented LLMs, showing when retrieval helps or hurts, introducing Tokens Per Correctness and mitigation strategies.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05503",
      "pdf_url": "https://arxiv.org/pdf/2601.05503",
      "github_links": [
        "https://github.com/ruoyuxie/OversearchQA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05503",
      "scraped_at": "2026-01-13T01:48:36.245536"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.04823",
    "authors": [
      "Linqi Song",
      "Huacan Wang",
      "Ronghao Chen",
      "Guanzhi Deng",
      "liboaccn"
    ],
    "stars": "0",
    "details": {
      "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
      "abstract": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04823",
      "pdf_url": "https://arxiv.org/pdf/2601.04823",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04823",
      "scraped_at": "2026-01-13T01:48:38.107896"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.04726",
    "authors": [
      "Zhicheng Dou",
      "Yutao Zhu",
      "Jiejun Tan",
      "Jiongnan Liu",
      "namespace-ERI"
    ],
    "stars": "0",
    "details": {
      "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
      "abstract": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04726",
      "pdf_url": "https://arxiv.org/pdf/2601.04726",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04726",
      "scraped_at": "2026-01-13T01:48:39.978598"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
    "paper_url": "https://huggingface.co/papers/2601.05637",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A Reason-then-Describe Instruction Interpreter for Controllable Video Generation (2025) EVE: A Generator-Verifier System for Generative Policies (2025) Eliciting Behaviors in Multi-Turn Conversations (2025) SkillWrapper: Generative Predicate Invention for Skill Abstraction (2025) From Word to World: Can Large Language Models be Implicit Text-based World Models? (2025) SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models (2025) Propose, Solve, Verify: Self-Play Through Formal Verification (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05637",
      "pdf_url": "https://arxiv.org/pdf/2601.05637",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05637",
      "scraped_at": "2026-01-13T01:48:41.784741"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.04544",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
      "abstract": "Code: https://github.com/Tencent/TCAndon-Router",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04544",
      "pdf_url": "https://arxiv.org/pdf/2601.04544",
      "github_links": [
        "https://github.com/kyegomez/awesome-multi-agent-papers",
        "https://github.com/Tencent/TCAndon-Router"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04544",
      "scraped_at": "2026-01-13T01:48:43.634411"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "paper_url": "https://huggingface.co/papers/2601.05960",
    "authors": [
      "vicgalle"
    ],
    "stars": "1",
    "details": {
      "title": "Distilling Feedback into Memory-as-a-Tool",
      "abstract": "Code: https://github.com/vicgalle/feedback-memory-as-a-tool Data: https://huggingface.co/datasets/vicgalle/rubric-feedback-bench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05960",
      "pdf_url": "https://arxiv.org/pdf/2601.05960",
      "github_links": [
        "https://github.com/vicgalle/feedback-memory-as-a-tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05960",
      "scraped_at": "2026-01-13T01:48:45.433282"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "paper_url": "https://huggingface.co/papers/2601.05899",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
      "abstract": "Some of the observations are :- i. TowerMind is a lightweight RTS-style benchmark for LLM agents It introduces a tower defense based environment that preserves long term planning and decision making challenges of RTS games, while requiring very low computational resources compared to StarCraft II based benchmarks. ii. Multimodal observations enable broader LLM evaluation TowerMind supports pixel-based, textual (JSON), and structured state observations, making it suitable for evaluating language-only and vision-language models under the same environment. iii. Hallucination is explicitly measured via action validity Beyond performance score, the benchmark introduces valid action rate to quantify hallucinations. i.e. actions that violate game rules or state constraints allowing simultaneous evaluation of capability and reliability. iv. LLMs significantly underperform human experts Even the best-performing models (e.g. GPT-4.1, Claude 3.7 Sonnet) show a large gap from human experts, especially on harder levels, revealing weaknesses in planning validation, multifinality, and efficient action use. v. TowerMind is challenging for both LLMs and RL agents Classic RL algorithms (Ape-X DQN, PPO) also fail to reach human level performance, confirming TowerMind as a non-trivial benchmark that complements existing LLM and RL evaluation environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05899",
      "pdf_url": "https://arxiv.org/pdf/2601.05899",
      "github_links": [
        "https://github.com/tb6147877/TowerMind"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05899",
      "scraped_at": "2026-01-13T01:48:47.327914"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
    "paper_url": "https://huggingface.co/papers/2601.05851",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
      "abstract": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05851",
      "pdf_url": "https://arxiv.org/pdf/2601.05851",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05851",
      "scraped_at": "2026-01-13T01:48:49.142532"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
    "paper_url": "https://huggingface.co/papers/2601.05741",
    "authors": [
      "Marco Huber",
      "Jan Niklas Kolf",
      "Tahar Chettaoui",
      "Eduarda Caldeira",
      "gurayozgur"
    ],
    "stars": "3",
    "details": {
      "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
      "abstract": "https://github.com/gurayozgur/ViTNT-FIQA",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05741",
      "pdf_url": "https://arxiv.org/pdf/2601.05741",
      "github_links": [
        "https://github.com/gurayozgur/ViTNT-FIQA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05741",
      "scraped_at": "2026-01-13T01:48:51.076876"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "paper_url": "https://huggingface.co/papers/2601.05870",
    "authors": [
      "Zhuoyue Chen",
      "Long Li",
      "Yue Zhu",
      "Hongchen Luo",
      "Huilin Deng"
    ],
    "stars": "0",
    "details": {
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReLaX: Reasoning with Latent Exploration for Large Reasoning Models (2025) Multi-Path Collaborative Reasoning via Reinforcement Learning (2025) Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies (2025) ESPO: Entropy Importance Sampling Policy Optimization (2025) Diversity or Precision? A Deep Dive into Next Token Prediction (2025) SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization (2025) Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05870",
      "pdf_url": "https://arxiv.org/pdf/2601.05870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05870",
      "scraped_at": "2026-01-13T01:48:52.895587"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
    "paper_url": "https://huggingface.co/papers/2601.05699",
    "authors": [
      "Jesujoba Oluwadara Alabi",
      "Israel Abebe Azime",
      "Emilio Villa-Cueva",
      "Srija Anand",
      "Atnafu"
    ],
    "stars": "0",
    "details": {
      "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG (2025) Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries (2025) HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples (2025) Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles (2025) IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages (2025) See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models (2025) Multilingual VLM Training: Adapting an English-Trained VLM to French (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05699",
      "pdf_url": "https://arxiv.org/pdf/2601.05699",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05699",
      "scraped_at": "2026-01-13T01:48:54.752545"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
    "paper_url": "https://huggingface.co/papers/2601.05376",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "abstract": "This paper investigates how \"persona conditioning\" (e.g., instructing an LLM to act as a specific medical professional) impacts clinical decision-making. The authors challenge the assumption that assigning a medical persona consistently improves accuracy or safety, labeling this inconsistency the \"Persona Paradox.\" Key Insights: Non-Monotonic Effects: Assigning a medical persona (like an Emergency Department physician) does not always improve performance. It acts as a behavioral prior that can help in some contexts but hurt in others. The Context Gap: Medical personas improved accuracy and calibration by up to 20% in critical-care tasks (triage) but degraded performance by a similar margin in primary-care settings. Interaction Styles: Adding styles such as \"bold\" or \"cautious\" changes the model‚Äôs risk propensity, but these effects vary widely across base models. The Alignment Gap: While \"LLM judges\" preferred medical personas for safety-critical cases, human clinicians were much more skeptical. Human experts showed low confidence in the AI's reasoning quality in 95.9% of cases, despite moderate agreement on safety compliance. Conclusion The study concludes that personas are not \"expertise switches\" but rather priors that introduce context-dependent trade-offs. Relying on personas for clinical safety is risky because they do not provide a universal guarantee of better judgment. Personas should be used with caution in high-stakes medicine, as they can inadvertently trigger biases or performance drops depending on the specific clinical task.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05376",
      "pdf_url": "https://arxiv.org/pdf/2601.05376",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05376",
      "scraped_at": "2026-01-13T01:48:56.593118"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Legal Alignment for Safe and Ethical AI",
    "paper_url": "https://huggingface.co/papers/2601.04175",
    "authors": [
      "Rishi Bommasani",
      "Cullen O'Keefe",
      "Jack Boeglin",
      "Nicholas Caputo",
      "Noam Kolt"
    ],
    "stars": "0",
    "details": {
      "title": "Legal Alignment for Safe and Ethical AI",
      "abstract": "Field-defining paper by researchers from Stanford, MIT, Harvard, Oxford, Princeton, and other leading institutions. More details at: https://www.legal-alignment.ai/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04175",
      "pdf_url": "https://arxiv.org/pdf/2601.04175",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04175",
      "scraped_at": "2026-01-13T01:48:58.401803"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06943",
    "authors": [
      "Zhe Huang",
      "Zhuoyue Chang",
      "HJH2CMD",
      "Yu2020",
      "POTATO66"
    ],
    "stars": "51",
    "details": {
      "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
      "abstract": "First video deep research benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06943",
      "pdf_url": "https://arxiv.org/pdf/2601.06943",
      "github_links": [
        "https://github.com/QuantaAlpha/VideoDR-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06943",
      "scraped_at": "2026-01-14T01:55:05.388627"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "paper_url": "https://huggingface.co/papers/2601.06521",
    "authors": [
      "Liang Chen",
      "Liuff23",
      "Ziqi",
      "ssz1111",
      "chenxz"
    ],
    "stars": "81",
    "details": {
      "title": "BabyVision: Visual Reasoning Beyond Language",
      "abstract": "Feel free to follow our GitHub repo: https://github.com/UniPat-AI/BabyVision",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06521",
      "pdf_url": "https://arxiv.org/pdf/2601.06521",
      "github_links": [
        "https://github.com/UniPat-AI/BabyVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06521",
      "scraped_at": "2026-01-14T01:55:07.338360"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05593",
    "authors": [],
    "stars": "261",
    "details": {
      "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
      "abstract": "üéâ Introducing Parallel Coordinated Reasoning (PaCoRe) üìà An 8B model beats GPT-5 on HMMT25 by unlocking parallel thinking for test-time scaling! üìÇ Open-source deep think: data + model + inference code! üÜì MIT-licensed ‚Äî use it however you want üîçKey findings: Message Passing Unlocks Scaling Without compaction, performance flatlines at the context limit. PaCoRe breaks the memory barrier and lets reasoning scale freely. Breadth > Depth All compute is not equal. Coordinated parallel reasoning delivers far higher returns than extending a single chain. Data as a Force Multiplier The PaCoRe corpus provides exceptionally valuable supervision‚Äî even baseline models see substantial gains when trained on it. üîó Links: GitHub: https://github.com/stepfun-ai/PaCoRe Data: https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k Model: https://huggingface.co/stepfun-ai/PaCoRe-8B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05593",
      "pdf_url": "https://arxiv.org/pdf/2601.05593",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05593",
      "scraped_at": "2026-01-14T01:55:09.273922"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "paper_url": "https://huggingface.co/papers/2601.07832",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2601.07832",
      "pdf_url": "https://arxiv.org/pdf/2601.07832",
      "github_links": [
        "https://github.com/DAGroup-PKU/MHLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07832",
      "scraped_at": "2026-01-14T01:55:11.365101"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
    "paper_url": "https://huggingface.co/papers/2601.06953",
    "authors": [
      "Jane Luo",
      "Jiani Guo",
      "Xin Zhang",
      "Jie Wu",
      "Ringo1110"
    ],
    "stars": "52",
    "details": {
      "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Tailored Primitive Initialization is the Secret Key to Reinforcement Learning (2025) Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling (2026) PerfCoder: Large Language Models for Interpretable Code Performance Optimization (2025) Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization (2026) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06953",
      "pdf_url": "https://arxiv.org/pdf/2601.06953",
      "github_links": [
        "https://github.com/JieWu02/X-Coder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06953",
      "scraped_at": "2026-01-14T01:55:13.303655"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
    "paper_url": "https://huggingface.co/papers/2601.05110",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
      "abstract": "LLM + SLM > LLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05110",
      "pdf_url": "https://arxiv.org/pdf/2601.05110",
      "github_links": [
        "https://github.com/Zengwh02/GlimpRouter"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05110",
      "scraped_at": "2026-01-14T01:55:16.049503"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "paper_url": "https://huggingface.co/papers/2601.07226",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
      "abstract": "The code and dataset will be released publicly.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07226",
      "pdf_url": "https://arxiv.org/pdf/2601.07226",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07226",
      "scraped_at": "2026-01-14T01:55:18.008918"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "paper_url": "https://huggingface.co/papers/2601.07779",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
      "abstract": "Despite VLM advances, current CUA frameworks remain brittle in long-horizon workflows and weak in novel domains due to coarse historical visual context management and missing visual-aware tutorial retrieval, so we propose OS-SYMPHONY, an orchestrated framework combining milestone-driven reflection memory for trajectory-level self-correction with a SeeAct-style multimodal searcher that synthesizes visually aligned live tutorials, achieving new SOTA across three online benchmarks (65.84% on OSWorld).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07779",
      "pdf_url": "https://arxiv.org/pdf/2601.07779",
      "github_links": [
        "https://github.com/OS-Copilot/OS-Symphony"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07779",
      "scraped_at": "2026-01-14T01:55:19.919265"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.07351",
    "authors": [
      "Chenchen Jing",
      "Tianjian Feng",
      "Bozhen Fang",
      "Linyu Wu",
      "zhongzero"
    ],
    "stars": "16",
    "details": {
      "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
      "abstract": "GitHub repo: https://github.com/aim-uofa/EvoTokenDLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07351",
      "pdf_url": "https://arxiv.org/pdf/2601.07351",
      "github_links": [
        "https://github.com/aim-uofa/EvoTokenDLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07351",
      "scraped_at": "2026-01-14T01:55:21.893924"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "paper_url": "https://huggingface.co/papers/2601.05107",
    "authors": [
      "Zhengkang Guo",
      "Jingwen Xu",
      "Xiaohua Wang",
      "Muzhao Tian",
      "zisuh"
    ],
    "stars": "0",
    "details": {
      "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
      "abstract": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05107",
      "pdf_url": "https://arxiv.org/pdf/2601.05107",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05107",
      "scraped_at": "2026-01-14T01:55:23.799775"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "paper_url": "https://huggingface.co/papers/2601.01528",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
      "abstract": "DrivingGen is a comprehensive benchmark for generative world models in the driving domain with a diverse data distribution and novel evaluation metrics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01528",
      "pdf_url": "https://arxiv.org/pdf/2601.01528",
      "github_links": [
        "https://github.com/youngzhou1999/DrivingGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01528",
      "scraped_at": "2026-01-14T01:55:25.767787"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "paper_url": "https://huggingface.co/papers/2601.07526",
    "authors": [
      "Jiawei Chen",
      "Ruisheng Cao",
      "Mouxiang Chen",
      "zjj1233",
      "Lemoncoke"
    ],
    "stars": "0",
    "details": {
      "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
      "abstract": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07526",
      "pdf_url": "https://arxiv.org/pdf/2601.07526",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07526",
      "scraped_at": "2026-01-14T01:55:27.656092"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2601.05823",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
      "abstract": "arXiv link: Boosting Latent Diffusion Models via Disentangled Representation Alignment Code (Coming Soon): https://github.com/Kwai-Kolors/Send-VAE",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05823",
      "pdf_url": "https://arxiv.org/pdf/2601.05823",
      "github_links": [
        "https://github.com/Kwai-Kolors/Send-VAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05823",
      "scraped_at": "2026-01-14T01:55:29.594871"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06165",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
      "abstract": "Users often ask VLMs under-specified, informal visual questions, which current clean-prompt benchmarks fail to capture. We introduce HAERAE-Vision (653 real Korean community queries + explicit rewrites) and show that making queries explicit boosts accuracy by 8‚Äì22 points, while web search cannot fully offset what users leave unsaid.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06165",
      "pdf_url": "https://arxiv.org/pdf/2601.06165",
      "github_links": [
        "https://github.com/HAE-RAE/HAERAE-VISION"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06165",
      "scraped_at": "2026-01-14T01:55:31.500284"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
    "paper_url": "https://huggingface.co/papers/2601.06860",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
      "abstract": "Most current TIR work only focuses on the accuracy of agents in downstream tasks, while lacking calibration of the agents' behavioral patterns in TIR tasks. To address this issue, we first quantitatively analyze several possible erroneous behavioral patterns in current TIR tasks, and classify them into two categories: \"improper tool use\" and \"flawed reasoning logic\". Based on this, we propose ET-Agent, a framework that fully calibrates the behavioral patterns of agents when performing TIR tasks from both data and algorithm levels. On the data side, we propose a self-evolving data flywheel, which enhances the training data by leveraging the agent's own reflective exploration capabilities. On the algorithm side, we propose a behavioral calibration training framework. It performs rejection sampling fine-tuning on the basis of enhanced training data to broaden the agent's exploration of the action space. Subsequently, we implement iterative behavioral calibration reinforcement learning to calibrate the actions of the fine-tuned agent to the optimal behavioral pattern. Our contributions are listed as follows: We provide a comprehensive quantitative analysis of erroneous behavioral patterns in TIR. Inspired by this, we propose ET-Agent, a framework for optimizing TIR's behavioral patterns. We introduce a self-evolving data flywheel, an iterative loop where the model continuously refines its previous trajectories. This mechanism effectively unfolds the model's action space coverage beyond its initial exploration. Based on the flywheel, we present a behavior calibration training framework with two phases, aiming to calibrate the model's exploration in tool-use action space to optimal trajectories. Extensive experiments demonstrate that ET-Agent substantially improves behavioral efficiency, reasoning conciseness, and execution success rates while maintaining high accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06860",
      "pdf_url": "https://arxiv.org/pdf/2601.06860",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06860",
      "scraped_at": "2026-01-14T01:55:33.429929"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
    "paper_url": "https://huggingface.co/papers/2601.07055",
    "authors": [
      "Shaoliang Nie",
      "Suyu Ge",
      "Xianjun Yang",
      "Kartikeya Upasani",
      "Zhenrui Yue"
    ],
    "stars": "74",
    "details": {
      "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
      "abstract": "Dr. Zero enables data-free self-evolving search agents through a self-evolution loop with HRPO, achieving strong multi-step reasoning while reducing compute.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07055",
      "pdf_url": "https://arxiv.org/pdf/2601.07055",
      "github_links": [
        "https://github.com/facebookresearch/drzero"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07055",
      "scraped_at": "2026-01-14T01:55:35.318742"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06803",
    "authors": [
      "Yankai Lin",
      "Yichen Wu",
      "Yubo Wang",
      "Yuhan",
      "ZION121"
    ],
    "stars": "0",
    "details": {
      "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
      "abstract": "We hope this work encourages a paradigm shift from explicit next-token prediction to latent visual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06803",
      "pdf_url": "https://arxiv.org/pdf/2601.06803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06803",
      "scraped_at": "2026-01-14T01:55:37.211022"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
    "paper_url": "https://huggingface.co/papers/2601.04698",
    "authors": [
      "Hao Wang",
      "Xiaoxi Li",
      "Wenxiang Jiao",
      "Mining Tan",
      "Yinuo Wang"
    ],
    "stars": "0",
    "details": {
      "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
      "abstract": "We propose TourPlanner , a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04698",
      "pdf_url": "https://arxiv.org/pdf/2601.04698",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04698",
      "scraped_at": "2026-01-14T01:55:39.117410"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2601.07376",
    "authors": [
      "Jiaxuan You",
      "zsqzz"
    ],
    "stars": "568",
    "details": {
      "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
      "abstract": "üéâ Introducing OpenTinker üöÄ A scalable RL infrastructure for LLM agents that separates what you build (agents + environments) from how it runs (training + inference)! üß© Composable RL-as-a-Service No more monolithic RL pipelines. OpenTinker decomposes agentic learning into lightweight, modular components with clean abstraction boundaries. Plug in new agents, environments, and interaction protocols with minimal friction. ‚öôÔ∏è Unified Runtime for Training + Inference A centralized scheduler manages shared compute across workloads like RL (LoRA / full-parameter), SFT, and high-throughput inference. Built for multi-tenant scaling and real-world iteration speed. ü§ñ Multi-Agent Ready by Design OpenTinker supports coordinator-driven multi-agent interaction. Each agent can optimize independently while coordination emerges through environment dynamics. This keeps MARL scalable, flexible, and system-friendly. üîó Links: üìÑ Paper (arXiv): https://arxiv.org/pdf/2601.07376 üíª GitHub: https://github.com/open-tinker/OpenTinker",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07376",
      "pdf_url": "https://arxiv.org/pdf/2601.07376",
      "github_links": [
        "https://github.com/open-tinker/OpenTinker?tab=readme-ov-file",
        "https://github.com/open-tinker/OpenTinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07376",
      "scraped_at": "2026-01-14T01:55:41.067699"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Are LLM Decisions Faithful to Verbal Confidence?",
    "paper_url": "https://huggingface.co/papers/2601.07767",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Are LLM Decisions Faithful to Verbal Confidence?",
      "abstract": "While LLMs can express their confidence levels, their actual decisions do not demonstrate risk sensitivity. Even with high error penalties, they rarely abstain from making choices, often leading to utility collapse.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07767",
      "pdf_url": "https://arxiv.org/pdf/2601.07767",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07767",
      "scraped_at": "2026-01-14T01:55:42.902306"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Structured Episodic Event Memory",
    "paper_url": "https://huggingface.co/papers/2601.06411",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Structured Episodic Event Memory",
      "abstract": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06411",
      "pdf_url": "https://arxiv.org/pdf/2601.06411",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06411",
      "scraped_at": "2026-01-14T01:55:44.777676"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "paper_url": "https://huggingface.co/papers/2601.03666",
    "authors": [
      "Zhicheng Dou",
      "Tetsuya Sakai",
      "Radu Timofte",
      "Sicheng Gao",
      "Haon-Chen"
    ],
    "stars": "0",
    "details": {
      "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
      "abstract": "A lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. Checkpoints: https://huggingface.co/Haon-Chen/e5-omni-3B https://huggingface.co/Haon-Chen/e5-omni-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03666",
      "pdf_url": "https://arxiv.org/pdf/2601.03666",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03666",
      "scraped_at": "2026-01-14T01:55:46.645891"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "paper_url": "https://huggingface.co/papers/2601.07786",
    "authors": [
      "Mia Mohammad Imran",
      "Abdullah Al Mujahid"
    ],
    "stars": "0",
    "details": {
      "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
      "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07786",
      "pdf_url": "https://arxiv.org/pdf/2601.07786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07786",
      "scraped_at": "2026-01-14T01:55:48.489091"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "ShowUI-Aloha: Human-Taught GUI Agent",
    "paper_url": "https://huggingface.co/papers/2601.07181",
    "authors": [
      "Zhiheng Chen",
      "Jessica Hu",
      "Yauhong Goh",
      "Xiangwu Guo",
      "Yichun Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "ShowUI-Aloha: Human-Taught GUI Agent",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2601.07181",
      "pdf_url": "https://arxiv.org/pdf/2601.07181",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07181",
      "scraped_at": "2026-01-14T01:55:50.358460"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Codified Foreshadowing-Payoff Text Generation",
    "paper_url": "https://huggingface.co/papers/2601.07033",
    "authors": [
      "Jingbo Shang",
      "Letian Peng",
      "Kun Zhou",
      "Longfei Yun",
      "hyp1231"
    ],
    "stars": "0",
    "details": {
      "title": "Codified Foreshadowing-Payoff Text Generation",
      "abstract": "Codified Foreshadowing-Payoff Text Generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07033",
      "pdf_url": "https://arxiv.org/pdf/2601.07033",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07033",
      "scraped_at": "2026-01-14T01:55:52.291930"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
    "paper_url": "https://huggingface.co/papers/2601.04577",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
      "abstract": "While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04577",
      "pdf_url": "https://arxiv.org/pdf/2601.04577",
      "github_links": [
        "https://github.com/AmberLJC/Sci-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04577",
      "scraped_at": "2026-01-14T01:55:54.190567"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "paper_url": "https://huggingface.co/papers/2601.03570",
    "authors": [
      "Zaishuo Xia",
      "Minqian Liu",
      "Yunzhi Yao",
      "Sha Li",
      "Barry Menglong Yao"
    ],
    "stars": "0",
    "details": {
      "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
      "abstract": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03570",
      "pdf_url": "https://arxiv.org/pdf/2601.03570",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03570",
      "scraped_at": "2026-01-14T01:55:56.059301"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
    "paper_url": "https://huggingface.co/papers/2601.07389",
    "authors": [
      "Weixi Zhang",
      "Wei Han",
      "Bo Bai",
      "Xueyan Niu"
    ],
    "stars": "0",
    "details": {
      "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
      "abstract": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07389",
      "pdf_url": "https://arxiv.org/pdf/2601.07389",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07389",
      "scraped_at": "2026-01-14T01:55:57.904888"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
    "paper_url": "https://huggingface.co/papers/2601.06993",
    "authors": [
      "Xiaoming Liu",
      "Yiyang Su",
      "Paipile"
    ],
    "stars": "1",
    "details": {
      "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
      "abstract": "In this work, we investigate the impact of CoT on Fine-Grained Visual Classification (FGVC), revealing a paradox: the degradation in FGVC performance due to CoT is primarily driven by reasoning length, with longer textual reasoning consistently reducing classification accuracy. We introduce the concept of the \"Cost of Thinking\" to describe this phenomenon. Building on this insight, we propose two key contributions: (1) MRN, a normalization method for multi-reward optimization that balances heterogeneous reward signals; and (2) ReFine-RFT, a framework that integrates ensemble rewards with MRN to constrain reasoning length while providing dense, accuracy-oriented feedback. Our extensive experiments across multiple FGVC benchmarks demonstrate the effectiveness of our approach, achieving state-of-the-art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06993",
      "pdf_url": "https://arxiv.org/pdf/2601.06993",
      "github_links": [
        "https://github.com/jiezhu23/ReFine-RFT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06993",
      "scraped_at": "2026-01-14T01:55:59.761826"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "paper_url": "https://huggingface.co/papers/2601.06966",
    "authors": [
      "Shaolei Zhang",
      "Zishan Xu",
      "Sen Hu",
      "Zhiyuan Yao",
      "Haonan-Bian"
    ],
    "stars": "0",
    "details": {
      "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory (2026) Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents (2026) Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI (2025) MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards (2026) KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions (2026) MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents (2026) Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06966",
      "pdf_url": "https://arxiv.org/pdf/2601.06966",
      "github_links": [
        "https://github.com/AvatarMemory/RealMemBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06966",
      "scraped_at": "2026-01-14T01:56:01.621095"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06944",
    "authors": [
      "Shixing Li",
      "Guozhang Li",
      "Yaoyao Zhong",
      "Mei Wang",
      "Yuhang Su"
    ],
    "stars": "0",
    "details": {
      "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ViRectify: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models (2025) PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding (2025) MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models (2026) AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding (2026) CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution (2025) PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models (2025) Evaluating large language models on multimodal chemistry olympiad exams (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06944",
      "pdf_url": "https://arxiv.org/pdf/2601.06944",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06944",
      "scraped_at": "2026-01-14T01:56:03.488898"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06788",
    "authors": [
      "Manling Li",
      "Zeguan Wu",
      "Canyu Chen",
      "Zihan Wang",
      "Min Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06788",
      "pdf_url": "https://arxiv.org/pdf/2601.06788",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06788",
      "scraped_at": "2026-01-14T01:56:05.320924"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "paper_url": "https://huggingface.co/papers/2601.06747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
      "abstract": "This paper introduces FinForge, a novel framework designed to address the scarcity of high-quality, domain-specific datasets for evaluating Large Language Models (LLMs) in finance. The authors propose a scalable, semi-synthetic pipeline that combines expert-guided data curation from authoritative sources with controlled question generation and validation using Gemini 2.5 Flash. Key Contributions: FinForge Framework: A hybrid pipeline integrating manual/programmatic corpus construction with rigorous LM-based synthesis. FinForge-5k Dataset: A new snapshot benchmark comprising over 5,000 human-validated Q&A pairs across 11 financial subdomains, derived from a curated corpus of 100,000 verified documents (143M tokens). Benchmarking Results: Evaluation of state-of-the-art open and closed-source models reveals significant variance in financial reasoning capabilities, with leading models achieving approximately 80% accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06747",
      "pdf_url": "https://arxiv.org/pdf/2601.06747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06747",
      "scraped_at": "2026-01-14T01:56:07.161587"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "paper_url": "https://huggingface.co/papers/2601.06463",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06463",
      "pdf_url": "https://arxiv.org/pdf/2601.06463",
      "github_links": [
        "https://github.com/XuezheMax/gecko-llm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06463",
      "scraped_at": "2026-01-14T01:56:09.049268"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "paper_url": "https://huggingface.co/papers/2601.06423",
    "authors": [
      "Deep Mehta"
    ],
    "stars": "0",
    "details": {
      "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
      "abstract": "We ask a question that hasn't been studied before: does inference scaling improve reasoning faithfulness or just accuracy? Self-consistency (majority voting over multiple reasoning paths) reliably boosts LLM accuracy on reasoning tasks. But does getting the right answer more often mean the model is actually reasoning better? We test 4 frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K problems and find a surprising tradeoff. Accuracy gains from self-consistency don't necessarily translate to more faithful reasoning. This discovery has important implications for AI safety. We may be building systems that appear smarter without actually reasoning more reliably.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06423",
      "pdf_url": "https://arxiv.org/pdf/2601.06423",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06423",
      "scraped_at": "2026-01-14T01:56:10.861313"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
    "paper_url": "https://huggingface.co/papers/2601.05747",
    "authors": [
      "Peter St\\√ºtz",
      "Marvin Brenner",
      "farooqhassaan"
    ],
    "stars": "0",
    "details": {
      "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
      "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasible models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ‚àº20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05747",
      "pdf_url": "https://arxiv.org/pdf/2601.05747",
      "github_links": [
        "https://github.com/farooqhassaan/FlyPose"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05747",
      "scraped_at": "2026-01-14T01:56:12.800773"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "paper_url": "https://huggingface.co/papers/2601.07790",
    "authors": [
      "Chaowei Yang",
      "Joseph Rogers",
      "Zifu Wang",
      "Emily Ma",
      "ymasri"
    ],
    "stars": "1",
    "details": {
      "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
      "abstract": "We evaluate 9 open-source models under zero-shot, few-shot, and RAG (FAISS) and measure both accuracy + per-log latency. Main takeaway: RAG can massively help small models (Qwen3-4B: 95.64%, Gemma3-1B: 85.28%), but some reasoning-focused models degrade with retrieval, showing that retrieval integration isn‚Äôt uniform across architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07790",
      "pdf_url": "https://arxiv.org/pdf/2601.07790",
      "github_links": [
        "https://github.com/stccenter/Benchmarking-SLMs-and-SRLMs-on-System-Log-Severity-Classification"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07790",
      "scraped_at": "2026-01-14T01:56:14.646904"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "paper_url": "https://huggingface.co/papers/2601.07239",
    "authors": [
      "Shreyash Dhoot",
      "Aadi Pandey",
      "Anusa Saha",
      "Shourya Aggarwal",
      "Tanmay Joshi"
    ],
    "stars": "0",
    "details": {
      "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "abstract": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07239",
      "pdf_url": "https://arxiv.org/pdf/2601.07239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07239",
      "scraped_at": "2026-01-14T01:56:16.492225"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.06496",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
      "abstract": "https://github.com/AIGeeksGroup/3DCoCav2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06496",
      "pdf_url": "https://arxiv.org/pdf/2601.06496",
      "github_links": [
        "https://github.com/AIGeeksGroup/3DCoCav2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06496",
      "scraped_at": "2026-01-14T01:56:18.356557"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.06329",
    "authors": [
      "Ju-Chieh Chou",
      "Yen-Chun Kuo",
      "Yi-Cheng Lin",
      "Liang-Hsuan Tseng",
      "Jeff Chan-Jan Sju"
    ],
    "stars": "0",
    "details": {
      "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
      "abstract": "Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ‚Äúglobal token perplexity‚Äù, which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06329",
      "pdf_url": "https://arxiv.org/pdf/2601.06329",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06329",
      "scraped_at": "2026-01-14T01:56:20.214536"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
    "paper_url": "https://huggingface.co/papers/2601.06307",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Dhruva Patil",
      "Zhenlin He",
      "Ishika Agarwal"
    ],
    "stars": "0",
    "details": {
      "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
      "abstract": "https://huggingface.co/collections/ishikaa/a-rising-tide-lifts-all-boats-mtqe-rewards-for-idioms",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06307",
      "pdf_url": "https://arxiv.org/pdf/2601.06307",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06307",
      "scraped_at": "2026-01-14T01:56:22.211331"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
    "paper_url": "https://huggingface.co/papers/2601.06238",
    "authors": [
      "Aman Chadha",
      "Vinija Jain",
      "Amit Dhanda",
      "Partha Pratim Saha",
      "Arion Das"
    ],
    "stars": "0",
    "details": {
      "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
      "abstract": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06238",
      "pdf_url": "https://arxiv.org/pdf/2601.06238",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06238",
      "scraped_at": "2026-01-14T01:56:24.053442"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "paper_url": "https://huggingface.co/papers/2601.06789",
    "authors": [
      "Yu2020",
      "KunyiWang",
      "shuozhang",
      "cadche",
      "jimson991"
    ],
    "stars": "19",
    "details": {
      "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
      "abstract": "code agent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06789",
      "pdf_url": "https://arxiv.org/pdf/2601.06789",
      "github_links": [
        "https://github.com/QuantaAlpha/MemGovern"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06789",
      "scraped_at": "2026-01-15T01:50:09.498721"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Solar Open Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.07022",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Solar Open Technical Report",
      "abstract": "huggingface model: https://huggingface.co/upstage/Solar-Open-100B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07022",
      "pdf_url": "https://arxiv.org/pdf/2601.07022",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07022",
      "scraped_at": "2026-01-15T01:50:11.630863"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
    "paper_url": "https://huggingface.co/papers/2601.04745",
    "authors": [
      "lanqz7766",
      "ChenglongLi",
      "Super-shuhe-v2",
      "Zhisheng888",
      "realty2333"
    ],
    "stars": "83",
    "details": {
      "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
      "abstract": "know me",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04745",
      "pdf_url": "https://arxiv.org/pdf/2601.04745",
      "github_links": [
        "https://github.com/QuantaAlpha/KnowMeBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04745",
      "scraped_at": "2026-01-15T01:50:13.446669"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "paper_url": "https://huggingface.co/papers/2601.08225",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
      "abstract": "While large language models have shown remarkable progress in tool use, maintaining high-quality, user-centric multi-turn conversations at scale remains a significant challenge. Our work focuses on: (1) Generating high-fidelity multi-turn dialogue datasets designed for practical tool-use scenarios. (2) Enhancing model performance in complex, user-oriented interactions. (3) Providing insights into scaling dialogue generation without compromising on user experience. Check out the full paper here: https://arxiv.org/abs/2601.08225",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08225",
      "pdf_url": "https://arxiv.org/pdf/2601.08225",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08225",
      "scraped_at": "2026-01-15T01:50:15.282705"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands",
    "paper_url": "https://huggingface.co/papers/2512.24965",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands",
      "abstract": "TL;DR: ShowUI-œÄ is a 450M flow-based vision-language-action model that treats GUI actions as continuous trajectories, generating smooth clicks and drags directly from screen observations. It unifies discrete and continuous actions, enabling precise drawing, rotation, sorting, and captcha solving without tokenized coordinates. arXiv: https://arxiv.org/abs/2512.24965 Website: https://showlab.github.io/showui-pi/ Github: https://github.com/showlab/showui-pi",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24965",
      "pdf_url": "https://arxiv.org/pdf/2512.24965",
      "github_links": [
        "https://github.com/showlab/showui-pi"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24965",
      "scraped_at": "2026-01-15T01:50:17.137473"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.08079",
    "authors": [
      "Zhao Cao",
      "lz1001",
      "TommyChien"
    ],
    "stars": "39",
    "details": {
      "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
      "abstract": "Project Repo: https://github.com/qhjqhj00/MemoBrain",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08079",
      "pdf_url": "https://arxiv.org/pdf/2601.08079",
      "github_links": [
        "https://github.com/qhjqhj00/MemoBrain"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08079",
      "scraped_at": "2026-01-15T01:50:18.963578"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "paper_url": "https://huggingface.co/papers/2601.06487",
    "authors": [],
    "stars": "49",
    "details": {
      "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
      "abstract": "As a key exploration of open-domain agents, our method has been validated within Amap's (Gaode Map) real-world business scenarios. Demonstrating significant practical value, we believe this paradigm represents one of the most important direction of AI agents in the future. Project Resources: Github: https://github.com/Alibaba-NLP/qqr Paper: https://arxiv.org/abs/2601.06487 Hugging Face: https://huggingface.co/collections/Alibaba-NLP/arenarl",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06487",
      "pdf_url": "https://arxiv.org/pdf/2601.06487",
      "github_links": [
        "https://github.com/Alibaba-NLP/qqr"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06487",
      "scraped_at": "2026-01-15T01:50:20.840380"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Ministral 3",
    "paper_url": "https://huggingface.co/papers/2601.08584",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Ministral 3",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T5Gemma 2: Seeing, Reading, and Understanding Longer (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs (2026) SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation (2025) Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08584",
      "pdf_url": "https://arxiv.org/pdf/2601.08584",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08584",
      "scraped_at": "2026-01-15T01:50:22.698526"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "paper_url": "https://huggingface.co/papers/2601.08831",
    "authors": [
      "Min-Hung Chen",
      "Fu-En Yang",
      "Chin-Yang Lin",
      "Cheng Sun",
      "Yang-Che Sun"
    ],
    "stars": "0",
    "details": {
      "title": "3AM: Segment Anything with Geometric Consistency in Videos",
      "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08831",
      "pdf_url": "https://arxiv.org/pdf/2601.08831",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08831",
      "scraped_at": "2026-01-15T01:50:24.602177"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.07264",
    "authors": [
      "Qingcheng Zeng",
      "Naotoyokoyama",
      "junjuewang",
      "lrzneedresearch",
      "weihao1115"
    ],
    "stars": "0",
    "details": {
      "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
      "abstract": "We reveal a \"confidence dichotomy\" in tool-use LLM agents, finding that evidence tools like web search systematically induce overconfidence due to noisy retrieval, while verification tools like code interpreters help ground reasoning and reduce miscalibration. To address this, we propose Calibration Agentic RL (CAR), a reinforcement learning framework that jointly optimizes task accuracy and calibration through a novel reward design, demonstrating significant reductions in calibration error while maintaining competitive performance across different domains and environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07264",
      "pdf_url": "https://arxiv.org/pdf/2601.07264",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07264",
      "scraped_at": "2026-01-15T01:50:26.533802"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "paper_url": "https://huggingface.co/papers/2601.08670",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
      "abstract": "Parallel Context-of-Experts Decoding (Pced) speeds up RAG by decoding in parallel from per-document KV-cache ‚Äúexperts‚Äù and selecting retrieval-supported tokens to recover cross-document reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08670",
      "pdf_url": "https://arxiv.org/pdf/2601.08670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08670",
      "scraped_at": "2026-01-15T01:50:28.367501"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Motion Attribution for Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.08828",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Motion Attribution for Video Generation",
      "abstract": "TL;DR: We propose MOTIVE, a scalable, motion-centric data attribution framework for video generation to identify which training clips improve or degrade motion dynamics, enabling curation and more.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08828",
      "pdf_url": "https://arxiv.org/pdf/2601.08828",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08828",
      "scraped_at": "2026-01-15T01:50:30.225015"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.08620",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
      "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://huggingface.co/vidore .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08620",
      "pdf_url": "https://arxiv.org/pdf/2601.08620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08620",
      "scraped_at": "2026-01-15T01:50:32.048862"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "paper_url": "https://huggingface.co/papers/2601.08303",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
      "abstract": "Proposes efficient diffusion transformers for edge devices via sparse attention, elastic training, and knowledge-guided distillation to achieve high-fidelity, fast on-device image generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08303",
      "pdf_url": "https://arxiv.org/pdf/2601.08303",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08303",
      "scraped_at": "2026-01-15T01:50:33.857107"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "paper_url": "https://huggingface.co/papers/2601.08665",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation (2025) MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots (2025) CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving (2025) NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction (2025) ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination (2025) Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation (2025) EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08665",
      "pdf_url": "https://arxiv.org/pdf/2601.08665",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08665",
      "scraped_at": "2026-01-15T01:50:35.696313"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "paper_url": "https://huggingface.co/papers/2601.08587",
    "authors": [],
    "stars": "550",
    "details": {
      "title": "End-to-End Video Character Replacement without Structural Guidance",
      "abstract": "End-to-End Video Character Replacement without Structural Guidance",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08587",
      "pdf_url": "https://arxiv.org/pdf/2601.08587",
      "github_links": [
        "https://github.com/Orange-3DV-Team/MoCha"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08587",
      "scraped_at": "2026-01-15T01:50:37.523465"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.08468",
    "authors": [
      "Sujian Li",
      "Yudong Wang",
      "Hailin Zhang",
      "Hanyu Li",
      "Jiangshan Duo"
    ],
    "stars": "0",
    "details": {
      "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Structured Reasoning for Large Language Models (2026) Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning (2026) Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning (2026) Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards (2025) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) ORION: Teaching Language Models to Reason Efficiently in the Language of Thought (2025) ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08468",
      "pdf_url": "https://arxiv.org/pdf/2601.08468",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08468",
      "scraped_at": "2026-01-15T01:50:39.376659"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "paper_url": "https://huggingface.co/papers/2601.07290",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
      "abstract": "Joint temporal understanding and spatial perception within a single framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07290",
      "pdf_url": "https://arxiv.org/pdf/2601.07290",
      "github_links": [
        "https://github.com/JPShi12/VideoLoom"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07290",
      "scraped_at": "2026-01-15T01:50:41.246788"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.06786",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
      "abstract": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemicallycalibrated reasoning (EPICAR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Paretosuperiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3√ó reduction in inference compute, matching the K = 30 performance of STaR with only K = 10 samples in capable models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06786",
      "pdf_url": "https://arxiv.org/pdf/2601.06786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06786",
      "scraped_at": "2026-01-15T01:50:43.224423"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "paper_url": "https://huggingface.co/papers/2601.08321",
    "authors": [
      "Ting Zhu",
      "Zipeng Guo",
      "Gaojing Zhou",
      "Xiaolong Fu",
      "Lichen Ma"
    ],
    "stars": "0",
    "details": {
      "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08321",
      "pdf_url": "https://arxiv.org/pdf/2601.08321",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08321",
      "scraped_at": "2026-01-15T01:50:45.007426"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
    "paper_url": "https://huggingface.co/papers/2601.04582",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
      "abstract": "Generating working visualization code is not enough. Charts must be semantically correct and visually meaningful. We introduce RL-Text2Vis, the first reinforcement-learning framework for Text-to-Visualization, using post-execution feedback to jointly optimize: ‚úîÔ∏è textual accuracy ‚úîÔ∏è code executability ‚úîÔ∏è visualization quality üìà Results: ‚Ä¢ +22% relative improvement in chart quality over GPT-4o ‚Ä¢ Code execution success boosted from 78% ‚Üí 97% ‚Ä¢ Strong generalization to out-of-domain benchmarks This work demonstrates the power of multi-objective RL for structured, multimodal reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04582",
      "pdf_url": "https://arxiv.org/pdf/2601.04582",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04582",
      "scraped_at": "2026-01-15T01:50:46.788335"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "paper_url": "https://huggingface.co/papers/2601.02669",
    "authors": [
      "Zhen Ye",
      "Ziyang Luo",
      "Zhiqi Shen",
      "Zixin Chen",
      "Hongzhan Lin"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
      "abstract": "Current automatic LLM fact-checking tests are too narrow, they only check if a model can verify a claim, ignoring the hard parts like finding evidence and decomposing check-worthy claims. FactArena is built to evaluate the full fact-checking pipeline. Testing 16 state-of-the-art models reveals that the whole process ranking can disagree with simple accuracy. The system also revealed fragility in LLM fact-checking through subtle claim modifications (\"claim flipping\"), tanked average accuracy to 68%, proving that trustworthy auditing of every stage in the process matters.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02669",
      "pdf_url": "https://arxiv.org/pdf/2601.02669",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02669",
      "scraped_at": "2026-01-15T01:50:48.582670"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.08173",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
      "abstract": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08173",
      "pdf_url": "https://arxiv.org/pdf/2601.08173",
      "github_links": [
        "https://github.com/KnowledgeXLab/EvoEnv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08173",
      "scraped_at": "2026-01-15T01:50:50.375117"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.07632",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07632",
      "pdf_url": "https://arxiv.org/pdf/2601.07632",
      "github_links": [
        "https://github.com/JYe16/GeoMotionGPT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07632",
      "scraped_at": "2026-01-15T01:50:52.181113"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "paper_url": "https://huggingface.co/papers/2601.07348",
    "authors": [],
    "stars": "79",
    "details": {
      "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
      "abstract": "arXiv explained breakdown of this paper üëâ https://arxivexplained.com/papers/controlled-self-evolution-for-algorithmic-code-optimization",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07348",
      "pdf_url": "https://arxiv.org/pdf/2601.07348",
      "github_links": [
        "https://github.com/QuantaAlpha/EvoControl"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07348",
      "scraped_at": "2026-01-16T01:52:19.823061"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.09688",
    "authors": [],
    "stars": "67",
    "details": {
      "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
      "abstract": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09688",
      "pdf_url": "https://arxiv.org/pdf/2601.09688",
      "github_links": [
        "https://github.com/Infinity-AILab/DeepResearchEval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09688",
      "scraped_at": "2026-01-16T01:52:21.780207"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.09259",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
      "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09259",
      "pdf_url": "https://arxiv.org/pdf/2601.09259",
      "github_links": [
        "https://github.com/exoskeletonzj/MAXS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09259",
      "scraped_at": "2026-01-16T01:52:23.826039"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
    "paper_url": "https://huggingface.co/papers/2601.09274",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
      "abstract": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A3-Bench, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09274",
      "pdf_url": "https://arxiv.org/pdf/2601.09274",
      "github_links": [
        "https://github.com/exoskeletonzj/A3-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09274",
      "scraped_at": "2026-01-16T01:52:25.818622"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09088",
    "authors": [],
    "stars": "16",
    "details": {
      "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
      "abstract": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09088",
      "pdf_url": "https://arxiv.org/pdf/2601.09088",
      "github_links": [
        "https://github.com/D2I-ai/dasd-thinking"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09088",
      "scraped_at": "2026-01-16T01:52:28.085648"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "paper_url": "https://huggingface.co/papers/2601.09708",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "abstract": "Project page: https://jasper0314-huang.github.io/fast-thinkact/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09708",
      "pdf_url": "https://arxiv.org/pdf/2601.09708",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09708",
      "scraped_at": "2026-01-16T01:52:30.028323"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "paper_url": "https://huggingface.co/papers/2601.09136",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
      "abstract": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09136",
      "pdf_url": "https://arxiv.org/pdf/2601.09136",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09136",
      "scraped_at": "2026-01-16T01:52:32.025174"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
    "paper_url": "https://huggingface.co/papers/2601.09575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
      "abstract": "OpenVoxel provides training-free grouping and captioning of sparse voxels for open-vocabulary 3D scene understanding using VLMs/MLLMs and text search, enabling RES and OVS without CLIP embeddings.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09575",
      "pdf_url": "https://arxiv.org/pdf/2601.09575",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09575",
      "scraped_at": "2026-01-16T01:52:34.005779"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
    "paper_url": "https://huggingface.co/papers/2601.09028",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
      "abstract": "OpenDecoder is a novel framework that directly 'opens' the LLM to modify its decoding process within RAG scenarios by leveraging relevance signals from retrieved documents. Through a robustness-oriented training algorithm, the model learns to perform answer decoding guided by explicit indicators, rather than relying solely on prompt engineering or internal attention scores. This approach significantly enhances the system's controllability, accuracy, and robustness across various noisy environments. Take Away: Opening LLM rather than solely relying on prompt engineering is important to improve the system‚Äôs robustness, since we cannot expect LLMs‚Äô implicit identification to be always correct. The external indicators, e.g., relevance score, confidence feature, faithful factors, are useful to incorporate with LLMs‚Äô internal information processing mechanism, e.g., attention, for output decoding, where the key problem is to obtain and integrate these indicators into LLMs with a sophisticated training algorithm (during post-training). Experimental results show the robustness enhancement in different levels of noisy environments of our initial investigation of OpenDecoder. An ideal situation is that the LLM can understand to what extent to rely on external retrieved knowledge and internal parametric knowledge during answer decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09028",
      "pdf_url": "https://arxiv.org/pdf/2601.09028",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09028",
      "scraped_at": "2026-01-16T01:52:36.082937"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
    "paper_url": "https://huggingface.co/papers/2601.08605",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
      "abstract": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08605",
      "pdf_url": "https://arxiv.org/pdf/2601.08605",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08605",
      "scraped_at": "2026-01-16T01:52:37.987916"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "paper_url": "https://huggingface.co/papers/2601.09465",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
      "abstract": "EvoFSM presents a controllable self-evolution framework using a finite state machine to guide adaptive problem-solving, separating macroscopic flow and microscopic skills with critic-guided updates and reusable priors.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09465",
      "pdf_url": "https://arxiv.org/pdf/2601.09465",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09465",
      "scraped_at": "2026-01-16T01:52:39.900524"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
    "paper_url": "https://huggingface.co/papers/2601.03928",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
      "abstract": "TL;DR: High-res UI screenshots (2K/4K) force VLMs to process thousands of visual tokens. Inspired by human vision, which selects only instruction-relevant image patches, FocusUI teaches VLMs where to look in UI screenshots smartly üîç üìÑ Paper: arXiv:2601.03928 üåê Project Page: showlab.github.io/FocusUI üíª Code: github.com/showlab/FocusUI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03928",
      "pdf_url": "https://arxiv.org/pdf/2601.03928",
      "github_links": [
        "https://github.com/showlab/FocusUI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03928",
      "scraped_at": "2026-01-16T01:52:41.892931"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
    "paper_url": "https://huggingface.co/papers/2601.06596",
    "authors": [
      "Chi Zhang",
      "Jiawei Shao",
      "Jiangan Chen",
      "Yiliang Song",
      "Hongjun An"
    ],
    "stars": "0",
    "details": {
      "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
      "abstract": "This paper treats preference undermining as an experimental object, not a vibe. A clean factorial design isolates manipulation factors and quantifies when truth yields to compliance. Conclusion, stated politely: yes, a large model can be PUA-ed, and it may rationalize the outcome as alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06596",
      "pdf_url": "https://arxiv.org/pdf/2601.06596",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06596",
      "scraped_at": "2026-01-16T01:52:43.833419"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "TranslateGemma Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09012",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TranslateGemma Technical Report",
      "abstract": "TranslateGemma extends Gemma 3 with two-stage fine-tuning (supervised then RL) for multilingual translation, achieving strong WMT performance and multimodal capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09012",
      "pdf_url": "https://arxiv.org/pdf/2601.09012",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09012",
      "scraped_at": "2026-01-16T01:52:45.793509"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "paper_url": "https://huggingface.co/papers/2601.08955",
    "authors": [
      "Wenjie Li",
      "Beichen Guo",
      "Hanlin Wang",
      "Youwei Liu",
      "jwanglvy"
    ],
    "stars": "0",
    "details": {
      "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
      "abstract": "TL;DR: An agent learning framework via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step \"imagined\" trajectories. This imagination is conducted via a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08955",
      "pdf_url": "https://arxiv.org/pdf/2601.08955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08955",
      "scraped_at": "2026-01-16T01:52:47.807412"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
    "paper_url": "https://huggingface.co/papers/2601.09697",
    "authors": [
      "Ayush Tewari",
      "Joan Lasenby",
      "Jeffrey Hu",
      "Jieying Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
      "abstract": "Proposes SRENDER: generate sparse diffusion keyframes for static scenes and render 3D views to produce long videos fast and consistently.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09697",
      "pdf_url": "https://arxiv.org/pdf/2601.09697",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09697",
      "scraped_at": "2026-01-16T01:52:49.731039"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Geometric Stability: The Missing Axis of Representations",
    "paper_url": "https://huggingface.co/papers/2601.09173",
    "authors": [
      "pcr2120"
    ],
    "stars": "0",
    "details": {
      "title": "Geometric Stability: The Missing Axis of Representations",
      "abstract": "DeepSeek got it half right with their mHC paper: stability matters for scaling. But they only measure stability DURING training. What about the stability of what models LEARN? I built Shesha to measure this - a geometric stability metric with SOTA results across AI Safety , Constitutional AI , Model selection , and CRISPR perturbation analysis. The core insight: Most evals check external similarity (does output X match Y?). But imagine a massive library where someone reshuffled all the books. A content-based audit would say that nothing's wrong since the inventory is identical. But the library is useless since nothing can be found. That's the gap Shesha fills. The implications are broad with SOTA results across 4 domains : AI Safety - Shesha is the best canary in the coal mine . Shesha outperforms CKA and Procrustes on drift detection. Detects 2x more drift than CKA, triggers earlier 73% of the time, catches subtle LoRA shifts at 90% sensitivity (5% FPR) - with only 7% false alarms vs Procrustes' 44%. Constitutional AI - Shesha provides the best steering prediction . Constitutional AI needs models you can actually steer. Most metrics ask: \"Are classes separable?\" Wrong question! Shesha asks: \"Is that separation STABLE under perturbation?\" Tested on 35-69 embedding models across 3 experiments. Shesha outperforms Fisher discriminant, silhouette score, Procrustes, and anisotropy. The correlations with intervention success are rho=0.89-0.96, and the partial correlations after controlling for separability are rho=0.67-0.76. Stability ‚â† separability. Model selection - Shesha exposes what LogME misses . The DINO Paradox - best transfer scores, worst geometric stability. Tested 94 vision models on 6 datasets: DINOv2 ranked #1 in LogME on 4/6 datasets but last or near last in stability on 5/6. SOTA transfer incurs a \"geometric tax.\" CRISPR perturbations - Shesha serves as a new filter for target selection . CRISPR screens find hits by magnitude, but magnitude alone can't distinguish clean lineage drivers from promiscuous regulators. Shesha adds precision. Tested on 811 perturbations, Shesha showed uniformly positive magnitude-stability correlations ranging from rho=0.746 in high-variance screens to rho=0.963 in cleaner activation settings. Notably, in discordant cases, Shesha separates KLF1 (stable, specific) from CEBPA (strong but messy) purely from geometry. Additional validation in neuroscience: Geometric stability predicted neural-behavioral coupling (rho=0.18, p=0.005) in Neuropixels data. Centroid drift showed no relationship (rho=0.00). Stability ‚â† consistency. Try it yourself: PyPI: pip install shesha-geometry Tutorials: https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials Preprint: https://arxiv.org/abs/2601.09173 Code: https://github.com/prashantcraju/geometric-stability",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09173",
      "pdf_url": "https://arxiv.org/pdf/2601.09173",
      "github_links": [
        "https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials",
        "https://github.com/prashantcraju/geometric-stability"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09173",
      "scraped_at": "2026-01-16T01:52:51.691110"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "The AI Hippocampus: How Far are We From Human Memory?",
    "paper_url": "https://huggingface.co/papers/2601.09113",
    "authors": [
      "Tong Wu",
      "Yuxuan Wang",
      "Yipeng Kang",
      "Jiaqi Li",
      "Zixia Jia"
    ],
    "stars": "0",
    "details": {
      "title": "The AI Hippocampus: How Far are We From Human Memory?",
      "abstract": "Survey of memory in LLMs and multimodal models, detailing implicit, explicit, and agentic memory, architectures, benchmarks, and challenges in persistence, alignment, and cross-modal retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09113",
      "pdf_url": "https://arxiv.org/pdf/2601.09113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09113",
      "scraped_at": "2026-01-16T01:52:53.593035"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.01075",
    "authors": [
      "Thomas Anderson Keller",
      "Yilun Du",
      "Fangneng Zhan",
      "Benhao Huang",
      "Hansen Jin Lillemark"
    ],
    "stars": "5",
    "details": {
      "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01075",
      "pdf_url": "https://arxiv.org/pdf/2601.01075",
      "github_links": [
        "https://github.com/hlillemark/flowm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01075",
      "scraped_at": "2026-01-16T01:52:55.619837"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
    "paper_url": "https://huggingface.co/papers/2601.09609",
    "authors": [
      "Ruihua Song",
      "Yi Zhao",
      "Wei Bi",
      "Yahui Liu",
      "Qian Cao"
    ],
    "stars": "0",
    "details": {
      "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
      "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09609",
      "pdf_url": "https://arxiv.org/pdf/2601.09609",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09609",
      "scraped_at": "2026-01-16T01:52:57.577292"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09536",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
      "abstract": "This paper proposes a unified generative multimodal reasoning paradigm, using a two-stage SFT+RL framework with perception alignment loss and perception reward, and explores bootstrapping step-wise visualizations from text-only reasoning data when multimodal annotation availability is extremely limited.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09536",
      "pdf_url": "https://arxiv.org/pdf/2601.09536",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09536",
      "scraped_at": "2026-01-16T01:52:59.576152"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2601.07287",
    "authors": [
      "Xiao Yang",
      "Kaipeng Zhang",
      "Shenghai Yuan",
      "Yuanyang Yin",
      "yfdeng10"
    ],
    "stars": "0",
    "details": {
      "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision (2025) Plan-X: Instruct Video Generation via Semantic Planning (2025) AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation (2025) Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models (2025) InstanceV: Instance-Level Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07287",
      "pdf_url": "https://arxiv.org/pdf/2601.07287",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07287",
      "scraped_at": "2026-01-16T01:53:02.313236"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
    "paper_url": "https://huggingface.co/papers/2601.06794",
    "authors": [
      "Yixia Li",
      "Xingchen Zeng",
      "Yulan Hu",
      "Lingjie Jiang",
      "Zhicong Li"
    ],
    "stars": "0",
    "details": {
      "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
      "abstract": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06794",
      "pdf_url": "https://arxiv.org/pdf/2601.06794",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06794",
      "scraped_at": "2026-01-16T01:53:04.140516"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.04809",
    "authors": [
      "Yixin Cao",
      "Xinrun Wang",
      "Zhongyuan Peng",
      "Changyi Xiao",
      "SII-Molu"
    ],
    "stars": "6",
    "details": {
      "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
      "abstract": "Scalable Environment Synthesis Given a programming problem (statement + reference solution), SCALER synthesizes a reasoning environment with: Verifiability: deterministic oracle / unit tests provide correctness signals. Difficulty control: explicit scale parameters discretized into difficulty levels. Unbounded instance generation: randomized testcase generation yields unlimited training instances. Adaptive Multi-Environment RL SCALER sustains learning signals at two levels: In-environment difficulty controller: keeps sampling near a target success regime. Environment curation: maintains an active set and replaces saturated/uninformative environments to preserve diversity and long-horizon improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04809",
      "pdf_url": "https://arxiv.org/pdf/2601.04809",
      "github_links": [
        "https://github.com/molumolua/SCALER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04809",
      "scraped_at": "2026-01-16T01:53:05.993611"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "paper_url": "https://huggingface.co/papers/2601.09282",
    "authors": [
      "Jolanta Mizeria-Pietraszko",
      "lsliwko"
    ],
    "stars": "0",
    "details": {
      "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
      "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09282",
      "pdf_url": "https://arxiv.org/pdf/2601.09282",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09282",
      "scraped_at": "2026-01-16T01:53:07.875759"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
    "paper_url": "https://huggingface.co/papers/2601.08472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08472",
      "pdf_url": "https://arxiv.org/pdf/2601.08472",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08472",
      "scraped_at": "2026-01-16T01:53:09.794070"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
    "paper_url": "https://huggingface.co/papers/2601.04469",
    "authors": [
      "Aleksey Komissarov",
      "Ekaterina Chelombitko",
      "Iaroslav Chelombitko"
    ],
    "stars": "0",
    "details": {
      "title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
      "abstract": "The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04469",
      "pdf_url": "https://arxiv.org/pdf/2601.04469",
      "github_links": [
        "https://github.com/AragonerUA/SampoNLP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04469",
      "scraped_at": "2026-01-16T01:53:11.668045"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "125",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-17T01:46:10.410290"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "152",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-17T01:46:12.281061"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-17T01:46:14.129704"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-17T01:46:15.994820"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "22",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-17T01:46:17.812994"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "34",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-17T01:46:19.609904"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "12",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-17T01:46:21.464351"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "332",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-17T01:46:23.272686"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-17T01:46:25.131294"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-17T01:46:26.958024"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-17T01:46:28.768874"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-17T01:46:30.627023"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-17T01:46:32.424107"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "20",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-17T01:46:34.429955"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-17T01:46:36.227451"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-17T01:46:38.096548"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-17T01:46:39.969567"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-17T01:46:41.766508"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "75",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-17T01:46:43.586501"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-17T01:46:45.366327"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "0",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-17T01:46:47.168503"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-17T01:46:48.943667"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-17T01:46:50.737198"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-17T01:46:52.610283"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "4",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-17T01:46:54.410994"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-17T01:46:56.176236"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-17T01:46:57.933744"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-17T01:46:59.691457"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-17T01:47:01.464358"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-17T01:47:03.230799"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "45",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-17T01:47:04.980548"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-17T01:47:06.760677"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-17T01:47:08.580845"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-17T01:47:10.393786"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-17T01:47:12.142262"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-17T01:47:13.912097"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-17T01:47:15.696403"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "20",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-17T01:47:17.436915"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "168",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-18T01:58:28.325011"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "135",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-18T01:58:30.488100"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-18T01:58:32.445211"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-18T01:58:34.356864"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "29",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-18T01:58:36.230150"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-18T01:58:38.101276"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "14",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-18T01:58:40.112824"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "336",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-18T01:58:42.067107"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-18T01:58:43.999136"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "86",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-18T01:58:45.918917"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-18T01:58:47.820357"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-18T01:58:49.722522"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-18T01:58:52.822014"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-18T01:58:54.734358"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "21",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-18T01:58:56.653857"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-18T01:58:58.523736"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-18T01:59:00.469539"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-18T01:59:02.337989"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-18T01:59:04.192079"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "164",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [
        "https://github.com/HeartMuLa/heartlib"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-18T01:59:06.066745"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "159",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-18T01:59:08.004724"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-18T01:59:09.867129"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-18T01:59:11.788446"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-18T01:59:13.656078"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-18T01:59:15.486341"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "4",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-18T01:59:17.319882"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-18T01:59:19.169455"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-18T01:59:21.091232"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-18T01:59:22.939202"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-18T01:59:24.819341"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-18T01:59:26.635216"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "55",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-18T01:59:28.561269"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-18T01:59:30.387446"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-18T01:59:32.244800"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-18T01:59:34.092644"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-18T01:59:35.925826"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-18T01:59:37.795150"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "20",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-18T01:59:39.653822"
    },
    "scraped_date": "2026-01-18"
  }
]