[
  {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "paper_url": "https://huggingface.co/papers/2601.15876",
    "authors": [],
    "stars": "154",
    "details": {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "EvoCUA: Evolving Computer Use Agent ü•á #1 Open-Source Model on OSWorld | A General-Purpose Multimodal Model Excelling at Computer Use üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/meituan/EvoCUA üåü Highlights ü•á #1 Open-Source Model on OSWorld : Achieves 56.7% task completion rate, #1 among all open-source models üìà Significant Improvements : +11.7% over OpenCUA-72B (45.0%‚Üí56.7%), +15.1% over Qwen3-VL thinking (41.6%‚Üí56.7%), with fewer parameters and half the steps üñ•Ô∏è End-to-End Multi-Turn Automation : Operates Chrome, Excel, PowerPoint, VSCode and more through screenshots and natural language instructions üß† Novel Training Method : Our data synthesis and training approach consistently improves Computer Use capability across multiple open-source VLMs without degrading general performance üìä Performance Comparison Rank Model Open/Closed Type Max Steps Score 1 Claude-sonnet-4-5 üîí Closed General 100 62.9% 2 Seed-1.8 üîí Closed General 100 61.9% 3 Claude-sonnet-4-5 üîí Closed General 50 58.1% 4 EvoCUA-20260105 (Ours) üü¢ Open General 50 56.7% ü•á 5 DeepMiner-Mano-72B üîí Closed Specialized 100 53.9% 6 UI-TARS-2-2509 üîí Closed General 100 53.1% 7 EvoCUA (Previous Version) üîí Closed General 50 50.3% 8 EvoCUA-8B-20260105 (Ours) üü¢ Open General 50 46.1% 9 OpenCUA-72B üü¢ Open Specialized 100 45.0% ... ... ... ... ... ... 13 Qwen3-VL-Flash üîí Closed General 100 41.6% EvoCUA is #1 among all open-source models , achieving competitive results with only 50 steps . Human-level performance remains significantly higher, indicating substantial room for improvement. üìù Citation If you find EvoCUA useful in your research, please consider citing: @ misc {evocua2026,\n  title={EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience},\n  author={Chong Peng* and Taofeng Xue*},\n  year={2026},\n  url={https://github.com/meituan/EvoCUA},\n  note={* Equal contribution}\n} üìú License This project is licensed under the Apache 2.0 License - see the LICENSE file for details. Built with ‚ù§Ô∏è by Meituan LongCat Team",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15876",
      "pdf_url": "https://arxiv.org/pdf/2601.15876",
      "github_links": [
        "https://github.com/meituan/EvoCUA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15876",
      "scraped_at": "2026-01-26T02:01:16.604428"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.14724",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "abstract": "üöÄ Introducing HERMES: The Future of Real-Time Streaming Video Understanding! While today's Multimodal Large Language Models (MLLMs) perform impressively at offline video comprehension, they often face a \"painful trade-off\" when it comes to real-time streaming video - balancing real-time responses, low memory usage, and high accuracy. To solve this, we introduce the following innovations: üí° The HERMES Breakthrough: 1Ô∏è‚É£ Novel memory architecture: By deeply analyzing attention mechanisms, we' ve introduced a \"Hierarchical Memory\" approach. The KV Cache is now reimagined as a multi-level memory framework: Shallow layers act as Sensory Memory (events that just happened). Deep layers focus on Long-term Memory (frame-level semantic anchors). Middle layers bridge the gap with Working Memory. 2Ô∏è‚É£ Plug-and-play architecture: HERMES achieves highly efficient KV Cache reuse and optimization strategies including cross-layer memory smoothing and position re-indexing , delivering instant responses without the need for additional training, or auxiliary computations when user queries arrive. 3Ô∏è‚É£ Incredible efficiency and performance: ‚ö° Blazing speed: HERMES is 10x faster than previous SOTA in terms of response latency (TTFT)! üöÄ Compact efficiency: Even with 68% fewer video tokens, the model remains rock-solid, achieving up to 11.4% improvement in streaming comprehension tasks! üíæ Memory-friendly: No matter the video length, memory usage stays constant, leaving OOM errors in the past. üî• Join us in exploring this breakthrough: If you're passionate about streaming video understanding and efficient inference, we'd love to discuss and collaborate! üîç Explore the Details : üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/haowei-freesky/HERMES üåê Project: https://hermes-streaming.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14724",
      "pdf_url": "https://arxiv.org/pdf/2601.14724",
      "github_links": [
        "https://github.com/haowei-freesky/HERMES"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14724",
      "scraped_at": "2026-01-26T02:01:18.574988"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.16206",
    "authors": [],
    "stars": "81",
    "details": {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "Introducing LLM-in-Sandbox ‚Äî put your LLM in a virtual computer to unlock general agentic intelligence for non-code tasks! Significant gains for chemistry, long-context QA, instruction following, and more. No extra training needed. üåê Demo: https://llm-in-sandbox.github.io üíª Code: https://github.com/llm-in-sandbox/llm-in-sandbox pip install llm-in-sandbox Feel free to open issues or discussions  ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16206",
      "pdf_url": "https://arxiv.org/pdf/2601.16206",
      "github_links": [
        "https://github.com/llm-in-sandbox/llm-in-sandbox"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16206",
      "scraped_at": "2026-01-26T02:01:20.577739"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15165",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "abstract": "Links üìÑ paper: https://arxiv.org/abs/2601.15165 üè† project page: https://nzl-thu.github.io/the-flexibility-trap üíª code: https://github.com/LeapLabTHU/JustGRPO ü§ó model: https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15165",
      "pdf_url": "https://arxiv.org/pdf/2601.15165",
      "github_links": [
        "https://github.com/LeapLabTHU/JustGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15165",
      "scraped_at": "2026-01-26T02:01:22.559804"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "paper_url": "https://huggingface.co/papers/2601.15197",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "abstract": "üèóÔ∏è Architecture BayesianVLA is a novel framework designed to solve the Vision Shortcut problem in Vision-Language-Action (VLA) models. In current VLA training, goal-driven datasets often make language instructions highly predictable from visual observations alone. This leads to Information Collapse, where the model ignores language and degenerates into a vision-only policy, failing miserably in out-of-distribution (OOD) scenarios. BayesianVLA addresses this by: Bayesian Decomposition : Explicitly modeling a vision-only prior $p(a|v)$ and a language-conditioned posterior $\\pi(a|v, \\ell)$. LLR Optimization : Maximizing the Log-Likelihood Ratio (LLR) to penalize actions that rely solely on visual cues and reward actions that are truly grounded in language instructions. ‚ú® Key Features Dual-Branch Architecture : Uses learnable Latent Action Queries to decouple vision-only and language-conditioned action distributions. Zero Extra Data : Achieves significant performance gains (e.g., +11.3% on SimplerEnv) using the exact same datasets as baselines. Preserves VLM Intelligence : Effectively regularizes the model to prevent the \"catastrophic forgetting\" of general multimodal reasoning capabilities common in standard VLA fine-tuning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15197",
      "pdf_url": "https://arxiv.org/pdf/2601.15197",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/BayesianVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15197",
      "scraped_at": "2026-01-26T02:01:24.489273"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "paper_url": "https://huggingface.co/papers/2601.16208",
    "authors": [],
    "stars": "129",
    "details": {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "abstract": "We scale RAE to text-to-image, and its advantage over VAEs still holds!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16208",
      "pdf_url": "https://arxiv.org/pdf/2601.16208",
      "github_links": [
        "https://github.com/ZitengWangNYU/Scale-RAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16208",
      "scraped_at": "2026-01-26T02:01:26.455999"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "paper_url": "https://huggingface.co/papers/2601.15892",
    "authors": [],
    "stars": "28",
    "details": {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs (2025) LLaDA2.0: Scaling Up Diffusion Language Models to 100B (2025) WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference (2025) CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models (2026) SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding (2025) Fast and Accurate Causal Parallel Decoding using Jacobi Forcing (2025) Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15892",
      "pdf_url": "https://arxiv.org/pdf/2601.15892",
      "github_links": [
        "https://github.com/ByteDance-Seed/Stable-DiffCoder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15892",
      "scraped_at": "2026-01-26T02:01:28.374246"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "paper_url": "https://huggingface.co/papers/2601.16093",
    "authors": [],
    "stars": "1.51k",
    "details": {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Project page: https://zhouyiks.github.io/projects/SAMTok/ Training Code: https://github.com/bytedance/Sa2VA/tree/main/projects/samtok Short Bio:   We present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16093",
      "pdf_url": "https://arxiv.org/pdf/2601.16093",
      "github_links": [
        "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16093",
      "scraped_at": "2026-01-26T02:01:30.339469"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Learning to Discover at Test Time",
    "paper_url": "https://huggingface.co/papers/2601.16175",
    "authors": [],
    "stars": "163",
    "details": {
      "title": "Learning to Discover at Test Time",
      "abstract": "New paper on scientific discovery with test time training. New discoveries on several open scientific problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16175",
      "pdf_url": "https://arxiv.org/pdf/2601.16175",
      "github_links": [
        "https://github.com/test-time-training/discover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16175",
      "scraped_at": "2026-01-26T02:01:32.194965"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Qwen3-TTS Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.15621",
    "authors": [],
    "stars": "4.27k",
    "details": {
      "title": "Qwen3-TTS Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IndexTTS 2.5 Technical Report (2026) FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2026) DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec (2025) VoiceSculptor: Your Voice, Designed By You (2026) GLM-TTS Technical Report (2025) MiMo-Audio: Audio Language Models are Few-Shot Learners (2025) JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15621",
      "pdf_url": "https://arxiv.org/pdf/2601.15621",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-TTS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15621",
      "scraped_at": "2026-01-26T02:01:34.141141"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "paper_url": "https://huggingface.co/papers/2601.11868",
    "authors": [
      "Boxuan Li",
      "Nicholas Carlini",
      "Alexander G. Shaw",
      "Mike A. Merrill",
      "menorf"
    ],
    "stars": "1.41k",
    "details": {
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts (2026) Real-Time Procedural Learning From Experience for AI Agents (2025) Benchmarking LLM Agents for Wealth-Management Workflows (2025) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (2025) SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (2025) The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11868",
      "pdf_url": "https://arxiv.org/pdf/2601.11868",
      "github_links": [
        "https://github.com/laude-institute/terminal-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11868",
      "scraped_at": "2026-01-26T02:01:36.017971"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "paper_url": "https://huggingface.co/papers/2601.15727",
    "authors": [
      "Yixin Shen",
      "Haiming Wu",
      "Chi Hsu Tsai",
      "Peiyu Zang",
      "Yang Yu"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "Summary of Key Points Kernel quality is a fundamental bottleneck for modern AI system performance, yet high-quality kernel engineering is expert-intensive, time-consuming, and difficult to scale. Recent advances in large language models (LLMs) and LLM-based agents enable automated kernel generation and optimization by capturing expert knowledge and supporting iterative, feedback-driven optimization loops. Despite rapid progress, existing work is fragmented and lacks a unified, systematic perspective. This survey provides a structured overview of LLM-based kernel generation methods and agentic optimization workflows, and compiles the key datasets and benchmarks used for training and evaluation. The paper further identifies open challenges and outlines future research directions, aiming to serve as a comprehensive reference for next-generation automated kernel optimization. Resources Open-source repository tracking this field: https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15727",
      "pdf_url": "https://arxiv.org/pdf/2601.15727",
      "github_links": [
        "https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15727",
      "scraped_at": "2026-01-26T02:01:37.896798"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.15369",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "abstract": "Project Page: https://ucsc-vlaa.github.io/OpenVision3/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15369",
      "pdf_url": "https://arxiv.org/pdf/2601.15369",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15369",
      "scraped_at": "2026-01-26T02:01:39.825845"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.16125",
    "authors": [
      "Dingkun Long",
      "Zhuoning Guo",
      "Mingxin Li",
      "Yanzhao Zhang",
      "songtingyu"
    ],
    "stars": "1",
    "details": {
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "abstract": "A new benchmark for Composed Image Retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16125",
      "pdf_url": "https://arxiv.org/pdf/2601.16125",
      "github_links": [
        "https://github.com/SighingSnow/edir"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16125",
      "scraped_at": "2026-01-26T02:01:41.667094"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "paper_url": "https://huggingface.co/papers/2601.16163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "abstract": "Cosmos Policy fine-tunes a pretrained video model in one stage for visuomotor control, enabling action latent frames, future state prediction, and planning, achieving state-of-the-art robotic benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16163",
      "pdf_url": "https://arxiv.org/pdf/2601.16163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16163",
      "scraped_at": "2026-01-26T02:01:43.638260"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.16148",
    "authors": [],
    "stars": "91",
    "details": {
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "abstract": "ü§óTry it out: https://huggingface.co/spaces/facebook/ActionMesh üåêProject Page: https://remysabathier.github.io/actionmesh/ üìÑPaper: https://remysabathier.github.io/actionmesh/actionmesh_2026.pdf üíªCode: https://github.com/facebookresearch/actionmesh",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16148",
      "pdf_url": "https://arxiv.org/pdf/2601.16148",
      "github_links": [
        "https://github.com/facebookresearch/actionmesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16148",
      "scraped_at": "2026-01-26T02:01:45.631171"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "paper_url": "https://huggingface.co/papers/2601.14255",
    "authors": [],
    "stars": "108",
    "details": {
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "abstract": "Demo: https://huggingface.co/spaces/SammyLim/VideoMaMa Git: https://github.com/cvlab-kaist/VideoMaMa Project Page: https://cvlab-kaist.github.io/VideoMaMa/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14255",
      "pdf_url": "https://arxiv.org/pdf/2601.14255",
      "github_links": [
        "https://github.com/cvlab-kaist/VideoMaMa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14255",
      "scraped_at": "2026-01-26T02:01:47.579204"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15224",
    "authors": [
      "Dingcheng Wang",
      "Haoran Lu",
      "Haosen Sun",
      "Jianshu Zhang",
      "Raymond-Qiancx"
    ],
    "stars": "76",
    "details": {
      "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
      "abstract": "Towards General Progress Understanding for Embodied Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15224",
      "pdf_url": "https://arxiv.org/pdf/2601.15224",
      "github_links": [
        "https://github.com/ProgressLM/ProgressLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15224",
      "scraped_at": "2026-01-26T02:01:49.425233"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Agentic Uncertainty Quantification",
    "paper_url": "https://huggingface.co/papers/2601.15703",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Uncertainty Quantification",
      "abstract": "üõë Stop the \"Spiral of Hallucination\" in Autonomous Agents! Long-horizon agents often fail because minor early errors snowball into irreversible failures. We introduce Agentic Uncertainty Quantification (AUQ) , a training-free Dual-Process framework inspired by System 1/System 2 thinking: üß† System 1 (Fast): Uncertainty-Aware Memory propagates doubt to prevent blind commitment. ü§î System 2 (Slow): Triggers active reflection only when confidence drops below a specific threshold. Key Wins: ‚úÖ SOTA Performance: Outperforms ReAct & Reflexion on ALFWorld, WebShop, and the new DeepResearch Bench . ‚úÖ Efficiency: Prevents long, futile failure loops, making it more token-efficient than standard methods. ‚úÖ Plug-and-Play: No fine-tuning required. From \"Passive Diagnosis\" to \"Active Control\" ‚Äî make your agents reliable! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15703",
      "pdf_url": "https://arxiv.org/pdf/2601.15703",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15703",
      "scraped_at": "2026-01-26T02:01:51.336711"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
    "paper_url": "https://huggingface.co/papers/2601.16192",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
      "abstract": "360Anything lifts arbitrary perspective images and videos to seamless, gravity-aligned 360¬∞ panoramas, without using any camera or 3D information. Project page: https://360anything.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16192",
      "pdf_url": "https://arxiv.org/pdf/2601.16192",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16192",
      "scraped_at": "2026-01-26T02:01:53.349634"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Agentic Confidence Calibration",
    "paper_url": "https://huggingface.co/papers/2601.15778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Confidence Calibration",
      "abstract": "üéØ Don't let your Agents be \"Confidently Wrong\"! Traditional calibration works for static text, but Autonomous Agents fail differently‚Äîerrors compound over long trajectories. We introduce Holistic Trajectory Calibration (HTC) , a new paradigm to diagnose the entire execution process. Why it matters: üîç Process-Centric: Extracts rich features (Dynamics, Stability) from the agent's thinking process, not just the final output. üìà SOTA Calibration: Consistently outperforms baselines across 8 benchmarks (SimpleQA, Math500, etc.). üåç Generalization: We release the General Agent Calibrator (GAC) , which achieves the best zero-shot calibration on the challenging GAIA benchmark. Achieve Interpretability, Transferability, and Trust in your AI Agents. üõ°Ô∏è",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15778",
      "pdf_url": "https://arxiv.org/pdf/2601.15778",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15778",
      "scraped_at": "2026-01-26T02:01:55.177454"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15690",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "üó∫Ô∏è The 2026 Roadmap for Reliable AI: Making Uncertainty Actionable We are witnessing a paradigm shift in LLMs: Uncertainty is no longer just a passive score for diagnosis‚Äîit is evolving into an Active Control Signal for real-time decision-making. Our comprehensive survey covers this transformation across three frontiers: üß† Reasoning: Triggering self-correction & optimizing \"thinking budget\" (System 2). ü§ñ Agents: Determining when to use tools, ask for help, or stop generation. üéØ Alignment: Using uncertainty as an intrinsic reward to mitigate reward hacking in RLHF. If you are building Agents or Reasoning Models, this is the functional evolution you need to know. üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15690",
      "pdf_url": "https://arxiv.org/pdf/2601.15690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15690",
      "scraped_at": "2026-01-26T02:01:56.990537"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "paper_url": "https://huggingface.co/papers/2601.15549",
    "authors": [
      "Ryo Hachiuma",
      "Hideo Saito",
      "Ryo Fujii"
    ],
    "stars": "0",
    "details": {
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "abstract": "Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15549",
      "pdf_url": "https://arxiv.org/pdf/2601.15549",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15549",
      "scraped_at": "2026-01-26T02:01:58.859945"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "paper_url": "https://huggingface.co/papers/2601.16134",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LLM Prompt Evaluation for Educational Applications",
      "abstract": "Some of the observations founded are :- -- Prompt design matters as much as the model : The study shows that different prompt templates using the same LLM produce significantly different educational outcomes, proving prompt engineering is a critical lever in AI supported learning. -- Persona + Context Manager is the strongest combination : The Strategic Reading Coach prompt combining Persona and Context Manager patterns outperformed all others with 81‚Äì100% win probability, making it the most effective for follow up educational questions. -- Systematic prompt evaluation beats ad-hoc refinement : The tournament style evaluation using comparative judgment + Glicko2 ranking provides a reproducible, evidence based alternative to informal trial and error prompt tuning. -- Learning theory grounded prompts perform better : Prompts explicitly grounded in adult learning theory, self directed learning, and metacognition consistently generated higher quality educational dialogue than theory light designs -- Theoretical alignment alone is not enough : Some prompts rooted in strong learning theories (e.g. constructivism) still underperformed, highlighting that empirical evaluation is essential good theory must be paired with effective prompt patterns.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16134",
      "pdf_url": "https://arxiv.org/pdf/2601.16134",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16134",
      "scraped_at": "2026-01-26T02:02:00.656760"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "paper_url": "https://huggingface.co/papers/2601.16004",
    "authors": [
      "Cohaerence"
    ],
    "stars": "5",
    "details": {
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts. Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16004",
      "pdf_url": "https://arxiv.org/pdf/2601.16004",
      "github_links": [
        "https://github.com/christopher-altman/ibm-qml-kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16004",
      "scraped_at": "2026-01-26T02:02:02.464187"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "paper_url": "https://huggingface.co/papers/2601.15440",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "abstract": "In this work, we address the performance limitations often encountered in Python-based DLA simulations. By utilizing Numba for just-in-time compilation, we developed an implementation that achieves computational speeds comparable to legacy Fortran codes, offering a speedup over pure Python. We also validated the solver by analyzing the fractal dimension of the generated clusters (D‚âà1.71). We have released the code as a PyPI package named dla-ideal-solver to facilitate easier use and reproducibility. We hope this tool proves useful to those working in computational physics and complex systems, and we welcome any feedback from the community.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15440",
      "pdf_url": "https://arxiv.org/pdf/2601.15440",
      "github_links": [
        "https://github.com/sandyherho/dla-ideal-solver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15440",
      "scraped_at": "2026-01-26T02:02:05.381786"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "paper_url": "https://huggingface.co/papers/2601.08118",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
      "abstract": "The framework is open-sourced at https://github.com/SAP/mirrorbench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08118",
      "pdf_url": "https://arxiv.org/pdf/2601.08118",
      "github_links": [
        "https://github.com/SAP/mirrorbench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08118",
      "scraped_at": "2026-01-26T02:02:07.168371"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LongCat-Flash-Thinking-2601 Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.16725",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LongCat-Flash-Thinking-2601 Technical Report",
      "abstract": "this is informative.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16725",
      "pdf_url": "https://arxiv.org/pdf/2601.16725",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16725",
      "scraped_at": "2026-01-27T01:57:53.550979"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
    "paper_url": "https://huggingface.co/papers/2601.16746",
    "authors": [],
    "stars": "35",
    "details": {
      "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
      "abstract": "wcÔºånb",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16746",
      "pdf_url": "https://arxiv.org/pdf/2601.16746",
      "github_links": [
        "https://github.com/Ayanami1314/swe-pruner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16746",
      "scraped_at": "2026-01-27T01:57:55.428156"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "paper_url": "https://huggingface.co/papers/2601.14133",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
      "abstract": "TwinBrainVLA , a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14133",
      "pdf_url": "https://arxiv.org/pdf/2601.14133",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/TwinBrainVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14133",
      "scraped_at": "2026-01-27T01:57:57.312958"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
    "paper_url": "https://huggingface.co/papers/2601.16973",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
      "abstract": "We released VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents. We systematically study the brittleness of vision-language models in multi-step visual interaction, analyze how training choices shape behavior, and open-source the full benchmark, models, and trajectories. X: https://x.com/zwcolin/status/2015812327338287227 Project: https://visgym.github.io/ Paper: https://arxiv.org/abs/2601.16973 Code: https://github.com/visgym/VisGym Data & models: https://huggingface.co/VisGym",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16973",
      "pdf_url": "https://arxiv.org/pdf/2601.16973",
      "github_links": [
        "https://github.com/visgym/VIsGym",
        "https://github.com/visgym/VisGym"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16973",
      "scraped_at": "2026-01-27T01:57:59.237093"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "paper_url": "https://huggingface.co/papers/2601.16296",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Plenoptic Video Generation (2026) Spatia: Video Generation with Updatable Spatial Memory (2025) StoryMem: Multi-shot Long Video Storytelling with Memory (2025) Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) BulletTime: Decoupled Control of Time and Camera Pose for Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16296",
      "pdf_url": "https://arxiv.org/pdf/2601.16296",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16296",
      "scraped_at": "2026-01-27T01:58:01.102913"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "paper_url": "https://huggingface.co/papers/2601.15808",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "abstract": "The scaling law of verification in deep research agent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15808",
      "pdf_url": "https://arxiv.org/pdf/2601.15808",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15808",
      "scraped_at": "2026-01-27T01:58:02.949107"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "paper_url": "https://huggingface.co/papers/2601.14243",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
      "abstract": "This paper analyzes why existing FP8 reinforcement learning methods fail. It proposes Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization by eliminating training-inference mismatch.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14243",
      "pdf_url": "https://arxiv.org/pdf/2601.14243",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14243",
      "scraped_at": "2026-01-27T01:58:04.828615"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.16515",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
      "abstract": "In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72√ó inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16515",
      "pdf_url": "https://arxiv.org/pdf/2601.16515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16515",
      "scraped_at": "2026-01-27T01:58:06.714348"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "GameTalk: Training LLMs for Strategic Conversation",
    "paper_url": "https://huggingface.co/papers/2601.16276",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GameTalk: Training LLMs for Strategic Conversation",
      "abstract": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16276",
      "pdf_url": "https://arxiv.org/pdf/2601.16276",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16276",
      "scraped_at": "2026-01-27T01:58:08.550432"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
    "paper_url": "https://huggingface.co/papers/2601.07251",
    "authors": [
      "Jianwen Sun",
      "Yukang Feng",
      "Yibin Wang",
      "Chuanhao Li",
      "Zizhen Li"
    ],
    "stars": "18",
    "details": {
      "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
      "abstract": "https://github.com/leroy9472/MeepleLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07251",
      "pdf_url": "https://arxiv.org/pdf/2601.07251",
      "github_links": [
        "https://github.com/leroy9472/MeepleLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07251",
      "scraped_at": "2026-01-27T01:58:10.584314"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
    "paper_url": "https://huggingface.co/papers/2601.16344",
    "authors": [
      "Yongchan Kwon",
      "Federico Bianchi",
      "Harper Hua",
      "Junlin Wang",
      "Fan Nie"
    ],
    "stars": "0",
    "details": {
      "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems (2026) SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence (2025) AInsteinBench: Benchmarking Coding Agents on Scientific Repositories (2025) DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows (2025) HeurekaBench: A Benchmarking Framework for AI Co-scientist (2026) LongDA: Benchmarking LLM Agents for Long-Document Data Analysis (2026) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16344",
      "pdf_url": "https://arxiv.org/pdf/2601.16344",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16344",
      "scraped_at": "2026-01-27T01:58:12.422648"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
    "paper_url": "https://huggingface.co/papers/2601.16018",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "abstract": "Mecellem Models propose Turkish legal-domain encoders and decoders trained from scratch and via continual pre-training. ModernBERT-based encoders (112.7B tokens) achieve top-3 Turkish retrieval results with high production efficiency, while Qwen3-based decoders show 36.2% perplexity reduction on legal text. Models and datasets are released via Hugging Face to support reproducible and cost-effective legal NLP for Turkish and other low-resource languages.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16018",
      "pdf_url": "https://arxiv.org/pdf/2601.16018",
      "github_links": [
        "https://github.com/newmindai/mecellem-models"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16018",
      "scraped_at": "2026-01-27T01:58:14.317360"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
    "paper_url": "https://huggingface.co/papers/2601.16443",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Failure to Mastery: Generating Hard Samples for Tool-use Agents (2026) Training Versatile Coding Agents in Synthetic Environments (2025) One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents (2025) Dr. Zero: Self-Evolving Search Agents without Training Data (2026) NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents (2025) SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning (2025) AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16443",
      "pdf_url": "https://arxiv.org/pdf/2601.16443",
      "github_links": [
        "https://github.com/kanishkg/endless-terminals"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16443",
      "scraped_at": "2026-01-27T01:58:16.138892"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
    "paper_url": "https://huggingface.co/papers/2601.13606",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
      "abstract": "High-quality synthetic Chart data and strong Chart reasoning model.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13606",
      "pdf_url": "https://arxiv.org/pdf/2601.13606",
      "github_links": [
        "https://github.com/starriver030515/ChartVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13606",
      "scraped_at": "2026-01-27T01:58:18.182354"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.11258",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
      "abstract": "Let Your LLMs Use New Knowledge with ‚ÄúPaST‚Äù Skills Paper: https://arxiv.org/abs/2601.11258 Blog: https://past-blog.notion.site",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11258",
      "pdf_url": "https://arxiv.org/pdf/2601.11258",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11258",
      "scraped_at": "2026-01-27T01:58:20.042830"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
    "paper_url": "https://huggingface.co/papers/2601.15715",
    "authors": [
      "Yi R Fung",
      "Zongwei Lyu",
      "Zhitao He"
    ],
    "stars": "0",
    "details": {
      "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance (2026) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) CogDoc: Towards Unified thinking in Documents (2025) Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models (2025) Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR (2026) REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation (2025) R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15715",
      "pdf_url": "https://arxiv.org/pdf/2601.15715",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15715",
      "scraped_at": "2026-01-27T01:58:21.862502"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
    "paper_url": "https://huggingface.co/papers/2601.13118",
    "authors": [
      "Gabriele Bavota",
      "Rosalia Tufano",
      "Fiorella Zampetti",
      "Alessandro Midolo",
      "Devy1"
    ],
    "stars": "0",
    "details": {
      "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
      "abstract": ".",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13118",
      "pdf_url": "https://arxiv.org/pdf/2601.13118",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13118",
      "scraped_at": "2026-01-27T01:58:23.760413"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
    "paper_url": "https://huggingface.co/papers/2601.16451",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
      "abstract": "üöÄ VISTA-PATH is introduced as the first interactive segmentation foundation model for pathology. It advances computational pathology workflows by enabling more accurate, interpretable, and human-guided quantitative measurements. Key highlights include: 1Ô∏è‚É£ Large-scale training: Trained on over 1.6M image-mask-text pairs 2Ô∏è‚É£ State-of-the-art performance: Accurate segmentation across both in-distribution and out-of-distribution tissues 3Ô∏è‚É£ Interactive refinement: Outputs can be efficiently refined using bounding-box prompts with only a few active-learning steps 4Ô∏è‚É£ Deployment-ready: Fully integrated into https://tissuelab.org and available for immediate use üìÑ Read the arXiv preprint: https://arxiv.org/abs/2601.16451",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16451",
      "pdf_url": "https://arxiv.org/pdf/2601.16451",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16451",
      "scraped_at": "2026-01-27T01:58:25.717710"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs",
    "paper_url": "https://huggingface.co/papers/2601.17058",
    "authors": [],
    "stars": "644",
    "details": {
      "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs",
      "abstract": "Please refer to our repository for more details: https://github.com/weAIDB/awesome-data-llm .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17058",
      "pdf_url": "https://arxiv.org/pdf/2601.17058",
      "github_links": [
        "https://github.com/weAIDB/awesome-data-llm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17058",
      "scraped_at": "2026-01-28T01:53:53.491369"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
    "paper_url": "https://huggingface.co/papers/2601.18418",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
      "abstract": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories.  While post-training methods have become the de facto approach for code agents, agentic mid-training -mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning.  A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development.  To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale.  Central to our approach is agent-native data -supervision comprising two complementary types of trajectories: contextually-native trajectories that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and environmentally-native trajectories collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity.  We verify the model's agentic capabilities on SWE-Bench Verified .  We demonstrate our superiority over the previous open software engineering mid-training recipe Kimi-Dev under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B).  Besides relative advantage, our best performing 32B and 72B models achieve 56.1% and 58.5% resolution rates, respectively, which are state-of-the-art among open training recipes using agentic scaffolds under their model sizes, despite starting from non-coder Qwen2.5-Base base models.  Beyond these agentic capabilities, we also observe performance gains on general code generation and scientific benchmarks.  We plan to open-source a significant portion of our datasets, recipes, and model checkpoints-resources representing substantial computational investment typically unavailable to the broader community-to facilitate further research in this underexplored paradigm.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18418",
      "pdf_url": "https://arxiv.org/pdf/2601.18418",
      "github_links": [
        "https://github.com/GAIR-NLP/daVinci-Dev"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18418",
      "scraped_at": "2026-01-28T01:53:55.426936"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.17737",
    "authors": [],
    "stars": "228",
    "details": {
      "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
      "abstract": "Convert dialogue to script for video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17737",
      "pdf_url": "https://arxiv.org/pdf/2601.17737",
      "github_links": [
        "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17737",
      "scraped_at": "2026-01-28T01:53:57.279708"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility",
    "paper_url": "https://huggingface.co/papers/2601.17027",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility",
      "abstract": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17027",
      "pdf_url": "https://arxiv.org/pdf/2601.17027",
      "github_links": [
        "https://github.com/SciGenBench/SciGenBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17027",
      "scraped_at": "2026-01-28T01:53:59.134985"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
    "paper_url": "https://huggingface.co/papers/2601.17367",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
      "abstract": "Elastic Attention enables models to achieve both strong performance and efficient inference by dynamically allocating computation modes (Full Attention or Sparse Attention) to each attention head through our designed Attention Router, adapting sparsity ratios based on input characteristics. Code & Training data: https://github.com/LCM-Lab/Elastic-Attention Model Collection: https://modelscope.cn/collections/LCM_group/Elastic-Attention",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17367",
      "pdf_url": "https://arxiv.org/pdf/2601.17367",
      "github_links": [
        "https://github.com/LCM-Lab/Elastic-Attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17367",
      "scraped_at": "2026-01-28T01:54:00.930545"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
    "paper_url": "https://huggingface.co/papers/2601.17124",
    "authors": [],
    "stars": "59",
    "details": {
      "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
      "abstract": "AR or Diffusion? It‚Äôs been hard to judge because different tokenizers (VQ vs. VAE) Enter iFSQ with just 1 line of code! We found: (1) AR wins on efficiency, but Diffusion hits a higher quality ceiling. (2) The sweet spot for representations is ~4 bits. We brought REPA to LlamaGen and solved the missing piece: Where to align? It turns out there‚Äôs no fixed layer, but a Golden Ratio! We found the optimal alignment depth is consistently 1/3 of total layers for both AR & Diffusion.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17124",
      "pdf_url": "https://arxiv.org/pdf/2601.17124",
      "github_links": [
        "https://github.com/Tencent-Hunyuan/iFSQ"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17124",
      "scraped_at": "2026-01-28T01:54:02.801537"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "paper_url": "https://huggingface.co/papers/2601.18778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
      "abstract": "Check out our blog post: https://ssundaram21.github.io/soar/ !",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18778",
      "pdf_url": "https://arxiv.org/pdf/2601.18778",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18778",
      "scraped_at": "2026-01-28T01:54:04.625178"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Self-Refining Video Sampling",
    "paper_url": "https://huggingface.co/papers/2601.18577",
    "authors": [
      "Sangwon Jang",
      "jaehong31",
      "sainx",
      "harry9704",
      "taekyungki"
    ],
    "stars": "43",
    "details": {
      "title": "Self-Refining Video Sampling",
      "abstract": "[TL;DR] We present self-refining video sampling method that reuses a pre-trained video generator as a denoising autoencoder to iteratively refine latents. With ~50% additional NFEs, it improves physical realism (e.g., motion coherence and physics alignment) without any external verifier, training, or dataset.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18577",
      "pdf_url": "https://arxiv.org/pdf/2601.18577",
      "github_links": [
        "https://github.com/agwmon/self-refine-video"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18577",
      "scraped_at": "2026-01-28T01:54:06.485659"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "VIBEVOICE-ASR Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.18184",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VIBEVOICE-ASR Technical Report",
      "abstract": "VibeVoice-ASR is a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18184",
      "pdf_url": "https://arxiv.org/pdf/2601.18184",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18184",
      "scraped_at": "2026-01-28T01:54:08.341973"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
    "paper_url": "https://huggingface.co/papers/2601.18137",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
      "abstract": "DeepPlanning ‚Äî a new benchmark for long-horizon agent planning in real-world scenarios!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18137",
      "pdf_url": "https://arxiv.org/pdf/2601.18137",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18137",
      "scraped_at": "2026-01-28T01:54:10.173169"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
    "paper_url": "https://huggingface.co/papers/2601.15849",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
      "abstract": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query‚Äìtable mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms the existing baselines with an average R@1 improvement of 16.54%. Under cross-domain evaluation, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15849",
      "pdf_url": "https://arxiv.org/pdf/2601.15849",
      "github_links": [
        "https://github.com/yumeow0122/CGPT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15849",
      "scraped_at": "2026-01-28T01:54:11.981623"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
    "paper_url": "https://huggingface.co/papers/2601.15860",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
      "abstract": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query‚Äìtable alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15860",
      "pdf_url": "https://arxiv.org/pdf/2601.15860",
      "github_links": [
        "https://github.com/adsl135789/STAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15860",
      "scraped_at": "2026-01-28T01:54:13.819967"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.18217",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
      "abstract": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18217",
      "pdf_url": "https://arxiv.org/pdf/2601.18217",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18217",
      "scraped_at": "2026-01-28T01:54:15.835567"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation",
    "paper_url": "https://huggingface.co/papers/2601.17761",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation",
      "abstract": "AR-Omni is a single-decoder, single-token-stream autoregressive any-to-any model. It unifies multimodal generation (text, images, and speech) as standard next-token prediction over interleaved sequences. It improves training and inference with task-aware loss reweighting, token-level perceptual alignment for image tokens, and a finite-state machine for decoding. Project page: https://modalitydance.github.io/AR-Omni",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17761",
      "pdf_url": "https://arxiv.org/pdf/2601.17761",
      "github_links": [
        "https://github.com/ModalityDance/AR-Omni"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17761",
      "scraped_at": "2026-01-28T01:54:17.669017"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
    "paper_url": "https://huggingface.co/papers/2601.18744",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18744",
      "pdf_url": "https://arxiv.org/pdf/2601.18744",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18744",
      "scraped_at": "2026-01-28T01:54:19.456320"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Agentic Very Long Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.18157",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Very Long Video Understanding",
      "abstract": "EGAgent uses entity scene graphs and structured search over long, multimodal video streams to enable cross-modal, temporally coherent reasoning for egocentric video understanding.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18157",
      "pdf_url": "https://arxiv.org/pdf/2601.18157",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18157",
      "scraped_at": "2026-01-28T01:54:21.308602"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal",
    "paper_url": "https://huggingface.co/papers/2601.18081",
    "authors": [
      "Jingjun Xu",
      "Yingjie Yu",
      "jiaxuanYou",
      "HakHan"
    ],
    "stars": "3",
    "details": {
      "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal",
      "abstract": "DRPG - An Agentic Framework for Academic Rebuttal",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18081",
      "pdf_url": "https://arxiv.org/pdf/2601.18081",
      "github_links": [
        "https://github.com/ulab-uiuc/DRPG-RebuttalAgent/tree/master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18081",
      "scraped_at": "2026-01-28T01:54:23.140140"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance",
    "paper_url": "https://huggingface.co/papers/2601.16207",
    "authors": [
      "yjang43",
      "cfmata",
      "jjh6297",
      "kahnchana",
      "jongwoopark7978"
    ],
    "stars": "0",
    "details": {
      "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance",
      "abstract": "IVRA is a training-free, inference-time drop-in that restores spatial structure in VLA models by injecting encoder affinity signals into selected LLM layers (no retraining, no extra parameters, ~3% latency). It generalizes across LLaRA, OpenVLA, and FLOWER, improving both 2D and 3D manipulation: +4.2% on VIMA (Novel Object) over LLaRA in a low-data regime, and consistent LIBERO gains over OpenVLA (76.5%‚Üí77.6%) and FLOWER even near saturation (96.3%‚Üí97.1%). Paper: https://arxiv.org/abs/2601.16207 Project: https://jongwoopark7978.github.io/IVRA",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16207",
      "pdf_url": "https://arxiv.org/pdf/2601.16207",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16207",
      "scraped_at": "2026-01-28T01:54:25.078185"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
    "paper_url": "https://huggingface.co/papers/2601.18202",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
      "abstract": "SAGE automatically generates difficulty-controlled deep-search QA pairs via an iterative agent-feedback loop, yielding higher-quality training data that improves deep search agent performance and adaptability.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18202",
      "pdf_url": "https://arxiv.org/pdf/2601.18202",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18202",
      "scraped_at": "2026-01-28T01:54:26.946872"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "SkyReels-V3 Technique Report",
    "paper_url": "https://huggingface.co/papers/2601.17323",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SkyReels-V3 Technique Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API KlingAvatar 2.0 Technical Report (2025) YingVideo-MV: Music-Driven Multi-Stage Video Generation (2025) ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions (2025) Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model (2025) Scaling Zero-Shot Reference-to-Video Generation (2025) DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17323",
      "pdf_url": "https://arxiv.org/pdf/2601.17323",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17323",
      "scraped_at": "2026-01-28T01:54:28.793576"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.17111",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts",
      "abstract": "Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17111",
      "pdf_url": "https://arxiv.org/pdf/2601.17111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17111",
      "scraped_at": "2026-01-28T01:54:30.576547"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
    "paper_url": "https://huggingface.co/papers/2601.18731",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
      "abstract": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18731",
      "pdf_url": "https://arxiv.org/pdf/2601.18731",
      "github_links": [
        "https://github.com/ModalityDance/MRM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18731",
      "scraped_at": "2026-01-28T01:54:32.368463"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions",
    "paper_url": "https://huggingface.co/papers/2601.17640",
    "authors": [
      "Shrikanth Narayanan",
      "Catherine Lord",
      "Somer Bishop",
      "Anfeng Xu",
      "tiantiaf"
    ],
    "stars": "0",
    "details": {
      "title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions",
      "abstract": "Accurate transcription and speaker diarization of child‚Äìadult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and automatic speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder‚Äìdecoder architecture to jointly model ASR and child‚Äìadult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whispersmall and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child‚Äìadult interactions at scale. The code and model weights are publicly available: https://github.com/usc-sail/joint-asr-diarization-child-adult",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17640",
      "pdf_url": "https://arxiv.org/pdf/2601.17640",
      "github_links": [
        "https://github.com/usc-sail/joint-asr-diarization-child-adult"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17640",
      "scraped_at": "2026-01-28T01:54:34.145624"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
    "paper_url": "https://huggingface.co/papers/2601.17067",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
      "abstract": "While large-scale video generation models show signs of emergent physical coherence, they remain distinct from true world models. A critical gap persists between modern \"stateless\" video architectures and the \"state-centric\" requirements of classic control theory. This survey bridges that divide. We propose a new taxonomy built on State Construction (implicit context vs. explicit latent compression) and Dynamics Modeling . We argue that the field must transition its evaluation standards from simple visual fidelity to functional benchmarks‚Äîspecifically testing for physical persistence and causal reasoning. Finally, we outline the path forward: solving data-driven memory for persistence and integrating reasoning priors for causality. These steps are essential to transition the field from merely generating visually plausible videos to building robust, general-purpose world simulators.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17067",
      "pdf_url": "https://arxiv.org/pdf/2601.17067",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17067",
      "scraped_at": "2026-01-28T01:54:35.970093"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.13599",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion",
      "abstract": "One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a 'draft-then-refine' framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models. Project page: https://noah-dllm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13599",
      "pdf_url": "https://arxiv.org/pdf/2601.13599",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13599",
      "scraped_at": "2026-01-28T01:54:37.780467"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing",
    "paper_url": "https://huggingface.co/papers/2601.18759",
    "authors": [
      "April Yi Wang",
      "Mustafa Doga Dogan",
      "Xiaotian Su",
      "Junling Wang",
      "HenryLhy"
    ],
    "stars": "0",
    "details": {
      "title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing",
      "abstract": "UI Remix enables interactive, example-driven design for mobile interfaces using multimodal retrieval-augmented generation to search, adapt, and remix interface components with source transparency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18759",
      "pdf_url": "https://arxiv.org/pdf/2601.18759",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18759",
      "scraped_at": "2026-01-28T01:54:39.593912"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Masked Depth Modeling for Spatial Perception",
    "paper_url": "https://huggingface.co/papers/2601.17895",
    "authors": [],
    "stars": "252",
    "details": {
      "title": "Masked Depth Modeling for Spatial Perception",
      "abstract": "Website: technology.robbyant.com/lingbot-depth Code: https://github.com/Robbyant/lingbot-depth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17895",
      "pdf_url": "https://arxiv.org/pdf/2601.17895",
      "github_links": [
        "https://github.com/Robbyant/lingbot-depth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17895",
      "scraped_at": "2026-01-28T01:54:41.529300"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues",
    "paper_url": "https://huggingface.co/papers/2601.17277",
    "authors": [
      "afaji",
      "gentaiscool",
      "faridlazuarda",
      "hanifmz0711",
      "rifqifarhansyah"
    ],
    "stars": "0",
    "details": {
      "title": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues",
      "abstract": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17277",
      "pdf_url": "https://arxiv.org/pdf/2601.17277",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17277",
      "scraped_at": "2026-01-28T01:54:43.375823"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control",
    "paper_url": "https://huggingface.co/papers/2601.15015",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control",
      "abstract": "FluidGym: Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control There is enormous potential for reinforcement learning and other data-driven control paradigms for controlling large-scale fluid flows. But RL research on such systems is often hindered by a complex and brittle software pipeline consisting of external solvers and multiple code bases, making this exciting field inaccessible for many RL researchers. To tackle this challenge, we have developed a standalone, fully differentiable, plug-and-play benchmark for RL in active flow control, implemented in a single PyTorch codebase via PICT, without external solver dependencies. Our FluidGym comes with a collection of standardized environment configurations spanning diverse 3D and multi-agent control tasks. We perform an extensive experimental study with multiple seeds, randomized initial conditions, and separate train/validate/test sets. We compare the default implementations of the two most popular algorithms, PPO and SAC, in the single and multi-agent settings, and also investigate the potential for transfer learning. We hope that this may be of interest to a large number of reinforcement learning researchers who are keen on assessing the most recent trends in basic RL research on a new set of challenging tasks, but otherwise find it difficult to enter the field of fluid mechanics Paper: https://arxiv.org/abs/2601.15015v1 GitHub: https://github.com/safe-autonomous-systems/fluidgym",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15015v1",
      "pdf_url": "https://arxiv.org/pdf/2601.15015",
      "github_links": [
        "https://github.com/safe-autonomous-systems/fluidgym"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15015",
      "scraped_at": "2026-01-28T01:54:45.411853"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.14127",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
      "abstract": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2{,}676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code is available at https://github.com/thu-coai/MIR-SafetyBench .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14127",
      "pdf_url": "https://arxiv.org/pdf/2601.14127",
      "github_links": [
        "https://github.com/thu-coai/MIR-SafetyBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14127",
      "scraped_at": "2026-01-28T01:54:47.283184"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.12042",
    "authors": [
      "Guanhong Tao",
      "Yanjun Zhang",
      "Leo Yu Zhang",
      "Xiaomei Zhang",
      "plll123"
    ],
    "stars": "0",
    "details": {
      "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models",
      "abstract": "Visual token compression is widely used to accelerate inference in Large Vision‚ÄìLanguage Models (LVLMs), enabling deployment in latency- and resource-constrained settings. This paper reveals that such compression introduces a previously overlooked security risk: models that are robust under full-token inference can become highly vulnerable once compression is enabled. We show that this vulnerability is compression-specific and stems from the instability of token-importance ranking, where small, imperceptible perturbations can cause task-critical visual tokens to be discarded. To systematically study this phenomenon, we propose Compression-Aware Attack (CAA), which explicitly targets the token selection mechanism and induces failures only under compressed inference. Extensive experiments across multiple LVLMs, datasets, and compression methods demonstrate a severe efficiency‚Äìsecurity trade-off, highlighting the need for robustness-aware compression design in practical LVLM deployments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12042",
      "pdf_url": "https://arxiv.org/pdf/2601.12042",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12042",
      "scraped_at": "2026-01-28T01:54:49.053067"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
    "paper_url": "https://huggingface.co/papers/2601.18790",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
      "abstract": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18790",
      "pdf_url": "https://arxiv.org/pdf/2601.18790",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18790",
      "scraped_at": "2026-01-28T01:54:50.846530"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.18753",
    "authors": [
      "Junhong Lin",
      "zhoudw",
      "liangshi",
      "yanyujun",
      "xyzeng2000"
    ],
    "stars": "0",
    "details": {
      "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
      "abstract": "üöÄ HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs Accepted at ICLR 2026 In this work, we introduce HalluGuard , a unified, theory-driven framework for hallucination detection in large language models , accepted at ICLR 2026 . Rather than treating hallucination as a single failure mode, HalluGuard explicitly decomposes hallucinations into data-driven and reasoning-driven components‚Äîand detects both at inference time , with no retraining , no labels , and no external references . üòÜ Key Takeaways üß† Two Sources of Hallucination LLM hallucinations arise from two fundamentally different mechanisms: Data-driven hallucinations Errors rooted in biased, incomplete, or mismatched knowledge acquired during pretraining or finetuning. Reasoning-driven hallucinations Errors caused by instability and error amplification during multi-step autoregressive decoding. Most existing detectors focus on only one of these. HalluGuard shows that real hallucinations often emerge from their interaction and evolve across decoding steps . üìê Hallucination Risk Bound (Theory) We introduce a Hallucination Risk Bound , which formally decomposes total hallucination risk into: a representation bias term (training-time mismatch), and a decoding instability term (inference-time amplification). The analysis reveals a key insight: hallucinations originate from semantic approximation gaps and are then exponentially amplified during long-horizon generation . This provides a principled explanation of how hallucinations emerge and evolve in LLMs. üîç HalluGuard Score (Method) Building on this theory, we propose HalluGuard , a lightweight NTK-based hallucination score : H A L L U G U A R D ( u h ) = det ‚Å° ( K ) + log ‚Å° œÉ max ‚Å° ‚àí log ‚Å° ‚Å£ ( Œ∫ ( K ) 2 ) . \\mathrm{HALLUGUARD}(u_h)\n=\n\\det(K)\n+\n\\log \\sigma_{\\max}\n-\n\\log\\!\\big(\\kappa(K)^2\\big). HALLUGUARD ( u h ‚Äã ) = det ( K ) + lo g œÉ m a x ‚Äã ‚àí lo g ( Œ∫ ( K ) 2 ) . Higher HalluGuard score ‚áí lower hallucination risk . üìä Strong Empirical Results We evaluate HalluGuard across: 10 benchmarks (QA, math reasoning, instruction following), 11 competitive baselines , and 9 LLM backbones (from GPT-2 to 70B-scale models). Results: üèÜ Consistent state-of-the-art AUROC / AUPRC across all task families üîç Especially strong gains on multi-step reasoning benchmarks (MATH-500, BBH) üß© Robust detection of fine-grained semantic hallucinations (PAWS), even when surface forms are nearly identical üß≠ Beyond Detection: Test-Time Guidance HalluGuard can also be used to guide test-time inference , significantly improving reasoning accuracy by steering generation away from unstable trajectories‚Äî without modifying or retraining the model . üîë Takeaway HalluGuard (ICLR 2026) provides: a theoretical lens for understanding how hallucinations emerge and evolve, and a practical, plug-and-play detector for modern LLMs. It bridges representation geometry and decoding dynamics, offering a unified foundation for reliable reasoning and uncertainty-aware inference . Feedback and discussion are very welcome üôå",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18753",
      "pdf_url": "https://arxiv.org/pdf/2601.18753",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18753",
      "scraped_at": "2026-01-28T01:54:52.678183"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents",
    "paper_url": "https://huggingface.co/papers/2601.18130",
    "authors": [
      "Yiming Song",
      "Han Wu",
      "larryle",
      "zhiyuanyou",
      "Jize1"
    ],
    "stars": "0",
    "details": {
      "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents",
      "abstract": "An efficient mixture-of-agents framework with dynamic routing.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18130",
      "pdf_url": "https://arxiv.org/pdf/2601.18130",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18130",
      "scraped_at": "2026-01-28T01:54:54.438640"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
    "paper_url": "https://huggingface.co/papers/2601.17958",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
      "abstract": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17958",
      "pdf_url": "https://arxiv.org/pdf/2601.17958",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17958",
      "scraped_at": "2026-01-28T01:54:56.218099"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests",
    "paper_url": "https://huggingface.co/papers/2601.17617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests",
      "abstract": "This paper presents a large-scale behavioral analysis of 14.44M agentic search interactions, characterizing how autonomous agents organize sessions by intent, execute query reformulations, and reuse retrieved evidence across multi-step trajectories.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17617",
      "pdf_url": "https://arxiv.org/pdf/2601.17617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17617",
      "scraped_at": "2026-01-28T01:54:57.983894"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing",
    "paper_url": "https://huggingface.co/papers/2601.14103",
    "authors": [
      "Wei Ji",
      "Jiayin Zhu",
      "Qiyuan He",
      "Yicong Li",
      "xiaolul2"
    ],
    "stars": "11",
    "details": {
      "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing",
      "abstract": "In this work, we propose Interp3D, a training-free approach that instantiates the progressive alignment principle based on generative priors for textured 3D morphing.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14103",
      "pdf_url": "https://arxiv.org/pdf/2601.14103",
      "github_links": [
        "https://github.com/xiaolul2/Interp3D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14103",
      "scraped_at": "2026-01-28T01:54:59.782980"
    },
    "scraped_date": "2026-01-28"
  },
  {
    "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
    "paper_url": "https://huggingface.co/papers/2601.18491",
    "authors": [],
    "stars": "178",
    "details": {
      "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
      "abstract": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18491",
      "pdf_url": "https://arxiv.org/pdf/2601.18491",
      "github_links": [
        "https://github.com/AI45Lab/AgentDoG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18491",
      "scraped_at": "2026-01-29T02:08:04.394750"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.18631",
    "authors": [],
    "stars": "44",
    "details": {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "abstract": "For more information, please visit our homepage: https://adareasoner.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18631",
      "pdf_url": "https://arxiv.org/pdf/2601.18631",
      "github_links": [
        "https://github.com/ssmisya/AdaReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18631",
      "scraped_at": "2026-01-29T02:08:06.400581"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "A Pragmatic VLA Foundation Model",
    "paper_url": "https://huggingface.co/papers/2601.18692",
    "authors": [],
    "stars": "245",
    "details": {
      "title": "A Pragmatic VLA Foundation Model",
      "abstract": "A Pragmatic VLA Foundation Model",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18692",
      "pdf_url": "https://arxiv.org/pdf/2601.18692",
      "github_links": [
        "https://github.com/robbyant/lingbot-vla"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18692",
      "scraped_at": "2026-01-29T02:08:08.410068"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "paper_url": "https://huggingface.co/papers/2601.19834",
    "authors": [],
    "stars": "36",
    "details": {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "abstract": "TL;DR : From a world-model perspective, we study when and how visual generation enabled by unified multimodal models (UMMs) benefits reasoning. Humans construct mental models of the world, representing information and knowledge through two complementary channels‚Äîverbal and visual‚Äîto support reasoning, planning, and decision-making. In contrast, recent advances in large language models (LLMs) and vision‚Äìlanguage models (VLMs) largely rely on verbal chain-of-thought reasoning, leveraging primarily symbolic and linguistic world knowledge. Unified multimodal models (UMMs) open a new paradigm by using visual generation for visual world modeling, advancing more human-like reasoning on tasks grounded in the physical world. In this work: We formalize the atomic capabilities of world models and world model-based chain-of-thought reasoning. We highlight the richer informativeness and complementary prior knowledge afforded by visual world modeling, leading to our visual superiority hypothesis for tasks grounded in the physical world. We identify and design tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval . Through controlled experiments on BAGEL, we show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, strongly supporting our insights. For more details, check our project page or paper .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19834",
      "pdf_url": "https://arxiv.org/pdf/2601.19834",
      "github_links": [
        "https://github.com/thuml/Reasoning-Visual-World"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19834",
      "scraped_at": "2026-01-29T02:08:10.368411"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision",
    "paper_url": "https://huggingface.co/papers/2601.19798",
    "authors": [],
    "stars": "35",
    "details": {
      "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision",
      "abstract": "Performs on par with Qwen3-VL-8B-Instruct on visual based tasks despite being half the size.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19798",
      "pdf_url": "https://arxiv.org/pdf/2601.19798",
      "github_links": [
        "https://github.com/TencentCloudADP/youtu-vl"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19798",
      "scraped_at": "2026-01-29T02:08:12.467514"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
    "paper_url": "https://huggingface.co/papers/2601.17645",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "abstract": "Demo: https://avmemeexam.github.io/public Dataset: https://huggingface.co/datasets/naplab/AVMeme-Exam",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17645",
      "pdf_url": "https://arxiv.org/pdf/2601.17645",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17645",
      "scraped_at": "2026-01-29T02:08:14.655068"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text",
    "paper_url": "https://huggingface.co/papers/2601.09150",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text",
      "abstract": "https://github.com/HerzogFL/World-Craft Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a \"dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (e.g., environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09150",
      "pdf_url": "https://arxiv.org/pdf/2601.09150",
      "github_links": [
        "https://github.com/HerzogFL/World-Craft"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09150",
      "scraped_at": "2026-01-29T02:08:16.735410"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
    "paper_url": "https://huggingface.co/papers/2601.19895",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
      "abstract": "üöÄ Only a few lines of code changed, and we pushed deep LLMs to the next level. üìà With Keel, we scaled LLM to 1000 layers. And the deeper we go, the more Keel pulls ahead of standard Pre-LN Transformers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19895",
      "pdf_url": "https://arxiv.org/pdf/2601.19895",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19895",
      "scraped_at": "2026-01-29T02:08:18.763802"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
    "paper_url": "https://huggingface.co/papers/2601.18292",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
      "abstract": "TriPlay-RL: Solving the Safety vs. Reasoning Trade-off with 3-Way Self-Play Really interesting take on automated safety alignment. We've seen plenty of \"Red Team vs. Blue Team\" setups, but they often suffer from two issues: the Red Team eventually finds one exploit and spams it (mode collapse), or the Blue Team becomes \"safe\" but loses its general reasoning abilities (the alignment tax). This paper introduces TriPlay-RL, which adds a third active player: an Evolving Evaluator. Instead of using a fixed reward model or human labels, all three roles (Attacker, Defender, Evaluator) co-evolve in a closed loop. Why this stands out: No Alignment Tax: The Defender improved safety performance by 10-30% without degrading general reasoning benchmarks. This is usually the hardest part of safety training. Diverse Attacks: They use diversity penalties to stop the Attacker from collapsing into a single pattern, keeping the pressure on the Defender high. Near-Zero Data: It works with minimal manual annotation, making it scalable. It feels like a step closer to an \"AlphaZero\" moment for safety alignment. Highly recommend checking the ablation studies on entropy collapse!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18292",
      "pdf_url": "https://arxiv.org/pdf/2601.18292",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18292",
      "scraped_at": "2026-01-29T02:08:20.927968"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.18116",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
      "abstract": "With the rise of 1M+ context windows in Gemini and Claude, the biggest debate in AI right now is: \"Do we still need RAG, or should we just dump everything into the prompt?\" Today's pick, FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning, provides a compelling answer: Long context is great, but structured navigation is better (and much cheaper). What makes FABLE stand out from the sea of RAG papers is that it reimagines the LLM's role. It stops treating the LLM as just a \"reader\" and turns it into a \"librarian\": Forests > Flat Chunks: Instead of the traditional \"chunk + vector search\" which loses global context, FABLE uses LLMs to pre-build Hierarchical Knowledge Forests. This allows the system to actively \"zoom in\" for granular details or \"zoom out\" for high-level synthesis depending on the query. The Bi-Path Innovation: It doesn't rely on just one retrieval method. It runs a Bi-Path Strategy: one path uses LLM reasoning to navigate the document structure (symbolic/logic), and the other uses vector propagation (semantic similarity). This hybrid approach captures subtle connections that vector databases often miss. Insane Efficiency: The results are the real hook here. FABLE achieves the same reasoning accuracy as full-context inference (517k tokens) while using only ~31k tokens. That is a 94% reduction in compute/cost without sacrificing quality. Why read this? If you are working on Agents or Multi-Document QA, you know that \"Lost-in-the-middle\" is a real pain. FABLE proves that giving LLMs a map (the semantic forest) is more effective than just giving them a bigger backpack (context window). It beats graph-based baselines like HippoRAG and is a refreshing take on how to scale reasoning without scaling costs. A must-read for anyone trying to optimize RAG pipelines for complex tasks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18116",
      "pdf_url": "https://arxiv.org/pdf/2601.18116",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18116",
      "scraped_at": "2026-01-29T02:08:22.808809"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
    "paper_url": "https://huggingface.co/papers/2601.19228",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
      "abstract": "Project Page: https://simpleseg.github.io/ Github: https://github.com/songtianhui/SimpleSeg HuggingFace: https://huggingface.co/collections/sthui/simpleseg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19228",
      "pdf_url": "https://arxiv.org/pdf/2601.19228",
      "github_links": [
        "https://github.com/songtianhui/SimpleSeg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19228",
      "scraped_at": "2026-01-29T02:08:24.705429"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection",
    "paper_url": "https://huggingface.co/papers/2601.19375",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection",
      "abstract": "We introduce Selective Steering, a principled norm-preserving activation steering method that enables stable, continuous control of LLM behavior while significantly improving adversarial attack effectiveness without sacrificing model capabilities. Page: https://knoveleng.github.io/steering/ Code: https://github.com/knoveleng/steering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19375",
      "pdf_url": "https://arxiv.org/pdf/2601.19375",
      "github_links": [
        "https://github.com/knoveleng/steering"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19375",
      "scraped_at": "2026-01-29T02:08:26.607468"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Revisiting Parameter Server in LLM Post-Training",
    "paper_url": "https://huggingface.co/papers/2601.19362",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Revisiting Parameter Server in LLM Post-Training",
      "abstract": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC) , which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at github .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19362",
      "pdf_url": "https://arxiv.org/pdf/2601.19362",
      "github_links": [
        "https://github.com/sail-sg/odc"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19362",
      "scraped_at": "2026-01-29T02:08:28.752540"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
    "paper_url": "https://huggingface.co/papers/2601.18724",
    "authors": [
      "Taro Watanabe",
      "Hidetaka Kamigaito",
      "Yusuke Sakai"
    ],
    "stars": "0",
    "details": {
      "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
      "abstract": "arXivlens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/hallucitation-matters-revealing-the-impact-of-hallucinated-references-with-300-hallucinated-papers-in-acl-conferences-2856-e431efdf Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18724",
      "pdf_url": "https://arxiv.org/pdf/2601.18724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18724",
      "scraped_at": "2026-01-29T02:08:30.853885"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2601.15968",
    "authors": [
      "Jiaxian Guo",
      "Xin Xie",
      "dginf"
    ],
    "stars": "0",
    "details": {
      "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
      "abstract": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15968",
      "pdf_url": "https://arxiv.org/pdf/2601.15968",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15968",
      "scraped_at": "2026-01-29T02:08:32.912590"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Self-Distillation Enables Continual Learning",
    "paper_url": "https://huggingface.co/papers/2601.19897",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Distillation Enables Continual Learning",
      "abstract": "Check out our website and code: www.idanshenfeld.com/SDFT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19897",
      "pdf_url": "https://arxiv.org/pdf/2601.19897",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19897",
      "scraped_at": "2026-01-29T02:08:35.100990"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
    "paper_url": "https://huggingface.co/papers/2601.19532",
    "authors": [
      "Vincent Ginis",
      "Brecht Verbeken",
      "Andres Algaba",
      "martheballon"
    ],
    "stars": "0",
    "details": {
      "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19532",
      "pdf_url": "https://arxiv.org/pdf/2601.19532",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19532",
      "scraped_at": "2026-01-29T02:08:37.064494"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery",
    "paper_url": "https://huggingface.co/papers/2601.19149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery",
      "abstract": "GPCR-Filter is a compound‚Äìprotein interaction model that couples ESM-3 GPCR sequence embeddings with ligand graph representations through attention-based feature interaction, trained on 90k+ curated GPCR‚Äìligand pairs. It shows stronger OOD generalization to unseen receptors and ligands than prior baselines and recovers micromolar 5-HT‚ÇÅA agonists with diverse scaffolds.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19149",
      "pdf_url": "https://arxiv.org/pdf/2601.19149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19149",
      "scraped_at": "2026-01-29T02:08:38.949971"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
    "paper_url": "https://huggingface.co/papers/2601.18923",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "abstract": "DeFM (Depth Foundation Model) is a vision backbone trained on 60M depth images via self-distillation. It is engineered for robotic perception, providing metric-aware representations that excel in sim-to-real transfer and cross-sensor generalization. TL;DR - A DINO-style encoder, but for depth image inputs. Works zero-shot on diverse robotics and computer vision tasks! webpage: https://de-fm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18923",
      "pdf_url": "https://arxiv.org/pdf/2601.18923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18923",
      "scraped_at": "2026-01-29T02:08:40.980016"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization",
    "paper_url": "https://huggingface.co/papers/2601.18067",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization",
      "abstract": "Verilog‚Äôs design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18067",
      "pdf_url": "https://arxiv.org/pdf/2601.18067",
      "github_links": [
        "https://github.com/weiber2002/ICRTL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18067",
      "scraped_at": "2026-01-29T02:08:42.879305"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "CooperBench: Why Coding Agents Cannot be Your Teammates Yet",
    "paper_url": "https://huggingface.co/papers/2601.13295",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "CooperBench: Why Coding Agents Cannot be Your Teammates Yet",
      "abstract": "Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13295",
      "pdf_url": "https://arxiv.org/pdf/2601.13295",
      "github_links": [
        "https://github.com/cooperbench/CooperBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13295",
      "scraped_at": "2026-01-29T02:08:44.800527"
    },
    "scraped_date": "2026-01-29"
  },
  {
    "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation",
    "paper_url": "https://huggingface.co/papers/2601.20614",
    "authors": [],
    "stars": "84",
    "details": {
      "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation",
      "abstract": "Accepted for ICLR 2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20614",
      "pdf_url": "https://arxiv.org/pdf/2601.20614",
      "github_links": [
        "https://github.com/AMAP-ML/MathForge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20614",
      "scraped_at": "2026-01-30T02:08:03.359880"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Advancing Open-source World Models",
    "paper_url": "https://huggingface.co/papers/2601.20540",
    "authors": [],
    "stars": "756",
    "details": {
      "title": "Advancing Open-source World Models",
      "abstract": "LingBot-World: Advancing Open-source World Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20540",
      "pdf_url": "https://arxiv.org/pdf/2601.20540",
      "github_links": [
        "https://github.com/Robbyant/lingbot-world/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20540",
      "scraped_at": "2026-01-30T02:08:05.280511"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
    "paper_url": "https://huggingface.co/papers/2601.19325",
    "authors": [],
    "stars": "70",
    "details": {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "abstract": "Homepage: https://innovatorlm.github.io/Innovator-VL Github: https://github.com/InnovatorLM/Innovator-VL",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19325",
      "pdf_url": "https://arxiv.org/pdf/2601.19325",
      "github_links": [
        "https://github.com/InnovatorLM/Innovator-VL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19325",
      "scraped_at": "2026-01-30T02:08:07.171795"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "DeepSeek-OCR 2: Visual Causal Flow",
    "paper_url": "https://huggingface.co/papers/2601.20552",
    "authors": [],
    "stars": "1.56k",
    "details": {
      "title": "DeepSeek-OCR 2: Visual Causal Flow",
      "abstract": "Proposes DeepSeek-OCR 2 with a causal, reordering encoder (DeepEncoder V2) to dynamically rearrange visual tokens for LLMs, enabling 2D reasoning via two cascaded 1D causal structures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20552",
      "pdf_url": "https://arxiv.org/pdf/2601.20552",
      "github_links": [
        "https://github.com/deepseek-ai/DeepSeek-OCR-2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20552",
      "scraped_at": "2026-01-30T02:08:09.078301"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
    "paper_url": "https://huggingface.co/papers/2601.20209",
    "authors": [
      "Shuai Zhang",
      "Yuhao Shen",
      "Changpeng Yang",
      "Shuo Yang",
      "Jinyang23"
    ],
    "stars": "0",
    "details": {
      "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20209",
      "pdf_url": "https://arxiv.org/pdf/2601.20209",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20209",
      "scraped_at": "2026-01-30T02:08:10.927956"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Linear representations in language models can change dramatically over a conversation",
    "paper_url": "https://huggingface.co/papers/2601.20834",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Linear representations in language models can change dramatically over a conversation",
      "abstract": "LM representations along linear directions evolve during conversations, with factual content shifting over time; changes depend on context, robust across models, and challenge static interpretability and steering.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20834",
      "pdf_url": "https://arxiv.org/pdf/2601.20834",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20834",
      "scraped_at": "2026-01-30T02:08:12.800498"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Reinforcement Learning via Self-Distillation",
    "paper_url": "https://huggingface.co/papers/2601.20802",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Reinforcement Learning via Self-Distillation",
      "abstract": "We introduce Self-Distillation Policy Optimization (SDPO), a method for online RL that leverages the model's own ability to interpret rich feedback to drastically speed up training and boost reasoning capabilities on hard tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20802",
      "pdf_url": "https://arxiv.org/pdf/2601.20802",
      "github_links": [
        "https://github.com/lasgroup/SDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20802",
      "scraped_at": "2026-01-30T02:08:14.741122"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "SERA: Soft-Verified Efficient Repository Agents",
    "paper_url": "https://huggingface.co/papers/2601.20789",
    "authors": [],
    "stars": "75",
    "details": {
      "title": "SERA: Soft-Verified Efficient Repository Agents",
      "abstract": "Introduces SERA for efficient open-weight coding agents trained via supervised finetuning; SVG generates synthetic trajectories to cheaply specialize agents to private codebases with strong open-model performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20789",
      "pdf_url": "https://arxiv.org/pdf/2601.20789",
      "github_links": [
        "https://github.com/allenai/SERA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20789",
      "scraped_at": "2026-01-30T02:08:16.570295"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.20055",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning",
      "abstract": "In this paper, we introduce VERGE, a neuro-symbolic framework designed to bridge the semantic gap between LLM fluency and formal correctness by addressing the \"all-or-nothing\" limitation of traditional solvers. A key innovation is Semantic Routing, which dynamically classifies atomic claims to handle non-formalizable natural language; verifiable logic is sent to SMT solvers (Z3) for proof, while ambiguous or commonsense claims are routed to a consensus-based \"soft verifier,\" preventing the system from breaking on vague predicates. Furthermore, we replace generic error signals with actionable feedback via Minimal Correction Subsets (MCS), which pinpoint the precise subset of conflicting claims to revise, enabling the model to iteratively refine its reasoning from \"probabilistic\" to \"provable\" with an 18.7% performance uplift",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20055",
      "pdf_url": "https://arxiv.org/pdf/2601.20055",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20055",
      "scraped_at": "2026-01-30T02:08:18.452732"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
    "paper_url": "https://huggingface.co/papers/2601.20380",
    "authors": [
      "Yusai Zhao",
      "Jingjia Cao",
      "Xinjiang Lu",
      "Yixiong Xiao",
      "Le Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
      "abstract": "OmegaUse is a general GUI agent enabling autonomous desktop and mobile task execution, trained with curated data, SFT and GRPO, MoE backbone, and evaluated on OS-Nav benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20380",
      "pdf_url": "https://arxiv.org/pdf/2601.20380",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20380",
      "scraped_at": "2026-01-30T02:08:20.311133"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.19280",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning",
      "abstract": "Beyond Uniform-data LLM Reasoning: We propose an optimization-first framework, based on Group Distributionally Robust Optimization (GDRO), that moves beyond uniform reasoning models by dynamically adapting the training distribution.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19280",
      "pdf_url": "https://arxiv.org/pdf/2601.19280",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19280",
      "scraped_at": "2026-01-30T02:08:22.188842"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.19949",
    "authors": [
      "mandipgoswami"
    ],
    "stars": "1",
    "details": {
      "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation",
      "abstract": "Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index (C_{50}) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results. Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19949",
      "pdf_url": "https://arxiv.org/pdf/2601.19949",
      "github_links": [
        "https://github.com/mandip42/rir-mega-speech"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19949",
      "scraped_at": "2026-01-30T02:08:24.021390"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
    "paper_url": "https://huggingface.co/papers/2601.17950",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
      "abstract": "Code: https://github.com/mwalmer-umd/UPLiFT/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17950",
      "pdf_url": "https://arxiv.org/pdf/2601.17950",
      "github_links": [
        "https://github.com/mwalmer-umd/UPLiFT/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17950",
      "scraped_at": "2026-01-30T02:08:25.865529"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
    "paper_url": "https://huggingface.co/papers/2601.20829",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
      "abstract": "TL;DR: We enable continued RL training on saturated reasoning tasks by conditioning on rare failure prefixes.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20829",
      "pdf_url": "https://arxiv.org/pdf/2601.20829",
      "github_links": [
        "https://github.com/minwukim/training-on-saturated-problems"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20829",
      "scraped_at": "2026-01-30T02:08:27.676752"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection",
    "paper_url": "https://huggingface.co/papers/2601.20618",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection",
      "abstract": "Existing multimodal sarcasm detection methods struggle with loosely related image-text pairs and noisy LLM-generated cues. GDCNet addresses this by using MLLM-generated factual captions as semantic anchors to compute semantic and sentiment discrepancies against the original text, fused via a gated module. The approach achieves state-of-the-art performance on the MMSD2.0 benchmark, demonstrating superior robustness in capturing cross-modal incongruities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20618",
      "pdf_url": "https://arxiv.org/pdf/2601.20618",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20618",
      "scraped_at": "2026-01-30T02:08:29.510352"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "How AI Impacts Skill Formation",
    "paper_url": "https://huggingface.co/papers/2601.20245",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "How AI Impacts Skill Formation",
      "abstract": "https://www.anthropic.com/research/AI-assistance-coding-skills",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20245",
      "pdf_url": "https://arxiv.org/pdf/2601.20245",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20245",
      "scraped_at": "2026-01-30T02:08:31.313413"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper",
    "paper_url": "https://huggingface.co/papers/2601.19194",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper",
      "abstract": "Accepted at ICASSP 2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19194",
      "pdf_url": "https://arxiv.org/pdf/2601.19194",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19194",
      "scraped_at": "2026-01-30T02:08:33.154397"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2601.18150",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
      "abstract": "We have developed FP8 rollout features for the two frameworks, verl and NeMo-RL. In this report, we introduce the implementation solutions and a series of validation experiments conducted (covering both Dense and MoE models), with analyses performed from the perspectives of precision alignment and performance. In the coming period, we will continue to explore and practice stable FP8 end-to-end training recipes for MoE models. Welcome to use these features and provide feedback.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18150",
      "pdf_url": "https://arxiv.org/pdf/2601.18150",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18150",
      "scraped_at": "2026-01-30T02:08:34.979980"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Persona Prompting as a Lens on LLM Social Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.20757",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Persona Prompting as a Lens on LLM Social Reasoning",
      "abstract": "Persona prompting can improve classification in socially-sensitive tasks, but it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20757",
      "pdf_url": "https://arxiv.org/pdf/2601.20757",
      "github_links": [
        "https://github.com/jingyng/PP-social-reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20757",
      "scraped_at": "2026-01-30T02:08:36.781949"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation",
    "paper_url": "https://huggingface.co/papers/2601.20622",
    "authors": [
      "Hongbo Fu",
      "Zeyu Wang",
      "Lin-Ping Yuan",
      "Boyu Li"
    ],
    "stars": "0",
    "details": {
      "title": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching (2025) Rewriting Video: Text-Driven Reauthoring of Video Footage (2026) Protosampling: Enabling Free-Form Convergence of Sampling and Prototyping through Canvas-Driven Visual AI Generation (2026) DepthScape: Authoring 2.5D Designs via Depth Estimation, Semantic Understanding, and Geometry Extraction (2025) SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing (2025) LAMP: Language-Assisted Motion Planning for Controllable Video Generation (2025) ShadowDraw: From Any Object to Shadow-Drawing Compositional Art (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20622",
      "pdf_url": "https://arxiv.org/pdf/2601.20622",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20622",
      "scraped_at": "2026-01-30T02:08:38.570627"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Shallow-œÄ: Knowledge Distillation for Flow-based VLAs",
    "paper_url": "https://huggingface.co/papers/2601.20262",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Shallow-œÄ: Knowledge Distillation for Flow-based VLAs",
      "abstract": "https://icsl-jeon.github.io/shallow-pi/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20262",
      "pdf_url": "https://arxiv.org/pdf/2601.20262",
      "github_links": [
        "https://github.com/icsl-Jeon/openpi"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20262",
      "scraped_at": "2026-01-30T02:08:40.401454"
    },
    "scraped_date": "2026-01-30"
  },
  {
    "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
    "paper_url": "https://huggingface.co/papers/2601.20833",
    "authors": [],
    "stars": "54",
    "details": {
      "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
      "abstract": "arXivLens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/idea2story-an-automated-pipeline-for-transforming-research-concepts-into-complete-scientific-narratives-2345-6407a884 Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20833",
      "pdf_url": "https://arxiv.org/pdf/2601.20833",
      "github_links": [
        "https://github.com/AgentAlphaAGI/Idea2Paper"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20833",
      "scraped_at": "2026-01-31T02:04:25.055427"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2601.20354",
    "authors": [],
    "stars": "93",
    "details": {
      "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
      "abstract": "A very interesting benchmark (ICLR2026) for T2I models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20354",
      "pdf_url": "https://arxiv.org/pdf/2601.20354",
      "github_links": [
        "https://github.com/AMAP-ML/SpatialGenEval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20354",
      "scraped_at": "2026-01-31T02:04:26.932159"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21204",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "abstract": "Embedding scaling can outperform mixture of experts for sparse language models, aided by system optimizations and speculative decoding, with LongCat-Flash-Lite achieving strong competitiveness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21204",
      "pdf_url": "https://arxiv.org/pdf/2601.21204",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21204",
      "scraped_at": "2026-01-31T02:04:29.327313"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.22153",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "abstract": "TL; DR: DynamicVLA enables open-ended dynamic object manipulation by pairing a compact 0.4B VLM with low-latency Continuous Inference and Latent-aware Action Streaming, evaluated at scale through the new DOM benchmark in both simulation and the real world. GitHub: https://github.com/hzxie/DynamicVLA Project Page: https://haozhexie.com/project/dynamic-vla Spotlight Video: https://www.youtube.com/watch?v=NmJnHcI04_Q",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22153",
      "pdf_url": "https://arxiv.org/pdf/2601.22153",
      "github_links": [
        "https://github.com/hzxie/DynamicVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22153",
      "scraped_at": "2026-01-31T02:04:31.292790"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21639",
    "authors": [
      "Liming Zheng",
      "Wenkang Han",
      "Xuanle Zhao",
      "Lei Chen",
      "Albert-Zhong"
    ],
    "stars": "13",
    "details": {
      "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
      "abstract": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21639",
      "pdf_url": "https://arxiv.org/pdf/2601.21639",
      "github_links": [
        "https://github.com/DocTron-hub/OCRVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21639",
      "scraped_at": "2026-01-31T02:04:33.172960"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
    "paper_url": "https://huggingface.co/papers/2601.21821",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
      "abstract": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21821",
      "pdf_url": "https://arxiv.org/pdf/2601.21821",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21821",
      "scraped_at": "2026-01-31T02:04:35.166150"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
    "paper_url": "https://huggingface.co/papers/2601.21420",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
      "abstract": "ConceptMoE shifts language model processing from uniform token-level to adaptive concept-level computation. By learning to merge semantically similar tokens into unified concepts while preserving fine-grained granularity for complex tokens, it performs implicit compute allocation‚Äîautomatically investing computation where needed. Key results: (1) Fair comparison under identical parameters and FLOPs shows consistent gains across language (+0.9), vision-language (+0.6, +2.3 on long context), and continual training (+5.5 with layer loops, +6.4 from scratch). (2) Inherent efficiency: at compression ratio R=2, attention computation reduces by R¬≤√ó and KV cache by R√ó, achieving prefill speedups up to 175% and decoding speedups up to 117%. (3) Minimal architectural changes (chunk module + decoder QKV projectors) enable straightforward deployment in existing MoE systems. Represents a paradigm shift toward hierarchical semantic processing in LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21420",
      "pdf_url": "https://arxiv.org/pdf/2601.21420",
      "github_links": [
        "https://github.com/ZihaoHuang-notabot/ConceptMoE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21420",
      "scraped_at": "2026-01-31T02:04:37.053847"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.22046",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "abstract": "PLANING introduces a loosely coupled triangle-Gaussian representation and a monocular streaming framework that jointly achieves accurate geometry, high-fidelity rendering, and efficient planar abstraction for embodied AI applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22046",
      "pdf_url": "https://arxiv.org/pdf/2601.22046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22046",
      "scraped_at": "2026-01-31T02:04:39.099965"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Qwen3-ASR Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.21337",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-ASR Technical Report",
      "abstract": "Qwen3-ASR delivers two all-in-one ASR models with 52-language support and a non-autoregressive forced-aligner; achieves competitive SOTA accuracy, fast TTFT, and open-source Apache 2.0 release.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21337",
      "pdf_url": "https://arxiv.org/pdf/2601.21337",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21337",
      "scraped_at": "2026-01-31T02:04:40.995879"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
    "paper_url": "https://huggingface.co/papers/2601.20730",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
      "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \\textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues. The code is available at https://github.com/euReKa025/AgentLongBench .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20730",
      "pdf_url": "https://arxiv.org/pdf/2601.20730",
      "github_links": [
        "https://github.com/euReKa025/AgentLongBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20730",
      "scraped_at": "2026-01-31T02:04:42.866671"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Exploring Reasoning Reward Model for Agents",
    "paper_url": "https://huggingface.co/papers/2601.22154",
    "authors": [
      "Zhixun Li",
      "Tianshuo Peng",
      "Manyuan Zhang",
      "Kaituo Feng",
      "bunny127"
    ],
    "stars": "14",
    "details": {
      "title": "Exploring Reasoning Reward Model for Agents",
      "abstract": "Github: https://github.com/kxfan2002/Reagent Paper: https://arxiv.org/pdf/2601.22154",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22154",
      "pdf_url": "https://arxiv.org/pdf/2601.22154",
      "github_links": [
        "https://github.com/kxfan2002/Reagent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22154",
      "scraped_at": "2026-01-31T02:04:44.820271"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
    "paper_url": "https://huggingface.co/papers/2601.16914",
    "authors": [
      "Xiaojie Li",
      "Tao Yang",
      "Ming Li",
      "Jie Wu",
      "Justin Cui"
    ],
    "stars": "0",
    "details": {
      "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
      "abstract": "Scaling up video generation to hour long, please checkout our paper at: https://arxiv.org/abs/2601.16914 Project Page and code will released at: https://github.com/justincui03/LoL",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16914",
      "pdf_url": "https://arxiv.org/pdf/2601.16914",
      "github_links": [
        "https://github.com/justincui03/LoL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16914",
      "scraped_at": "2026-01-31T02:04:46.771257"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
    "paper_url": "https://huggingface.co/papers/2601.21754",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
      "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21754",
      "pdf_url": "https://arxiv.org/pdf/2601.21754",
      "github_links": [
        "https://github.com/Harry-mic/SCOUT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21754",
      "scraped_at": "2026-01-31T02:04:48.673338"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "paper_url": "https://huggingface.co/papers/2601.22157",
    "authors": [
      "Yedid Hoshen",
      "Eliahu Horwitz",
      "Jonathan Kahana"
    ],
    "stars": "0",
    "details": {
      "title": "Discovering Hidden Gems in Model Repositories",
      "abstract": "An investigation of the available fine-tunes of popular foundation models. While over 90% of downloads are directed to the official base versions the paper shows the existence of other, rarely downloaded fine-tunes that significantly outperform them.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22157",
      "pdf_url": "https://arxiv.org/pdf/2601.22157",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22157",
      "scraped_at": "2026-01-31T02:04:51.312485"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "paper_url": "https://huggingface.co/papers/2601.22083",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Latent Adversarial Regularization for Offline Preference Optimization",
      "abstract": "Most offline preference optimization methods (e.g., DPO) constrain policy updates using token-level divergences. However, token-space similarity is often a weak proxy for semantic or structural behavior. We propose GANPO, a plug-and-play regularizer that introduces latent-space adversarial regularization, aligning the latent representation distributions of a policy and a reference model via a principled GAN-style divergence. We find consistent performance improvements. GANPO yields consistent gains across model architectures when integrated into OPO-style methods on AlpacaEval. We also find that structure is preserved. The adversarial objective acts as a geometry-preserving regularizer. Unlike DPO, which often degrades at high sampling temperatures (T ‚â• 1.0), GANPO maintains structural coherence in high-entropy settings. If you‚Äôre interested in alignment, GANs, or the limitations of KL-divergence‚Äìbased regularization, feel free to check out the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22083",
      "pdf_url": "https://arxiv.org/pdf/2601.22083",
      "github_links": [
        "https://github.com/enyijiang/GANPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22083",
      "scraped_at": "2026-01-31T02:04:53.289612"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
    "paper_url": "https://huggingface.co/papers/2601.21590",
    "authors": [
      "Haitham Bou Ammar",
      "Matthieu Zimmer",
      "Rasul Tutunov",
      "xtongji"
    ],
    "stars": "0",
    "details": {
      "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
      "abstract": "What if RL isn‚Äôt teaching LLMs how to reason, but just sharpening what‚Äôs already there? Most recent progress in LLM reasoning comes from RL post-training (GRPO, verifiers, rewards). But there‚Äôs growing evidence that these gains may come less from learning new capabilities and more from reshaping the distribution of outputs. In our new work, we take that idea seriously. We show that: Reasoning trajectories already exist in base models What matters is how you sample, not how you retrain The global power distribution can be approximated autoregressively, without MCMC The result is a training-free, verifier-free inference-time method that: ‚ö° Matches GRPO-style post-training ‚è± Is ~10√ó faster than MCMC-based power sampling üß™ Requires no rewards, no finetuning, no verifier Conceptually, the key insight is simple: Power sampling ‚âà low-temperature sampling √ó future-aware token scaling This lets us recover global reasoning behaviour token by token, without expensive trajectory-level inference.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21590",
      "pdf_url": "https://arxiv.org/pdf/2601.21590",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21590",
      "scraped_at": "2026-01-31T02:04:55.321407"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Shaping capabilities with token-level data filtering",
    "paper_url": "https://huggingface.co/papers/2601.21571",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "Shaping capabilities with token-level data filtering",
      "abstract": "Key Findings: 1. Token-level Filtering vs Document-level Filtering (Figure 3) Token filtering Pareto-dominates document filtering : Can achieve equal reduction in undesired capabilities (equal medical loss) at lower cost to desired capabilities (lower biology loss) More precise filtering preserves beneficial content better 2. Scaling Effects (Figures 1, 4, 5, 6) Filtering gets more effective with scale : 1.8B parameter models see 7,000√ó compute slowdown on medical domain Document filtering: ~30√ó slowdown Token removal: >7,000√ó slowdown Multiple choice evaluation : Models score near chance on MedMCQA and MedQA-USMLE (medical), but maintain performance on retain domains Free response : Token filtering reduces medical answer correctness up to 20√ó, relevance/coherence 3√ó compared to baseline 3. Robustness to Attacks (Figure 7) 10√ó more robust than unlearning against adversarial finetuning attacks for 1.8B models State-of-the-art unlearning (RMU) requires 13√ó fewer tokens to recover capabilities compared to token removal 4. Alignment Compatibility (Figures 8, 9) Models can still be aligned on forget domain : Token-level filtering makes refusal training easier (2√ó better refusal generalization) Document filtering struggles with alignment generalization Linear probes show models can distinguish forget vs. retain tokens despite filtering 5. Classifier Training (Table 1, Figure 11) Small, task-specific models outperform large general ones : 224M parameter biLM achieves 0.894 F1 on test set Outperforms 395M ModernBERT-large (0.794 F1) Domain-specific pretraining improves performance 6. Label Quality Tolerance (Figures 12, 13, 14, 15) Robust to imperfect labels : Aggressive filtering with sufficient compute can overcome label noise Token-level classifiers generalize from weak labels better than document-level Can trade precision for recall to maintain effectiveness",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21571",
      "pdf_url": "https://arxiv.org/pdf/2601.21571",
      "github_links": [
        "https://github.com/neilrathi/token-filtering"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21571",
      "scraped_at": "2026-01-31T02:04:57.253161"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.21051",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "abstract": "Model card: https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21051",
      "pdf_url": "https://arxiv.org/pdf/2601.21051",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21051",
      "scraped_at": "2026-01-31T02:04:59.333958"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.18129",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
      "abstract": "Code: https://github.com/scb-10x/typhoon-s Artifact: https://huggingface.co/collections/typhoon-ai/typhoon-s",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18129",
      "pdf_url": "https://arxiv.org/pdf/2601.18129",
      "github_links": [
        "https://github.com/scb-10x/typhoon-s"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18129",
      "scraped_at": "2026-01-31T02:05:01.248336"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.22069",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
      "abstract": "We propose VTC-R1, an efficient long-context reasoning paradigm that integrates vision-text compression into iterative reasoning. By rendering previous reasoning segments into compact visual representations, VTC-R1 replaces long textual contexts with significantly fewer vision tokens in a lightweight and model-free manner. Extensive experiments show that VTC-R1 consistently improves reasoning accuracy across multiple benchmarks while achieving up to 3.4x token compression and 2.7x end-to-end inference speedup. The results demonstrate that VTC-R1 provides an effective alternative representation for scalable long-context reasoning. We hope our work would inspire further exploration of efficient reasoning beyond pure text-based paradigms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22069",
      "pdf_url": "https://arxiv.org/pdf/2601.22069",
      "github_links": [
        "https://github.com/w-yibo/VTC-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22069",
      "scraped_at": "2026-01-31T02:05:04.875316"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21181",
    "authors": [
      "Yong Man Ro",
      "Youngchae Chee",
      "Se Yeon Kim",
      "topyun"
    ],
    "stars": "0",
    "details": {
      "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8% and 2.0% improvements for VideoLLaMA2-AV, 8.7% and 4.7% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21181",
      "pdf_url": "https://arxiv.org/pdf/2601.21181",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21181",
      "scraped_at": "2026-01-31T02:05:07.147597"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
    "paper_url": "https://huggingface.co/papers/2601.20975",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
      "abstract": "Proposes DeepSearchQA, a 900-prompt benchmark across 17 fields to test long-horizon search, info synthesis, deduplication, and stopping criteria for open-web research agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20975",
      "pdf_url": "https://arxiv.org/pdf/2601.20975",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20975",
      "scraped_at": "2026-01-31T02:05:09.153443"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "paper_url": "https://huggingface.co/papers/2601.17883",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
      "abstract": "We propose fair and comprehensive benchmarking for open source EEG foundation models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17883",
      "pdf_url": "https://arxiv.org/pdf/2601.17883",
      "github_links": [
        "https://github.com/Dingkun0817/EEG-FM-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17883",
      "scraped_at": "2026-01-31T02:05:11.291822"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
    "paper_url": "https://huggingface.co/papers/2601.21598",
    "authors": [
      "Wee Sun Lee",
      "zz1358m"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
      "abstract": "Our recent work on Latent Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21598",
      "pdf_url": "https://arxiv.org/pdf/2601.21598",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21598",
      "scraped_at": "2026-01-31T02:05:14.352845"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "paper_url": "https://huggingface.co/papers/2601.22158",
    "authors": [
      "Zhicheng Jiang",
      "Hanhong Zhao",
      "Qiao Sun",
      "Susie Lu",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "abstract": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22158",
      "pdf_url": "https://arxiv.org/pdf/2601.22158",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22158",
      "scraped_at": "2026-01-31T02:05:16.302853"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "paper_url": "https://huggingface.co/papers/2601.22156",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "abstract": "Code: https://www.github.com/THUNLP/hybrid-linear-attention",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22156",
      "pdf_url": "https://arxiv.org/pdf/2601.22156",
      "github_links": [
        "https://github.com/thunlp/hybrid-linear-attention",
        "https://www.github.com/THUNLP/hybrid-linear-attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22156",
      "scraped_at": "2026-01-31T02:05:18.673947"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
    "paper_url": "https://huggingface.co/papers/2601.22146",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "abstract": "@ AjayP13 and @ craffel really interesting work and approach, do you plan to add support for multilingual instructions ü§î",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22146",
      "pdf_url": "https://arxiv.org/pdf/2601.22146",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22146",
      "scraped_at": "2026-01-31T02:05:20.808062"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
    "paper_url": "https://huggingface.co/papers/2601.21579",
    "authors": [
      "Danilo Mandic",
      "Giorgos Iacovides",
      "Yuxuan Gu",
      "WuyangZzzz"
    ],
    "stars": "1",
    "details": {
      "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
      "abstract": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21579",
      "pdf_url": "https://arxiv.org/pdf/2601.21579",
      "github_links": [
        "https://github.com/wz1119/KromHC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21579",
      "scraped_at": "2026-01-31T02:05:23.326409"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
    "paper_url": "https://huggingface.co/papers/2601.21343",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
      "abstract": "Streaming pretraining uses a strong post-trained model to judge next-token generations with RL, improving quality, safety, and factuality earlier in training.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21343",
      "pdf_url": "https://arxiv.org/pdf/2601.21343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21343",
      "scraped_at": "2026-01-31T02:05:25.784093"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "ECO: Quantized Training without Full-Precision Master Weights",
    "paper_url": "https://huggingface.co/papers/2601.22101",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ECO: Quantized Training without Full-Precision Master Weights",
      "abstract": "We present Error-Compensating Optimizer (ECO), which integrates with standard optimizers and, for the first time, enables quantized training of large-scale LLMs without requiring high-precision master weights.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22101",
      "pdf_url": "https://arxiv.org/pdf/2601.22101",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22101",
      "scraped_at": "2026-01-31T02:05:28.392176"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "paper_url": "https://huggingface.co/papers/2601.22054",
    "authors": [
      "Jianxun Cui",
      "Xuancheng Zhang",
      "Donglin Di",
      "Baorui Ma",
      "yjh001"
    ],
    "stars": "41",
    "details": {
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "abstract": "Project Page: https://metric-anything.github.io/metric-anything-io/ Code: https: https://github.com/metric-anything/metric-anything",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22054",
      "pdf_url": "https://arxiv.org/pdf/2601.22054",
      "github_links": [
        "https://github.com/metric-anything/metric-anything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22054",
      "scraped_at": "2026-01-31T02:05:31.122578"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
    "paper_url": "https://huggingface.co/papers/2601.21996",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "abstract": "We introduce Mechanistic Data Attribution (MDA), a new paradigm that shifts the focus of mechanistic interpretability from post-hoc circuit analysis to the causal formation of these mechanisms during training.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21996",
      "pdf_url": "https://arxiv.org/pdf/2601.21996",
      "github_links": [
        "https://github.com/chenjianhuii/Mechanistic-Data-Attribution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21996",
      "scraped_at": "2026-01-31T02:05:33.127604"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
    "paper_url": "https://huggingface.co/papers/2601.21406",
    "authors": [
      "Guanhua Chen",
      "Yong Wang",
      "Kangrui Cen",
      "Hongyang Wei",
      "Zihan Su"
    ],
    "stars": "6",
    "details": {
      "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
      "abstract": "Paper: https://arxiv.org/abs/2601.21406 Github: https://github.com/Sugewud/UniMRG Project: https://sugewud.github.io/UniMRG-Project/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21406",
      "pdf_url": "https://arxiv.org/pdf/2601.21406",
      "github_links": [
        "https://github.com/Sugewud/UniMRG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21406",
      "scraped_at": "2026-01-31T02:05:34.975076"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
    "paper_url": "https://huggingface.co/papers/2601.20465",
    "authors": [
      "Mingkun Xu",
      "Yujie Wu",
      "Yusong Wang",
      "Jiaxiang Liu",
      "innovation64"
    ],
    "stars": "0",
    "details": {
      "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
      "abstract": "We introduce BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture designed to solve \"soul erosion\"‚Äîthe loss of temporal grounding and consistency in long-term agent interactions. üß† Key Innovations: Cognitive-inspired Architecture: Decomposes memory into episodic, semantic, salience-aware, and control-oriented components. Temporal Grounding: Operates at complementary time scales to maintain behavioral consistency. Plug-and-play: A general framework for LLM-based multi-agent systems. Check out our preprint for details on how we bridge the gap between biological memory systems and AI agents!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20465",
      "pdf_url": "https://arxiv.org/pdf/2601.20465",
      "github_links": [
        "https://github.com/innovation64/BMAM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20465",
      "scraped_at": "2026-01-31T02:05:36.757303"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.22143",
    "authors": [
      "Urska Jelercic",
      "Matan Ben Yosef",
      "Tavi Halperin",
      "Naomi Ken Korem",
      "Anthony Chen"
    ],
    "stars": "0",
    "details": {
      "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22143",
      "pdf_url": "https://arxiv.org/pdf/2601.22143",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22143",
      "scraped_at": "2026-01-31T02:05:38.700079"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.19001",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
      "abstract": "ICLR2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19001",
      "pdf_url": "https://arxiv.org/pdf/2601.19001",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19001",
      "scraped_at": "2026-01-31T02:05:40.680731"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
    "paper_url": "https://huggingface.co/papers/2601.21268",
    "authors": [
      "Jesse Roberts",
      "Micah Rentschler"
    ],
    "stars": "0",
    "details": {
      "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
      "abstract": "We present Reinforcement Learning from Meta-Evaluation (RLME), a label-free RL framework that trains LLMs using evaluator judgments to natural-language meta-questions, achieving performance comparable to supervised rewards while scaling to ambiguous, open-domain tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21268",
      "pdf_url": "https://arxiv.org/pdf/2601.21268",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21268",
      "scraped_at": "2026-01-31T02:05:42.509455"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
    "paper_url": "https://huggingface.co/papers/2601.20103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
      "abstract": "We show that contrasting reward hacks in an outlier detection setting helps LLMs detect code hacking behaviors. We further show that a cluster's benign-to-hacked trajectory ratio influences this detection rate. Finally we perform thorough QA and show that semantically contextualized hacks are more difficult to detect as compared to syntactic ones. We release TRACE, a synthetic, human verified dataset of 517 trajectories spanning 54 code reward hack categories to help the community build robust automated RL orchestration pipelines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20103",
      "pdf_url": "https://arxiv.org/pdf/2601.20103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20103",
      "scraped_at": "2026-01-31T02:05:44.372489"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
    "paper_url": "https://huggingface.co/papers/2601.17690",
    "authors": [
      "Melody Ma",
      "Iram Kamdar",
      "Yunyan Ouyang",
      "Ziling Gong",
      "Franck-Dernoncourt"
    ],
    "stars": "0",
    "details": {
      "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning (2026) BEST-STD2.0: Balanced and Efficient Speech Tokenizer for Spoken Term Detection (2025) VIBEVOICE-ASR Technical Report (2026) DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification (2026) Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding (2025) LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech (2026) SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17690",
      "pdf_url": "https://arxiv.org/pdf/2601.17690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17690",
      "scraped_at": "2026-01-31T02:05:46.530167"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
    "paper_url": "https://huggingface.co/papers/2601.11747",
    "authors": [
      "Stefano Petrangeli",
      "Yu Shen",
      "Sunav Choudhary",
      "Huaxiaoyue Wang",
      "Franck-Dernoncourt"
    ],
    "stars": "0",
    "details": {
      "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing (2026) Styles + Persona-plug = Customized LLMs (2026) Step-by-step Layered Design Generation (2025) Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs (2025) Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation (2025) ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement (2025) Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11747",
      "pdf_url": "https://arxiv.org/pdf/2601.11747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11747",
      "scraped_at": "2026-01-31T02:05:48.392500"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
    "paper_url": "https://huggingface.co/papers/2601.21872",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
      "abstract": "Accepted at ICLR 2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21872",
      "pdf_url": "https://arxiv.org/pdf/2601.21872",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21872",
      "scraped_at": "2026-01-31T02:05:50.473689"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.21416",
    "authors": [
      "Liming Chen",
      "Emmanuel Dellandr√©a",
      "Bruno Machado",
      "Beegbrain"
    ],
    "stars": "0",
    "details": {
      "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
      "abstract": "The ability of visuomotor policies to generalize across tasks and environments critically depends on the structure of the underlying visual representations. While most state-of-the-art robot policies rely on either global or dense features from pre-trained vision models, these approaches often entangle task-relevant and irrelevant signals, limiting their robustness to visual distribution shifts. In this paper, we investigate Slot-based Object-Centric Representations (SOCRs) as a structured alternative that decomposes scenes into a finite set of object-like entities. We present the first large-scale, systematic comparison of global, dense, and object-centric representations across diverse simulated (METAWORLD, LIBERO) and real-world robotic manipulation tasks. Our results show that SOCRs enable superior policy performance and significantly improve generalization under shifts in lighting, texture, and clutter‚Äî even without task-specific tuning. We further demonstrate that pretraining SOCRs on robotic video datasets amplifies these benefits. Our findings highlight the importance of structured visual abstraction in robotic perception and suggest that SOCRs offer a promising pathway toward more robust and generaliz- able manipulation policies.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21416",
      "pdf_url": "https://arxiv.org/pdf/2601.21416",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21416",
      "scraped_at": "2026-01-31T02:05:52.548123"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "paper_url": "https://huggingface.co/papers/2601.21282",
    "authors": [
      "Pranay Boreddy",
      "Ayush Agrawal",
      "Jim Solomon",
      "Howard Zhang",
      "Rishi Upadhyay"
    ],
    "stars": "0",
    "details": {
      "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
      "abstract": "WorldBench provides a disentangled, concept-specific video benchmark to rigorously evaluate physical reasoning in world models and their video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21282",
      "pdf_url": "https://arxiv.org/pdf/2601.21282",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21282",
      "scraped_at": "2026-01-31T02:05:54.363536"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.20381",
    "authors": [
      "Liming Chen",
      "Emmanuel Dellandr√©a",
      "Beegbrain"
    ],
    "stars": "0",
    "details": {
      "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
      "abstract": "We introduce a slot-based object-centric method with a \"task-awareness\" alignment in order to learn robotic manipulation. Our method obtains strong generalization improvements over existing VFM by simply adding a few layers of structure and keeping the backbone frozen. We hope this work can lead to more work going in the direction of adding structure in the visual inputs for robotics manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20381",
      "pdf_url": "https://arxiv.org/pdf/2601.20381",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20381",
      "scraped_at": "2026-01-31T02:05:57.004313"
    },
    "scraped_date": "2026-01-31"
  },
  {
    "title": "Flow-based Extremal Mathematical Structure Discovery",
    "paper_url": "https://huggingface.co/papers/2601.18005",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Flow-based Extremal Mathematical Structure Discovery",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18005",
      "pdf_url": "https://arxiv.org/pdf/2601.18005",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18005",
      "scraped_at": "2026-01-31T02:05:59.056116"
    },
    "scraped_date": "2026-01-31"
  }
]