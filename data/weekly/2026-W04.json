[
  {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "paper_url": "https://huggingface.co/papers/2601.15876",
    "authors": [],
    "stars": "154",
    "details": {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "EvoCUA: Evolving Computer Use Agent ü•á #1 Open-Source Model on OSWorld | A General-Purpose Multimodal Model Excelling at Computer Use üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/meituan/EvoCUA üåü Highlights ü•á #1 Open-Source Model on OSWorld : Achieves 56.7% task completion rate, #1 among all open-source models üìà Significant Improvements : +11.7% over OpenCUA-72B (45.0%‚Üí56.7%), +15.1% over Qwen3-VL thinking (41.6%‚Üí56.7%), with fewer parameters and half the steps üñ•Ô∏è End-to-End Multi-Turn Automation : Operates Chrome, Excel, PowerPoint, VSCode and more through screenshots and natural language instructions üß† Novel Training Method : Our data synthesis and training approach consistently improves Computer Use capability across multiple open-source VLMs without degrading general performance üìä Performance Comparison Rank Model Open/Closed Type Max Steps Score 1 Claude-sonnet-4-5 üîí Closed General 100 62.9% 2 Seed-1.8 üîí Closed General 100 61.9% 3 Claude-sonnet-4-5 üîí Closed General 50 58.1% 4 EvoCUA-20260105 (Ours) üü¢ Open General 50 56.7% ü•á 5 DeepMiner-Mano-72B üîí Closed Specialized 100 53.9% 6 UI-TARS-2-2509 üîí Closed General 100 53.1% 7 EvoCUA (Previous Version) üîí Closed General 50 50.3% 8 EvoCUA-8B-20260105 (Ours) üü¢ Open General 50 46.1% 9 OpenCUA-72B üü¢ Open Specialized 100 45.0% ... ... ... ... ... ... 13 Qwen3-VL-Flash üîí Closed General 100 41.6% EvoCUA is #1 among all open-source models , achieving competitive results with only 50 steps . Human-level performance remains significantly higher, indicating substantial room for improvement. üìù Citation If you find EvoCUA useful in your research, please consider citing: @ misc {evocua2026,\n  title={EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience},\n  author={Chong Peng* and Taofeng Xue*},\n  year={2026},\n  url={https://github.com/meituan/EvoCUA},\n  note={* Equal contribution}\n} üìú License This project is licensed under the Apache 2.0 License - see the LICENSE file for details. Built with ‚ù§Ô∏è by Meituan LongCat Team",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15876",
      "pdf_url": "https://arxiv.org/pdf/2601.15876",
      "github_links": [
        "https://github.com/meituan/EvoCUA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15876",
      "scraped_at": "2026-01-26T02:01:16.604428"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.14724",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "abstract": "üöÄ Introducing HERMES: The Future of Real-Time Streaming Video Understanding! While today's Multimodal Large Language Models (MLLMs) perform impressively at offline video comprehension, they often face a \"painful trade-off\" when it comes to real-time streaming video - balancing real-time responses, low memory usage, and high accuracy. To solve this, we introduce the following innovations: üí° The HERMES Breakthrough: 1Ô∏è‚É£ Novel memory architecture: By deeply analyzing attention mechanisms, we' ve introduced a \"Hierarchical Memory\" approach. The KV Cache is now reimagined as a multi-level memory framework: Shallow layers act as Sensory Memory (events that just happened). Deep layers focus on Long-term Memory (frame-level semantic anchors). Middle layers bridge the gap with Working Memory. 2Ô∏è‚É£ Plug-and-play architecture: HERMES achieves highly efficient KV Cache reuse and optimization strategies including cross-layer memory smoothing and position re-indexing , delivering instant responses without the need for additional training, or auxiliary computations when user queries arrive. 3Ô∏è‚É£ Incredible efficiency and performance: ‚ö° Blazing speed: HERMES is 10x faster than previous SOTA in terms of response latency (TTFT)! üöÄ Compact efficiency: Even with 68% fewer video tokens, the model remains rock-solid, achieving up to 11.4% improvement in streaming comprehension tasks! üíæ Memory-friendly: No matter the video length, memory usage stays constant, leaving OOM errors in the past. üî• Join us in exploring this breakthrough: If you're passionate about streaming video understanding and efficient inference, we'd love to discuss and collaborate! üîç Explore the Details : üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/haowei-freesky/HERMES üåê Project: https://hermes-streaming.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14724",
      "pdf_url": "https://arxiv.org/pdf/2601.14724",
      "github_links": [
        "https://github.com/haowei-freesky/HERMES"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14724",
      "scraped_at": "2026-01-26T02:01:18.574988"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.16206",
    "authors": [],
    "stars": "81",
    "details": {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "Introducing LLM-in-Sandbox ‚Äî put your LLM in a virtual computer to unlock general agentic intelligence for non-code tasks! Significant gains for chemistry, long-context QA, instruction following, and more. No extra training needed. üåê Demo: https://llm-in-sandbox.github.io üíª Code: https://github.com/llm-in-sandbox/llm-in-sandbox pip install llm-in-sandbox Feel free to open issues or discussions  ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16206",
      "pdf_url": "https://arxiv.org/pdf/2601.16206",
      "github_links": [
        "https://github.com/llm-in-sandbox/llm-in-sandbox"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16206",
      "scraped_at": "2026-01-26T02:01:20.577739"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15165",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "abstract": "Links üìÑ paper: https://arxiv.org/abs/2601.15165 üè† project page: https://nzl-thu.github.io/the-flexibility-trap üíª code: https://github.com/LeapLabTHU/JustGRPO ü§ó model: https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15165",
      "pdf_url": "https://arxiv.org/pdf/2601.15165",
      "github_links": [
        "https://github.com/LeapLabTHU/JustGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15165",
      "scraped_at": "2026-01-26T02:01:22.559804"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "paper_url": "https://huggingface.co/papers/2601.15197",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "abstract": "üèóÔ∏è Architecture BayesianVLA is a novel framework designed to solve the Vision Shortcut problem in Vision-Language-Action (VLA) models. In current VLA training, goal-driven datasets often make language instructions highly predictable from visual observations alone. This leads to Information Collapse, where the model ignores language and degenerates into a vision-only policy, failing miserably in out-of-distribution (OOD) scenarios. BayesianVLA addresses this by: Bayesian Decomposition : Explicitly modeling a vision-only prior $p(a|v)$ and a language-conditioned posterior $\\pi(a|v, \\ell)$. LLR Optimization : Maximizing the Log-Likelihood Ratio (LLR) to penalize actions that rely solely on visual cues and reward actions that are truly grounded in language instructions. ‚ú® Key Features Dual-Branch Architecture : Uses learnable Latent Action Queries to decouple vision-only and language-conditioned action distributions. Zero Extra Data : Achieves significant performance gains (e.g., +11.3% on SimplerEnv) using the exact same datasets as baselines. Preserves VLM Intelligence : Effectively regularizes the model to prevent the \"catastrophic forgetting\" of general multimodal reasoning capabilities common in standard VLA fine-tuning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15197",
      "pdf_url": "https://arxiv.org/pdf/2601.15197",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/BayesianVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15197",
      "scraped_at": "2026-01-26T02:01:24.489273"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "paper_url": "https://huggingface.co/papers/2601.16208",
    "authors": [],
    "stars": "129",
    "details": {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "abstract": "We scale RAE to text-to-image, and its advantage over VAEs still holds!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16208",
      "pdf_url": "https://arxiv.org/pdf/2601.16208",
      "github_links": [
        "https://github.com/ZitengWangNYU/Scale-RAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16208",
      "scraped_at": "2026-01-26T02:01:26.455999"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "paper_url": "https://huggingface.co/papers/2601.15892",
    "authors": [],
    "stars": "28",
    "details": {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs (2025) LLaDA2.0: Scaling Up Diffusion Language Models to 100B (2025) WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference (2025) CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models (2026) SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding (2025) Fast and Accurate Causal Parallel Decoding using Jacobi Forcing (2025) Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15892",
      "pdf_url": "https://arxiv.org/pdf/2601.15892",
      "github_links": [
        "https://github.com/ByteDance-Seed/Stable-DiffCoder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15892",
      "scraped_at": "2026-01-26T02:01:28.374246"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "paper_url": "https://huggingface.co/papers/2601.16093",
    "authors": [],
    "stars": "1.51k",
    "details": {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Project page: https://zhouyiks.github.io/projects/SAMTok/ Training Code: https://github.com/bytedance/Sa2VA/tree/main/projects/samtok Short Bio:   We present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16093",
      "pdf_url": "https://arxiv.org/pdf/2601.16093",
      "github_links": [
        "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16093",
      "scraped_at": "2026-01-26T02:01:30.339469"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Learning to Discover at Test Time",
    "paper_url": "https://huggingface.co/papers/2601.16175",
    "authors": [],
    "stars": "163",
    "details": {
      "title": "Learning to Discover at Test Time",
      "abstract": "New paper on scientific discovery with test time training. New discoveries on several open scientific problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16175",
      "pdf_url": "https://arxiv.org/pdf/2601.16175",
      "github_links": [
        "https://github.com/test-time-training/discover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16175",
      "scraped_at": "2026-01-26T02:01:32.194965"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Qwen3-TTS Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.15621",
    "authors": [],
    "stars": "4.27k",
    "details": {
      "title": "Qwen3-TTS Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IndexTTS 2.5 Technical Report (2026) FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2026) DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec (2025) VoiceSculptor: Your Voice, Designed By You (2026) GLM-TTS Technical Report (2025) MiMo-Audio: Audio Language Models are Few-Shot Learners (2025) JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15621",
      "pdf_url": "https://arxiv.org/pdf/2601.15621",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-TTS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15621",
      "scraped_at": "2026-01-26T02:01:34.141141"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "paper_url": "https://huggingface.co/papers/2601.11868",
    "authors": [
      "Boxuan Li",
      "Nicholas Carlini",
      "Alexander G. Shaw",
      "Mike A. Merrill",
      "menorf"
    ],
    "stars": "1.41k",
    "details": {
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts (2026) Real-Time Procedural Learning From Experience for AI Agents (2025) Benchmarking LLM Agents for Wealth-Management Workflows (2025) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (2025) SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (2025) The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11868",
      "pdf_url": "https://arxiv.org/pdf/2601.11868",
      "github_links": [
        "https://github.com/laude-institute/terminal-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11868",
      "scraped_at": "2026-01-26T02:01:36.017971"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "paper_url": "https://huggingface.co/papers/2601.15727",
    "authors": [
      "Yixin Shen",
      "Haiming Wu",
      "Chi Hsu Tsai",
      "Peiyu Zang",
      "Yang Yu"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "Summary of Key Points Kernel quality is a fundamental bottleneck for modern AI system performance, yet high-quality kernel engineering is expert-intensive, time-consuming, and difficult to scale. Recent advances in large language models (LLMs) and LLM-based agents enable automated kernel generation and optimization by capturing expert knowledge and supporting iterative, feedback-driven optimization loops. Despite rapid progress, existing work is fragmented and lacks a unified, systematic perspective. This survey provides a structured overview of LLM-based kernel generation methods and agentic optimization workflows, and compiles the key datasets and benchmarks used for training and evaluation. The paper further identifies open challenges and outlines future research directions, aiming to serve as a comprehensive reference for next-generation automated kernel optimization. Resources Open-source repository tracking this field: https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15727",
      "pdf_url": "https://arxiv.org/pdf/2601.15727",
      "github_links": [
        "https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15727",
      "scraped_at": "2026-01-26T02:01:37.896798"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.15369",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "abstract": "Project Page: https://ucsc-vlaa.github.io/OpenVision3/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15369",
      "pdf_url": "https://arxiv.org/pdf/2601.15369",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15369",
      "scraped_at": "2026-01-26T02:01:39.825845"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.16125",
    "authors": [
      "Dingkun Long",
      "Zhuoning Guo",
      "Mingxin Li",
      "Yanzhao Zhang",
      "songtingyu"
    ],
    "stars": "1",
    "details": {
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "abstract": "A new benchmark for Composed Image Retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16125",
      "pdf_url": "https://arxiv.org/pdf/2601.16125",
      "github_links": [
        "https://github.com/SighingSnow/edir"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16125",
      "scraped_at": "2026-01-26T02:01:41.667094"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "paper_url": "https://huggingface.co/papers/2601.16163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "abstract": "Cosmos Policy fine-tunes a pretrained video model in one stage for visuomotor control, enabling action latent frames, future state prediction, and planning, achieving state-of-the-art robotic benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16163",
      "pdf_url": "https://arxiv.org/pdf/2601.16163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16163",
      "scraped_at": "2026-01-26T02:01:43.638260"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.16148",
    "authors": [],
    "stars": "91",
    "details": {
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "abstract": "ü§óTry it out: https://huggingface.co/spaces/facebook/ActionMesh üåêProject Page: https://remysabathier.github.io/actionmesh/ üìÑPaper: https://remysabathier.github.io/actionmesh/actionmesh_2026.pdf üíªCode: https://github.com/facebookresearch/actionmesh",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16148",
      "pdf_url": "https://arxiv.org/pdf/2601.16148",
      "github_links": [
        "https://github.com/facebookresearch/actionmesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16148",
      "scraped_at": "2026-01-26T02:01:45.631171"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "paper_url": "https://huggingface.co/papers/2601.14255",
    "authors": [],
    "stars": "108",
    "details": {
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "abstract": "Demo: https://huggingface.co/spaces/SammyLim/VideoMaMa Git: https://github.com/cvlab-kaist/VideoMaMa Project Page: https://cvlab-kaist.github.io/VideoMaMa/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14255",
      "pdf_url": "https://arxiv.org/pdf/2601.14255",
      "github_links": [
        "https://github.com/cvlab-kaist/VideoMaMa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14255",
      "scraped_at": "2026-01-26T02:01:47.579204"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15224",
    "authors": [
      "Dingcheng Wang",
      "Haoran Lu",
      "Haosen Sun",
      "Jianshu Zhang",
      "Raymond-Qiancx"
    ],
    "stars": "76",
    "details": {
      "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
      "abstract": "Towards General Progress Understanding for Embodied Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15224",
      "pdf_url": "https://arxiv.org/pdf/2601.15224",
      "github_links": [
        "https://github.com/ProgressLM/ProgressLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15224",
      "scraped_at": "2026-01-26T02:01:49.425233"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Agentic Uncertainty Quantification",
    "paper_url": "https://huggingface.co/papers/2601.15703",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Uncertainty Quantification",
      "abstract": "üõë Stop the \"Spiral of Hallucination\" in Autonomous Agents! Long-horizon agents often fail because minor early errors snowball into irreversible failures. We introduce Agentic Uncertainty Quantification (AUQ) , a training-free Dual-Process framework inspired by System 1/System 2 thinking: üß† System 1 (Fast): Uncertainty-Aware Memory propagates doubt to prevent blind commitment. ü§î System 2 (Slow): Triggers active reflection only when confidence drops below a specific threshold. Key Wins: ‚úÖ SOTA Performance: Outperforms ReAct & Reflexion on ALFWorld, WebShop, and the new DeepResearch Bench . ‚úÖ Efficiency: Prevents long, futile failure loops, making it more token-efficient than standard methods. ‚úÖ Plug-and-Play: No fine-tuning required. From \"Passive Diagnosis\" to \"Active Control\" ‚Äî make your agents reliable! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15703",
      "pdf_url": "https://arxiv.org/pdf/2601.15703",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15703",
      "scraped_at": "2026-01-26T02:01:51.336711"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
    "paper_url": "https://huggingface.co/papers/2601.16192",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
      "abstract": "360Anything lifts arbitrary perspective images and videos to seamless, gravity-aligned 360¬∞ panoramas, without using any camera or 3D information. Project page: https://360anything.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16192",
      "pdf_url": "https://arxiv.org/pdf/2601.16192",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16192",
      "scraped_at": "2026-01-26T02:01:53.349634"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Agentic Confidence Calibration",
    "paper_url": "https://huggingface.co/papers/2601.15778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Confidence Calibration",
      "abstract": "üéØ Don't let your Agents be \"Confidently Wrong\"! Traditional calibration works for static text, but Autonomous Agents fail differently‚Äîerrors compound over long trajectories. We introduce Holistic Trajectory Calibration (HTC) , a new paradigm to diagnose the entire execution process. Why it matters: üîç Process-Centric: Extracts rich features (Dynamics, Stability) from the agent's thinking process, not just the final output. üìà SOTA Calibration: Consistently outperforms baselines across 8 benchmarks (SimpleQA, Math500, etc.). üåç Generalization: We release the General Agent Calibrator (GAC) , which achieves the best zero-shot calibration on the challenging GAIA benchmark. Achieve Interpretability, Transferability, and Trust in your AI Agents. üõ°Ô∏è",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15778",
      "pdf_url": "https://arxiv.org/pdf/2601.15778",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15778",
      "scraped_at": "2026-01-26T02:01:55.177454"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15690",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "üó∫Ô∏è The 2026 Roadmap for Reliable AI: Making Uncertainty Actionable We are witnessing a paradigm shift in LLMs: Uncertainty is no longer just a passive score for diagnosis‚Äîit is evolving into an Active Control Signal for real-time decision-making. Our comprehensive survey covers this transformation across three frontiers: üß† Reasoning: Triggering self-correction & optimizing \"thinking budget\" (System 2). ü§ñ Agents: Determining when to use tools, ask for help, or stop generation. üéØ Alignment: Using uncertainty as an intrinsic reward to mitigate reward hacking in RLHF. If you are building Agents or Reasoning Models, this is the functional evolution you need to know. üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15690",
      "pdf_url": "https://arxiv.org/pdf/2601.15690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15690",
      "scraped_at": "2026-01-26T02:01:56.990537"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "paper_url": "https://huggingface.co/papers/2601.15549",
    "authors": [
      "Ryo Hachiuma",
      "Hideo Saito",
      "Ryo Fujii"
    ],
    "stars": "0",
    "details": {
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "abstract": "Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15549",
      "pdf_url": "https://arxiv.org/pdf/2601.15549",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15549",
      "scraped_at": "2026-01-26T02:01:58.859945"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "paper_url": "https://huggingface.co/papers/2601.16134",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LLM Prompt Evaluation for Educational Applications",
      "abstract": "Some of the observations founded are :- -- Prompt design matters as much as the model : The study shows that different prompt templates using the same LLM produce significantly different educational outcomes, proving prompt engineering is a critical lever in AI supported learning. -- Persona + Context Manager is the strongest combination : The Strategic Reading Coach prompt combining Persona and Context Manager patterns outperformed all others with 81‚Äì100% win probability, making it the most effective for follow up educational questions. -- Systematic prompt evaluation beats ad-hoc refinement : The tournament style evaluation using comparative judgment + Glicko2 ranking provides a reproducible, evidence based alternative to informal trial and error prompt tuning. -- Learning theory grounded prompts perform better : Prompts explicitly grounded in adult learning theory, self directed learning, and metacognition consistently generated higher quality educational dialogue than theory light designs -- Theoretical alignment alone is not enough : Some prompts rooted in strong learning theories (e.g. constructivism) still underperformed, highlighting that empirical evaluation is essential good theory must be paired with effective prompt patterns.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16134",
      "pdf_url": "https://arxiv.org/pdf/2601.16134",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16134",
      "scraped_at": "2026-01-26T02:02:00.656760"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "paper_url": "https://huggingface.co/papers/2601.16004",
    "authors": [
      "Cohaerence"
    ],
    "stars": "5",
    "details": {
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts. Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16004",
      "pdf_url": "https://arxiv.org/pdf/2601.16004",
      "github_links": [
        "https://github.com/christopher-altman/ibm-qml-kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16004",
      "scraped_at": "2026-01-26T02:02:02.464187"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "paper_url": "https://huggingface.co/papers/2601.15440",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "abstract": "In this work, we address the performance limitations often encountered in Python-based DLA simulations. By utilizing Numba for just-in-time compilation, we developed an implementation that achieves computational speeds comparable to legacy Fortran codes, offering a speedup over pure Python. We also validated the solver by analyzing the fractal dimension of the generated clusters (D‚âà1.71). We have released the code as a PyPI package named dla-ideal-solver to facilitate easier use and reproducibility. We hope this tool proves useful to those working in computational physics and complex systems, and we welcome any feedback from the community.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15440",
      "pdf_url": "https://arxiv.org/pdf/2601.15440",
      "github_links": [
        "https://github.com/sandyherho/dla-ideal-solver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15440",
      "scraped_at": "2026-01-26T02:02:05.381786"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "paper_url": "https://huggingface.co/papers/2601.08118",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
      "abstract": "The framework is open-sourced at https://github.com/SAP/mirrorbench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08118",
      "pdf_url": "https://arxiv.org/pdf/2601.08118",
      "github_links": [
        "https://github.com/SAP/mirrorbench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08118",
      "scraped_at": "2026-01-26T02:02:07.168371"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LongCat-Flash-Thinking-2601 Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.16725",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LongCat-Flash-Thinking-2601 Technical Report",
      "abstract": "this is informative.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16725",
      "pdf_url": "https://arxiv.org/pdf/2601.16725",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16725",
      "scraped_at": "2026-01-27T01:57:53.550979"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
    "paper_url": "https://huggingface.co/papers/2601.16746",
    "authors": [],
    "stars": "35",
    "details": {
      "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
      "abstract": "wcÔºånb",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16746",
      "pdf_url": "https://arxiv.org/pdf/2601.16746",
      "github_links": [
        "https://github.com/Ayanami1314/swe-pruner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16746",
      "scraped_at": "2026-01-27T01:57:55.428156"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "paper_url": "https://huggingface.co/papers/2601.14133",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
      "abstract": "TwinBrainVLA , a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14133",
      "pdf_url": "https://arxiv.org/pdf/2601.14133",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/TwinBrainVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14133",
      "scraped_at": "2026-01-27T01:57:57.312958"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
    "paper_url": "https://huggingface.co/papers/2601.16973",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
      "abstract": "We released VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents. We systematically study the brittleness of vision-language models in multi-step visual interaction, analyze how training choices shape behavior, and open-source the full benchmark, models, and trajectories. X: https://x.com/zwcolin/status/2015812327338287227 Project: https://visgym.github.io/ Paper: https://arxiv.org/abs/2601.16973 Code: https://github.com/visgym/VisGym Data & models: https://huggingface.co/VisGym",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16973",
      "pdf_url": "https://arxiv.org/pdf/2601.16973",
      "github_links": [
        "https://github.com/visgym/VIsGym",
        "https://github.com/visgym/VisGym"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16973",
      "scraped_at": "2026-01-27T01:57:59.237093"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "paper_url": "https://huggingface.co/papers/2601.16296",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Plenoptic Video Generation (2026) Spatia: Video Generation with Updatable Spatial Memory (2025) StoryMem: Multi-shot Long Video Storytelling with Memory (2025) Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) BulletTime: Decoupled Control of Time and Camera Pose for Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16296",
      "pdf_url": "https://arxiv.org/pdf/2601.16296",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16296",
      "scraped_at": "2026-01-27T01:58:01.102913"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "paper_url": "https://huggingface.co/papers/2601.15808",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "abstract": "The scaling law of verification in deep research agent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15808",
      "pdf_url": "https://arxiv.org/pdf/2601.15808",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15808",
      "scraped_at": "2026-01-27T01:58:02.949107"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "paper_url": "https://huggingface.co/papers/2601.14243",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
      "abstract": "This paper analyzes why existing FP8 reinforcement learning methods fail. It proposes Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization by eliminating training-inference mismatch.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14243",
      "pdf_url": "https://arxiv.org/pdf/2601.14243",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14243",
      "scraped_at": "2026-01-27T01:58:04.828615"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.16515",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
      "abstract": "In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72√ó inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16515",
      "pdf_url": "https://arxiv.org/pdf/2601.16515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16515",
      "scraped_at": "2026-01-27T01:58:06.714348"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "GameTalk: Training LLMs for Strategic Conversation",
    "paper_url": "https://huggingface.co/papers/2601.16276",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GameTalk: Training LLMs for Strategic Conversation",
      "abstract": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16276",
      "pdf_url": "https://arxiv.org/pdf/2601.16276",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16276",
      "scraped_at": "2026-01-27T01:58:08.550432"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
    "paper_url": "https://huggingface.co/papers/2601.07251",
    "authors": [
      "Jianwen Sun",
      "Yukang Feng",
      "Yibin Wang",
      "Chuanhao Li",
      "Zizhen Li"
    ],
    "stars": "18",
    "details": {
      "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
      "abstract": "https://github.com/leroy9472/MeepleLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07251",
      "pdf_url": "https://arxiv.org/pdf/2601.07251",
      "github_links": [
        "https://github.com/leroy9472/MeepleLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07251",
      "scraped_at": "2026-01-27T01:58:10.584314"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
    "paper_url": "https://huggingface.co/papers/2601.16344",
    "authors": [
      "Yongchan Kwon",
      "Federico Bianchi",
      "Harper Hua",
      "Junlin Wang",
      "Fan Nie"
    ],
    "stars": "0",
    "details": {
      "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems (2026) SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence (2025) AInsteinBench: Benchmarking Coding Agents on Scientific Repositories (2025) DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows (2025) HeurekaBench: A Benchmarking Framework for AI Co-scientist (2026) LongDA: Benchmarking LLM Agents for Long-Document Data Analysis (2026) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16344",
      "pdf_url": "https://arxiv.org/pdf/2601.16344",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16344",
      "scraped_at": "2026-01-27T01:58:12.422648"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
    "paper_url": "https://huggingface.co/papers/2601.16018",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "abstract": "Mecellem Models propose Turkish legal-domain encoders and decoders trained from scratch and via continual pre-training. ModernBERT-based encoders (112.7B tokens) achieve top-3 Turkish retrieval results with high production efficiency, while Qwen3-based decoders show 36.2% perplexity reduction on legal text. Models and datasets are released via Hugging Face to support reproducible and cost-effective legal NLP for Turkish and other low-resource languages.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16018",
      "pdf_url": "https://arxiv.org/pdf/2601.16018",
      "github_links": [
        "https://github.com/newmindai/mecellem-models"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16018",
      "scraped_at": "2026-01-27T01:58:14.317360"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
    "paper_url": "https://huggingface.co/papers/2601.16443",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Failure to Mastery: Generating Hard Samples for Tool-use Agents (2026) Training Versatile Coding Agents in Synthetic Environments (2025) One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents (2025) Dr. Zero: Self-Evolving Search Agents without Training Data (2026) NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents (2025) SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning (2025) AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16443",
      "pdf_url": "https://arxiv.org/pdf/2601.16443",
      "github_links": [
        "https://github.com/kanishkg/endless-terminals"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16443",
      "scraped_at": "2026-01-27T01:58:16.138892"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
    "paper_url": "https://huggingface.co/papers/2601.13606",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
      "abstract": "High-quality synthetic Chart data and strong Chart reasoning model.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13606",
      "pdf_url": "https://arxiv.org/pdf/2601.13606",
      "github_links": [
        "https://github.com/starriver030515/ChartVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13606",
      "scraped_at": "2026-01-27T01:58:18.182354"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.11258",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
      "abstract": "Let Your LLMs Use New Knowledge with ‚ÄúPaST‚Äù Skills Paper: https://arxiv.org/abs/2601.11258 Blog: https://past-blog.notion.site",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11258",
      "pdf_url": "https://arxiv.org/pdf/2601.11258",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11258",
      "scraped_at": "2026-01-27T01:58:20.042830"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
    "paper_url": "https://huggingface.co/papers/2601.15715",
    "authors": [
      "Yi R Fung",
      "Zongwei Lyu",
      "Zhitao He"
    ],
    "stars": "0",
    "details": {
      "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance (2026) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) CogDoc: Towards Unified thinking in Documents (2025) Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models (2025) Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR (2026) REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation (2025) R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15715",
      "pdf_url": "https://arxiv.org/pdf/2601.15715",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15715",
      "scraped_at": "2026-01-27T01:58:21.862502"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
    "paper_url": "https://huggingface.co/papers/2601.13118",
    "authors": [
      "Gabriele Bavota",
      "Rosalia Tufano",
      "Fiorella Zampetti",
      "Alessandro Midolo",
      "Devy1"
    ],
    "stars": "0",
    "details": {
      "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
      "abstract": ".",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13118",
      "pdf_url": "https://arxiv.org/pdf/2601.13118",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13118",
      "scraped_at": "2026-01-27T01:58:23.760413"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
    "paper_url": "https://huggingface.co/papers/2601.16451",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
      "abstract": "üöÄ VISTA-PATH is introduced as the first interactive segmentation foundation model for pathology. It advances computational pathology workflows by enabling more accurate, interpretable, and human-guided quantitative measurements. Key highlights include: 1Ô∏è‚É£ Large-scale training: Trained on over 1.6M image-mask-text pairs 2Ô∏è‚É£ State-of-the-art performance: Accurate segmentation across both in-distribution and out-of-distribution tissues 3Ô∏è‚É£ Interactive refinement: Outputs can be efficiently refined using bounding-box prompts with only a few active-learning steps 4Ô∏è‚É£ Deployment-ready: Fully integrated into https://tissuelab.org and available for immediate use üìÑ Read the arXiv preprint: https://arxiv.org/abs/2601.16451",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16451",
      "pdf_url": "https://arxiv.org/pdf/2601.16451",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16451",
      "scraped_at": "2026-01-27T01:58:25.717710"
    },
    "scraped_date": "2026-01-27"
  }
]