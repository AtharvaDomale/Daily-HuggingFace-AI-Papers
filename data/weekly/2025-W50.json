[
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "paper_url": "https://huggingface.co/papers/2512.10430",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
      "abstract": "T-pro 2.0 is an open-weight Russian LLM with hybrid reasoning and fast inference, released with datasets, benchmarks, and an optimized decoding pipeline to support reproducible research and practical applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10430",
      "pdf_url": "https://arxiv.org/pdf/2512.10430",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10430",
      "scraped_at": "2025-12-15T01:51:12.704072"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2512.10739",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
      "abstract": "Due to a user error, the abstract displayed in this paper contains some errors üò≠ (the abstract in the PDF is correct). The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO.  Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10739",
      "pdf_url": "https://arxiv.org/pdf/2512.10739",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10739",
      "scraped_at": "2025-12-15T01:51:14.519089"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "paper_url": "https://huggingface.co/papers/2512.10949",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "abstract": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1 . Model is released at https://huggingface.co/IvanTang/3DGen-R1 .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10949",
      "pdf_url": "https://arxiv.org/pdf/2512.10949",
      "github_links": [
        "https://github.com/Ivan-Tang-3D/3DGen-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10949",
      "scraped_at": "2025-12-15T01:51:16.372862"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "paper_url": "https://huggingface.co/papers/2512.10756",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
      "abstract": "We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10756",
      "pdf_url": "https://arxiv.org/pdf/2512.10756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10756",
      "scraped_at": "2025-12-15T01:51:18.171391"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.10534",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "abstract": "InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10534",
      "pdf_url": "https://arxiv.org/pdf/2512.10534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10534",
      "scraped_at": "2025-12-15T01:51:20.001218"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "paper_url": "https://huggingface.co/papers/2512.05439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BEAVER: An Efficient Deterministic LLM Verifier",
      "abstract": "BEAVER is the first practical framework to formally verify an LLM‚Äôs output distribution. It enables rigorous assessment and comparison beyond traditional sampling-based evaluation. BEAVER computes deterministic, sound bounds on the total probability that a model‚Äôs responses satisfy given specifications. Across popular benchmarks, it yields tight certificates for correctness, security, and privacy and scales efficiently to large open-weight LLMs (e.g., 70B).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05439",
      "pdf_url": "https://arxiv.org/pdf/2512.05439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05439",
      "scraped_at": "2025-12-15T01:51:21.929291"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2512.10881",
    "authors": [
      "Mingxi Xu",
      "DonaldLian",
      "weixia111111",
      "wzy27",
      "kehong"
    ],
    "stars": "0",
    "details": {
      "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
      "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10881",
      "pdf_url": "https://arxiv.org/pdf/2512.10881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10881",
      "scraped_at": "2025-12-15T01:51:23.926766"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "paper_url": "https://huggingface.co/papers/2512.08511",
    "authors": [
      "Qixiang Ye",
      "Fang Wan",
      "callsys",
      "ywenxi"
    ],
    "stars": "12",
    "details": {
      "title": "Thinking with Images via Self-Calling Agent",
      "abstract": "üß†üñºÔ∏è Vision-language models are getting smarter‚Äîbut also harder to train. Many recent systems ‚Äúthink with images,‚Äù weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually requires LLMs to reason across modalites. ‚ú® This paper introduces thinking-with-images-through-self-calling (sCoT) -- a simpler idea: let the model think in language, break problems into atomic steps, and call itself to solve them . Instead of mixing text and images throughout its reasoning, a main agent splits a visual problem into small pieces‚Äîlike reading text or spotting an object‚Äîand delegates them to lightweight subagents ü§ñ. These subagents are virtual copies of the same model that answer one focused visual question and return a short text response. The main agent then combines everything through pure language reasoning. üöÄ The result? Easier training and stronger performance. The sCoT-based model trained with end-to-end RL, named as SubagentVL , outperforms previous state-of-the-art methods on challenging high-resolution benchmarks (V* and HR-Bench) with less GPU hours. üëâ Bottom line: smarter visual reasoning doesn‚Äôt require more complex multimodal thinking‚Äî letting models reason in language and ask for help from its virtual replicas . Code is available at github repo . Paper is available at arxiv",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08511",
      "pdf_url": "https://arxiv.org/pdf/2512.08511",
      "github_links": [
        "https://github.com/YWenxi/think-with-images-through-self-calling"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08511",
      "scraped_at": "2025-12-15T01:51:25.763062"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "paper_url": "https://huggingface.co/papers/2512.10938",
    "authors": [
      "Zhuang Liu",
      "Mingjie Sun",
      "Jiachen Zhu",
      "TaiMingLu",
      "Fishloong"
    ],
    "stars": "47",
    "details": {
      "title": "Stronger Normalization-Free Transformers",
      "abstract": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x)=erf(Œ±x+s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10938",
      "pdf_url": "https://arxiv.org/pdf/2512.10938",
      "github_links": [
        "https://github.com/zlab-princeton/Derf"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10938",
      "scraped_at": "2025-12-15T01:51:27.569161"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10867",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
      "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark frameworkMiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10867",
      "pdf_url": "https://arxiv.org/pdf/2512.10867",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10867",
      "scraped_at": "2025-12-15T01:51:29.477912"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "paper_url": "https://huggingface.co/papers/2511.23386",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
      "abstract": "arXiv: https://arxiv.org/pdf/2511.23386 Overall Architecture",
      "arxiv_page_url": "https://arxiv.org/abs/2511.23386",
      "pdf_url": "https://arxiv.org/pdf/2511.23386",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.23386",
      "scraped_at": "2025-12-15T01:51:31.292465"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "paper_url": "https://huggingface.co/papers/2512.10959",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
      "abstract": "Project page: https://huggingface.co/spaces/prs-eth/stereospace_web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10959",
      "pdf_url": "https://arxiv.org/pdf/2512.10959",
      "github_links": [
        "https://github.com/prs-eth/stereospace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10959",
      "scraped_at": "2025-12-15T01:51:33.104448"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "paper_url": "https://huggingface.co/papers/2512.10675",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
      "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10675",
      "pdf_url": "https://arxiv.org/pdf/2512.10675",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10675",
      "scraped_at": "2025-12-15T01:51:34.910861"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "paper_url": "https://huggingface.co/papers/2512.10955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "abstract": "This work can isolate a specific attribute from any image and merge those selected attributes from multiple images into a coherent generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10955",
      "pdf_url": "https://arxiv.org/pdf/2512.10955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10955",
      "scraped_at": "2025-12-15T01:51:36.679902"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "paper_url": "https://huggingface.co/papers/2512.04537",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
      "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04537",
      "pdf_url": "https://arxiv.org/pdf/2512.04537",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04537",
      "scraped_at": "2025-12-15T01:51:38.611203"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "paper_url": "https://huggingface.co/papers/2512.10791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10791",
      "pdf_url": "https://arxiv.org/pdf/2512.10791",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10791",
      "scraped_at": "2025-12-15T01:51:40.432391"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "paper_url": "https://huggingface.co/papers/2512.09270",
    "authors": [
      "Won-Sik Cheong",
      "Geonho Kim",
      "shurek20",
      "klavna",
      "sangwoonkwak"
    ],
    "stars": "6",
    "details": {
      "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression (2025) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting (2025) MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting (2025) UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction (2025) StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video (2025) Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09270",
      "pdf_url": "https://arxiv.org/pdf/2512.09270",
      "github_links": [
        "https://github.com/CMLab-Korea/MoRel-arXiv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09270",
      "scraped_at": "2025-12-15T01:51:42.273910"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "paper_url": "https://huggingface.co/papers/2512.10359",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
      "abstract": "Tool-augmented VideoQA system, accepted by NeurIPS'25 main track.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10359",
      "pdf_url": "https://arxiv.org/pdf/2512.10359",
      "github_links": [
        "https://github.com/fansunqi/VideoTool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10359",
      "scraped_at": "2025-12-15T01:51:44.717904"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "paper_url": "https://huggingface.co/papers/2512.09924",
    "authors": [
      "SuaLily",
      "whluo",
      "Yanbiao",
      "LewisPan",
      "JacobYuan"
    ],
    "stars": "5",
    "details": {
      "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
      "abstract": "Code: https://github.com/Liuxinyv/ReViSE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09924",
      "pdf_url": "https://arxiv.org/pdf/2512.09924",
      "github_links": [
        "https://github.com/Liuxinyv/ReViSE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09924",
      "scraped_at": "2025-12-15T01:51:46.477889"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "paper_url": "https://huggingface.co/papers/2512.09406",
    "authors": [
      "Pei Yang",
      "Xiaokang Liu",
      "AnalMom",
      "yiren98",
      "HaiCi"
    ],
    "stars": "14",
    "details": {
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "abstract": "A framework to translate human object interaction (HOI) videos into grounded robot object interaction (ROI) videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09406",
      "pdf_url": "https://arxiv.org/pdf/2512.09406",
      "github_links": [
        "https://github.com/showlab/H2R-Grounder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09406",
      "scraped_at": "2025-12-15T01:51:48.269947"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "paper_url": "https://huggingface.co/papers/2512.08870",
    "authors": [
      "Xiaodong Gu",
      "Yuchao Qiu",
      "Xiang Chen",
      "lanqz7766",
      "YerbaPage"
    ],
    "stars": "0",
    "details": {
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "abstract": "Check this out!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08870",
      "pdf_url": "https://arxiv.org/pdf/2512.08870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08870",
      "scraped_at": "2025-12-15T01:51:50.042296"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "paper_url": "https://huggingface.co/papers/2512.10894",
    "authors": [
      "Jing Liao",
      "Yiran Xu",
      "Matthew Fisher",
      "Nanxuan Zhao",
      "Peiying Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
      "abstract": "We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10894",
      "pdf_url": "https://arxiv.org/pdf/2512.10894",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10894",
      "scraped_at": "2025-12-15T01:51:51.913914"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "paper_url": "https://huggingface.co/papers/2512.10398",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
      "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10398",
      "pdf_url": "https://arxiv.org/pdf/2512.10398",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10398",
      "scraped_at": "2025-12-15T01:51:53.696504"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "paper_url": "https://huggingface.co/papers/2512.09756",
    "authors": [
      "Fei Huang",
      "Ke Wang",
      "Yongbin-Li",
      "yuchuan123",
      "ChonghuaLiao"
    ],
    "stars": "0",
    "details": {
      "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
      "abstract": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09756",
      "pdf_url": "https://arxiv.org/pdf/2512.09756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09756",
      "scraped_at": "2025-12-15T01:51:55.466731"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "DragMesh: Interactive 3D Generation Made Easy",
    "paper_url": "https://huggingface.co/papers/2512.06424",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DragMesh: Interactive 3D Generation Made Easy",
      "abstract": "DragMesh enables real time, physically valid 3D object articulation by decoupling kinematic reasoning from motion generation and producing plausible motions via a dual quaternion based generative model.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06424",
      "pdf_url": "https://arxiv.org/pdf/2512.06424",
      "github_links": [
        "https://github.com/AIGeeksGroup/DragMesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06424",
      "scraped_at": "2025-12-15T01:51:57.292467"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "paper_url": "https://huggingface.co/papers/2512.08269",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08269",
      "pdf_url": "https://arxiv.org/pdf/2512.08269",
      "github_links": [
        "https://github.com/KEH0T0/EgoX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08269",
      "scraped_at": "2025-12-16T01:48:10.403542"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "paper_url": "https://huggingface.co/papers/2512.11558",
    "authors": [
      "Yanchao Li",
      "Junjie Zhao",
      "Jiaming Zhang",
      "Zhenyang Cai",
      "CocoNutZENG"
    ],
    "stars": "0",
    "details": {
      "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
      "abstract": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT , a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11558",
      "pdf_url": "https://arxiv.org/pdf/2512.11558",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11558",
      "scraped_at": "2025-12-16T01:48:12.307882"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "paper_url": "https://huggingface.co/papers/2512.11749",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11749",
      "pdf_url": "https://arxiv.org/pdf/2512.11749",
      "github_links": [
        "https://github.com/KlingTeam/SVG-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11749",
      "scraped_at": "2025-12-16T01:48:14.370200"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "paper_url": "https://huggingface.co/papers/2512.11799",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
      "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11799",
      "pdf_url": "https://arxiv.org/pdf/2512.11799",
      "github_links": [
        "https://github.com/Aleafy/V-RGBX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11799",
      "scraped_at": "2025-12-16T01:48:16.319310"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Sliding Window Attention Adaptation",
    "paper_url": "https://huggingface.co/papers/2512.10411",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Sliding Window Attention Adaptation",
      "abstract": "We propose a set of practical recipes that can let a full-attention LLM use sliding window attention to improve efficiency. For example, some can achieve nearly 100% acceleration of LLM long-context inference speed with 90% accuracy retainment; some can only achieve about 30% acceleration but with nearly 100% accuracy retainment. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10411",
      "pdf_url": "https://arxiv.org/pdf/2512.10411",
      "github_links": [
        "https://github.com/yuyijiong/sliding-window-attention-adaptation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10411",
      "scraped_at": "2025-12-16T01:48:18.230209"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
    "paper_url": "https://huggingface.co/papers/2512.11253",
    "authors": [],
    "stars": "206",
    "details": {
      "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
      "abstract": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11253",
      "pdf_url": "https://arxiv.org/pdf/2512.11253",
      "github_links": [
        "https://github.com/GVCLab/PersonaLive"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11253",
      "scraped_at": "2025-12-16T01:48:20.225617"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "paper_url": "https://huggingface.co/papers/2512.11464",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
      "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11464",
      "pdf_url": "https://arxiv.org/pdf/2512.11464",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11464",
      "scraped_at": "2025-12-16T01:48:22.165015"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.11792",
    "authors": [
      "Qifeng Chen",
      "Jingyuan Liu",
      "George Stoica",
      "Tim666",
      "sunfly"
    ],
    "stars": "0",
    "details": {
      "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
      "abstract": "We introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11792",
      "pdf_url": "https://arxiv.org/pdf/2512.11792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11792",
      "scraped_at": "2025-12-16T01:48:24.130045"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "paper_url": "https://huggingface.co/papers/2512.06818",
    "authors": [
      "Matheus Gadelha",
      "Daniel Rebain",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Jan Held"
    ],
    "stars": "273",
    "details": {
      "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
      "abstract": "MeshSplatting introduces a differentiable rendering approach that reconstructs connected, fully opaque triangle meshes for fast, memory efficient, high quality novel view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06818",
      "pdf_url": "https://arxiv.org/pdf/2512.06818",
      "github_links": [
        "https://github.com/meshsplatting/mesh-splatting"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06818",
      "scraped_at": "2025-12-16T01:48:26.152847"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
    "paper_url": "https://huggingface.co/papers/2512.10605",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
      "abstract": "A general-purpose robotic agent framework based on LLMs. The LLM can independently reason, plan, and execute actions to operate diverse robot types across various scenarios to complete unpredictable, complex tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10605",
      "pdf_url": "https://arxiv.org/pdf/2512.10605",
      "github_links": [
        "https://github.com/LegendLeoChen/LEO-RobotAgent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10605",
      "scraped_at": "2025-12-16T01:48:28.199459"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
    "paper_url": "https://huggingface.co/papers/2512.11150",
    "authors": [
      "elandy"
    ],
    "stars": "7",
    "details": {
      "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
      "abstract": "LLM-as-judge evals are convenient, but meaningful (fixable) failure modes lurk beneath the surface. CJE treats LLM-judge evaluation as a statistics problem: ‚Ä¢ calibrate a cheap judge to a small oracle slice of high-quality labels ‚Ä¢ quantify uncertainty ‚Ä¢ flag when the method is breaking On Chatbot Arena prompts, we match oracle-quality pairwise policy ranking (99%) while cutting oracle labeling cost by ~14√ó. If you run an eval pipeline: what are the most important failure modes you‚Äôve seen? I‚Äôd love to hear where this breaks first.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11150",
      "pdf_url": "https://arxiv.org/pdf/2512.11150",
      "github_links": [
        "https://github.com/cimo-labs/cje"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11150",
      "scraped_at": "2025-12-16T01:48:30.171508"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
    "paper_url": "https://huggingface.co/papers/2512.02901",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
      "abstract": "Is it possible to run LLMs at 2-bit with virtually NO loss in accuracy? ü§î No with Real numbers, but Yes with Complex ones! üöÄ Meet Fairy2i-W2(2bit): QAT from LLaMA-2 7B with Complex Phase quant PPL: 7.85 (vs FP16's 6.63) Accuracy: 62.00% (vs FP16's 64.72%) But isn't LLaMA real-valued? Yes, but we built a bridge. üåâ We prove a mathematical equivalence: Any real linear layer can be losslessly re-parameterized into a \"Widely-Linear Complex Form\". Which means no retraining needed! Another secret sauce: Recursive Residual Quantization. üéØ Instead of just quantize once.We also quantize the remaining error to wipe out noise. Best part? These stages are Data-Independent, so they run in PARALLEL. You get high accuracy with virtually NO latency penalty. But isn't Complex arithmetic slow?\" ü§îNot with Fairy2i.Since weights are quantized to the unit circle ${ \\pm 1, \\pm i }$, we achieve Multiplication-Free Inference.Heavy Matrix Muls turn into simple Adds, Subs, and Swaps. This efficiency is key for running LLMs on edge devices We've only scratched the surface (QAT on just 30B tokens) We believe with more training data, surpassing the full-precision model is just around the corner Resources: arXiv: https://arxiv.org/pdf/2512.02901 huggingface: https://huggingface.co/PKU-DS-LAB/Fairy2i-W2 github: https://github.com/PKULab1806/Fairy2i-W2",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02901",
      "pdf_url": "https://arxiv.org/pdf/2512.02901",
      "github_links": [
        "https://github.com/PKULab1806/Fairy2i-W2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02901",
      "scraped_at": "2025-12-16T01:48:32.083569"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
    "paper_url": "https://huggingface.co/papers/2512.11437",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
      "abstract": "First and largest multilingual trustworthiness benchmark for healthcare",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11437",
      "pdf_url": "https://arxiv.org/pdf/2512.11437",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11437",
      "scraped_at": "2025-12-16T01:48:33.973540"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
    "paper_url": "https://huggingface.co/papers/2512.06951",
    "authors": [
      "Akash Karnatak",
      "Gleb Zarin",
      "IliaLarchenko"
    ],
    "stars": "111",
    "details": {
      "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
      "abstract": "We present our 1st place solution to the 2025 NeurIPS BEHAVIOR Challenge, where a single Vision-Language-Action robotics policy is trained to perform 50 household manipulation tasks in a photorealistic simulator. The approach builds on Pi0.5 with several practical architecture, training, and inference modifications. We open-source the full solution, including code, model weights, and a detailed technical report, for anyone exploring or adapting VLAs to real tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06951",
      "pdf_url": "https://arxiv.org/pdf/2512.06951",
      "github_links": [
        "https://github.com/IliaLarchenko/behavior-1k-solution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06951",
      "scraped_at": "2025-12-16T01:48:36.073880"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
    "paper_url": "https://huggingface.co/papers/2512.11130",
    "authors": [],
    "stars": "45",
    "details": {
      "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
      "abstract": "A real-time foundation model for stereo depth estimation, which is crucial for robotics/humanoid 3D spatial perception.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11130",
      "pdf_url": "https://arxiv.org/pdf/2512.11130",
      "github_links": [
        "https://github.com/NVlabs/Fast-FoundationStereo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11130",
      "scraped_at": "2025-12-16T01:48:37.954375"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Scaling Behavior of Discrete Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10858",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Scaling Behavior of Discrete Diffusion Language Models",
      "abstract": "We scale diffusion language models up to 3B (masked and uniform diffusion) and 10B (uniform diffusion) parameters,  pre-trained on a pure diffusion objective (mixture of unconditional and conditional) via Nemotron-CC. ü§ñ GitHub: https://github.com/dvruette/gidd-easydel ü§ó Huggingface: https://huggingface.co/collections/dvruette/scaling-behavior-of-discrete-diffusion-language-models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10858",
      "pdf_url": "https://arxiv.org/pdf/2512.10858",
      "github_links": [
        "https://github.com/dvruette/gidd-easydel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10858",
      "scraped_at": "2025-12-16T01:48:39.888994"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
    "paper_url": "https://huggingface.co/papers/2512.10715",
    "authors": [
      "Enzo Ferrante",
      "Rodrigo Echeveste",
      "Nicolas Gaggion",
      "Matias Cosarinsky"
    ],
    "stars": "1",
    "details": {
      "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
      "abstract": "We present CheXmask-U , a framework for quantifying uncertainty in landmark-based anatomical segmentation models on chest X-rays and release the CheXmask-U dataset providing per-node uncertainty estimates to support research in robust and safe medical imaging.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10715",
      "pdf_url": "https://arxiv.org/pdf/2512.10715",
      "github_links": [
        "https://github.com/mcosarinsky/CheXmask-U"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10715",
      "scraped_at": "2025-12-16T01:48:41.791510"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
    "paper_url": "https://huggingface.co/papers/2512.11393",
    "authors": [
      "Dima Damen",
      "Yoichi Sato",
      "Yifei Huang",
      "Zhifan Zhu"
    ],
    "stars": "0",
    "details": {
      "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
      "abstract": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11393",
      "pdf_url": "https://arxiv.org/pdf/2512.11393",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11393",
      "scraped_at": "2025-12-16T01:48:43.710961"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "paper_url": "https://huggingface.co/papers/2512.10685",
    "authors": [],
    "stars": "139",
    "details": {
      "title": "Sharp Monocular View Synthesis in Less Than a Second",
      "abstract": "Sharp Monocular View Synthesis in Less Than a Second https://huggingface.co/papers/2512.10685 Real-time photorealistic view synthesis from a single image. Given a single photograph, regresses the parameters of a 3D Gaussian representation of the depicted scene. Synthesis in less than a second on a standard GPU via a single feedforward pass through a neural network. The synthesized representation is then rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Robust zero-shot generalization. SOTA on multiple datasets while lowering the synthesis time by three orders of magnitude. Learn mode at  and https://huggingface.co/apple/Sharp",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10685",
      "pdf_url": "https://arxiv.org/pdf/2512.10685",
      "github_links": [
        "https://github.com/apple/ml-sharp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10685",
      "scraped_at": "2025-12-16T01:48:45.671499"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
    "paper_url": "https://huggingface.co/papers/2512.10092",
    "authors": [
      "Neel Nanda",
      "Lewis Smith",
      "Lisa Dunlap",
      "Xiaoqing Sun",
      "Nick Jiang"
    ],
    "stars": "9",
    "details": {
      "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
      "abstract": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data. Project page: https://www.interp-embed.com Code: https://github.com/nickjiang2378/interp_embed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10092",
      "pdf_url": "https://arxiv.org/pdf/2512.10092",
      "github_links": [
        "https://github.com/nickjiang2378/interp_embed"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10092",
      "scraped_at": "2025-12-16T01:48:47.552453"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "paper_url": "https://huggingface.co/papers/2512.11798",
    "authors": [
      "Joan Lasenby",
      "Christian Rupprecht",
      "Chuanxia Zheng",
      "Yuxin Yao",
      "Ruining Li"
    ],
    "stars": "0",
    "details": {
      "title": "Particulate: Feed-Forward 3D Object Articulation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11798",
      "pdf_url": "https://arxiv.org/pdf/2512.11798",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11798",
      "scraped_at": "2025-12-16T01:48:49.433966"
    },
    "scraped_date": "2025-12-16"
  }
]