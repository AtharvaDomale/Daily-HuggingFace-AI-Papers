[
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "paper_url": "https://huggingface.co/papers/2512.10430",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
      "abstract": "T-pro 2.0 is an open-weight Russian LLM with hybrid reasoning and fast inference, released with datasets, benchmarks, and an optimized decoding pipeline to support reproducible research and practical applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10430",
      "pdf_url": "https://arxiv.org/pdf/2512.10430",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10430",
      "scraped_at": "2025-12-15T01:51:12.704072"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2512.10739",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
      "abstract": "Due to a user error, the abstract displayed in this paper contains some errors üò≠ (the abstract in the PDF is correct). The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO.  Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10739",
      "pdf_url": "https://arxiv.org/pdf/2512.10739",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10739",
      "scraped_at": "2025-12-15T01:51:14.519089"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "paper_url": "https://huggingface.co/papers/2512.10949",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "abstract": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1 . Model is released at https://huggingface.co/IvanTang/3DGen-R1 .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10949",
      "pdf_url": "https://arxiv.org/pdf/2512.10949",
      "github_links": [
        "https://github.com/Ivan-Tang-3D/3DGen-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10949",
      "scraped_at": "2025-12-15T01:51:16.372862"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "paper_url": "https://huggingface.co/papers/2512.10756",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
      "abstract": "We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10756",
      "pdf_url": "https://arxiv.org/pdf/2512.10756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10756",
      "scraped_at": "2025-12-15T01:51:18.171391"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.10534",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "abstract": "InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10534",
      "pdf_url": "https://arxiv.org/pdf/2512.10534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10534",
      "scraped_at": "2025-12-15T01:51:20.001218"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "paper_url": "https://huggingface.co/papers/2512.05439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BEAVER: An Efficient Deterministic LLM Verifier",
      "abstract": "BEAVER is the first practical framework to formally verify an LLM‚Äôs output distribution. It enables rigorous assessment and comparison beyond traditional sampling-based evaluation. BEAVER computes deterministic, sound bounds on the total probability that a model‚Äôs responses satisfy given specifications. Across popular benchmarks, it yields tight certificates for correctness, security, and privacy and scales efficiently to large open-weight LLMs (e.g., 70B).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05439",
      "pdf_url": "https://arxiv.org/pdf/2512.05439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05439",
      "scraped_at": "2025-12-15T01:51:21.929291"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2512.10881",
    "authors": [
      "Mingxi Xu",
      "DonaldLian",
      "weixia111111",
      "wzy27",
      "kehong"
    ],
    "stars": "0",
    "details": {
      "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
      "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10881",
      "pdf_url": "https://arxiv.org/pdf/2512.10881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10881",
      "scraped_at": "2025-12-15T01:51:23.926766"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "paper_url": "https://huggingface.co/papers/2512.08511",
    "authors": [
      "Qixiang Ye",
      "Fang Wan",
      "callsys",
      "ywenxi"
    ],
    "stars": "12",
    "details": {
      "title": "Thinking with Images via Self-Calling Agent",
      "abstract": "üß†üñºÔ∏è Vision-language models are getting smarter‚Äîbut also harder to train. Many recent systems ‚Äúthink with images,‚Äù weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually requires LLMs to reason across modalites. ‚ú® This paper introduces thinking-with-images-through-self-calling (sCoT) -- a simpler idea: let the model think in language, break problems into atomic steps, and call itself to solve them . Instead of mixing text and images throughout its reasoning, a main agent splits a visual problem into small pieces‚Äîlike reading text or spotting an object‚Äîand delegates them to lightweight subagents ü§ñ. These subagents are virtual copies of the same model that answer one focused visual question and return a short text response. The main agent then combines everything through pure language reasoning. üöÄ The result? Easier training and stronger performance. The sCoT-based model trained with end-to-end RL, named as SubagentVL , outperforms previous state-of-the-art methods on challenging high-resolution benchmarks (V* and HR-Bench) with less GPU hours. üëâ Bottom line: smarter visual reasoning doesn‚Äôt require more complex multimodal thinking‚Äî letting models reason in language and ask for help from its virtual replicas . Code is available at github repo . Paper is available at arxiv",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08511",
      "pdf_url": "https://arxiv.org/pdf/2512.08511",
      "github_links": [
        "https://github.com/YWenxi/think-with-images-through-self-calling"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08511",
      "scraped_at": "2025-12-15T01:51:25.763062"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "paper_url": "https://huggingface.co/papers/2512.10938",
    "authors": [
      "Zhuang Liu",
      "Mingjie Sun",
      "Jiachen Zhu",
      "TaiMingLu",
      "Fishloong"
    ],
    "stars": "47",
    "details": {
      "title": "Stronger Normalization-Free Transformers",
      "abstract": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x)=erf(Œ±x+s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10938",
      "pdf_url": "https://arxiv.org/pdf/2512.10938",
      "github_links": [
        "https://github.com/zlab-princeton/Derf"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10938",
      "scraped_at": "2025-12-15T01:51:27.569161"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10867",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
      "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark frameworkMiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10867",
      "pdf_url": "https://arxiv.org/pdf/2512.10867",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10867",
      "scraped_at": "2025-12-15T01:51:29.477912"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "paper_url": "https://huggingface.co/papers/2511.23386",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
      "abstract": "arXiv: https://arxiv.org/pdf/2511.23386 Overall Architecture",
      "arxiv_page_url": "https://arxiv.org/abs/2511.23386",
      "pdf_url": "https://arxiv.org/pdf/2511.23386",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.23386",
      "scraped_at": "2025-12-15T01:51:31.292465"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "paper_url": "https://huggingface.co/papers/2512.10959",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
      "abstract": "Project page: https://huggingface.co/spaces/prs-eth/stereospace_web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10959",
      "pdf_url": "https://arxiv.org/pdf/2512.10959",
      "github_links": [
        "https://github.com/prs-eth/stereospace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10959",
      "scraped_at": "2025-12-15T01:51:33.104448"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "paper_url": "https://huggingface.co/papers/2512.10675",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
      "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10675",
      "pdf_url": "https://arxiv.org/pdf/2512.10675",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10675",
      "scraped_at": "2025-12-15T01:51:34.910861"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "paper_url": "https://huggingface.co/papers/2512.10955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "abstract": "This work can isolate a specific attribute from any image and merge those selected attributes from multiple images into a coherent generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10955",
      "pdf_url": "https://arxiv.org/pdf/2512.10955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10955",
      "scraped_at": "2025-12-15T01:51:36.679902"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "paper_url": "https://huggingface.co/papers/2512.04537",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
      "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04537",
      "pdf_url": "https://arxiv.org/pdf/2512.04537",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04537",
      "scraped_at": "2025-12-15T01:51:38.611203"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "paper_url": "https://huggingface.co/papers/2512.10791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10791",
      "pdf_url": "https://arxiv.org/pdf/2512.10791",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10791",
      "scraped_at": "2025-12-15T01:51:40.432391"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "paper_url": "https://huggingface.co/papers/2512.09270",
    "authors": [
      "Won-Sik Cheong",
      "Geonho Kim",
      "shurek20",
      "klavna",
      "sangwoonkwak"
    ],
    "stars": "6",
    "details": {
      "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression (2025) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting (2025) MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting (2025) UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction (2025) StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video (2025) Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09270",
      "pdf_url": "https://arxiv.org/pdf/2512.09270",
      "github_links": [
        "https://github.com/CMLab-Korea/MoRel-arXiv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09270",
      "scraped_at": "2025-12-15T01:51:42.273910"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "paper_url": "https://huggingface.co/papers/2512.10359",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
      "abstract": "Tool-augmented VideoQA system, accepted by NeurIPS'25 main track.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10359",
      "pdf_url": "https://arxiv.org/pdf/2512.10359",
      "github_links": [
        "https://github.com/fansunqi/VideoTool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10359",
      "scraped_at": "2025-12-15T01:51:44.717904"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "paper_url": "https://huggingface.co/papers/2512.09924",
    "authors": [
      "SuaLily",
      "whluo",
      "Yanbiao",
      "LewisPan",
      "JacobYuan"
    ],
    "stars": "5",
    "details": {
      "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
      "abstract": "Code: https://github.com/Liuxinyv/ReViSE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09924",
      "pdf_url": "https://arxiv.org/pdf/2512.09924",
      "github_links": [
        "https://github.com/Liuxinyv/ReViSE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09924",
      "scraped_at": "2025-12-15T01:51:46.477889"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "paper_url": "https://huggingface.co/papers/2512.09406",
    "authors": [
      "Pei Yang",
      "Xiaokang Liu",
      "AnalMom",
      "yiren98",
      "HaiCi"
    ],
    "stars": "14",
    "details": {
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "abstract": "A framework to translate human object interaction (HOI) videos into grounded robot object interaction (ROI) videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09406",
      "pdf_url": "https://arxiv.org/pdf/2512.09406",
      "github_links": [
        "https://github.com/showlab/H2R-Grounder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09406",
      "scraped_at": "2025-12-15T01:51:48.269947"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "paper_url": "https://huggingface.co/papers/2512.08870",
    "authors": [
      "Xiaodong Gu",
      "Yuchao Qiu",
      "Xiang Chen",
      "lanqz7766",
      "YerbaPage"
    ],
    "stars": "0",
    "details": {
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "abstract": "Check this out!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08870",
      "pdf_url": "https://arxiv.org/pdf/2512.08870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08870",
      "scraped_at": "2025-12-15T01:51:50.042296"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "paper_url": "https://huggingface.co/papers/2512.10894",
    "authors": [
      "Jing Liao",
      "Yiran Xu",
      "Matthew Fisher",
      "Nanxuan Zhao",
      "Peiying Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
      "abstract": "We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10894",
      "pdf_url": "https://arxiv.org/pdf/2512.10894",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10894",
      "scraped_at": "2025-12-15T01:51:51.913914"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "paper_url": "https://huggingface.co/papers/2512.10398",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
      "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10398",
      "pdf_url": "https://arxiv.org/pdf/2512.10398",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10398",
      "scraped_at": "2025-12-15T01:51:53.696504"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "paper_url": "https://huggingface.co/papers/2512.09756",
    "authors": [
      "Fei Huang",
      "Ke Wang",
      "Yongbin-Li",
      "yuchuan123",
      "ChonghuaLiao"
    ],
    "stars": "0",
    "details": {
      "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
      "abstract": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09756",
      "pdf_url": "https://arxiv.org/pdf/2512.09756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09756",
      "scraped_at": "2025-12-15T01:51:55.466731"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "DragMesh: Interactive 3D Generation Made Easy",
    "paper_url": "https://huggingface.co/papers/2512.06424",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DragMesh: Interactive 3D Generation Made Easy",
      "abstract": "DragMesh enables real time, physically valid 3D object articulation by decoupling kinematic reasoning from motion generation and producing plausible motions via a dual quaternion based generative model.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06424",
      "pdf_url": "https://arxiv.org/pdf/2512.06424",
      "github_links": [
        "https://github.com/AIGeeksGroup/DragMesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06424",
      "scraped_at": "2025-12-15T01:51:57.292467"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "paper_url": "https://huggingface.co/papers/2512.08269",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08269",
      "pdf_url": "https://arxiv.org/pdf/2512.08269",
      "github_links": [
        "https://github.com/KEH0T0/EgoX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08269",
      "scraped_at": "2025-12-16T01:48:10.403542"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "paper_url": "https://huggingface.co/papers/2512.11558",
    "authors": [
      "Yanchao Li",
      "Junjie Zhao",
      "Jiaming Zhang",
      "Zhenyang Cai",
      "CocoNutZENG"
    ],
    "stars": "0",
    "details": {
      "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
      "abstract": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT , a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11558",
      "pdf_url": "https://arxiv.org/pdf/2512.11558",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11558",
      "scraped_at": "2025-12-16T01:48:12.307882"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "paper_url": "https://huggingface.co/papers/2512.11749",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11749",
      "pdf_url": "https://arxiv.org/pdf/2512.11749",
      "github_links": [
        "https://github.com/KlingTeam/SVG-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11749",
      "scraped_at": "2025-12-16T01:48:14.370200"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "paper_url": "https://huggingface.co/papers/2512.11799",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
      "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11799",
      "pdf_url": "https://arxiv.org/pdf/2512.11799",
      "github_links": [
        "https://github.com/Aleafy/V-RGBX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11799",
      "scraped_at": "2025-12-16T01:48:16.319310"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Sliding Window Attention Adaptation",
    "paper_url": "https://huggingface.co/papers/2512.10411",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Sliding Window Attention Adaptation",
      "abstract": "We propose a set of practical recipes that can let a full-attention LLM use sliding window attention to improve efficiency. For example, some can achieve nearly 100% acceleration of LLM long-context inference speed with 90% accuracy retainment; some can only achieve about 30% acceleration but with nearly 100% accuracy retainment. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10411",
      "pdf_url": "https://arxiv.org/pdf/2512.10411",
      "github_links": [
        "https://github.com/yuyijiong/sliding-window-attention-adaptation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10411",
      "scraped_at": "2025-12-16T01:48:18.230209"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
    "paper_url": "https://huggingface.co/papers/2512.11253",
    "authors": [],
    "stars": "206",
    "details": {
      "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
      "abstract": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11253",
      "pdf_url": "https://arxiv.org/pdf/2512.11253",
      "github_links": [
        "https://github.com/GVCLab/PersonaLive"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11253",
      "scraped_at": "2025-12-16T01:48:20.225617"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "paper_url": "https://huggingface.co/papers/2512.11464",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
      "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11464",
      "pdf_url": "https://arxiv.org/pdf/2512.11464",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11464",
      "scraped_at": "2025-12-16T01:48:22.165015"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.11792",
    "authors": [
      "Qifeng Chen",
      "Jingyuan Liu",
      "George Stoica",
      "Tim666",
      "sunfly"
    ],
    "stars": "0",
    "details": {
      "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
      "abstract": "We introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11792",
      "pdf_url": "https://arxiv.org/pdf/2512.11792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11792",
      "scraped_at": "2025-12-16T01:48:24.130045"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "paper_url": "https://huggingface.co/papers/2512.06818",
    "authors": [
      "Matheus Gadelha",
      "Daniel Rebain",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Jan Held"
    ],
    "stars": "273",
    "details": {
      "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
      "abstract": "MeshSplatting introduces a differentiable rendering approach that reconstructs connected, fully opaque triangle meshes for fast, memory efficient, high quality novel view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06818",
      "pdf_url": "https://arxiv.org/pdf/2512.06818",
      "github_links": [
        "https://github.com/meshsplatting/mesh-splatting"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06818",
      "scraped_at": "2025-12-16T01:48:26.152847"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
    "paper_url": "https://huggingface.co/papers/2512.10605",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
      "abstract": "A general-purpose robotic agent framework based on LLMs. The LLM can independently reason, plan, and execute actions to operate diverse robot types across various scenarios to complete unpredictable, complex tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10605",
      "pdf_url": "https://arxiv.org/pdf/2512.10605",
      "github_links": [
        "https://github.com/LegendLeoChen/LEO-RobotAgent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10605",
      "scraped_at": "2025-12-16T01:48:28.199459"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
    "paper_url": "https://huggingface.co/papers/2512.11150",
    "authors": [
      "elandy"
    ],
    "stars": "7",
    "details": {
      "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
      "abstract": "LLM-as-judge evals are convenient, but meaningful (fixable) failure modes lurk beneath the surface. CJE treats LLM-judge evaluation as a statistics problem: ‚Ä¢ calibrate a cheap judge to a small oracle slice of high-quality labels ‚Ä¢ quantify uncertainty ‚Ä¢ flag when the method is breaking On Chatbot Arena prompts, we match oracle-quality pairwise policy ranking (99%) while cutting oracle labeling cost by ~14√ó. If you run an eval pipeline: what are the most important failure modes you‚Äôve seen? I‚Äôd love to hear where this breaks first.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11150",
      "pdf_url": "https://arxiv.org/pdf/2512.11150",
      "github_links": [
        "https://github.com/cimo-labs/cje"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11150",
      "scraped_at": "2025-12-16T01:48:30.171508"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
    "paper_url": "https://huggingface.co/papers/2512.02901",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
      "abstract": "Is it possible to run LLMs at 2-bit with virtually NO loss in accuracy? ü§î No with Real numbers, but Yes with Complex ones! üöÄ Meet Fairy2i-W2(2bit): QAT from LLaMA-2 7B with Complex Phase quant PPL: 7.85 (vs FP16's 6.63) Accuracy: 62.00% (vs FP16's 64.72%) But isn't LLaMA real-valued? Yes, but we built a bridge. üåâ We prove a mathematical equivalence: Any real linear layer can be losslessly re-parameterized into a \"Widely-Linear Complex Form\". Which means no retraining needed! Another secret sauce: Recursive Residual Quantization. üéØ Instead of just quantize once.We also quantize the remaining error to wipe out noise. Best part? These stages are Data-Independent, so they run in PARALLEL. You get high accuracy with virtually NO latency penalty. But isn't Complex arithmetic slow?\" ü§îNot with Fairy2i.Since weights are quantized to the unit circle ${ \\pm 1, \\pm i }$, we achieve Multiplication-Free Inference.Heavy Matrix Muls turn into simple Adds, Subs, and Swaps. This efficiency is key for running LLMs on edge devices We've only scratched the surface (QAT on just 30B tokens) We believe with more training data, surpassing the full-precision model is just around the corner Resources: arXiv: https://arxiv.org/pdf/2512.02901 huggingface: https://huggingface.co/PKU-DS-LAB/Fairy2i-W2 github: https://github.com/PKULab1806/Fairy2i-W2",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02901",
      "pdf_url": "https://arxiv.org/pdf/2512.02901",
      "github_links": [
        "https://github.com/PKULab1806/Fairy2i-W2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02901",
      "scraped_at": "2025-12-16T01:48:32.083569"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
    "paper_url": "https://huggingface.co/papers/2512.11437",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
      "abstract": "First and largest multilingual trustworthiness benchmark for healthcare",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11437",
      "pdf_url": "https://arxiv.org/pdf/2512.11437",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11437",
      "scraped_at": "2025-12-16T01:48:33.973540"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
    "paper_url": "https://huggingface.co/papers/2512.06951",
    "authors": [
      "Akash Karnatak",
      "Gleb Zarin",
      "IliaLarchenko"
    ],
    "stars": "111",
    "details": {
      "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
      "abstract": "We present our 1st place solution to the 2025 NeurIPS BEHAVIOR Challenge, where a single Vision-Language-Action robotics policy is trained to perform 50 household manipulation tasks in a photorealistic simulator. The approach builds on Pi0.5 with several practical architecture, training, and inference modifications. We open-source the full solution, including code, model weights, and a detailed technical report, for anyone exploring or adapting VLAs to real tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06951",
      "pdf_url": "https://arxiv.org/pdf/2512.06951",
      "github_links": [
        "https://github.com/IliaLarchenko/behavior-1k-solution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06951",
      "scraped_at": "2025-12-16T01:48:36.073880"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
    "paper_url": "https://huggingface.co/papers/2512.11130",
    "authors": [],
    "stars": "45",
    "details": {
      "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
      "abstract": "A real-time foundation model for stereo depth estimation, which is crucial for robotics/humanoid 3D spatial perception.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11130",
      "pdf_url": "https://arxiv.org/pdf/2512.11130",
      "github_links": [
        "https://github.com/NVlabs/Fast-FoundationStereo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11130",
      "scraped_at": "2025-12-16T01:48:37.954375"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Scaling Behavior of Discrete Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10858",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Scaling Behavior of Discrete Diffusion Language Models",
      "abstract": "We scale diffusion language models up to 3B (masked and uniform diffusion) and 10B (uniform diffusion) parameters,  pre-trained on a pure diffusion objective (mixture of unconditional and conditional) via Nemotron-CC. ü§ñ GitHub: https://github.com/dvruette/gidd-easydel ü§ó Huggingface: https://huggingface.co/collections/dvruette/scaling-behavior-of-discrete-diffusion-language-models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10858",
      "pdf_url": "https://arxiv.org/pdf/2512.10858",
      "github_links": [
        "https://github.com/dvruette/gidd-easydel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10858",
      "scraped_at": "2025-12-16T01:48:39.888994"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
    "paper_url": "https://huggingface.co/papers/2512.10715",
    "authors": [
      "Enzo Ferrante",
      "Rodrigo Echeveste",
      "Nicolas Gaggion",
      "Matias Cosarinsky"
    ],
    "stars": "1",
    "details": {
      "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
      "abstract": "We present CheXmask-U , a framework for quantifying uncertainty in landmark-based anatomical segmentation models on chest X-rays and release the CheXmask-U dataset providing per-node uncertainty estimates to support research in robust and safe medical imaging.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10715",
      "pdf_url": "https://arxiv.org/pdf/2512.10715",
      "github_links": [
        "https://github.com/mcosarinsky/CheXmask-U"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10715",
      "scraped_at": "2025-12-16T01:48:41.791510"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
    "paper_url": "https://huggingface.co/papers/2512.11393",
    "authors": [
      "Dima Damen",
      "Yoichi Sato",
      "Yifei Huang",
      "Zhifan Zhu"
    ],
    "stars": "0",
    "details": {
      "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
      "abstract": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11393",
      "pdf_url": "https://arxiv.org/pdf/2512.11393",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11393",
      "scraped_at": "2025-12-16T01:48:43.710961"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "paper_url": "https://huggingface.co/papers/2512.10685",
    "authors": [],
    "stars": "139",
    "details": {
      "title": "Sharp Monocular View Synthesis in Less Than a Second",
      "abstract": "Sharp Monocular View Synthesis in Less Than a Second https://huggingface.co/papers/2512.10685 Real-time photorealistic view synthesis from a single image. Given a single photograph, regresses the parameters of a 3D Gaussian representation of the depicted scene. Synthesis in less than a second on a standard GPU via a single feedforward pass through a neural network. The synthesized representation is then rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Robust zero-shot generalization. SOTA on multiple datasets while lowering the synthesis time by three orders of magnitude. Learn mode at  and https://huggingface.co/apple/Sharp",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10685",
      "pdf_url": "https://arxiv.org/pdf/2512.10685",
      "github_links": [
        "https://github.com/apple/ml-sharp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10685",
      "scraped_at": "2025-12-16T01:48:45.671499"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
    "paper_url": "https://huggingface.co/papers/2512.10092",
    "authors": [
      "Neel Nanda",
      "Lewis Smith",
      "Lisa Dunlap",
      "Xiaoqing Sun",
      "Nick Jiang"
    ],
    "stars": "9",
    "details": {
      "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
      "abstract": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data. Project page: https://www.interp-embed.com Code: https://github.com/nickjiang2378/interp_embed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10092",
      "pdf_url": "https://arxiv.org/pdf/2512.10092",
      "github_links": [
        "https://github.com/nickjiang2378/interp_embed"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10092",
      "scraped_at": "2025-12-16T01:48:47.552453"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "paper_url": "https://huggingface.co/papers/2512.11798",
    "authors": [
      "Joan Lasenby",
      "Christian Rupprecht",
      "Chuanxia Zheng",
      "Yuxin Yao",
      "Ruining Li"
    ],
    "stars": "0",
    "details": {
      "title": "Particulate: Feed-Forward 3D Object Articulation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11798",
      "pdf_url": "https://arxiv.org/pdf/2512.11798",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11798",
      "scraped_at": "2025-12-16T01:48:49.433966"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "paper_url": "https://huggingface.co/papers/2512.13586",
    "authors": [
      "Chongxuan Li",
      "Wei Wu",
      "Jian Guan",
      "JinaLeejnl"
    ],
    "stars": "18",
    "details": {
      "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
      "abstract": "ReFusion is a masked diffusion model that achieves superior performance and efficiency, featuring full KV cache reuse while simultaneously supporting any-order generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13586",
      "pdf_url": "https://arxiv.org/pdf/2512.13586",
      "github_links": [
        "https://github.com/ML-GSAI/ReFusion"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13586",
      "scraped_at": "2025-12-17T01:43:44.759416"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
    "paper_url": "https://huggingface.co/papers/2512.13687",
    "authors": [],
    "stars": "92",
    "details": {
      "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
      "abstract": "GitHub codes: https://github.com/MiniMax-AI/VTP Huggingface weights: https://huggingface.co/collections/MiniMaxAI/vtp collaborated with HUST Vision Lab: https://github.com/hustvl",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13687",
      "pdf_url": "https://arxiv.org/pdf/2512.13687",
      "github_links": [
        "https://github.com/hustvl",
        "https://github.com/MiniMax-AI/VTP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13687",
      "scraped_at": "2025-12-17T01:43:46.637297"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Memory in the Age of AI Agents",
    "paper_url": "https://huggingface.co/papers/2512.13564",
    "authors": [
      "Jeryi",
      "zstanjj",
      "KYLN24",
      "Liusc2020",
      "namespace-ERI"
    ],
    "stars": "115",
    "details": {
      "title": "Memory in the Age of AI Agents",
      "abstract": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13564",
      "pdf_url": "https://arxiv.org/pdf/2512.13564",
      "github_links": [
        "https://github.com/Shichun-Liu/Agent-Memory-Paper-List"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13564",
      "scraped_at": "2025-12-17T01:43:48.475578"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "paper_url": "https://huggingface.co/papers/2512.12967",
    "authors": [],
    "stars": "312",
    "details": {
      "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
      "abstract": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12967",
      "pdf_url": "https://arxiv.org/pdf/2512.12967",
      "github_links": [
        "https://github.com/Tongyi-Zhiwen/Qwen-Doc",
        "https://github.com/Tongyi-Zhiwen/Qwen-Doc/tree/main/QwenLong-L1.5"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12967",
      "scraped_at": "2025-12-17T01:43:50.394126"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "paper_url": "https://huggingface.co/papers/2512.13604",
    "authors": [
      "Xian Liu",
      "Zhaoxi Chen",
      "Jianxiong Gao",
      "ChenyangSi",
      "JunhaoZhuang"
    ],
    "stars": "0",
    "details": {
      "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
      "abstract": "Page: https://vchitect.github.io/LongVie2-project/ Github: https://github.com/Vchitect/LongVie Huggingface: https://huggingface.co/Vchitect/LongVie2",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13604",
      "pdf_url": "https://arxiv.org/pdf/2512.13604",
      "github_links": [
        "https://github.com/Vchitect/LongVie"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13604",
      "scraped_at": "2025-12-17T01:43:52.206654"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "paper_url": "https://huggingface.co/papers/2512.13168",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
      "abstract": "Real-world F&A work is messy, spanning heterogeneous and large-scale artifacts such as spreadsheets and PDFs. It's also long-horizon and knowledge-intensive: workflows interleave multiple tasks and span diverse domains such as budgeting, trading, asset management, and operations. The workflows are derived from real-world enterprise workspaces (primarily Enron, as well as corporations in the EUSES Corpus, investment and securities companies, World Bank, Canadian/British government agencies, and more).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13168",
      "pdf_url": "https://arxiv.org/pdf/2512.13168",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13168",
      "scraped_at": "2025-12-17T01:43:54.033557"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "paper_url": "https://huggingface.co/papers/2512.12730",
    "authors": [
      "yo37",
      "kkish",
      "YueHou",
      "coffiney",
      "JingzheDing"
    ],
    "stars": "25",
    "details": {
      "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
      "abstract": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents. https://github.com/multimodal-art-projection/NL2RepoBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12730",
      "pdf_url": "https://arxiv.org/pdf/2512.12730",
      "github_links": [
        "https://github.com/multimodal-art-projection/NL2RepoBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12730",
      "scraped_at": "2025-12-17T01:43:55.846055"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
    "paper_url": "https://huggingface.co/papers/2512.12602",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
      "abstract": "Error-Free Linear Attention is a Free Lunch!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12602",
      "pdf_url": "https://arxiv.org/pdf/2512.12602",
      "github_links": [
        "https://github.com/declare-lab/EFLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12602",
      "scraped_at": "2025-12-17T01:43:57.608577"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "KlingAvatar 2.0 Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.13313",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "KlingAvatar 2.0 Technical Report",
      "abstract": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13313",
      "pdf_url": "https://arxiv.org/pdf/2512.13313",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13313",
      "scraped_at": "2025-12-17T01:43:59.445920"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
    "paper_url": "https://huggingface.co/papers/2512.09636",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09636",
      "pdf_url": "https://arxiv.org/pdf/2512.09636",
      "github_links": [
        "https://github.com/elsa66666/MentraSuite"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09636",
      "scraped_at": "2025-12-17T01:44:01.296073"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
    "paper_url": "https://huggingface.co/papers/2512.10071",
    "authors": [
      "Jinwei Gu",
      "Qizhi Chen",
      "Yu-Wei Chao",
      "Junjie Bai",
      "delinqu"
    ],
    "stars": "148",
    "details": {
      "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
      "abstract": "OpenPi Comet is the submission of Team Comet for the 2025 BEHAVIOR Challenge . We provides a unified framework for pre-training, post-training, data generation and evaluation of œÄ0.5 (Pi05) models on BEHAVIOR-1K. üìÑ Arxiv: https://arxiv.org/pdf/2512.10071 ü§ó Code: https://github.com/mli0603/openpi-comet üìù Blog: https://lnkd.in/gSv2K5ua Below are 8 representative tasks that showcase some of the most interesting results from our system.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10071",
      "pdf_url": "https://arxiv.org/pdf/2512.10071",
      "github_links": [
        "https://github.com/mli0603/openpi-comet"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10071",
      "scraped_at": "2025-12-17T01:44:03.209280"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "paper_url": "https://huggingface.co/papers/2512.13080",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
      "abstract": "We propose VIPA-VLA , which learns 2D‚Äìto‚Äì3D visual‚Äìphysical grounding from human videos with Spatial-Aware VLA Pretraining, enabling robot policies with stronger spatial understanding and generalization. Website: https://beingbeyond.github.io/VIPA-VLA arXiv: https://arxiv.org/abs/2512.13080",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13080",
      "pdf_url": "https://arxiv.org/pdf/2512.13080",
      "github_links": [
        "https://github.com/BeingBeyond/VIPA-VLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13080",
      "scraped_at": "2025-12-17T01:44:05.071292"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
    "paper_url": "https://huggingface.co/papers/2512.12692",
    "authors": [
      "Md Rizwan Parvez",
      "Mohammed Eunus Ali",
      "Tanzima Hashem",
      "mahirlabibdihan"
    ],
    "stars": "9",
    "details": {
      "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
      "abstract": "We are excited to share our recent work titled \"WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment\". üìÉ Paper: https://arxiv.org/abs/2512.12692 üíª Code: https://github.com/kagnlp/WebOperator üè† Homepage: https://kagnlp.github.io/WebOperator",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12692",
      "pdf_url": "https://arxiv.org/pdf/2512.12692",
      "github_links": [
        "https://github.com/kagnlp/WebOperator"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12692",
      "scraped_at": "2025-12-17T01:44:06.846918"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "paper_url": "https://huggingface.co/papers/2512.12799",
    "authors": [
      "Zining Wang",
      "Siming Yan",
      "Rui Yang",
      "Runhui Huang",
      "happinessqq"
    ],
    "stars": "22",
    "details": {
      "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
      "abstract": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12799",
      "pdf_url": "https://arxiv.org/pdf/2512.12799",
      "github_links": [
        "https://github.com/happinesslz/DrivePI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12799",
      "scraped_at": "2025-12-17T01:44:08.645067"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "paper_url": "https://huggingface.co/papers/2512.11995",
    "authors": [
      "Kwesi Cobbina",
      "Shweta Bhardwaj",
      "Yijun Liang",
      "zhoutianyi",
      "Fcr09"
    ],
    "stars": "2",
    "details": {
      "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
      "abstract": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11995",
      "pdf_url": "https://arxiv.org/pdf/2512.11995",
      "github_links": [
        "https://github.com/tianyi-lab/VREX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11995",
      "scraped_at": "2025-12-17T01:44:10.467977"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
    "paper_url": "https://huggingface.co/papers/2512.13250",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
      "abstract": "Project page: https://active-view-selection.github.io Arxiv: https://arxiv.org/abs/2512.13250 Code: https://github.com/KAIST-Visual-AI-Group/VG-AVS",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13250",
      "pdf_url": "https://arxiv.org/pdf/2512.13250",
      "github_links": [
        "https://github.com/KAIST-Visual-AI-Group/VG-AVS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13250",
      "scraped_at": "2025-12-17T01:44:12.235666"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Image Diffusion Preview with Consistency Solver",
    "paper_url": "https://huggingface.co/papers/2512.13592",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "Image Diffusion Preview with Consistency Solver",
      "abstract": "The slow inference process of image diffusion models significantly degrades interactive user experiences. We introduce Diffusion Preview , a novel preview-and-refine paradigm that generates rapid, low-step preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. This workflow enables users to quickly iterate through different prompts or random seeds with minimal computational cost, only triggering expensive full-step sampling when a preview meets their expectations. Diffusion Preview framework: Fast preview generation followed by full-step refinement. To achieve high-quality and consistent previews, we propose ConsistencySolver , a learnable high-order ODE solver derived from Linear Multistep Methods and optimized via Reinforcement Learning. Unlike existing training-free solvers that rely on rigid numerical schemes or distillation methods that sacrifice consistency, ConsistencySolver dynamically adapts its integration strategy to maximize alignment between low-step previews and high-step reference generations, ensuring previews serve as reliable proxies for final outputs. Overview of our RL framework for optimizing a learnable ODE solver in diffusion sampling. Empirical validation demonstrates that ConsistencySolver significantly outperforms training-free ODE solvers (e.g., DDIM, UniPC, Multistep DPM), distillation-based methods (e.g., LCM, PCM, DMD2), and distillation-based solvers (e.g., AMED) across both consistency metrics and FID scores. Quantitative Results on Stable Diffusion v1-5 for Text-to-Image Generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13592",
      "pdf_url": "https://arxiv.org/pdf/2512.13592",
      "github_links": [
        "https://github.com/G-U-N/consolver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13592",
      "scraped_at": "2025-12-17T01:44:14.044524"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
    "paper_url": "https://huggingface.co/papers/2512.11891",
    "authors": [
      "Zihan Meng",
      "Jun Cen",
      "Shuang Liu",
      "Zeyi Liu",
      "Songqiao Hu"
    ],
    "stars": "0",
    "details": {
      "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
      "abstract": "Project Page: https://vlsa-aegis.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11891",
      "pdf_url": "https://arxiv.org/pdf/2512.11891",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11891",
      "scraped_at": "2025-12-17T01:44:15.788582"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.12751",
    "authors": [
      "Chenxuan Miao",
      "Liping Hou",
      "Yuxiang Lu",
      "Zhe Liu",
      "ANIYA673"
    ],
    "stars": "0",
    "details": {
      "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12751",
      "pdf_url": "https://arxiv.org/pdf/2512.12751",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12751",
      "scraped_at": "2025-12-17T01:44:17.535516"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
    "paper_url": "https://huggingface.co/papers/2512.11883",
    "authors": [
      "Shan Du",
      "Khalad Hasan",
      "Qingyun Qian",
      "weathon"
    ],
    "stars": "0",
    "details": {
      "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
      "abstract": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11883",
      "pdf_url": "https://arxiv.org/pdf/2512.11883",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11883",
      "scraped_at": "2025-12-17T01:44:19.312596"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Towards Interactive Intelligence for Digital Humans",
    "paper_url": "https://huggingface.co/papers/2512.13674",
    "authors": [
      "Yifei Huang",
      "Sitong Gong",
      "Xiwei Gao",
      "Xuangeng Chu",
      "Yiyi Cai"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Interactive Intelligence for Digital Humans",
      "abstract": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13674",
      "pdf_url": "https://arxiv.org/pdf/2512.13674",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13674",
      "scraped_at": "2025-12-17T01:44:21.116402"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "RecTok: Reconstruction Distillation along Rectified Flow",
    "paper_url": "https://huggingface.co/papers/2512.13421",
    "authors": [
      "Yujing Wang",
      "Kaidong Yu",
      "Size Wu",
      "BryanW",
      "QingyuShi"
    ],
    "stars": "6",
    "details": {
      "title": "RecTok: Reconstruction Distillation along Rectified Flow",
      "abstract": "arXiv: https://arxiv.org/abs/2512.13421 Project: https://shi-qingyu.github.io/rectok.github.io/ Code: https://github.com/Shi-qingyu/RecTok",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13421",
      "pdf_url": "https://arxiv.org/pdf/2512.13421",
      "github_links": [
        "https://github.com/Shi-qingyu/RecTok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13421",
      "scraped_at": "2025-12-17T01:44:22.970902"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
    "paper_url": "https://huggingface.co/papers/2512.13006",
    "authors": [],
    "stars": "91",
    "details": {
      "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
      "abstract": "A Systematic Study of Diffusion Distillation for Text-to-Image Synthesis towards truly applicable few steps distillation, casting existing distillation methods (sCM, MeanFlow and IMM) into a unified framework for fair comparison. Code is available at https://github.com/alibaba-damo-academy/T2I-Distill.git",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13006",
      "pdf_url": "https://arxiv.org/pdf/2512.13006",
      "github_links": [
        "https://github.com/alibaba-damo-academy/T2I-Distill.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13006",
      "scraped_at": "2025-12-17T01:44:24.814497"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.11438",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction (2025) VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory (2025) Uniform Discrete Diffusion with Metric Path for Video Generation (2025) Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context (2025) FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion (2025) Generative Neural Video Compression via Video Diffusion Prior (2025) JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11438",
      "pdf_url": "https://arxiv.org/pdf/2512.11438",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11438",
      "scraped_at": "2025-12-17T01:44:27.047805"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "paper_url": "https://huggingface.co/papers/2512.10794",
    "authors": [
      "Richard Zhang",
      "Liang Zheng",
      "Zongze Wu",
      "Xingjian Leng",
      "Jaskirat Singh"
    ],
    "stars": "80",
    "details": {
      "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10794",
      "pdf_url": "https://arxiv.org/pdf/2512.10794",
      "github_links": [
        "https://github.com/end2end-diffusion/irepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10794",
      "scraped_at": "2025-12-17T01:44:29.079121"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.10655",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
      "abstract": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10655",
      "pdf_url": "https://arxiv.org/pdf/2512.10655",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10655",
      "scraped_at": "2025-12-17T01:44:30.921512"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "paper_url": "https://huggingface.co/papers/2512.13690",
    "authors": [
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "Chongjian Ge",
      "Susung Hong"
    ],
    "stars": "0",
    "details": {
      "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
      "abstract": "Project page: https://susunghong.github.io/DiffusionBrowser",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13690",
      "pdf_url": "https://arxiv.org/pdf/2512.13690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13690",
      "scraped_at": "2025-12-17T01:44:32.692263"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "paper_url": "https://huggingface.co/papers/2512.13689",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "LitePT: Lighter Yet Stronger Point Transformer",
      "abstract": "LitePT: Lighter Yet Stronger Point Transformer LitePT is a lightweight, high-performance 3D point cloud architecture for various point cloud processing tasks. It embodies the simple principle \"convolutions for low-level geometry, attention for high-level relations\" and strategically places only the required operations at each hierarchy level, avoiding wasted computations. We equip LitePT with parameter-free PointROPE positional encoding to compensate for the loss of spatial layout information that occurs when discarding convolutional layers. Together, these integrated designs give rise to a state-of-the-art backbone for point cloud analysis. Arxiv: https://arxiv.org/abs/2512.13689 Project page: https://litept.github.io/ Code: https://github.com/prs-eth/LitePT",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13689",
      "pdf_url": "https://arxiv.org/pdf/2512.13689",
      "github_links": [
        "https://github.com/prs-eth/LitePT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13689",
      "scraped_at": "2025-12-17T01:44:34.444564"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
    "paper_url": "https://huggingface.co/papers/2512.13683",
    "authors": [
      "Aniket Bera",
      "Yichen Sheng",
      "Yunhao Ge",
      "Lu Ling"
    ],
    "stars": "0",
    "details": {
      "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13683",
      "pdf_url": "https://arxiv.org/pdf/2512.13683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13683",
      "scraped_at": "2025-12-17T01:44:36.235573"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.13672",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
      "abstract": "Hi everyone! üëã We investigated why Textual Inversion (TI) often ignores context and traced the issue to embedding norm inflation. We found that standard TI learns tokens with massive magnitudes (often >20) compared to the model's native vocabulary (‚âà0.4), which we prove theoretically breaks the representation update in pre-norm Transformers. Our solution, Directional Textual Inversion (DTI) , fixes the magnitude to an in-distribution scale and optimizes only the direction on the hypersphere using Riemannian SGD. This simple change significantly improves prompt fidelity and enables smooth spherical interpolation (slerp) between concepts. We‚Äôd love for you to try it out! Code is available here: https://github.com/kunheek/dti",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13672",
      "pdf_url": "https://arxiv.org/pdf/2512.13672",
      "github_links": [
        "https://github.com/kunheek/dti"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13672",
      "scraped_at": "2025-12-17T01:44:37.974113"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.12196",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
      "abstract": "arxiv: https://arxiv.org/abs/2512.12196v1 GitHub: https://github.com/multimodal-art-projection/AutoMV Website: https://m-a-p.ai/AutoMV/ Apache-2.0 license",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12196v1",
      "pdf_url": "https://arxiv.org/pdf/2512.12196",
      "github_links": [
        "https://github.com/multimodal-art-projection/AutoMV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12196",
      "scraped_at": "2025-12-17T01:44:39.759871"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
    "paper_url": "https://huggingface.co/papers/2512.10927",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
      "abstract": "FoundationMotion offers a scalable way to curate detailed motion datasets, enabling effective fine-tuning of diverse models (VLM / VLA / world models) to improve motion and spatial reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10927",
      "pdf_url": "https://arxiv.org/pdf/2512.10927",
      "github_links": [
        "https://github.com/Wolfv0/FoundationMotion/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10927",
      "scraped_at": "2025-12-17T01:44:41.559612"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "START: Spatial and Textual Learning for Chart Understanding",
    "paper_url": "https://huggingface.co/papers/2512.07186",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "START: Spatial and Textual Learning for Chart Understanding",
      "abstract": "Does visual grounding help visual reasoning in Chart Understanding? üìäüß† I am excited to share our latest paper, \"START: Spatial and Textual Learning for Chart Understanding,\" which explores how we can teach Multimodal LLMs (MLLMs) to better understand complex, real-world charts. The Challenge: In real-world scenarios (like scientific papers), charts often have complex layouts with multiple subplots. Current models often fail because they jump to reasoning without first \"grounding\" (locating) the correct visual elements or understanding the underlying data. Our Solution - START: We propose a spatial and textual learning framework that trains MLLMs using two auxiliary tasks alongside Chart QA: Chart Element Grounding (Spatial): Explicitly teaching the model to locate specific components (legends, subplots), which boosts spatial reasoning. Chart-to-Code Generation (Textual): Recovering the Python code used to render the chart to understand data details. Key Contributions: START-Dataset: We developed a novel pipeline that converts real chart images (from ArXiv) into executable Python code and precise element locations, preserving real-world visual complexity. CS-Bench: A new benchmark specifically designed to evaluate chart spatial understanding. SOTA Results: Our model, START-RL-7B, outperforms previous state-of-the-art models (like Chart-R1) by a clear margin on benchmarks like CharXiv, ChartMimic, and ChartQAPro. This work has been accepted to WACV2026.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07186",
      "pdf_url": "https://arxiv.org/pdf/2512.07186",
      "github_links": [
        "https://github.com/dragonlzm/START"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07186",
      "scraped_at": "2025-12-17T01:44:43.352197"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
    "paper_url": "https://huggingface.co/papers/2512.05272",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
      "abstract": "Our method turns videos into compositional 4D scenes with explicit meshes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05272",
      "pdf_url": "https://arxiv.org/pdf/2512.05272",
      "github_links": [
        "https://github.com/insait-institute/COM4D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05272",
      "scraped_at": "2025-12-17T01:44:45.245087"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.13330",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
      "abstract": "Our paper introduces FIN-bench-v2, a unified and robust benchmark suite for evaluating large language models in Finnish, addressing the scarcity of high-quality evaluation resources for low-resource languages. This new suite modernizes the original FIN-bench, migrating it to the LM Evaluation Harness and converting all retained and new datasets into the consistent HuggingFace Datasets format for long-term maintainability. A key feature is the inclusion of both Cloze Formulation (CF) and Multiple-Choice Formulation (MCF) prompts and following the practice established in NorEval ( https://aclanthology.org/2025.findings-acl.181/ ) and HPLT 3.0 ( https://arxiv.org/abs/2511.01066 ) to create five separate variants to account for prompt sensitivity. We utilize the FineTasks selection process ( https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks ) to ensure only robust, high-signal tasks are included. üìù‚Äã Our task configurations can be found at https://github.com/LumiOpen/lm-evaluation-harness/tree/main/lm_eval/tasks/finbench_v2 .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.01066",
      "pdf_url": "https://arxiv.org/pdf/2512.13330",
      "github_links": [
        "https://github.com/LumiOpen/lm-evaluation-harness/tree/main/lm_eval/tasks/finbench_v2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13330",
      "scraped_at": "2025-12-17T01:44:47.023422"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "paper_url": "https://huggingface.co/papers/2512.12777",
    "authors": [
      "Yoav Goldberg",
      "Shauli Ravfogel",
      "Zohar Elyoseph",
      "Mosh Levy"
    ],
    "stars": "0",
    "details": {
      "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
      "abstract": "One of the most captivating features of recent chatbot models is their apparent transparency when they \"think\" out loud, generating step-by-step text before their answer. This might suggest we can trust them because we can verify their logic, but growing evidence shows this is an illusion. The text looks like a human explanation, but it functions as something fundamentally different: a computational mechanism we suggest calling State over Tokens. Mistaking this mechanical state for a transparent account of reasoning is a category error‚Äîone that risks undermining AI safety, regulation, and public trust. This paper characterizes what this \"text\" actually is, and why it doesn't do what you think it does.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12777",
      "pdf_url": "https://arxiv.org/pdf/2512.12777",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12777",
      "scraped_at": "2025-12-17T01:44:48.835998"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.12768",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
      "abstract": "A dual semantic + geometric reasoning framework with octant-based 3D tokens and multi-critic GRPO, achieving SoTA on text-to-3D, image-to-3D, and 3D captioning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12768",
      "pdf_url": "https://arxiv.org/pdf/2512.12768",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12768",
      "scraped_at": "2025-12-17T01:44:50.645470"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "paper_url": "https://huggingface.co/papers/2512.11470",
    "authors": [
      "Qi Zhu",
      "Jiyao Yuan",
      "Jiayang Lv",
      "Yuhan Chen",
      "Bowen Ding"
    ],
    "stars": "5",
    "details": {
      "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
      "abstract": "The systematic study of expert trajectory utilization in LLM post-training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11470",
      "pdf_url": "https://arxiv.org/pdf/2512.11470",
      "github_links": [
        "https://github.com/LINs-lab/RETU"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11470",
      "scraped_at": "2025-12-17T01:44:52.448484"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Learning Robot Manipulation from Audio World Models",
    "paper_url": "https://huggingface.co/papers/2512.08405",
    "authors": [
      "Michael Gienger",
      "Fanzhri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning Robot Manipulation from Audio World Models",
      "abstract": "Paper page: https://arxiv.org/abs/2409.01083",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08405",
      "pdf_url": "https://arxiv.org/pdf/2512.08405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08405",
      "scraped_at": "2025-12-17T01:44:54.296182"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
    "paper_url": "https://huggingface.co/papers/2512.08400",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
      "abstract": "Link to the AutoFish dataset: https://huggingface.co/datasets/vapaau/autofish",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08400",
      "pdf_url": "https://arxiv.org/pdf/2512.08400",
      "github_links": [
        "https://github.com/msamdk/Fish_Re_Identification.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08400",
      "scraped_at": "2025-12-17T01:44:56.105225"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
    "paper_url": "https://huggingface.co/papers/2512.09069",
    "authors": [
      "Ali Nourbakhsh",
      "Nasrin Sanjari",
      "Erfan-Nourbakhsh"
    ],
    "stars": "0",
    "details": {
      "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
      "abstract": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09069",
      "pdf_url": "https://arxiv.org/pdf/2512.09069",
      "github_links": [
        "https://github.com/erfan-nourbakhsh/KD-OCT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09069",
      "scraped_at": "2025-12-17T01:44:57.863705"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.14691",
    "authors": [
      "Haozhe Zhao",
      "Haoyi Qiu",
      "Zefan Cai",
      "ZGZzz",
      "SueMintony"
    ],
    "stars": "0",
    "details": {
      "title": "MMGR: Multi-Modal Generative Reasoning",
      "abstract": "MMGR proposes a principled, multi-domain benchmark for evaluating generative models' physical, logical, and spatial reasoning in video and image generation, diagnosing global consistency and causal correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14691",
      "pdf_url": "https://arxiv.org/pdf/2512.14691",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14691",
      "scraped_at": "2025-12-18T01:44:00.300528"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "paper_url": "https://huggingface.co/papers/2512.13281",
    "authors": [
      "Rui Zhao",
      "Yi Zhan",
      "Weijia Wu",
      "Jiaqi Wang",
      "KevinQHLin"
    ],
    "stars": "13",
    "details": {
      "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
      "abstract": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56% accuracy (random 50%), far below that of human experts (81.25%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13281",
      "pdf_url": "https://arxiv.org/pdf/2512.13281",
      "github_links": [
        "https://github.com/video-reality-test/video-reality-test"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13281",
      "scraped_at": "2025-12-18T01:44:02.511598"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.14614",
    "authors": [
      "Zehan Wang",
      "Junta Wu",
      "Haoyuan Wang",
      "Haiyu Zhang",
      "wenqsun"
    ],
    "stars": "302",
    "details": {
      "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
      "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14614",
      "pdf_url": "https://arxiv.org/pdf/2512.14614",
      "github_links": [
        "https://github.com/Tencent-Hunyuan/HY-WorldPlay"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14614",
      "scraped_at": "2025-12-18T01:44:04.445977"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
    "paper_url": "https://huggingface.co/papers/2512.12675",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
      "abstract": "Code: https://github.com/Ryann-Ran/Scone",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12675",
      "pdf_url": "https://arxiv.org/pdf/2512.12675",
      "github_links": [
        "https://github.com/Ryann-Ran/Scone"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12675",
      "scraped_at": "2025-12-18T01:44:06.318404"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "paper_url": "https://huggingface.co/papers/2512.13660",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
      "abstract": "Project Page: https://zhoues.github.io/RoboTracer/ We present RoboTracer, the first 3D-aware VLM for multi-step metric-grounded spatial tracing with explicit reasoning. Highlights: RoboTracer first acquires both 3D spatial referring and measuring via SFT, and further advances multi-step metric-grounded spatial tracing via RFT. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes, and containing complex reasoning processes (up to 9 steps). SFT-trained RoboTracer achieves SOTA spatial understanding/measuring/referring, and RFT-trained RoboTracer exhibits strong spatial tracing under novel cluttered and dynamic scenes with complex reasoning processes. Motivation: Model Framework: Dataset Construction:",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13660",
      "pdf_url": "https://arxiv.org/pdf/2512.13660",
      "github_links": [
        "https://github.com/Zhoues/RoboTracer"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13660",
      "scraped_at": "2025-12-18T01:44:08.268676"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
    "paper_url": "https://huggingface.co/papers/2512.14051",
    "authors": [
      "Xin Gao",
      "Mengzhang Cai",
      "ChampionZhong",
      "Xiaoyang318",
      "Word2Li"
    ],
    "stars": "80",
    "details": {
      "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
      "abstract": "https://opendataarena.github.io/index.html",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14051",
      "pdf_url": "https://arxiv.org/pdf/2512.14051",
      "github_links": [
        "https://github.com/OpenDataArena/OpenDataArena-Tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14051",
      "scraped_at": "2025-12-18T01:44:10.190645"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
    "paper_url": "https://huggingface.co/papers/2512.12980",
    "authors": [
      "Hua Fan",
      "Haotian Wu",
      "Jiahua Wu",
      "Cong Fu",
      "Tingyang-Chen"
    ],
    "stars": "1",
    "details": {
      "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
      "abstract": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice. We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12980",
      "pdf_url": "https://arxiv.org/pdf/2512.12980",
      "github_links": [
        "https://github.com/ZJU-DAILY/Iceberg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12980",
      "scraped_at": "2025-12-18T01:44:12.160681"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
    "paper_url": "https://huggingface.co/papers/2512.14336",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
      "abstract": "Project page: https://yeolj00.github.io/personal-projects/vector-prism/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14336",
      "pdf_url": "https://arxiv.org/pdf/2512.14336",
      "github_links": [
        "https://github.com/YeolJ00/vector-prism"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14336",
      "scraped_at": "2025-12-18T01:44:14.032624"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
    "paper_url": "https://huggingface.co/papers/2512.14699",
    "authors": [
      "Xin Tao",
      "Shuai Yang",
      "Xi Chen",
      "Sihui Ji",
      "Hengshuang"
    ],
    "stars": "0",
    "details": {
      "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
      "abstract": "MemFlow uses a retrieval-driven adaptive memory and selective attention to maintain narrative coherence in long-streaming video generation with minimal overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14699",
      "pdf_url": "https://arxiv.org/pdf/2512.14699",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14699",
      "scraped_at": "2025-12-18T01:44:15.916926"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "RecGPT-V2 Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.14503",
    "authors": [
      "Dian Chen",
      "Chao Yi",
      "zhjgao",
      "TangJiakai5704",
      "hairlatic"
    ],
    "stars": "0",
    "details": {
      "title": "RecGPT-V2 Technical Report",
      "abstract": "üåü RecGPT-V2: A Major Leap in LLM-Powered Recommendation (RecGPT-V1‚Äôs Power Upgrade!) üåü Thrilled to unveil RecGPT-V2‚Äîthe highly anticipated successor to RecGPT-V1! This agentic framework addresses V1‚Äôs core limitations, fusing cognitive reasoning with industrial scalability for next-gen intent-centric recommendations. üî• Core Innovations: Hierarchical Multi-Agent + Hybrid Representation: 60% less GPU usage, 9.39%‚Üí10.99% exclusive recall, and 32K‚Üí11K token compression (context intact). Meta-Prompting: +7.3% explanation diversity with adaptive, non-generic prompts. Constrained RL: Resolves multi-reward conflicts‚Äî+24.1% better tag prediction and +13.0% higher explanation acceptance vs. V1. Agent-as-a-Judge: Human-like multi-step evaluation, closer alignment with real-world standards. üöÄ Taobao A/B Test Results: +2.98% CTR | +3.71% IPV | +2.19% TV | +11.46% NER (Novelty Exposure Rate) Validated for large-scale deployment‚Äîbridging cognitive AI and practical utility, with room to evolve. üéØ Why It Matters: Fixes V1‚Äôs pain points (computational bloat, rigid explanations, weak generalization, oversimplified evaluation) to deliver a scalable, efficient, human-aligned paradigm. Perfect for researchers and engineers‚Äîthis is just a key milestone in refining intent-driven AI! üëâ Dive into the full technical report to unlock scalable intent-driven recommendations. Let‚Äôs shape personalized AI4Rec‚Äôs future!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14503",
      "pdf_url": "https://arxiv.org/pdf/2512.14503",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14503",
      "scraped_at": "2025-12-18T01:44:18.114477"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
    "paper_url": "https://huggingface.co/papers/2512.13303",
    "authors": [
      "Zhaohe Liao",
      "Junjie Zhou",
      "Pandeng Li",
      "Xiaoyi Bao",
      "lntzm"
    ],
    "stars": "0",
    "details": {
      "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
      "abstract": "Nano Banana Pro excels at this. We hope our methods and bench can draw more community's attention to this type of genenration ability.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13303",
      "pdf_url": "https://arxiv.org/pdf/2512.13303",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13303",
      "scraped_at": "2025-12-18T01:44:20.041763"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
    "paper_url": "https://huggingface.co/papers/2512.13678",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
      "abstract": "Very cool model that lets you edit 3D digital objects into whatever way you like, using natural language instructions! Project Home: https://glab-caltech.github.io/steer3d/ Demo: https://glab-caltech.github.io/steer3d/#demo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13678",
      "pdf_url": "https://arxiv.org/pdf/2512.13678",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13678",
      "scraped_at": "2025-12-18T01:44:21.943698"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "paper_url": "https://huggingface.co/papers/2512.13607",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
      "abstract": "The Nemotron-Cascade models and the full collection of training data are released at: https://huggingface.co/collections/nvidia/nemotron-cascade",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13607",
      "pdf_url": "https://arxiv.org/pdf/2512.13607",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13607",
      "scraped_at": "2025-12-18T01:44:23.850652"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Olmo 3",
    "paper_url": "https://huggingface.co/papers/2512.13961",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Olmo 3",
      "abstract": "We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13961",
      "pdf_url": "https://arxiv.org/pdf/2512.13961",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13961",
      "scraped_at": "2025-12-18T01:44:25.721331"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Differentiable Evolutionary Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.13399",
    "authors": [
      "Difan Zou",
      "Xunjian Yin",
      "Xuhan Huang",
      "Tianle Li",
      "sitao"
    ],
    "stars": "0",
    "details": {
      "title": "Differentiable Evolutionary Reinforcement Learning",
      "abstract": "Code: https://github.com/sitaocheng/DERL Models: https://huggingface.co/DifferentiableEvolutionaryRL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13399",
      "pdf_url": "https://arxiv.org/pdf/2512.13399",
      "github_links": [
        "https://github.com/sitaocheng/DERL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13399",
      "scraped_at": "2025-12-18T01:44:27.512913"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
    "paper_url": "https://huggingface.co/papers/2512.14531",
    "authors": [],
    "stars": "924",
    "details": {
      "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
      "abstract": "The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at this https URL.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14531",
      "pdf_url": "https://arxiv.org/pdf/2512.14531",
      "github_links": [
        "https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14531",
      "scraped_at": "2025-12-18T01:44:29.387073"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.14442",
    "authors": [
      "Hanqing Wang",
      "Kanghao Chen",
      "Chenfei-Liao",
      "Harold328",
      "zhangzixin02"
    ],
    "stars": "17",
    "details": {
      "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
      "abstract": "Project Page: https://zixinzhang02.github.io/A4-Agent-page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14442",
      "pdf_url": "https://arxiv.org/pdf/2512.14442",
      "github_links": [
        "https://github.com/EnVision-Research/A4-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14442",
      "scraped_at": "2025-12-18T01:44:31.201991"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
    "paper_url": "https://huggingface.co/papers/2512.14284",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
      "abstract": "project page: https://lizb6626.github.io/SS4D/ code: https://github.com/Lizb6626/SS4D/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14284",
      "pdf_url": "https://arxiv.org/pdf/2512.14284",
      "github_links": [
        "https://github.com/Lizb6626/SS4D/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14284",
      "scraped_at": "2025-12-18T01:44:33.080941"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.14008",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
      "abstract": "Efficient Training and Inference for unified multi-modal diffusion language models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14008",
      "pdf_url": "https://arxiv.org/pdf/2512.14008",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14008",
      "scraped_at": "2025-12-18T01:44:34.899723"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "paper_url": "https://huggingface.co/papers/2512.14697",
    "authors": [
      "Chutong Yang",
      "Zhenlin Xu",
      "Hanwen Jiang",
      "eadeli42",
      "zhaoyue-zephyrus"
    ],
    "stars": "2",
    "details": {
      "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
      "abstract": "Blog: https://ai.stanford.edu/~yzz/blog/articles/npq.html Code for reconstruction and compression: https://github.com/zhaoyue-zephyrus/bsq-vit Code for generation with InfinityCC: https://github.com/zhaoyue-zephyrus/InfinityCC",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14697",
      "pdf_url": "https://arxiv.org/pdf/2512.14697",
      "github_links": [
        "https://github.com/zhaoyue-zephyrus/InfinityCC",
        "https://github.com/zhaoyue-zephyrus/bsq-vit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14697",
      "scraped_at": "2025-12-18T01:44:36.703348"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
    "paper_url": "https://huggingface.co/papers/2512.14696",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
      "abstract": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2% to 6.9% on human-centric video benchmarks (EMDB, PROX), while delivering a 43% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR. Code and interactive demos are available at our project website: \\href{ https://crisp-real2sim.github.io/CRISP-Real2Sim/}{\\textcolor{cyan}{{crisp-real2sim.github.io/CRISP-Real2Sim}}} .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14696",
      "pdf_url": "https://arxiv.org/pdf/2512.14696",
      "github_links": [
        "https://github.com/Z1hanW/CRISP-Real2Sim"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14696",
      "scraped_at": "2025-12-18T01:44:38.570918"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2512.14666",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
      "abstract": "EVOLVE-VLA is a test-time training framework that enables Vision-Language-Action models to continuously adapt through environment interaction with minimal or no task-specific demonstrations, overcoming the limitations of static supervised finetuning. By using a learned progress estimator with mechanisms to stabilize noisy feedback, it achieves significant performance gains, cross-task generalization, and emergent adaptive behaviors such as error recovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14666",
      "pdf_url": "https://arxiv.org/pdf/2512.14666",
      "github_links": [
        "https://github.com/showlab/EVOLVE-VLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14666",
      "scraped_at": "2025-12-18T01:44:40.486862"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
    "paper_url": "https://huggingface.co/papers/2512.14550",
    "authors": [
      "Bingzheng Wei",
      "Jian Liang",
      "Yang Yi",
      "Jiaju",
      "upyzwup"
    ],
    "stars": "29",
    "details": {
      "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14550",
      "pdf_url": "https://arxiv.org/pdf/2512.14550",
      "github_links": [
        "https://github.com/Yaziwel/TAT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14550",
      "scraped_at": "2025-12-18T01:44:42.387225"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
    "paper_url": "https://huggingface.co/papers/2512.14273",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
      "abstract": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14273",
      "pdf_url": "https://arxiv.org/pdf/2512.14273",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14273",
      "scraped_at": "2025-12-18T01:44:44.155664"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
    "paper_url": "https://huggingface.co/papers/2512.14067",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
      "abstract": "Proposes Efficient-DLM: converting autoregressive LMs to fast diffusion LMs via block-wise continuous pretraining and token masking, achieving higher accuracy and throughput than AR and existing dLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14067",
      "pdf_url": "https://arxiv.org/pdf/2512.14067",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14067",
      "scraped_at": "2025-12-18T01:44:45.941113"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
    "paper_url": "https://huggingface.co/papers/2512.13525",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
      "abstract": "Paper page: https://arxiv.org/pdf/2512.13525",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13525",
      "pdf_url": "https://arxiv.org/pdf/2512.13525",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13525",
      "scraped_at": "2025-12-18T01:44:47.917365"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "RePo: Language Models with Context Re-Positioning",
    "paper_url": "https://huggingface.co/papers/2512.14391",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RePo: Language Models with Context Re-Positioning",
      "abstract": "TL;DR: We want to give LLMs the architectural ability to reorganize input context just like humans do. Our solution is to incorporate a lightweight RePo module to dynamically assign positions before position encoding functions.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14391",
      "pdf_url": "https://arxiv.org/pdf/2512.14391",
      "github_links": [
        "https://github.com/SakanaAI/repo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14391",
      "scraped_at": "2025-12-18T01:44:49.821387"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
    "paper_url": "https://huggingface.co/papers/2512.14620",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
      "abstract": "‚ÄúBro, Benchmarks like MMMU-Pro are too expensive to build, right?‚Äù One month ago: Yes. Now: No üöÄ Proposing Vibe Benchmark Construction! NanoBanana Pro generates VQA itself, and humans only check or lightly edit prompts for regeneration. üöÄBuilding JMMMU-Pro incredibly quickly! JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, requiring integrated visual-textual understanding through visual perception. üßê Most open-source LMMs seem to perform close to random guessing on JMMMU-Pro. Let's take on the challenge! Paper: https://arxiv.org/pdf/2512.14620 Project Page: https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14620",
      "pdf_url": "https://arxiv.org/pdf/2512.14620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14620",
      "scraped_at": "2025-12-18T01:44:51.734895"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "paper_url": "https://huggingface.co/papers/2512.14014",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
      "abstract": "A benchmark for world modeling of mobile GUI agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14014",
      "pdf_url": "https://arxiv.org/pdf/2512.14014",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14014",
      "scraped_at": "2025-12-18T01:44:53.520919"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.13655",
    "authors": [
      "richardyoung"
    ],
    "stars": "0",
    "details": {
      "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
      "abstract": "TL;DR We benchmark 4 open-source LLM abliteration implementations across 16 instruction-tuned models. Key results: ‚Ä¢ Coverage differs a lot (Heretic 16/16; DECCP 11/16; ErisForge 9/16; FailSpy 5/16).  Ôøº ‚Ä¢ Single-pass methods preserved capabilities best on the benchmarked subset (avg GSM8K ‚àÜ: DECCP ‚àí0.13 pp, ErisForge ‚àí0.28 pp; Heretic ‚àí7.81 pp avg driven by Yi).  Ôøº ‚Ä¢ Math reasoning is the most sensitive axis (GSM8K swings from +1.51 pp to ‚àí18.81 pp depending on tool/model).  Ôøº",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13655",
      "pdf_url": "https://arxiv.org/pdf/2512.13655",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13655",
      "scraped_at": "2025-12-18T01:44:55.267099"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.13106",
    "authors": [
      "Zhongqi Chen",
      "Yingfan MA",
      "Xing Zheng",
      "Guangcheng Zhu",
      "Shenzhi"
    ],
    "stars": "1",
    "details": {
      "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
      "abstract": "We‚Äôve come up with a semi-supervised RLVR training method that uses just a few labeled examples to help pick out trustworthy samples from the unlabeled ones. Feel free to jump in with thoughts or suggestions‚Äî all feedback is welcome! ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13106",
      "pdf_url": "https://arxiv.org/pdf/2512.13106",
      "github_links": [
        "https://github.com/ShenzhiYang2000/TRAPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13106",
      "scraped_at": "2025-12-18T01:44:57.042085"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
    "paper_url": "https://huggingface.co/papers/2512.12941",
    "authors": [
      "Wenqi Ren",
      "Shengjie Li",
      "Taotao Li",
      "Dongxiu Liu",
      "Siyuan Yao"
    ],
    "stars": "0",
    "details": {
      "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
      "abstract": "UAGLNet Repository: https://github.com/Dstate/UAGLNet Paper: ‚ÄúUAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction‚Äù ( arXiv:2512.12941 )",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12941",
      "pdf_url": "https://arxiv.org/pdf/2512.12941",
      "github_links": [
        "https://github.com/Dstate/UAGLNet"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12941",
      "scraped_at": "2025-12-18T01:44:58.969163"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.14440",
    "authors": [
      "Timo Ropinski",
      "phermosilla",
      "xeTaiz",
      "lhoyer",
      "leonsick"
    ],
    "stars": "1",
    "details": {
      "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
      "abstract": "Project page: https://leonsick.github.io/s2d Code: https://github.com/leonsick/s2d",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14440",
      "pdf_url": "https://arxiv.org/pdf/2512.14440",
      "github_links": [
        "https://github.com/leonsick/s2d"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14440",
      "scraped_at": "2025-12-18T01:45:00.811365"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "paper_url": "https://huggingface.co/papers/2512.11934",
    "authors": [
      "Erfan Nourbakhsh",
      "Adeleh Mazaherian"
    ],
    "stars": "0",
    "details": {
      "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
      "abstract": "Hello everyone, I hope you enjoy reading our paper! These are the helpful links: https://arxiv.org/abs/2512.11934 https://github.com/erfan-nourbakhsh/GenAI-EdSent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11934",
      "pdf_url": "https://arxiv.org/pdf/2512.11934",
      "github_links": [
        "https://github.com/erfan-nourbakhsh/GenAI-EdSent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11934",
      "scraped_at": "2025-12-18T01:45:02.577452"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "paper_url": "https://huggingface.co/papers/2512.10952",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
      "abstract": "How do you decide which datasets to train on when data comes from many noisy, heterogeneous sources? In this work, we formalize dataset selection as its own problem and introduce DaSH (Dataset Selection via Hierarchies), a method that models dataset-level utility and group-level utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10952",
      "pdf_url": "https://arxiv.org/pdf/2512.10952",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10952",
      "scraped_at": "2025-12-18T01:45:04.337572"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.10945",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
      "abstract": "MeViSv2 Dataset, Project Page: https://henghuiding.com/MeViS/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10945",
      "pdf_url": "https://arxiv.org/pdf/2512.10945",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10945",
      "scraped_at": "2025-12-18T01:45:06.099138"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
    "paper_url": "https://huggingface.co/papers/2512.10342",
    "authors": [
      "Yogesh S Rawat",
      "Vibhav Vineet",
      "Akash Kumar",
      "Shresth Grover",
      "ppriyank"
    ],
    "stars": "0",
    "details": {
      "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
      "abstract": "LLM benchmark on sequence completion (Spoiler: They can't)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10342",
      "pdf_url": "https://arxiv.org/pdf/2512.10342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10342",
      "scraped_at": "2025-12-18T01:45:08.978991"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.07328",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
      "abstract": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07328",
      "pdf_url": "https://arxiv.org/pdf/2512.07328",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07328",
      "scraped_at": "2025-12-18T01:45:10.828372"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Step-GUI Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.15431",
    "authors": [],
    "stars": "1.44k",
    "details": {
      "title": "Step-GUI Technical Report",
      "abstract": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15431",
      "pdf_url": "https://arxiv.org/pdf/2512.15431",
      "github_links": [
        "https://github.com/stepfun-ai/gelab-zero"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15431",
      "scraped_at": "2025-12-19T01:47:15.248931"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
    "paper_url": "https://huggingface.co/papers/2512.15176",
    "authors": [
      "Zhijie Deng",
      "Jia Li",
      "Guo-Wei Yang",
      "Zicong Cheng",
      "menghao22"
    ],
    "stars": "0",
    "details": {
      "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
      "abstract": "Simultaneously leveraging the efficiency of dLLM and the performance of AR models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15176",
      "pdf_url": "https://arxiv.org/pdf/2512.15176",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15176",
      "scraped_at": "2025-12-19T01:47:17.221958"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
    "paper_url": "https://huggingface.co/papers/2512.14681",
    "authors": [
      "Tajana Rosing",
      "Samyam Rajbhandari",
      "Yichao Fu",
      "Siqi Kou",
      "Lanxiang Hu"
    ],
    "stars": "52",
    "details": {
      "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
      "abstract": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from either generation quality or limited wall-clock speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding benchmarks with minimal loss in performance. Based on Jacobi Forcing Model‚Äôs trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14681",
      "pdf_url": "https://arxiv.org/pdf/2512.14681",
      "github_links": [
        "https://github.com/hao-ai-lab/JacobiForcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14681",
      "scraped_at": "2025-12-19T01:47:19.119322"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.14944",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
      "abstract": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14944",
      "pdf_url": "https://arxiv.org/pdf/2512.14944",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14944",
      "scraped_at": "2025-12-19T01:47:20.987926"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
    "paper_url": "https://huggingface.co/papers/2512.14052",
    "authors": [
      "Yuhang Dong",
      "Zhiqiang Xia",
      "Kaiyang Han",
      "Yuchen Liu",
      "HyperAI Team"
    ],
    "stars": "0",
    "details": {
      "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
      "abstract": "üöÄ [New Paper] HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices Current multimodal large language models (MLLMs) possess strong perceptual and reasoning capabilities, but their high computational and memory requirements make them difficult to deploy directly on edge devices. HyperVL aims to tackle this challenge by introducing an efficient multimodal large language model tailored for on-device inference. ‚ú® The Core Intuition: HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: 1Ô∏è‚É£ Visual Resolution Compressor (VRC): Adaptively predicts optimal encoding resolutions to eliminate redundant computation. 2Ô∏è‚É£ Dual Consistency Learning (DCL): Aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. üìà Highlights: State-of-the-Art Performance: HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Resource Efficient: It significantly reduces latency and power consumption on real mobile devices, demonstrating a 6.8x reduction in peak memory overhead. Quantization Robustness: The model demonstrates exceptional robustness to low-bit precision under W4A16 quantization with negligible performance drops. Broad Applications: HyperVL shows strong generalization for on-device tasks such as UI understanding and parsing, intent recommendation, and image-text creation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14052",
      "pdf_url": "https://arxiv.org/pdf/2512.14052",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14052",
      "scraped_at": "2025-12-19T01:47:22.930324"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Universal Reasoning Model",
    "paper_url": "https://huggingface.co/papers/2512.14693",
    "authors": [],
    "stars": "23",
    "details": {
      "title": "Universal Reasoning Model",
      "abstract": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art‚àó 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14693",
      "pdf_url": "https://arxiv.org/pdf/2512.14693",
      "github_links": [
        "https://github.com/zitian-gao/URM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14693",
      "scraped_at": "2025-12-19T01:47:24.889066"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
    "paper_url": "https://huggingface.co/papers/2512.15635",
    "authors": [],
    "stars": "25",
    "details": {
      "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15635",
      "pdf_url": "https://arxiv.org/pdf/2512.15635",
      "github_links": [
        "https://github.com/CUC-MIPG/IC-Effect"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15635",
      "scraped_at": "2025-12-19T01:47:26.801519"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.15693",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
      "abstract": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning https://huggingface.co/papers/2512.15693 Explainable AI-generated video detection with a specialized multimodal LLM. Given an input video, Skyra explicitly identifies human-perceivable spatio-temporal artifacts (e.g., texture/structure inconsistencies, motion irregularities) and uses them as grounded evidence to produce both a real/fake decision and a human-interpretable explanation with localized cues. To train this capability, we introduce ViF-CoT-4K, the first large-scale AI-generated video artifact dataset with fine-grained human annotations, enabling supervised fine-tuning (Skyra-SFT). We further apply a second-stage reinforcement learning procedure to encourage the model to actively mine discriminative artifacts, improving both detection and explanation quality (Skyra-RL). For rigorous evaluation, we release ViF-Bench (3K high-quality samples from 10+ state-of-the-art video generators) with aligned real/fake semantics and formats to reduce shortcut signals, and demonstrate consistent gains over prior binary detectors and MLLM-based baselines. Learn more at https://joeleelyf.github.io/Skyra and https://github.com/JoeLeelyf/Skyra .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15693",
      "pdf_url": "https://arxiv.org/pdf/2512.15693",
      "github_links": [
        "https://github.com/JoeLeelyf/Skyra"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15693",
      "scraped_at": "2025-12-19T01:47:28.689499"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
    "paper_url": "https://huggingface.co/papers/2512.15603",
    "authors": [
      "Xiao Xu",
      "Kaiyuan Gao",
      "Zecheng Tang",
      "Zekai Zhang",
      "Shengming Yin"
    ],
    "stars": "0",
    "details": {
      "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
      "abstract": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15603",
      "pdf_url": "https://arxiv.org/pdf/2512.15603",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15603",
      "scraped_at": "2025-12-19T01:47:30.629216"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
    "paper_url": "https://huggingface.co/papers/2512.15182",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
      "abstract": "The paper ‚ÄúRobust and Calibrated Detection of Authentic Multimedia Content‚Äù presents a new framework for identifying whether multimedia particularly deepfakes produced by generative models is genuinely authentic or can be plausibly denied as fake, addressing key shortcomings of current detection methods which suffer from unbounded false positive rates and are easily defeated by adaptive attackers; by introducing a calibrated resynthesis approach that focuses on high precision and adversarial robustness under realistic (compute-limited) threat models, the authors demonstrate that their method reliably verifies authentic samples with controllable false positive rates while resisting evasion by efficient adversaries, supports multiple modalities, and leverages cutting-edge inversion techniques to improve robustness and calibration compared to prior work.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15182",
      "pdf_url": "https://arxiv.org/pdf/2512.15182",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15182",
      "scraped_at": "2025-12-19T01:47:32.576517"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.13874",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
      "abstract": "üìú explainer thread: https://x.com/allen_ai/status/2001351082916630586 üîó Project page: https://lnkd.in/eff-DjHx üíª Code: github.com/allenai/SAGE üì¶ Models & data: https://lnkd.in/eT9iVVRk üìù Paper: arxiv.org/abs/2512.13874",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13874",
      "pdf_url": "https://arxiv.org/pdf/2512.13874",
      "github_links": [
        "https://github.com/allenai/SAGE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13874",
      "scraped_at": "2025-12-19T01:47:34.472068"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.15687",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
      "abstract": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15687",
      "pdf_url": "https://arxiv.org/pdf/2512.15687",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15687",
      "scraped_at": "2025-12-19T01:47:36.334475"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
    "paper_url": "https://huggingface.co/papers/2512.13884",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
      "abstract": "GitHub Repo: https://github.com/whoisjones/FiNERweb HF Collection: https://huggingface.co/collections/whoisjones/finerweb",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13884",
      "pdf_url": "https://arxiv.org/pdf/2512.13884",
      "github_links": [
        "https://github.com/whoisjones/FiNERweb"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13884",
      "scraped_at": "2025-12-19T01:47:38.197928"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.10863",
    "authors": [
      "Peizhou Cao",
      "Sihan Yang",
      "Shaohao Zhu",
      "Runsen Xu",
      "rbler"
    ],
    "stars": "0",
    "details": {
      "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
      "abstract": "Our homepage: https://rbler1234.github.io/MMSI-VIdeo-Bench.github.io GitHub Page: https://github.com/InternRobotics/MMSI-Video-Bench HuggingFace: https://huggingface.co/datasets/rbler/MMSI-Video-Bench Arxiv: https://arxiv.org/abs/2512.10863",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10863",
      "pdf_url": "https://arxiv.org/pdf/2512.10863",
      "github_links": [
        "https://github.com/InternRobotics/MMSI-Video-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10863",
      "scraped_at": "2025-12-19T01:47:40.150604"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "paper_url": "https://huggingface.co/papers/2512.15713",
    "authors": [],
    "stars": "30",
    "details": {
      "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
      "abstract": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15713",
      "pdf_url": "https://arxiv.org/pdf/2512.15713",
      "github_links": [
        "https://github.com/hustvl/DiffusionVL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15713",
      "scraped_at": "2025-12-19T01:47:42.162776"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
    "paper_url": "https://huggingface.co/papers/2512.12072",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
      "abstract": "Diverse data is ALL you NEED",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12072",
      "pdf_url": "https://arxiv.org/pdf/2512.12072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12072",
      "scraped_at": "2025-12-19T01:47:44.037410"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "paper_url": "https://huggingface.co/papers/2512.15702",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
      "abstract": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15702",
      "pdf_url": "https://arxiv.org/pdf/2512.15702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15702",
      "scraped_at": "2025-12-19T01:47:45.898986"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.09299",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
      "abstract": "code link: https://github.com/tanABCC/VABench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09299",
      "pdf_url": "https://arxiv.org/pdf/2512.09299",
      "github_links": [
        "https://github.com/tanABCC/VABench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09299",
      "scraped_at": "2025-12-19T01:47:47.763167"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "paper_url": "https://huggingface.co/papers/2512.15715",
    "authors": [
      "Dong Wang",
      "Xinjie Lei",
      "Yang Li",
      "Shang-Wen Li",
      "Lihe Yang"
    ],
    "stars": "50",
    "details": {
      "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
      "abstract": "arXiv lens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/in-pursuit-of-pixel-supervision-for-visual-pre-training-8810-5e30657e Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15715",
      "pdf_url": "https://arxiv.org/pdf/2512.15715",
      "github_links": [
        "https://github.com/facebookresearch/pixio"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15715",
      "scraped_at": "2025-12-19T01:47:49.666981"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
    "paper_url": "https://huggingface.co/papers/2512.15649",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
      "abstract": "A comprehensive benchmark to study VLM's visual text compression ability. Code: https://github.com/Moenupa/VTCBench Huggingface: https://huggingface.co/datasets/MLLM-CL/VTCBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15649",
      "pdf_url": "https://arxiv.org/pdf/2512.15649",
      "github_links": [
        "https://github.com/Moenupa/VTCBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15649",
      "scraped_at": "2025-12-19T01:47:51.545725"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
    "paper_url": "https://huggingface.co/papers/2512.15110",
    "authors": [
      "Yicheng Zhang",
      "Jiaxin Zhu",
      "Hanyu Zhou",
      "Haoyou Deng",
      "Jialong Zuo"
    ],
    "stars": "0",
    "details": {
      "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
      "abstract": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \\textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15110",
      "pdf_url": "https://arxiv.org/pdf/2512.15110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15110",
      "scraped_at": "2025-12-19T01:47:53.384221"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
    "paper_url": "https://huggingface.co/papers/2512.13190",
    "authors": [
      "Sung Won Han",
      "Dongil Park",
      "Wooseok Shin",
      "Hyun Joon Park",
      "sadPororo"
    ],
    "stars": "3",
    "details": {
      "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
      "abstract": "A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13190",
      "pdf_url": "https://arxiv.org/pdf/2512.13190",
      "github_links": [
        "https://github.com/sadPororo/WAY"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13190",
      "scraped_at": "2025-12-19T01:47:55.228395"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.15699",
    "authors": [
      "Shang Zhou",
      "Huanzhi Mao",
      "Zhifei Li",
      "Wenhao Chai",
      "Qiuyang Mang"
    ],
    "stars": "0",
    "details": {
      "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
      "abstract": "https://github.com/FrontierCS/Frontier-CS Introducing FrontierCS. LiveCodeBench Pro is already a challenging competitive programming benchmark, so why do we still need to push one step further? The motivation behind FrontierCS is actually pretty simple: we love measuring intelligence with problems that have a \"single\", \"correct\",  \"optimal\" answer, but what really matters at the frontier in practice is often open-ended problems where the optimum is unknown, yet every step can be objectively scored and verified. In our experiments, we kept running into a sobering pattern: simply scaling up reasoning compute doesn‚Äôt close the gap. Models often settle for a locally feasible \"it runs\" solution, then stall on algorithmic and system choices that are still clearly bad. We still have a long way to go. Let‚Äôs build Evolving Challenges for Evolving Intelligence!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15699",
      "pdf_url": "https://arxiv.org/pdf/2512.15699",
      "github_links": [
        "https://github.com/FrontierCS/Frontier-CS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15699",
      "scraped_at": "2025-12-19T01:47:57.040151"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
    "paper_url": "https://huggingface.co/papers/2512.15374",
    "authors": [
      "Yunhe Wang",
      "Sinno Jialin Pan",
      "Shixiong Kai",
      "Hui-Ling Zhen",
      "Zehua Pei"
    ],
    "stars": "4",
    "details": {
      "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
      "abstract": "We introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution), a framework that automatically evolves agent prompts by learning from execution traces. Try it now: pip install scope-optimizer üìÑ Paper: https://arxiv.org/abs/2512.15374 üíª Code: https://github.com/JarvisPei/SCOPE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15374",
      "pdf_url": "https://arxiv.org/pdf/2512.15374",
      "github_links": [
        "https://github.com/JarvisPei/SCOPE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15374",
      "scraped_at": "2025-12-19T01:47:58.898057"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.14202",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
      "abstract": "tl;dr : We analytically show that large-norm embeddings destabilize hyperbolic representations in deep RL. In PPO, this coincides with trust-region violations. Existing methods based on SpectralNorm mitigate these issues only partially. We propose a theoretically principled combination of stabilization techniques, Hyper++. Hyper++ substantially outperforms existing hyperbolic agents on ProcGen (PPO) and Atari (DDQN). Because we do not have the power iteration overhead from SpectralNorm, Hyper++ is also faster. Happy to answer any questions :)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14202",
      "pdf_url": "https://arxiv.org/pdf/2512.14202",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14202",
      "scraped_at": "2025-12-19T01:48:00.784957"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
    "paper_url": "https://huggingface.co/papers/2512.14719",
    "authors": [
      "Yuanxing Zhang",
      "Shangyuan Li",
      "Feng Zhang",
      "Zhuoran Zhang",
      "DogNeverSleep"
    ],
    "stars": "0",
    "details": {
      "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
      "abstract": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14719",
      "pdf_url": "https://arxiv.org/pdf/2512.14719",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14719",
      "scraped_at": "2025-12-19T01:48:02.608177"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
    "paper_url": "https://huggingface.co/papers/2512.13077",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
      "abstract": "Memory ‚â† likability. LikeBench shows that models can remember more but still feel worse to talk to, and even SOTA models struggle to become likable over time despite having more information about a user.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13077",
      "pdf_url": "https://arxiv.org/pdf/2512.13077",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13077",
      "scraped_at": "2025-12-19T01:48:04.499671"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2512.09851",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
      "abstract": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09851",
      "pdf_url": "https://arxiv.org/pdf/2512.09851",
      "github_links": [
        "https://github.com/YuyangLee/TacThru"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09851",
      "scraped_at": "2025-12-19T01:48:06.352295"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
    "paper_url": "https://huggingface.co/papers/2512.15340",
    "authors": [
      "Kun Li",
      "Qing Zhou",
      "Zhihao Huang",
      "Fei Wang",
      "Junjie Chen"
    ],
    "stars": "6",
    "details": {
      "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
      "abstract": "Human conversation is a continuous exchange of speech and nonverbal cues‚Äîincluding head nods, gaze shifts, and subtle expressions. Most existing approaches, however, treat talking-head and listening-head generation as separate problems, or rely on non-causal full-sequence modeling that is unsuitable for real-time interaction. We propose a causal, turn-level framework for interactive 3D conversational head generation. Our method models dialogue as a sequence of causally linked turns, where each turn accumulates multimodal context from both participants to produce coherent, responsive, and humanlike 3D head dynamics.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15340",
      "pdf_url": "https://arxiv.org/pdf/2512.15340",
      "github_links": [
        "https://github.com/CoderChen01/towards-seamleass-interaction/blob/main/README.md",
        "https://github.com/CoderChen01/towards-seamleass-interaction"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15340",
      "scraped_at": "2025-12-19T01:48:08.178873"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
    "paper_url": "https://huggingface.co/papers/2512.14080",
    "authors": [
      "Tri Dao",
      "Ion Stoica",
      "Xinle Cheng",
      "Mayank Mishra",
      "Wentao Guo"
    ],
    "stars": "0",
    "details": {
      "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
      "abstract": "We propose to co-design the MoE architecture with a GPU kernel tailored to NVIDIA Blackwell and Hopper generation GPUs and a novel routing method. (1) We derive an algorithm to compute the MoE backward pass more efficiently leading to a much smaller activation memory footprint that does not increase with increasing expert granularity. (2) We leverage new hardware features on Blackwell and Hopper GPUs to overlap memory IO with computation which can benefit all MoEs, and, in particular, fine-grained MoEs. (3) We propose a hardware-aware token rounding routing method where the routed number of tokens to an expert is always a multiple of the GEMM tile size. looks amazing!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14080",
      "pdf_url": "https://arxiv.org/pdf/2512.14080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14080",
      "scraped_at": "2025-12-19T01:48:10.046129"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Kling-Omni Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kling-Omni Technical Report",
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16776",
      "pdf_url": "https://arxiv.org/pdf/2512.16776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16776",
      "scraped_at": "2025-12-20T01:41:55.235420"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Adaptation of Agentic AI",
    "paper_url": "https://huggingface.co/papers/2512.16301",
    "authors": [
      "XueqiangXu",
      "p-song1",
      "Gabshi",
      "linjc16",
      "pat-jj"
    ],
    "stars": "262",
    "details": {
      "title": "Adaptation of Agentic AI",
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16301",
      "pdf_url": "https://arxiv.org/pdf/2512.16301",
      "github_links": [
        "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16301",
      "scraped_at": "2025-12-20T01:41:57.144311"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "paper_url": "https://huggingface.co/papers/2512.15745",
    "authors": [],
    "stars": "159",
    "details": {
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15745",
      "pdf_url": "https://arxiv.org/pdf/2512.15745",
      "github_links": [
        "https://github.com/inclusionAI/LLaDA2.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15745",
      "scraped_at": "2025-12-20T01:41:59.124430"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "paper_url": "https://huggingface.co/papers/2512.16922",
    "authors": [],
    "stars": "43",
    "details": {
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "abstract": "Make SSL great again.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16922",
      "pdf_url": "https://arxiv.org/pdf/2512.16922",
      "github_links": [
        "https://github.com/SihanXU/nepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16922",
      "scraped_at": "2025-12-20T01:42:01.031451"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "paper_url": "https://huggingface.co/papers/2512.16915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "abstract": "StereoPilot replaces the fragile \"Depth-Warp-Inpaint\" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo dataset with a learnable domain switcher, it provides a unified and efficient solution for both parallel and converged 3D video formats.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16915",
      "pdf_url": "https://arxiv.org/pdf/2512.16915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16915",
      "scraped_at": "2025-12-20T01:42:02.925012"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "paper_url": "https://huggingface.co/papers/2512.13507",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
      "abstract": "Seedance 1.5 pro Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13507",
      "pdf_url": "https://arxiv.org/pdf/2512.13507",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13507",
      "scraped_at": "2025-12-20T01:42:04.842028"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "paper_url": "https://huggingface.co/papers/2512.16913",
    "authors": [
      "Wenxuan Lu",
      "Dizhe Zhang",
      "Meixi Song",
      "Xin Lin",
      "haodongli"
    ],
    "stars": "0",
    "details": {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP website/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16913",
      "pdf_url": "https://arxiv.org/pdf/2512.16913",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16913",
      "scraped_at": "2025-12-20T01:42:06.700701"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.16923",
    "authors": [
      "Yu-Lun Liu",
      "Jia-Bin Huang",
      "rayray9999"
    ],
    "stars": "27",
    "details": {
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16923",
      "pdf_url": "https://arxiv.org/pdf/2512.16923",
      "github_links": [
        "https://github.com/rayray9999/Genfocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16923",
      "scraped_at": "2025-12-20T01:42:08.570320"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16625",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
      "abstract": "‚ú® Image editing is awesome; but it can leak user information! üõ°Ô∏è Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. üìÑ Paper: https://arxiv.org/abs/2512.16625 üíª Code: https://github.com/LinghuiiShen/DeContext üåê Project Page: https://linghuiishen.github.io/decontext_project_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16625",
      "pdf_url": "https://arxiv.org/pdf/2512.16625",
      "github_links": [
        "https://github.com/LinghuiiShen/DeContext"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16625",
      "scraped_at": "2025-12-20T01:42:10.471123"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.16636",
    "authors": [
      "Giorgos Sfikas",
      "Theodoros Giannakopoulos",
      "Bill Psomas",
      "Christos Sgouropoulos",
      "Giorgos Petsangourakis"
    ],
    "stars": "0",
    "details": {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16636",
      "pdf_url": "https://arxiv.org/pdf/2512.16636",
      "github_links": [
        "https://github.com/giorgospets/reglue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16636",
      "scraped_at": "2025-12-20T01:42:12.346186"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "paper_url": "https://huggingface.co/papers/2512.16905",
    "authors": [
      "Jiarong Ou",
      "Miao Yang",
      "Xi Chen",
      "Yang Zhou",
      "Kaixin Ding"
    ],
    "stars": "0",
    "details": {
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "abstract": "data selection",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16905",
      "pdf_url": "https://arxiv.org/pdf/2512.16905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16905",
      "scraped_at": "2025-12-20T01:42:14.238040"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "paper_url": "https://huggingface.co/papers/2512.16924",
    "authors": [],
    "stars": "54",
    "details": {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "abstract": "Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16924",
      "pdf_url": "https://arxiv.org/pdf/2512.16924",
      "github_links": [
        "https://github.com/pPetrichor/WorldCanvas"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16924",
      "scraped_at": "2025-12-20T01:42:16.096041"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.16561",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "abstract": "Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16561",
      "pdf_url": "https://arxiv.org/pdf/2512.16561",
      "github_links": [
        "https://github.com/W-Ted/N3D-VLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16561",
      "scraped_at": "2025-12-20T01:42:17.998665"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "paper_url": "https://huggingface.co/papers/2512.16649",
    "authors": [],
    "stars": "70",
    "details": {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "abstract": "‚ú®What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2√ó less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16649",
      "pdf_url": "https://arxiv.org/pdf/2512.16649",
      "github_links": [
        "https://github.com/thunlp/JustRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16649",
      "scraped_at": "2025-12-20T01:42:19.922832"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "paper_url": "https://huggingface.co/papers/2512.16918",
    "authors": [
      "Zhixun Li",
      "Zhongyu Wang",
      "Dongyang Chen",
      "Kaituo Feng",
      "Chaoyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "abstract": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16918",
      "pdf_url": "https://arxiv.org/pdf/2512.16918",
      "github_links": [
        "https://github.com/CYWang735/AdaTooler-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16918",
      "scraped_at": "2025-12-20T01:42:21.777535"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "paper_url": "https://huggingface.co/papers/2512.16899",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "abstract": "Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta Superintelligence Labs released their reward benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16899",
      "pdf_url": "https://arxiv.org/pdf/2512.16899",
      "github_links": [
        "https://github.com/facebookresearch/MMRB2/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16899",
      "scraped_at": "2025-12-20T01:42:23.598937"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "paper_url": "https://huggingface.co/papers/2512.16920",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16920",
      "pdf_url": "https://arxiv.org/pdf/2512.16920",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16920",
      "scraped_at": "2025-12-20T01:42:25.490781"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "paper_url": "https://huggingface.co/papers/2512.16912",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16912",
      "pdf_url": "https://arxiv.org/pdf/2512.16912",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16912",
      "scraped_at": "2025-12-20T01:42:27.312082"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "paper_url": "https://huggingface.co/papers/2512.16900",
    "authors": [],
    "stars": "51",
    "details": {
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16900",
      "pdf_url": "https://arxiv.org/pdf/2512.16900",
      "github_links": [
        "https://github.com/Francis-Rings/FlashPortrait"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16900",
      "scraped_at": "2025-12-20T01:42:29.189155"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "paper_url": "https://huggingface.co/papers/2512.16864",
    "authors": [
      "Yuqi Liu",
      "Longxiang Tang",
      "Xiaohang Zhan",
      "Lei Ke",
      "TainU"
    ],
    "stars": "8",
    "details": {
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "abstract": "üöß The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). üìâ The Gap: Global Semantic Guidance Existing methods, including unified VLM-diffusion architectures, predominantly rely on Global Semantic Guidance . They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. üöÄ Our Solution: Region-Aligned Guidance RePlan introduces a Plan-then-Execute framework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performs Chain-of-Thought (CoT) reasoning to decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enables precise, multi-region parallel edits in a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities using Group Relative Policy Optimization (GRPO) . Remarkably, we achieve strong planning performance using only ~1k instruction-only samples , bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance is fully editable , enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supports regional negative prompts to prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establish IV-Edit , the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16864",
      "pdf_url": "https://arxiv.org/pdf/2512.16864",
      "github_links": [
        "https://github.com/dvlab-research/RePlan"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16864",
      "scraped_at": "2025-12-20T01:42:31.186498"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "paper_url": "https://huggingface.co/papers/2512.16501",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16501",
      "pdf_url": "https://arxiv.org/pdf/2512.16501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16501",
      "scraped_at": "2025-12-20T01:42:33.122365"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "paper_url": "https://huggingface.co/papers/2512.16106",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "ModelTables: A Corpus of Tables about Models",
      "abstract": "ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and configuration tables. The benchmark defines model and table relatedness using multiple signals, including paper citations, model-card links and inheritance, and shared training datasets, and supports downstream applications such as table discovery and semantic retrieval from both structural and semantic perspectives.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16106",
      "pdf_url": "https://arxiv.org/pdf/2512.16106",
      "github_links": [
        "https://github.com/RJMillerLab/ModelTables"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16106",
      "scraped_at": "2025-12-20T01:42:35.008094"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_url": "https://huggingface.co/papers/2512.16378",
    "authors": [
      "Carlos Escolano",
      "Vil√©m Zouhar",
      "zhopto3",
      "javi8979",
      "spapi"
    ],
    "stars": "13",
    "details": {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "abstract": "Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challenging conditions (noise, accents, disfluencies, long-form). Results show that cascades remain the most reliable overall, while SpeechLLMs close the gap in specific settings (notably noise and code-switching).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16378",
      "pdf_url": "https://arxiv.org/pdf/2512.16378",
      "github_links": [
        "https://github.com/sarapapi/hearing2translate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16378",
      "scraped_at": "2025-12-20T01:42:36.921178"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "paper_url": "https://huggingface.co/papers/2512.16921",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. Project Page: https://auditdm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16921",
      "pdf_url": "https://arxiv.org/pdf/2512.16921",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16921",
      "scraped_at": "2025-12-20T01:42:38.734491"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "paper_url": "https://huggingface.co/papers/2512.11251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
      "abstract": "Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptions.  Inspired by LLaVA, we showed instruction-tuning on TS-Insights can enable LLMs to understand time series as a native input modality and generate textual descriptions. This work was originally done in Summer 2023.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11251",
      "pdf_url": "https://arxiv.org/pdf/2512.11251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11251",
      "scraped_at": "2025-12-20T01:42:40.587194"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "paper_url": "https://huggingface.co/papers/2512.16767",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16767",
      "pdf_url": "https://arxiv.org/pdf/2512.16767",
      "github_links": [
        "https://github.com/jasongzy/Make-It-Poseable"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16767",
      "scraped_at": "2025-12-20T01:42:42.388264"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "paper_url": "https://huggingface.co/papers/2512.16670",
    "authors": [
      "Hendrik P. A. Lensch",
      "Ole Beisswenger",
      "JDihlmann"
    ],
    "stars": "0",
    "details": {
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "abstract": "FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination from a stream of G-buffer data. We overfit SD on single scenes and show that it learns the illumination setting for the scene and can transfer it to OOD views of the scene.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16670",
      "pdf_url": "https://arxiv.org/pdf/2512.16670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16670",
      "scraped_at": "2025-12-20T01:42:44.229155"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16615",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
      "abstract": "Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16615",
      "pdf_url": "https://arxiv.org/pdf/2512.16615",
      "github_links": [
        "https://github.com/SingleZombie/LLSA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16615",
      "scraped_at": "2025-12-20T01:42:46.106366"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.12576",
    "authors": [
      "Ben He",
      "Hongyu Lin",
      "Yanjiang Liu",
      "Jie Lou",
      "Aunderline"
    ],
    "stars": "0",
    "details": {
      "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
      "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12576",
      "pdf_url": "https://arxiv.org/pdf/2512.12576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12576",
      "scraped_at": "2025-12-20T01:42:48.019489"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "paper_url": "https://huggingface.co/papers/2512.16909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "abstract": "Project Page: https://hybridrobotics.github.io/MomaGraph/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16909",
      "pdf_url": "https://arxiv.org/pdf/2512.16909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16909",
      "scraped_at": "2025-12-20T01:42:49.825834"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.15907",
    "authors": [
      "Vivek Gupta",
      "Aparna Garimella",
      "Juhna Park",
      "Tejas Anvekar"
    ],
    "stars": "1",
    "details": {
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15907",
      "pdf_url": "https://arxiv.org/pdf/2512.15907",
      "github_links": [
        "https://github.com/CoRAL-ASU/TabReX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15907",
      "scraped_at": "2025-12-20T01:42:51.619129"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "paper_url": "https://huggingface.co/papers/2512.14884",
    "authors": [
      "Yutong Bai",
      "Michael D. Grossberg",
      "Andrew Lu",
      "Katherine Xu",
      "Huzheng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
      "abstract": "what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14884",
      "pdf_url": "https://arxiv.org/pdf/2512.14884",
      "github_links": [
        "https://github.com/huzeyann/VibeSpace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14884",
      "scraped_at": "2025-12-20T01:42:53.427990"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "paper_url": "https://huggingface.co/papers/2512.10953",
    "authors": [
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Qiao Sun",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "abstract": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10953",
      "pdf_url": "https://arxiv.org/pdf/2512.10953",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10953",
      "scraped_at": "2025-12-20T01:42:55.227843"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "paper_url": "https://huggingface.co/papers/2512.15528",
    "authors": [
      "Can Ma. Yu Zhou",
      "Dongbao Yang",
      "Daiqing Wu"
    ],
    "stars": "1",
    "details": {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "abstract": "Update the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15528",
      "pdf_url": "https://arxiv.org/pdf/2512.15528",
      "github_links": [
        "https://github.com/wdqqdw/EmoCaliber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15528",
      "scraped_at": "2025-12-20T01:42:57.027138"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "paper_url": "https://huggingface.co/papers/2512.15489",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "abstract": "Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization‚Äîespecially on HLE-Math‚Äîwhile maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15489",
      "pdf_url": "https://arxiv.org/pdf/2512.15489",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15489",
      "scraped_at": "2025-12-20T01:42:58.892206"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "paper_url": "https://huggingface.co/papers/2512.14805",
    "authors": [
      "Michael Carbin",
      "Tian Jin",
      "Logan Weber",
      "ellieyhc"
    ],
    "stars": "3",
    "details": {
      "title": "Sharing State Between Prompts and Programs",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14805",
      "pdf_url": "https://arxiv.org/pdf/2512.14805",
      "github_links": [
        "https://github.com/psg-mit/nightjarpy/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14805",
      "scraped_at": "2025-12-20T01:43:00.877943"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "paper_url": "https://huggingface.co/papers/2512.12880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Improving Recursive Transformers with Mixture of LoRAs",
      "abstract": "Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected shared FFNs with a small set of token-routed LoRA experts (sparse routing), allowing conditional computation while keeping the backbone compact. We pretrain ModernALBERT (50M to 120M) with RoPE, GeGLU, FlashAttention, and distillation-based initialisation, and report state-of-the-art results among compact models on GLUE, SQuAD-v2, and BEIR, often surpassing larger fully parameterised baselines. For deployment, we introduce expert merging (including an EMA-based strategy) that compresses MoL into a single adapter at inference, removing routing overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12880",
      "pdf_url": "https://arxiv.org/pdf/2512.12880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12880",
      "scraped_at": "2025-12-20T01:43:02.725068"
    },
    "scraped_date": "2025-12-20"
  }
]