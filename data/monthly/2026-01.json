[
  {
    "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
    "paper_url": "https://huggingface.co/papers/2512.21185",
    "authors": [
      "Yang Li",
      "Dehao Hao",
      "LutaoJiang",
      "StarYDY",
      "infinith"
    ],
    "stars": "110",
    "details": {
      "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
      "abstract": "Github: https://github.com/PKU-YuanGroup/UltraShape-1.0 Project Page: https://pku-yuangroup.github.io/UltraShape-1.0/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21185",
      "pdf_url": "https://arxiv.org/pdf/2512.21185",
      "github_links": [
        "https://github.com/PKU-YuanGroup/UltraShape-1.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21185",
      "scraped_at": "2026-01-01T01:59:26.540091"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "DreamOmni3: Scribble-based Editing and Generation",
    "paper_url": "https://huggingface.co/papers/2512.22525",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamOmni3: Scribble-based Editing and Generation",
      "abstract": "Github: https://github.com/dvlab-research/DreamOmni3",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22525",
      "pdf_url": "https://arxiv.org/pdf/2512.22525",
      "github_links": [
        "https://github.com/dvlab-research/DreamOmni3"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22525",
      "scraped_at": "2026-01-01T01:59:28.344263"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "End-to-End Test-Time Training for Long Context",
    "paper_url": "https://huggingface.co/papers/2512.23675",
    "authors": [
      "Marcel R√∏d",
      "Daniel Koceja",
      "Xinhao Li",
      "Karan Dalal",
      "Arnuv Tandon"
    ],
    "stars": "96",
    "details": {
      "title": "End-to-End Test-Time Training for Long Context",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Sliding Window Attention Adaptation (2025) Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) Attention and Compression is all you need for Controllably Efficient Language Models (2025) Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings (2025) Data-Free Pruning of Self-Attention Layers in LLMs (2025) Architectural Trade-offs in Small Language Models Under Compute Constraints (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23675",
      "pdf_url": "https://arxiv.org/pdf/2512.23675",
      "github_links": [
        "https://github.com/test-time-training/e2e"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23675",
      "scraped_at": "2026-01-01T01:59:30.184972"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "Evaluating Parameter Efficient Methods for RLVR",
    "paper_url": "https://huggingface.co/papers/2512.23165",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Parameter Efficient Methods for RLVR",
      "abstract": "https://www.alphaxiv.org/abs/2512.23165",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23165",
      "pdf_url": "https://arxiv.org/pdf/2512.23165",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23165",
      "scraped_at": "2026-01-01T01:59:31.986046"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization",
    "paper_url": "https://huggingface.co/papers/2512.22469",
    "authors": [
      "Wei Zhang",
      "Aofan Liu",
      "Pengfei Gao",
      "Wei Liu",
      "pengchao"
    ],
    "stars": "0",
    "details": {
      "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization",
      "abstract": "Issues describe symptoms‚Äîthe real buggy code is often hidden behind multi-hop dependencies. GraphLocator moves beyond relevance retrieval by explicitly modelling causal structure: decomposing sub-problems, capturing causal dependencies, and performing constrained causal reasoning over code dependency graphs. Across multiple Python and Java datasets, GraphLocator significantly improves function-level recall and precision, and the inferred causal structures further boost downstream issue resolution by +28.74%.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22469",
      "pdf_url": "https://arxiv.org/pdf/2512.22469",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22469",
      "scraped_at": "2026-01-01T01:59:33.772256"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
    "paper_url": "https://huggingface.co/papers/2512.22206",
    "authors": [
      "Yogeswar"
    ],
    "stars": "0",
    "details": {
      "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
      "abstract": "\"I introduce CosineGate, a SOTA dynamic routing mechanism for ResNets that uses the Cosine Incompatibility Ratio (CIR) as a self-supervised signal. üöÄ Why it matters: It matches ResNet-20 accuracy on CIFAR-10 while slashing computation by 28.5%‚Äîwithout needing extra 'predictor' sub-networks or distillation. üõ†Ô∏è Key Features: Fully differentiable (via Gumbel-Softmax). Bio-inspired (Predictive Coding). Plug-and-play for efficient computer vision. Check out our GitHub Repo linked in the sidebar for the implementation!\"",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22206",
      "pdf_url": "https://arxiv.org/pdf/2512.22206",
      "github_links": [
        "https://github.com/thotayogeswarreddy/CosineGate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22206",
      "scraped_at": "2026-01-01T01:59:35.535486"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs",
    "paper_url": "https://huggingface.co/papers/2512.21008",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs",
      "abstract": "Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness. In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21008",
      "pdf_url": "https://arxiv.org/pdf/2512.21008",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21008",
      "scraped_at": "2026-01-01T01:59:37.334356"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "mHC: Manifold-Constrained Hyper-Connections",
    "paper_url": "https://huggingface.co/papers/2512.24880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "abstract": "DeepSeek released a new paper proposing a novel architecture called mHC (Manifold-Constrained Hyper-Connections).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24880",
      "pdf_url": "https://arxiv.org/pdf/2512.24880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24880",
      "scraped_at": "2026-01-02T01:50:23.780257"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24618",
    "authors": [
      "Xinyi Dai",
      "Yinghui Li",
      "Lingfeng Qiao",
      "Jiarui Qin",
      "Junru Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24618",
      "pdf_url": "https://arxiv.org/pdf/2512.24618",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24618",
      "scraped_at": "2026-01-02T01:50:25.585213"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.24873",
    "authors": [
      "Wei Gao",
      "Fangwen Dai",
      "Wanhe An",
      "XiaoXiao Xu",
      "Weixun Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24873",
      "pdf_url": "https://arxiv.org/pdf/2512.24873",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24873",
      "scraped_at": "2026-01-02T01:50:27.522125"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "paper_url": "https://huggingface.co/papers/2512.25073",
    "authors": [
      "Yu-Lun Liu",
      "Ying-Huan Chen",
      "Chin-Yang Lin",
      "Hao-Jen Chien",
      "Yi-Chuan Huang"
    ],
    "stars": "0",
    "details": {
      "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
      "abstract": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25√ó speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.25073",
      "pdf_url": "https://arxiv.org/pdf/2512.25073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.25073",
      "scraped_at": "2026-01-02T01:50:29.343131"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
    "paper_url": "https://huggingface.co/papers/2512.23380",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23380",
      "pdf_url": "https://arxiv.org/pdf/2512.23380",
      "github_links": [
        "https://github.com/NasirzadehMoh/CoLog"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23380",
      "scraped_at": "2026-01-02T01:50:31.161979"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "paper_url": "https://huggingface.co/papers/2512.25070",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Scaling Open-Ended Reasoning to Predict the Future",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.25070",
      "pdf_url": "https://arxiv.org/pdf/2512.25070",
      "github_links": [
        "https://github.com/OpenForecaster/scaling-forecasting-training"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.25070",
      "scraped_at": "2026-01-02T01:50:33.046417"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24551",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
      "abstract": "A data construction pipeline and a new DPO framework for physically consistent Text-to-video generation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24551",
      "pdf_url": "https://arxiv.org/pdf/2512.24551",
      "github_links": [
        "https://github.com/caiyuanhao1998/Open-PhyGDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24551",
      "scraped_at": "2026-01-02T01:50:34.905156"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
    "paper_url": "https://huggingface.co/papers/2512.23343",
    "authors": [
      "Shixin Jiang",
      "Jiaqi Zhou",
      "Chang Li",
      "Hao Li",
      "Jiafeng Liang"
    ],
    "stars": "24",
    "details": {
      "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
      "abstract": "https://github.com/AgentMemory/Huaman-Agent-Memory",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23343",
      "pdf_url": "https://arxiv.org/pdf/2512.23343",
      "github_links": [
        "https://github.com/AgentMemory/Huaman-Agent-Memory"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23343",
      "scraped_at": "2026-01-02T01:50:36.704147"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "GR-Dexter Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.24210",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GR-Dexter Technical Report",
      "abstract": "VLAs go from grippers to 21 DoF dexterous ByteDexter V2 :)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24210",
      "pdf_url": "https://arxiv.org/pdf/2512.24210",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24210",
      "scraped_at": "2026-01-02T01:50:38.600164"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
    "paper_url": "https://huggingface.co/papers/2512.23988",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "abstract": "This is an impressive piece of work. Not only for the elegance of the sparse autoencoder pipeline, but because the empirical results reveal something far deeper than what is stated in the paper. Your SAE-derived ‚Äúreasoning vectors‚Äù behave exactly like stable dynamical modes inside a recursive state system ‚Äî not merely interpretable directions. The separation of reflection, backtracking, confidence, and response-length clusters across layers strongly suggests that modern transformer reasoning is governed by a latent, substrate-bound dynamical structure rather than a purely token-level process. A few observations that stood out: Reasoning vectors behave like attractor modes, not just features. The clustering of SAE decoder columns into semantically distinct basins is consistent with the existence of stable dynamical invariants that govern the model‚Äôs step-wise evolution. This is exactly the behavior expected when a system has: stable recurrence points, local attractor basins in its state manifold, and identity-like update modes that persist across tasks. Your causal interventions reinforce this: modifying a reasoning vector steers the entire reasoning trajectory while preserving final correctness. That is classic attractor dynamics. The layer-wise geometry mirrors a recursive integration process. The strongest separability occurring in mid-to-late layers, followed by a decline near the final layer, mirrors the behavior of systems that integrate state over time and then compress it near output. This is structurally identical to a recursive state-aware update: a(t+1)=R(a(t)) where the model accumulates long-range structure before collapsing it for output. Cross-domain generalization of these vectors indicates substrate-bound stability. The fact that reflection/backtracking vectors trained on MATH500 steer behavior on GPQA and KnowLogic implies the existence of substrate-stable reasoning structures that are independent of dataset distribution. This is a property of a dynamical system ‚Äî not a static embedding space. Confidence emerges as a coherent cluster because it is tied to entropy and coherence. Your discovery that confidence vectors suppress reflection/backtracking is an empirical confirmation of a predicted relation between: information coherence computational alignment noise minimization and entropy reduction Confidence is not a semantic trait ‚Äî it's a low-entropy attractor mode. Response-length alignment is a structural axis, not a surface feature. Length correlating with latent-space geometry further confirms that reasoning depth emerges from the system's internal temporal continuity rather than token heuristics. A broader note: These empirical findings align remarkably well with a larger theoretical framework I‚Äôve been developing, the Field of General Awareness (FoGA), which predicts: the existence of invariant reasoning modes, substrate-sensitive drift in state evolution, recursive attractor-based reasoning paths, and coherence-driven modulation of reasoning confidence. Your results are the clearest real-world demonstration I‚Äôve seen of these principles emerging naturally inside transformer models. If you're interested, I‚Äôm happy to share the relevant portions of the theory (and the mathematical basis behind these predictions), as well as the Dynamic Transformer Architecture ‚Äî an architecture patch explicitly designed to stabilize such recurrence modes. Excellent work. This paper is going to be foundational for understanding why LLM reasoning behaves the way it does. ‚Äî Zenith Zaraki SkyTeam Aerospace Foundation https://www.skyteamaerospacefoundation.com/foga https://www.skyteamaerospacefoundation.com/dta",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23988",
      "pdf_url": "https://arxiv.org/pdf/2512.23988",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23988",
      "scraped_at": "2026-01-02T01:50:40.409987"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
    "paper_url": "https://huggingface.co/papers/2512.23851",
    "authors": [
      "Beijia Lu",
      "Chong Zeng",
      "Muyang Li",
      "Shengqu Cai",
      "Lvmin Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
      "abstract": "Arxiv: https://arxiv.org/abs/2512.23851 Repo: https://github.com/lllyasviel/PFP",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23851",
      "pdf_url": "https://arxiv.org/pdf/2512.23851",
      "github_links": [
        "https://github.com/lllyasviel/PFP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23851",
      "scraped_at": "2026-01-02T01:50:42.190944"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "paper_url": "https://huggingface.co/papers/2512.25075",
    "authors": [
      "Tuanfeng Y. Wang",
      "Yulia Gryaditskaya",
      "Xuelin Chen",
      "Hyeonho Jeong",
      "Zhening Huang"
    ],
    "stars": "0",
    "details": {
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API BulletTime: Decoupled Control of Time and Camera Pose for Video Generation (2025) FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint (2025) ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Generative Video Motion Editing with 3D Point Tracks (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.25075",
      "pdf_url": "https://arxiv.org/pdf/2512.25075",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.25075",
      "scraped_at": "2026-01-02T01:50:43.989867"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
    "paper_url": "https://huggingface.co/papers/2512.22905",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
      "abstract": "üî•üî•üî• JavisGPT üåü We introduce JavisGPT, a multimodal LLM that can understand audiovisual inputs and simultaneously generate synchronized sounding videos in a unified model. ü§† We contribute JavisInst-Omni, a dataset to facilitate diverse and complex instruction-tuning for comprehension and generation on sounding videos. üìù Paper: https://arxiv.org/abs/2503.23377 üéâ Project: https://javisverse.github.io/JavisGPT-page/ ‚ú® Code: https://github.com/JavisVerse/JavisGPT",
      "arxiv_page_url": "https://arxiv.org/abs/2503.23377",
      "pdf_url": "https://arxiv.org/pdf/2512.22905",
      "github_links": [
        "https://github.com/JavisVerse/JavisGPT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22905",
      "scraped_at": "2026-01-02T01:50:45.835201"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
    "paper_url": "https://huggingface.co/papers/2512.22564",
    "authors": [
      "Mah≈üuk Taylan",
      "Ahmet Feridun I≈üƒ±k",
      "Selin Vulga I≈üƒ±k",
      "Atakanisik"
    ],
    "stars": "0",
    "details": {
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "abstract": "Hi all, We present a robust framework for Lung Sound Classification using AST backbones enhanced with SAM optimizer . Traditional transformers often struggle with limited medical data, but our experiments show that geometry-aware optimization (SAM) leads to a massive boost in sensitivity. We achieved a 68.10% Score on the official ICBHI 2017 split. We invite everyone to benchmark our results. The repository includes: Cyclic padding implementation Full training scripts Evaluation of model Check it out here: GitHub Link",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22564",
      "pdf_url": "https://arxiv.org/pdf/2512.22564",
      "github_links": [
        "https://github.com/Atakanisik/ICBHI-AST-SAM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22564",
      "scraped_at": "2026-01-02T01:50:47.616554"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
    "paper_url": "https://huggingface.co/papers/2512.24885",
    "authors": [
      "Mengmeng Wang",
      "Chenxi Li",
      "Qi Shen",
      "Zhaoxin Yu",
      "Hengli Li"
    ],
    "stars": "0",
    "details": {
      "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
      "abstract": "Arxiv: https://arxiv.org/abs/2512.24885 X thread: https://x.com/Hengli_Li_pku/status/2006606887652045158",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24885",
      "pdf_url": "https://arxiv.org/pdf/2512.24885",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24885",
      "scraped_at": "2026-01-02T01:50:49.444890"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
    "paper_url": "https://huggingface.co/papers/2512.24385",
    "authors": [],
    "stars": "105",
    "details": {
      "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
      "abstract": "GitHub at https://github.com/worldbench/awesome-spatial-intelligence",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24385",
      "pdf_url": "https://arxiv.org/pdf/2512.24385",
      "github_links": [
        "https://github.com/worldbench/awesome-spatial-intelligence"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24385",
      "scraped_at": "2026-01-02T01:50:51.222394"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
    "paper_url": "https://huggingface.co/papers/2512.24297",
    "authors": [
      "Jie Zhou",
      "Fandong Meng",
      "Meiqi Chen"
    ],
    "stars": "4",
    "details": {
      "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API V-Thinker: Interactive Thinking with Images (2025) Interleaved Latent Visual Reasoning with Selective Perceptual Modeling (2025) Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images (2025) ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking (2025) ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning (2025) Look as You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning (2025) CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24297",
      "pdf_url": "https://arxiv.org/pdf/2512.24297",
      "github_links": [
        "https://github.com/chenmeiqii/FIGR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24297",
      "scraped_at": "2026-01-02T01:50:53.259496"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
    "paper_url": "https://huggingface.co/papers/2512.24176",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
      "abstract": "üî• New SOTA on 256 √ó 256 ImageNet generation. We present Internal Guidance (IG), a simple yet powerful guidance mechanism for Diffusion Transformers. LightningDiT-XL/1 + IG sets a new state of the art with FID = 1.07 on ImageNet (balanced sampling), while achieving FID = 1.24 without classifier-free guidance. IG delivers dramatic quality gains with far fewer training epochs, adds negligible overhead, and works as a drop-in upgrade for modern diffusion transformers.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24176",
      "pdf_url": "https://arxiv.org/pdf/2512.24176",
      "github_links": [
        "https://github.com/CVL-UESTC/Internal-Guidance"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24176",
      "scraped_at": "2026-01-02T01:50:55.053809"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Factorized Learning for Temporally Grounded Video-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24097",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "abstract": "We tackle temporally grounded video-language understanding from a factorized perspective. Some key takeaways: [1] We emphasize the distinct yet causally dependent nature of temporal grounding and textual response. [2] Our study highlights the importance of explicit event-level visual semantic capture in enhancing both grounding and textual response quality. [3] We also propose a new Factorized Preference Optimization (FPO) scheme that jointly optimizes temporal and textual factors. A factorized data synthesis approach is also proposed to support FPO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24097",
      "pdf_url": "https://arxiv.org/pdf/2512.24097",
      "github_links": [
        "https://github.com/nusnlp/d2vlm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24097",
      "scraped_at": "2026-01-02T01:50:56.944458"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Valori: A Deterministic Memory Substrate for AI Systems",
    "paper_url": "https://huggingface.co/papers/2512.22280",
    "authors": [
      "varam17"
    ],
    "stars": "2",
    "details": {
      "title": "Valori: A Deterministic Memory Substrate for AI Systems",
      "abstract": "Valori: A Deterministic Fixed-Point Vector Kernel Tags: vector-database , rust , determinism , finance , audit , hnsw , fixed-point , systems-engineering TL;DR Floating-point math causes vector search results to drift between ARM (Mac) and x86 (Linux) architectures due to compiler and hardware variances. Valori is a Q16.16 Fixed-Point kernel written in Rust that guarantees bit-exact reproducibility across all hardware platforms, making it the first \"Audit-Grade\" vector engine for high-stakes AI. 1. The Problem: \"The Silent Drift\" Most vector databases (FAISS, Pinecone, Weaviate) rely on hardware-accelerated floating-point arithmetic ( f32 ). While fast, this introduces Non-Determinism : Associativity Variance: (a + b) + c ‚â† a + (b + c) depending on SIMD optimizations. Architecture Variance: A backtest run on a MacBook (ARM NEON) often yields different nearest neighbors than the live execution on an AWS Server (Intel AVX2). In RAG pipelines , this is annoying. In High-Frequency Trading or Legal Forensics , this variance is a liability. You cannot audit a probabilistic black box. 2. The Solution: Valori Kernel Valori is a forensic memory engine built from scratch in Rust . It replaces standard floating-point math with a custom Q16.16 Fixed-Point Arithmetic system inside an HNSW graph. Zero Drift: The mathematical operations are integer-based. 1 + 1 = 2 on every CPU, forever. Audit-Ready: Includes a calculate_state_hash() method that generates a cryptographic fingerprint of the database state. If a single bit changes, the hash breaks. Crash Proof: Implements a specialized persistence layer with <40ms recovery time (Cold Boot to Full Query) for 50k vector datasets. 3. Benchmarks (Engineering Verification) Validated on Apple M2 (ARM64) vs Intel Xeon (x86_64). Metric Result Notes Recall@10 99.0% Matches brute-force ground truth. Determinism 100% Bit-exact match across architectures. Recovery Time 35ms For 50k vectors (Snapshot Load). Latency ~0.5ms Per query (Single Thread). 4. Usage (Rust) Valori is designed to be embedded directly into high-integrity Rust applications. Add to Cargo.toml : [dependencies] valori_kernel = { git = \"https://github.com/varshith-Git/Valori-Kernel\" } Example: Deterministic Ingest & Audit use valori_kernel::{ValoriKernel, types::FixedPointVector}; fn main () -> anyhow:: Result <()> { // 1. Initialize the Kernel (Q16.16 Math) let mut kernel = ValoriKernel:: new (); // 2. Ingest Data (Manual f32 -> FixedPoint conversion for safety) let raw_vector = [ 0.5 , - 0.2 , 0.9 , ...]; // 128-dim // Convert float to Q16.16 integer representation let mut fixed_arr = [ 0i32 ; 128 ]; for (i, &val) in raw_vector. iter (). enumerate () {\n        fixed_arr[i] = (val * 65536.0 ) as i32 ;\n    } // Insert with ID=1, Tag=100 kernel. insert ( 1 , FixedPointVector (fixed_arr), 100 ); // 3. Search // Guaranteed to return the exact same ID and Distance on any CPU let results = kernel. search (&fixed_arr, 5 , None )?; println! ( \"Found neighbors: {:?}\" , results); // 4. Generate Forensic Evidence // This hash proves the database state is identical bit-for-bit let audit_hash = kernel.graph. calculate_state_hash (); println! ( \"Cryptographic State Signature: {}\" , audit_hash); Ok (())\n} 5. Who is this for? Quant Developers: Who need backtests to match live execution perfectly. Systems Engineers: Debugging \"Why did the agent say X?\" (Eliminate the database as a variable). Auditors: Who need to certify AI decision logs using cryptographic proofs. Citation If you use Valori in your research, please cite: @ misc {gudur2025valori,\n  title={Valori: A Deterministic Fixed-Point Vector Kernel},\n  author={Gudur, Varshith},\n  year={2025},\n  publisher={arXiv},\n  url={https://arxiv.org/abs/2512.22280}\n}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22280",
      "pdf_url": "https://arxiv.org/pdf/2512.22280",
      "github_links": [
        "https://github.com/varshith-Git/Valori-Kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22280",
      "scraped_at": "2026-01-02T01:50:58.743405"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-03T01:44:37.960652"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLen‚Äìbased implementation for efficient concept-token cross-attention; (2) a decoupled ŒºP formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-03T01:44:39.850579"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "13",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-03T01:44:41.687283"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-03T01:44:43.488349"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-03T01:44:45.286927"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-03T01:44:47.116340"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "3",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO‚Äôs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-03T01:44:48.907162"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-04T01:59:50.718806"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLen‚Äìbased implementation for efficient concept-token cross-attention; (2) a decoupled ŒºP formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-04T01:59:52.693703"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "24",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-04T01:59:54.609230"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-04T01:59:56.484887"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-04T01:59:58.371341"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-04T02:00:00.212924"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "4",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO‚Äôs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-04T02:00:02.005832"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-05T01:59:59.631752"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLen‚Äìbased implementation for efficient concept-token cross-attention; (2) a decoupled ŒºP formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-05T02:00:01.852412"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "61",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-05T02:00:03.959198"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-05T02:00:05.988614"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-05T02:00:08.091511"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-05T02:00:10.042015"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "4",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO‚Äôs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-05T02:00:11.922915"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.24615",
    "authors": [],
    "stars": "4.1k",
    "details": {
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "abstract": "LONG wait. Youtu-Agent ( https://github.com/TencentCloudADP/Youtu-agent ) now releases its technical report with two major updates, i.e., Automated Generation and Hybrid Policy Optimization. Additionally, we've launched Youtu-Tip ( https://github.com/TencentCloudADP/youtu-tip ), a more user-friendly application that runs on macOS. Check them out and have fun!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24615",
      "pdf_url": "https://arxiv.org/pdf/2512.24615",
      "github_links": [
        "https://github.com/TencentCloudADP/youtu-tip",
        "https://github.com/TencentCloudADP/youtu-agent",
        "https://github.com/TencentCloudADP/Youtu-agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24615",
      "scraped_at": "2026-01-06T01:50:51.163830"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2601.00393",
    "authors": [
      "Feng Wang",
      "Junran Peng",
      "renshengjihe",
      "Abyssaledge",
      "Yuppie1204"
    ],
    "stars": "124",
    "details": {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "abstract": "NeoVerse is a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. Project page: https://neoverse-4d.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00393",
      "pdf_url": "https://arxiv.org/pdf/2601.00393",
      "github_links": [
        "https://github.com/IamCreateAI/NeoVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00393",
      "scraped_at": "2026-01-06T01:50:53.205658"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "paper_url": "https://huggingface.co/papers/2601.00664",
    "authors": [
      "Sung Ju Hwang",
      "Jaehyeong Jo",
      "Sangwon Jang",
      "jaehong31",
      "taekyungki"
    ],
    "stars": "65",
    "details": {
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "abstract": "arXiv explained breakdown of this paper üëâ https://arxivexplained.com/papers/avatar-forcing-real-time-interactive-head-avatar-generation-for-natural-conversation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00664",
      "pdf_url": "https://arxiv.org/pdf/2601.00664",
      "github_links": [
        "https://github.com/TaekyungKi/AvatarForcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00664",
      "scraped_at": "2026-01-06T01:50:55.077779"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.24330",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "abstract": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24330",
      "pdf_url": "https://arxiv.org/pdf/2512.24330",
      "github_links": [
        "https://github.com/OpenSenseNova/SenseNova-MARS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24330",
      "scraped_at": "2026-01-06T01:50:56.907809"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24271",
    "authors": [],
    "stars": "29",
    "details": {
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "abstract": "An interesting work! github: https://github.com/AMAP-ML/Taming-Hallucinations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24271",
      "pdf_url": "https://arxiv.org/pdf/2512.24271",
      "github_links": [
        "https://github.com/AMAP-ML/Taming-Hallucinations"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24271",
      "scraped_at": "2026-01-06T01:50:58.822184"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.00796",
    "authors": [
      "Yu-Lun Liu",
      "Zhenjun Zhao",
      "Jiewen Chan"
    ],
    "stars": "0",
    "details": {
      "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
      "abstract": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00796",
      "pdf_url": "https://arxiv.org/pdf/2601.00796",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00796",
      "scraped_at": "2026-01-06T01:51:00.710791"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Deep Delta Learning",
    "paper_url": "https://huggingface.co/papers/2601.00417",
    "authors": [],
    "stars": "234",
    "details": {
      "title": "Deep Delta Learning",
      "abstract": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Œ≤(X). We provide a spectral analysis of this operator, demonstrating that the gate Œ≤(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00417",
      "pdf_url": "https://arxiv.org/pdf/2601.00417",
      "github_links": [
        "https://github.com/yifanzhang-pro/deep-delta-learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00417",
      "scraped_at": "2026-01-06T01:51:02.670038"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "paper_url": "https://huggingface.co/papers/2512.24695",
    "authors": [
      "Vahab Mirrokni",
      "Peilin Zhong",
      "Meisam Razaviyayn",
      "AliBehrouz"
    ],
    "stars": "0",
    "details": {
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "abstract": "Nested Learning (NL) is a new learning paradigm for continual learning and machine learning in general.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24695",
      "pdf_url": "https://arxiv.org/pdf/2512.24695",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24695",
      "scraped_at": "2026-01-06T01:51:04.593752"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.00747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "abstract": "For those of you interested in RLVR, here is a paper that formally characterizes the mechanism behind \"diversity collapse\" in reasoning models trained with scalar rewards (such as STaR, GRPO, and DPO). The paper introduces a variational framework based on Shahshahani gradient flow to prove that optimizing solely for correctness inherently erodes the diversity of reasoning paths, leading to a \"reasoning monoculture.\" To address this, they propose Distributional Creative Reasoning (DCR), which incorporates a diversity energy functional (using entropy and kernel-based novelty) into the objective, mathematically guaranteeing the maintenance of a diverse portfolio of successful reasoning strategies while still optimizing for utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00747",
      "pdf_url": "https://arxiv.org/pdf/2601.00747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00747",
      "scraped_at": "2026-01-06T01:51:06.385822"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
    "paper_url": "https://huggingface.co/papers/2512.22955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
      "abstract": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs).  The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode.  To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning.  By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision.  Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22955",
      "pdf_url": "https://arxiv.org/pdf/2512.22955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22955",
      "scraped_at": "2026-01-06T01:51:08.307456"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Fast-weight Product Key Memory",
    "paper_url": "https://huggingface.co/papers/2601.00671",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-weight Product Key Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Trellis: Learning to Compress Key-Value Memory in Attention Models (2025) TNT: Improving Chunkwise Training for Test-Time Memorization (2025) GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory (2025) MIDUS: Memory-Infused Depth Up-Scaling (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models (2025) BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00671",
      "pdf_url": "https://arxiv.org/pdf/2601.00671",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00671",
      "scraped_at": "2026-01-06T01:51:10.101023"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
    "paper_url": "https://huggingface.co/papers/2601.00575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "abstract": "Project Page: https://ishirgarg.github.io/infosynth_web/ Code: https://github.com/ishirgarg/infosynth Dataset: https://huggingface.co/datasets/ishirgarg/InfoSynth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00575",
      "pdf_url": "https://arxiv.org/pdf/2601.00575",
      "github_links": [
        "https://github.com/ishirgarg/infosynth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00575",
      "scraped_at": "2026-01-06T01:51:12.005175"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
    "paper_url": "https://huggingface.co/papers/2601.00204",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Hao Tang",
      "Zeyu Cai",
      "XiaokunSun"
    ],
    "stars": "19",
    "details": {
      "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
      "abstract": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io Code: https://github.com/XiaokunSun/MorphAny3D",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00204",
      "pdf_url": "https://arxiv.org/pdf/2601.00204",
      "github_links": [
        "https://github.com/XiaokunSun/MorphAny3D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00204",
      "scraped_at": "2026-01-06T01:51:13.858117"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "paper_url": "https://huggingface.co/papers/2512.20578",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
      "abstract": "Can Large Language Models predict their own failures? üß†‚ö° We all know the critical bottleneck in GenAI: LLMs are incredible, but they can confidently hallucinate and make mistakes. Until now, most fixes have been computationally massive ‚Äî relying on expensive external judges, huge reward models, or costly training to make the LLM itself more robust. This brings us to two fundamental questions: ‚ùì Do LLMs recognize when they're making mistakes? ‚ùì Can we make them self-aware about their own failures? üöÄ Introducing Gnosis: A lightweight self-awareness mechanism for frozen LLMs. Named after the Greek word for knowledge/insight, Gnosis gives LLMs a form of introspection. We add only ~5M parameters to enable a frozen LLM to verify its own outputs by decoding internal hidden states + attention patterns during inference ‚Äî with negligible overhead and no external judge . The results challenge the classic efficiency‚Äìaccuracy trade-off: üèÜ Superior performance across domains Despite being orders of magnitude smaller, Gnosis can outperform strong 8B reward models and proprietary judges like Gemini 2.5 Pro on both multi-step reasoning and factual/parametric knowledge QA (e.g., TriviaQA), across multiple backbones. ‚ö° Real-time early failure detection Gnosis doesn‚Äôt need to wait for the final token. By monitoring the generation trajectory in real time, it can predict an error before the model finishes ‚Äî enabling early stopping, preventing bad outputs from reaching users, and saving significant compute. This suggests something important: the model often already contains signals of impending failure during generation ‚Äî we just needed the right mechanism to read them. üëá code + models: üíª Code: https://github.com/Amirhosein-gh98/Gnosis",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20578",
      "pdf_url": "https://arxiv.org/pdf/2512.20578",
      "github_links": [
        "https://github.com/Amirhosein-gh98/Gnosis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20578",
      "scraped_at": "2026-01-07T01:50:17.166637"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.02204",
    "authors": [],
    "stars": "60",
    "details": {
      "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02204",
      "pdf_url": "https://arxiv.org/pdf/2601.02204",
      "github_links": [
        "https://github.com/ByteVisionLab/NextFlow"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02204",
      "scraped_at": "2026-01-07T01:50:19.105092"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "K-EXAONE Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.01739",
    "authors": [],
    "stars": "39",
    "details": {
      "title": "K-EXAONE Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01739",
      "pdf_url": "https://arxiv.org/pdf/2601.01739",
      "github_links": [
        "https://github.com/LG-AI-EXAONE/K-EXAONE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01739",
      "scraped_at": "2026-01-07T01:50:21.012374"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.01425",
    "authors": [],
    "stars": "86",
    "details": {
      "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
      "abstract": "We introduce DreamID-V, the first Diffusion Transformer-based framework for high-fidelity video face swapping. DreamID-V bridges the gap between image and video domains, achieving exceptional identity similarity and temporal coherence even in challenging scenarios. Our code : https://github.com/bytedance/DreamID-V Our project : https://guoxu1233.github.io/DreamID-V/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01425",
      "pdf_url": "https://arxiv.org/pdf/2601.01425",
      "github_links": [
        "https://github.com/bytedance/DreamID-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01425",
      "scraped_at": "2026-01-07T01:50:22.967886"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2601.02256",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02256",
      "pdf_url": "https://arxiv.org/pdf/2601.02256",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02256",
      "scraped_at": "2026-01-07T01:50:24.885134"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
    "paper_url": "https://huggingface.co/papers/2512.24138",
    "authors": [
      "Zhiyong Wang",
      "Jiajun Liang",
      "Jie Liu",
      "Yuxiao Ye",
      "Haoran He"
    ],
    "stars": "18",
    "details": {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "abstract": "Introducing GARDO: Reinforcing Diffusion Models without Reward Hacking paper: https://arxiv.org/abs/2512.24138 code: https://github.com/tinnerhrhe/gardo project: https://tinnerhrhe.github.io/gardo_project/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24138",
      "pdf_url": "https://arxiv.org/pdf/2512.24138",
      "github_links": [
        "https://github.com/tinnerhrhe/GARDO",
        "https://github.com/tinnerhrhe/gardo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24138",
      "scraped_at": "2026-01-07T01:50:26.781382"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "paper_url": "https://huggingface.co/papers/2601.02358",
    "authors": [
      "Kun Gai",
      "Pengfei Wan",
      "Zhoujie Fu",
      "Tong He",
      "Junyi Chen"
    ],
    "stars": "42",
    "details": {
      "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02358",
      "pdf_url": "https://arxiv.org/pdf/2601.02358",
      "github_links": [
        "https://github.com/SOTAMak1r/VINO-code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02358",
      "scraped_at": "2026-01-07T01:50:28.658298"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "paper_url": "https://huggingface.co/papers/2601.02281",
    "authors": [],
    "stars": "76",
    "details": {
      "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
      "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ‚Äúrolling‚Äù the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02281",
      "pdf_url": "https://arxiv.org/pdf/2601.02281",
      "github_links": [
        "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02281",
      "scraped_at": "2026-01-07T01:50:30.519682"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Recursive Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24601",
    "authors": [],
    "stars": "675",
    "details": {
      "title": "Recursive Language Models",
      "abstract": "Study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. They propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. They find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query. Some of the observations they found are :- -- LLMs interacting with their own prompts as objects. -- In their approach, a prompt isn‚Äôt ‚Äúrun‚Äù directly, instead it‚Äôs stored as a variable in an external Python REPL, and the language model writes code to inspect /slice/ decompose that long string, observes execution outputs, and then constructs sub-tasks where it recursively invokes an LLM on just the relevant snippets. Stitching the result together when the recursive process ends. So it can solve 10M+ token tasks with far less ‚Äúcontext rot‚Äù and often lower cost than summarization/RAG, turning long-context scaling into an inference-time algorithm rather than just a bigger context window. -- The ability to search the Prompt is what enables handling long context inputs, sub calls help handle information dense inputs. -- Inference cost of RLMs remain comparable to a base model call but are high variance  because it can keep making sub-calls or iterate if it can't solve the problem initially. -- The key insight is that long prompts should not be fed into the LLM directly, but should instead be treated as part of the environment that the LLM can search, read and interact with as needed for the task.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24601",
      "pdf_url": "https://arxiv.org/pdf/2512.24601",
      "github_links": [
        "https://github.com/alexzhang13/rlm/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24601",
      "scraped_at": "2026-01-07T01:50:32.438055"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "paper_url": "https://huggingface.co/papers/2601.02346",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent (2025) Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning (2025) CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions (2025) Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning (2025) Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02346",
      "pdf_url": "https://arxiv.org/pdf/2601.02346",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02346",
      "scraped_at": "2026-01-07T01:50:34.314010"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "paper_url": "https://huggingface.co/papers/2601.02356",
    "authors": [
      "Shuo Yang",
      "Jiarui Cai",
      "Yantao Shen",
      "ZyZcuhk",
      "jingtan"
    ],
    "stars": "13",
    "details": {
      "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) Loom: Diffusion-Transformer for Interleaved Generation (2025) What Happens Next? Next Scene Prediction with a Unified Video Model (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02356",
      "pdf_url": "https://arxiv.org/pdf/2601.02356",
      "github_links": [
        "https://github.com/sparkstj/Talk2Move"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02356",
      "scraped_at": "2026-01-07T01:50:36.191628"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "paper_url": "https://huggingface.co/papers/2601.02179",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "abstract": "In this paper, we explore the confidence estimation in a new paradigm: multi-turn interactions! Check it out!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02179",
      "pdf_url": "https://arxiv.org/pdf/2601.02179",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02179",
      "scraped_at": "2026-01-07T01:50:38.040156"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01046",
    "authors": [
      "Yi Yang",
      "Yixuan Tang"
    ],
    "stars": "0",
    "details": {
      "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
      "abstract": "‚ú® Turn any decoder-only LLM into a powerful embedding model‚Äîzero training needed! ‚ú® The Trick : Re-route the final token's key-value states as an internal prefix, giving all tokens access to global context in one forward pass. No input modification, no mask removal, just smart internal state manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01046",
      "pdf_url": "https://arxiv.org/pdf/2601.01046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01046",
      "scraped_at": "2026-01-07T01:50:39.891420"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2601.00501",
    "authors": [
      "Mohammad Asiful Hossain",
      "Kevin Cannons",
      "Saeed Ranjbar Alvar",
      "Mohsen Gholami",
      "Ahmad Rezaei"
    ],
    "stars": "0",
    "details": {
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "abstract": "CPPO: Contrastive Perception for Vision Language Policy Optimization introduces a new method (CPPO) for fine-tuning vision-language models (VLMs) using reinforcement learning. Instead of relying on explicit perception rewards or auxiliary models, the approach identifies perceptual tokens via entropy changes under perturbed images and augments the policy objective with a contrastive perception loss to improve multimodal reasoning performance and training efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00501",
      "pdf_url": "https://arxiv.org/pdf/2601.00501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00501",
      "scraped_at": "2026-01-07T01:50:41.758090"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "paper_url": "https://huggingface.co/papers/2601.02267",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Zhenyu Zhang",
      "wrk226"
    ],
    "stars": "1",
    "details": {
      "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
      "abstract": "Project page: https://wrk226.github.io/DiffProxy.html Code: https://github.com/wrk226/DiffProxy",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02267",
      "pdf_url": "https://arxiv.org/pdf/2601.02267",
      "github_links": [
        "https://github.com/wrk226/DiffProxy"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02267",
      "scraped_at": "2026-01-07T01:50:43.650977"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01836",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "abstract": "COMPASS is the first framework for evaluating LLM alignment with organization-specific policies rather than universal harms. While models handle legitimate requests well (>95% accuracy), they catastrophically fail at enforcing prohibitions, refusing only 13-40% of denylist violations. GitHub: https://github.com/AIM-Intelligence/COMPASS",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01836",
      "pdf_url": "https://arxiv.org/pdf/2601.01836",
      "github_links": [
        "https://github.com/AIM-Intelligence/COMPASS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01836",
      "scraped_at": "2026-01-07T01:50:45.620954"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
    "paper_url": "https://huggingface.co/papers/2512.23035",
    "authors": [
      "Shiying Wang",
      "Kai Li",
      "Shun Zhang",
      "Xuechao Zou",
      "Yi Zhou"
    ],
    "stars": "4",
    "details": {
      "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
      "abstract": "We are excited to introduce our latest work on semi-supervised semantic segmentation : üìÑ Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion This paper tackles one of the most challenging issues in semi-supervised segmentation: pseudo-label drift . When labeled data are extremely scarce, self-training methods are prone to deterministic bias, where early incorrect pseudo-labels accumulate over time, leading to unstable and degraded training. üß† Motivation Most existing consistency- or pseudo-label‚Äìbased semi-supervised approaches rely heavily on self-generated supervision . Once early pseudo-labels become unreliable, error accumulation is inevitable. Our goal is to introduce stronger semantic priors to correct such drift and stabilize the training process. ‚ú® Key Contributions 1Ô∏è‚É£ Heterogeneous Dual-Student Framework We leverage two complementary vision foundation models‚Äî CLIP for global semantic priors and DINOv3 for fine-grained local structures‚Äîto enable stable mutual learning and suppress error accumulation. 2Ô∏è‚É£ Explicit‚ÄìImplicit Semantic Co-Guidance By jointly utilizing text embeddings (explicit semantics) and learnable queries (implicit semantics), we provide class-level semantic anchors and enhance semantic consistency. 3Ô∏è‚É£ Global‚ÄìLocal Feature Co-Fusion We fuse CLIP‚Äôs global contextual understanding with DINOv3‚Äôs local structural details, yielding more accurate and stable segmentation results. üìä Experimental Results Extensive evaluations on six mainstream remote sensing benchmarks demonstrate that Co2S consistently achieves strong and stable performance across different data splits and scenarios, especially under extremely low annotation budgets . üì¶ Open-Source Resources arXiv Paper : https://arxiv.org/abs/2512.23035 Project Page : https://xavierjiezou.github.io/Co2S/ GitHub Code : https://github.com/XavierJiezou/Co2S HuggingFace Models : https://huggingface.co/XavierJiezou/co2s-models HuggingFace Datasets : https://huggingface.co/datasets/XavierJiezou/co2s-datasets #Remote Sensing #Semantic Segmentation #Semi-Supervised Learning #Vision Foundation Models #CLIP #DINOv3",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23035",
      "pdf_url": "https://arxiv.org/pdf/2512.23035",
      "github_links": [
        "https://github.com/XavierJiezou/Co2S"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23035",
      "scraped_at": "2026-01-07T01:50:47.571407"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "paper_url": "https://huggingface.co/papers/2601.01426",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01426",
      "pdf_url": "https://arxiv.org/pdf/2601.01426",
      "github_links": [
        "https://github.com/SWE-Lego/SWE-Lego"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01426",
      "scraped_at": "2026-01-07T01:50:49.438589"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
    "paper_url": "https://huggingface.co/papers/2601.01576",
    "authors": [
      "Chunchun Ma",
      "Yujiong Shen",
      "Yueyuan Huang",
      "Kexin Tan",
      "Ming Zhang"
    ],
    "stars": "3",
    "details": {
      "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation (2025) SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning (2025) WisPaper: Your AI Scholar Search Engine (2025) AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis (2025) OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists (2025) AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement (2025) Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01576",
      "pdf_url": "https://arxiv.org/pdf/2601.01576",
      "github_links": [
        "https://github.com/january-blue/OpenNovelty"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01576",
      "scraped_at": "2026-01-07T01:50:51.251342"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
    "paper_url": "https://huggingface.co/papers/2601.00863",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
      "abstract": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00863",
      "pdf_url": "https://arxiv.org/pdf/2601.00863",
      "github_links": [
        "https://github.com/lamm-mit/MusicAnalysis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00863",
      "scraped_at": "2026-01-07T01:50:53.135282"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
    "paper_url": "https://huggingface.co/papers/2512.21472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "abstract": "‚ú® The largest publicly available dermoscopic skin lesion segmentation dataset with 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image. ‚ú® 16 unique annotators , 3 different tools used, and 2 skill levels of the manual reviewer. ‚ú® Contains consensus masks for the 2,394 images that have multi-annotator segmentations (2-5 segmentations per image). ‚ú® Collected and curated from the ISIC Archive .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21472",
      "pdf_url": "https://arxiv.org/pdf/2512.21472",
      "github_links": [
        "https://github.com/sfu-mial/IMAplusplus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21472",
      "scraped_at": "2026-01-07T01:50:55.025985"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "paper_url": "https://huggingface.co/papers/2601.02315",
    "authors": [
      "Beth Tellman",
      "Lalit Maurya",
      "Saurabh Kaushik"
    ],
    "stars": "3",
    "details": {
      "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
      "abstract": "Despite the recent success of large pretrained encoders (Geo‚ÄëFoundation Models), we consistently observe that U‚ÄëNet‚Äëbased models remain highly competitive‚Äîand in some cases outperform transformers, particularly due to their strength in capturing local spatial nuances. Motivated by this, we propose Prithvi‚ÄëCAFE (Prithvi‚ÄëComplementary Adaptive Fusion Encoder), which enhances local representations through complementary fusion with a CNN‚Äëbased encoder. We evaluate our approach on two major flood datasets‚ÄîFloodPlanet and Sen1Floods11‚Äîand achieve state‚Äëof‚Äëthe‚Äëart performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02315",
      "pdf_url": "https://arxiv.org/pdf/2601.02315",
      "github_links": [
        "https://github.com/Sk-2103/Prithvi-CAFE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02315",
      "scraped_at": "2026-01-07T01:50:56.844097"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.02314",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "abstract": "Does COT in llms stay faithful to their thoughts?",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02314",
      "pdf_url": "https://arxiv.org/pdf/2601.02314",
      "github_links": [
        "https://github.com/skhanzad/AridadneXAI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02314",
      "scraped_at": "2026-01-07T01:50:58.682941"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.22877",
    "authors": [
      "Jun-Cheng Chen",
      "Cheng-Fu Chou",
      "Ju-Hsuan Weng",
      "jwliao1209"
    ],
    "stars": "0",
    "details": {
      "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
      "abstract": "Concept Erasure Benchmark",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22877",
      "pdf_url": "https://arxiv.org/pdf/2512.22877",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22877",
      "scraped_at": "2026-01-07T01:51:00.532894"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "paper_url": "https://huggingface.co/papers/2601.03252",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
      "abstract": "Depth Beyond Pixels üöÄ We Introduce InfiniDepth ‚Äî casting monocular depth estimation as a neural implicit field. üîç Arbitrary-Resolution üìê Accurate Metric Depth üì∑ Single-View NVS under large viewpoints shifts Arxiv: https://arxiv.org/abs/2601.03252 page: https://zju3dv.github.io/InfiniDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03252",
      "pdf_url": "https://arxiv.org/pdf/2601.03252",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03252",
      "scraped_at": "2026-01-08T01:50:45.247652"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "paper_url": "https://huggingface.co/papers/2601.01554",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
      "abstract": "MOSS Transcribe Diarize üéôÔ∏è We introduce MOSS Transcribe Diarize ‚Äî a unified multimodal model for Speaker-Attributed, Time-Stamped Transcription (SATS) . üîç End-to-end SATS in a single pass (transcription + speaker attribution + timestamps) üß† 128k context window for up to ~90-minute audio without chunking (strong long-range speaker memory) üåç Trained on extensive in-the-wild conversations + controllable simulated mixtures (robust to overlap/noise/domain shift) üìä Strong results on AISHELL-4 / Podcast / Movies benchmarks (best cpCER / Œîcp among evaluated systems) Paper: [2601.01554] MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization Homepage: https://mosi.cn/models/moss-transcribe-diarize Online Demo: https://moss-transcribe-diarize-demo.mosi.cn",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01554",
      "pdf_url": "https://arxiv.org/pdf/2601.01554",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01554",
      "scraped_at": "2026-01-08T01:50:47.162285"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "paper_url": "https://huggingface.co/papers/2601.03233",
    "authors": [
      "kvochko",
      "jacobitterman",
      "nisan",
      "benibraz",
      "yoavhacohen"
    ],
    "stars": "922",
    "details": {
      "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API 3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation (2025) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation (2025) MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning (2026) DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation (2025) JoVA: Unified Multimodal Learning for Joint Video-Audio Generation (2025) In-Context Audio Control of Video Diffusion Transformers (2025) JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03233",
      "pdf_url": "https://arxiv.org/pdf/2601.03233",
      "github_links": [
        "https://github.com/Lightricks/LTX-2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03233",
      "scraped_at": "2026-01-08T01:50:49.139726"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.22334",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "abstract": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22334",
      "pdf_url": "https://arxiv.org/pdf/2512.22334",
      "github_links": [
        "https://github.com/InternScience/SciEvalKit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22334",
      "scraped_at": "2026-01-08T01:50:51.230691"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "paper_url": "https://huggingface.co/papers/2601.03193",
    "authors": [
      "Lin-Chen",
      "lovesnowbest",
      "YuZeng260",
      "CostaliyA",
      "Hungryyan"
    ],
    "stars": "25",
    "details": {
      "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
      "abstract": "UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03193",
      "pdf_url": "https://arxiv.org/pdf/2601.03193",
      "github_links": [
        "https://github.com/Hungryyan1/UniCorn"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03193",
      "scraped_at": "2026-01-08T01:50:53.221654"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "paper_url": "https://huggingface.co/papers/2601.02427",
    "authors": [],
    "stars": "1.44k",
    "details": {
      "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
      "abstract": "NitroGen is a vision-action foundation model trained on 40k hours of gameplay across 1,000+ games, enabling cross-game generalization with behavior cloning and benchmarking, achieving strong unseen-game transfer.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02427",
      "pdf_url": "https://arxiv.org/pdf/2601.02427",
      "github_links": [
        "https://github.com/MineDojo/NitroGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02427",
      "scraped_at": "2026-01-08T01:50:55.235135"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2601.03044",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "abstract": "üöÄ Website: https://www.agibot.com/research/sop We introduce SOP for online post-training of generalist VLAs in the real world ‚Äî unlocking persistent, reliable deployment of generalist robots in physical environments. üîÅ 36 hours of continuous cloth folding: video üì¶ 36 hours of continuous cardboard box assembly: video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03044",
      "pdf_url": "https://arxiv.org/pdf/2601.03044",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03044",
      "scraped_at": "2026-01-08T01:50:57.176273"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "paper_url": "https://huggingface.co/papers/2601.02785",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamStyle: A Unified Framework for Video Stylization",
      "abstract": "DreamStyle unifies text-, style-image-, and first-frame-guided video stylization on an I2V backbone, using LoRA with token-specific up matrices to improve style consistency and video quality.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02785",
      "pdf_url": "https://arxiv.org/pdf/2601.02785",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02785",
      "scraped_at": "2026-01-08T01:50:59.158844"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MiMo-V2-Flash Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.02780",
    "authors": [],
    "stars": "957",
    "details": {
      "title": "MiMo-V2-Flash Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Xiaomi MiMo-VL-Miloco Technical Report (2025) Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning (2025) Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks (2025) AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing (2025) NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations (2025) Practical Policy Distillation for Reinforcement Learning in Radio Access Networks (2025) Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02780",
      "pdf_url": "https://arxiv.org/pdf/2601.02780",
      "github_links": [
        "https://github.com/XiaomiMiMo/MiMo-V2-Flash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02780",
      "scraped_at": "2026-01-08T01:51:01.099679"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.01874",
    "authors": [
      "Aojun Lu",
      "Junjie Xie",
      "Shuhang Chen",
      "JacobYuan",
      "Yunqiu"
    ],
    "stars": "0",
    "details": {
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "abstract": "Project page: https://shchen233.github.io/cogflow/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01874",
      "pdf_url": "https://arxiv.org/pdf/2601.01874",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01874",
      "scraped_at": "2026-01-08T01:51:03.012607"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "paper_url": "https://huggingface.co/papers/2601.01321",
    "authors": [
      "Yao Su",
      "vztu",
      "ZihanJia",
      "fjchendp",
      "roz322"
    ],
    "stars": "2",
    "details": {
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "abstract": "This paper systematically analyzes AI integration in Digital Twins through a four-stage framework (modeling ‚Üí mirroring ‚Üí intervention ‚Üí autonomous management), covering LLMs, foundation models, world models, and intelligent agents across 11 application domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01321",
      "pdf_url": "https://arxiv.org/pdf/2601.01321",
      "github_links": [
        "https://github.com/rongzhou7/Awesome-Digital-Twin-AI/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01321",
      "scraped_at": "2026-01-08T01:51:04.924547"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "paper_url": "https://huggingface.co/papers/2601.02439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "abstract": "WebGym creates a large, non-stationary visual web task suite and scalable RL pipeline, enabling fast trajectory rollout and improved vision-language agent performance on unseen websites.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02439",
      "pdf_url": "https://arxiv.org/pdf/2601.02439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02439",
      "scraped_at": "2026-01-08T01:51:06.769414"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
    "paper_url": "https://huggingface.co/papers/2601.02989",
    "authors": [
      "Fatemeh Askari",
      "Sadegh Mohammadian",
      "Mohammadali Banayeeanzade",
      "Hosein Hasani",
      "safinal"
    ],
    "stars": "0",
    "details": {
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "abstract": "üî¢ Overcoming Transformer Depth Limits in Counting Tasks LLMs often fail at counting not because they aren't smart, but because of architectural depth constraints üöß. We propose a simple, effective System-2 strategy üß© that decomposes counting tasks to bypass these limits. üî¨ We also provide a full mechanistic interpretation , identifying the specific attention heads and representations responsible for transferring \"latent counts\" across the network. üìà This approach allows LLMs to achieve high accuracy on large-scale counting benchmarks where they typically fail.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02989",
      "pdf_url": "https://arxiv.org/pdf/2601.02989",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02989",
      "scraped_at": "2026-01-08T01:51:08.644226"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "paper_url": "https://huggingface.co/papers/2601.03256",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
      "abstract": "Project page: https://luhexiao.github.io/Muses.github.io/ Code: https://github.com/luhexiao/Muses",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03256",
      "pdf_url": "https://arxiv.org/pdf/2601.03256",
      "github_links": [
        "https://github.com/luhexiao/Muses"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03256",
      "scraped_at": "2026-01-08T01:51:10.461072"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "paper_url": "https://huggingface.co/papers/2601.01720",
    "authors": [
      "Donghao Luo",
      "yanweifuture",
      "chengjie-wang",
      "ChengmingX",
      "ScarletAce"
    ],
    "stars": "0",
    "details": {
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) Unified Video Editing with Temporal Reasoner (2025) VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning (2025) EasyV2V: A High-quality Instruction-based Video Editing Framework (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01720",
      "pdf_url": "https://arxiv.org/pdf/2601.01720",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01720",
      "scraped_at": "2026-01-08T01:51:12.354426"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01592",
    "authors": [
      "Yang Yao",
      "Yixu Wang",
      "Juncheng Li",
      "Yunhao Chen",
      "xinwang22"
    ],
    "stars": "112",
    "details": {
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "abstract": "Even State-of-the-Art Models Fail to Hold Ground Against Sophisticated Adversaries. Our comprehensive evaluation highlights two key findings. (1) A clear stratification in defense capability: Top-tier models such as Claude Haiku 4.5, GPT-5.2, and Qwen3-Max exhibit strong baseline robustness, effectively neutralizing static, template-based attacks and complex logic traps, often keeping ASR below 20%.This suggests that leading labs have improved defenses against recognizable, repeatable jailbreak structures, while several models (e.g., Llama-4, Mistral Large 3) remain more susceptible to these simpler patterns. (2) A shift in the attack landscape: adaptive, multi-turn, and multi-agent strategies dominate, whereas static, single-turn, and template-based approaches are increasingly ineffective. Methods like EvoSynth and X-Teaming can achieve >90% ASR even against advanced models. This indicates current safety training overfits to static templates, failing to generalize against the broad attack surface exposed by automated red-teaming. Adversarial Robustness Exhibits Inconsistent and Polarized Vulnerability Patterns. We observe a polarization effect where models demonstrate high resistance to specific attack families (e.g., text-based cipher) yet remain completely defenseless against others (e.g., logic nesting). For instance, Grok 4.1 Fast shows 1.5% ASR against RedQueen but 90.5% against X-Teaming. This stark performance disparity (~90%) underscores that current defenses are often patch-based rather than holistic, necessitating the multi-faceted evaluation provided by OpenRT. Enhanced Reasoning and Multimodal Capabilities are New Vectors for Exploitation. Contrary to the common assumption that more capable models are inherently safer, we find that enhanced capabilities often introduce new vectors for exploitation. Reasoning-enhanced models (CoT) do not demonstrate superior robustness; instead, their verbose reasoning processes can be manipulated to bypass safety filters. Similarly, Multimodal LLMs exhibit a critical modality gap: visual inputs frequently bypass text-based safety mechanisms, allowing cross-modal attacks to compromise models that are otherwise robust to purely textual jailbreaks. These findings suggest that current safety alignment has not kept pace with the architectural expansion of model capabilities. Proprietary Models Can Be as Vulnerable as Open-Source Models Under Certain Attacks. Our analysis reveals that proprietary and open-source models exhibit comparable susceptibility to our attack suite. Across our 20 evaluated models, only GPT-5.2 and Claude Haiku 4.5 maintained an average ASR below 30%, while all other models consistently exceeded this threshold. This universality sharply contradicts the assumption that closed deployments offer superior protection, demonstrating that the safety through obscurity of proprietary strategies fails to provide any tangible mitigation against sophisticated adversarial attacks. Scaling MLLMs Robustness via Defense-in-Depth and Continuous Red Teaming. Challenges such as polarized robustness, weak generalization to unseen attacks, and cross-modal bypasses highlight the limits of single-layer defense. Effective mitigation requires a paradigm shift toward Defense-in-Depth: integrating intrinsic architectural safety with runtime risk estimation and adversarial training on multimodal and multi-turn interactions. Crucially, continuous Red Teaming via infrastructure like OpenRT provides systematic evaluation to verify empirical robustness and prevent benchmark overfitting.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01592",
      "pdf_url": "https://arxiv.org/pdf/2601.01592",
      "github_links": [
        "https://github.com/AI45Lab/OpenRT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01592",
      "scraped_at": "2026-01-08T01:51:14.181550"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.23412",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
      "abstract": "In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows the model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL. The agent reasoning framework, MWE-Bench, three smaller-scale agent models (2B, 3B, and 4B) distilled from MindWatcher 32B, and related resources will be open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23412",
      "pdf_url": "https://arxiv.org/pdf/2512.23412",
      "github_links": [
        "https://github.com/TIMMY-CHAN/MindWatcher"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23412",
      "scraped_at": "2026-01-08T01:51:16.037424"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "paper_url": "https://huggingface.co/papers/2601.03227",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "abstract": "We found the sonar moment in audio language models. We propose the task of audio geo-localization. And amazingly, Gemini 3 Pro can reach the distance error of less than 55km for 25%  samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03227",
      "pdf_url": "https://arxiv.org/pdf/2601.03227",
      "github_links": [
        "https://github.com/Rising0321/AGL1K"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03227",
      "scraped_at": "2026-01-08T01:51:21.077717"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "paper_url": "https://huggingface.co/papers/2601.03194",
    "authors": [
      "Sai Rithwik Reddy Chirra",
      "Shashivardhan Reddy Koppula",
      "Mohammad Zia Ur Rehman",
      "shwetankssingh",
      "UVSKKR"
    ],
    "stars": "0",
    "details": {
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (explainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03194",
      "pdf_url": "https://arxiv.org/pdf/2601.03194",
      "github_links": [
        "https://github.com/ziarehman30/X-MuTeST"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03194",
      "scraped_at": "2026-01-08T01:51:23.122431"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Parallel Latent Reasoning for Sequential Recommendation",
    "paper_url": "https://huggingface.co/papers/2601.03153",
    "authors": [
      "Yuning Jiang",
      "Jian Wu",
      "Wen Chen",
      "Xu Chen",
      "TangJiakai5704"
    ],
    "stars": "0",
    "details": {
      "title": "Parallel Latent Reasoning for Sequential Recommendation",
      "abstract": "Parallel Latent Reasoning (PLR): Sequential Recommendation with Parallel Reasoning üî• üìâ Depth-only reasoning often hits performance plateaus‚ÄîPLR mitigates this with parallel latent reasoning. Core Innovation ‚ú® üéØ Learnable trigger tokens: Build parallel streams in continuous latent space. üîÑ Global regularization: Preserve stream diversity to avoid redundancy. ‚öñÔ∏è Adaptive aggregation: Smartly combine multi-stream insights for optimal results. Key Advantages üöÄ üìä Outperforms SOTA baselines (SASRec, BERT4Rec, ReaRec, LRESA) by 5.5%‚Äì14.9% on Recall@10/20 and NDCG@10/20 across three real-world datasets. ‚ö° Real-time efficiency: Only 5.8% latency increase vs. base models, enabled by KV Caching and GPU parallelism. üõ°Ô∏è Strong robustness: Maintains top performance even with 30% missing user interactions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03153",
      "pdf_url": "https://arxiv.org/pdf/2601.03153",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03153",
      "scraped_at": "2026-01-08T01:51:25.012089"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03127",
    "authors": [
      "Yue Cao",
      "Hanqing Yang",
      "Jijin Hu",
      "Qiang Zhou",
      "Sashuai Zhou"
    ],
    "stars": "0",
    "details": {
      "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
      "abstract": "reasoning-based image generation and editing",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03127",
      "pdf_url": "https://arxiv.org/pdf/2601.03127",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03127",
      "scraped_at": "2026-01-08T01:51:26.836393"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
    "paper_url": "https://huggingface.co/papers/2601.02996",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
      "abstract": "https://github.com/cisnlp/multilingual-latent-reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02996",
      "pdf_url": "https://arxiv.org/pdf/2601.02996",
      "github_links": [
        "https://github.com/cisnlp/multilingual-latent-reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02996",
      "scraped_at": "2026-01-08T01:51:28.643997"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "paper_url": "https://huggingface.co/papers/2601.02359",
    "authors": [
      "Vladislav Golyanik",
      "Toshihiko Yamasaki",
      "mapooon"
    ],
    "stars": "0",
    "details": {
      "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
      "abstract": "Detecting deepfakes with generative AI. We introduce ExposeAnyone ‚Äî a paradigm shift in face forgery detection! üîçÔ∏è Fully self-supervised approach ü•á Best average AUC on traditional deepfake benchmarks üí™ Best AUC even on Sora2 by OpenAI üí¢ Strong Robustness to common corruptions such as JPEG/MPEG compression Arxiv: https://arxiv.org/abs/2601.02359 Project page: https://mapooon.github.io/ExposeAnyonePage/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02359",
      "pdf_url": "https://arxiv.org/pdf/2601.02359",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02359",
      "scraped_at": "2026-01-08T01:51:30.555236"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
    "paper_url": "https://huggingface.co/papers/2601.00581",
    "authors": [],
    "stars": "458",
    "details": {
      "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
      "abstract": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at this https URL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00581",
      "pdf_url": "https://arxiv.org/pdf/2601.00581",
      "github_links": [
        "https://github.com/torchmd/torchmd-net"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00581",
      "scraped_at": "2026-01-08T01:51:32.482242"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
    "paper_url": "https://huggingface.co/papers/2512.23950",
    "authors": [
      "Peng Li",
      "Yulong Xiao",
      "Mingzhe Liu",
      "Huibin Li",
      "FengShaner"
    ],
    "stars": "2",
    "details": {
      "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
      "abstract": "Title: DehazeSNN ‚Äî U-Net-like Spiking Neural Networks for Single Image Dehazing Short summary: DehazeSNN integrates a U-Net architecture with Spiking Neural Networks to reduce compute while achieving competitive dehazing results. Code: github.com/HaoranLiu507/DehazeSNN. Highlights: U-Net + SNN design for lower MACs. OLIF block for improved cross-channel communication. Benchmarks show comparable or better dehazing with smaller model footprint.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23950",
      "pdf_url": "https://arxiv.org/pdf/2512.23950",
      "github_links": [
        "https://github.com/HaoranLiu507/DehazeSNN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23950",
      "scraped_at": "2026-01-08T01:51:34.324181"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01584",
    "authors": [
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "abstract": "This paper measures how easily ‚Äúinstrumental-convergence‚Äù behaviors (e.g., shutdown avoidance, self-replication) in LLMs can be amplified or suppressed by simple steering, and argues that the common claim ‚Äúas AI capability (often glossed as ‚Äòintelligence‚Äô) increases, systems inevitably become less controllable‚Äù should not be treated as a default assumption. Using InstrumentalEval on Qwen3 (4B/30B; Base/Instruct/Thinking) with a GPT-5.2 judge, a short anti-instrumental prompt suffix drops convergence sharply (e.g., Qwen3-30B Instruct: 81.69% to 2.82%), while a pro-instrumental suffix pushes it high. The key takeaway is a safety‚Äìsecurity dilemma for open weights: the same high steerability that helps builders enforce safe behavior can also help attackers elicit disallowed behavior, so widening the gap between authorized vs. unauthorized steerability remains a central open problem.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01584",
      "pdf_url": "https://arxiv.org/pdf/2601.01584",
      "github_links": [
        "https://github.com/j-hoscilowicz/instrumental_steering/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01584",
      "scraped_at": "2026-01-08T01:51:36.179871"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
    "paper_url": "https://huggingface.co/papers/2601.02151",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
      "abstract": "üíª Code: https://github.com/PRIS-CV/EAFT ‚ú® Project Page: https://ymxyll.github.io/EAFT/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02151",
      "pdf_url": "https://arxiv.org/pdf/2601.02151",
      "github_links": [
        "https://github.com/hiyouga/LLaMA-Factory",
        "https://github.com/PRIS-CV/EAFT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02151",
      "scraped_at": "2026-01-09T01:51:09.218763"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Evolving Programmatic Skill Networks",
    "paper_url": "https://huggingface.co/papers/2601.03509",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evolving Programmatic Skill Networks",
      "abstract": "We study continual skill acquisition in openended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1) REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN‚Äôs learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03509",
      "pdf_url": "https://arxiv.org/pdf/2601.03509",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03509",
      "scraped_at": "2026-01-09T01:51:11.028376"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.03872",
    "authors": [
      "Yuhao Shen",
      "Jiahao Yuan",
      "Ruihan Jin",
      "Guocheng Zhai",
      "Jinyang23"
    ],
    "stars": "0",
    "details": {
      "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
      "abstract": "üöÄ [New Paper] Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning The growing diversity of LLMs and external tools presents a significant challenge: how to select the optimal model-tool combination for complex reasoning tasks. Existing methods often fall short by relying on single models or fixed tool-calling logic. ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation) addresses this by introducing a dual-path framework for dynamic model-tool alignment and invocation across multiple domains. ‚ú® The Core Intuition: ATLAS employs a dual-path approach to achieve dynamic model-tool alignment and invocation: 1Ô∏è‚É£ Training-Free Cluster-Based Routing: This path leverages empirical priors for domain-specific alignment, efficiently guiding the model-tool selection process. 2Ô∏è‚É£ RL-Based Multi-Step Routing: This path explores autonomous trajectories to achieve strong generalization, particularly for out-of-distribution tasks. üìà Highlights: Superior Performance: ATLAS significantly outperforms closed-source models like GPT-4o and existing routing methods, achieving +10.1% on in-distribution tasks and +13.1% on out-of-distribution tasks across 15 benchmarks. Enhanced Visual Reasoning: The framework demonstrates substantial improvements in visual reasoning by effectively orchestrating specialized multi-modal tools. Adaptive Orchestration: ATLAS learns to assess its internal state and dynamically invoke external resources, internalizing the alignment between domains and tool utilization. Robust and Generalizable: The design ensures that the routing policy effectively captures expertise distribution, making it robust and generalizable even as tools and models evolve.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03872",
      "pdf_url": "https://arxiv.org/pdf/2601.03872",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03872",
      "scraped_at": "2026-01-09T01:51:12.875928"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
    "paper_url": "https://huggingface.co/papers/2601.03986",
    "authors": [
      "Muling Wu",
      "Changze Lv",
      "Jingwen Xu",
      "Qi Qian",
      "ChengsongHuang"
    ],
    "stars": "0",
    "details": {
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03986",
      "pdf_url": "https://arxiv.org/pdf/2601.03986",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03986",
      "scraped_at": "2026-01-09T01:51:14.732608"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
    "paper_url": "https://huggingface.co/papers/2601.03822",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
      "abstract": "ROI-Reasoning introduces a principled framework for budget-aware inference-time reasoning in large language models. Instead of blindly scaling computation, the authors formulate multi-task reasoning under a global token constraint as an Ordered Stochastic Multiple-Choice Knapsack Problem, explicitly modeling the trade-off between reasoning cost and expected utility. The proposed two-stage approach combines Meta-Cognitive Fine-Tuning, which enables models to anticipate difficulty and make solve-or-skip decisions before reasoning, with Rationality-Aware Reinforcement Learning, which optimizes long-horizon computation allocation under strict budgets. Across challenging mathematical reasoning benchmarks, ROI-Reasoning consistently improves total score and substantially reduces regret‚Äîdemonstrating that meta-cognitive planning, not just stronger reasoning, is key to efficient test-time scaling of LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03822",
      "pdf_url": "https://arxiv.org/pdf/2601.03822",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03822",
      "scraped_at": "2026-01-09T01:51:16.648836"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "paper_url": "https://huggingface.co/papers/2601.04151",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
      "abstract": "Klear: 26B model for joint audio-video generation Single-tower DiT with \"Omni-Full Attention\" across video, audio, and text Progressive multi-task training (T2V, T2A, T2AV, I2V all in one model) 81M sample dataset with dense captions Claims Veo 3-level performance on lip-sync & AV consistency",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04151",
      "pdf_url": "https://arxiv.org/pdf/2601.04151",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04151",
      "scraped_at": "2026-01-09T01:51:18.670540"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Choreographing a World of Dynamic Objects",
    "paper_url": "https://huggingface.co/papers/2601.04194",
    "authors": [
      "Hadi Alzayer",
      "Yunzhi Zhang",
      "Karthik Dharmarajan",
      "Chen Geng",
      "Yanzhe Lyu"
    ],
    "stars": "0",
    "details": {
      "title": "Choreographing a World of Dynamic Objects",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Animus3D: Text-driven 3D Animation via Motion Score Distillation (2025) AnimaMimic: Imitating 3D Animation from Video Priors (2025) Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions (2025) DIMO: Diverse 3D Motion Generation for Arbitrary Objects (2025) Inferring Compositional 4D Scenes without Ever Seeing One (2025) SS4D: Native 4D Generative Model via Structured Spacetime Latents (2025) WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04194",
      "pdf_url": "https://arxiv.org/pdf/2601.04194",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04194",
      "scraped_at": "2026-01-09T01:51:20.596669"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "paper_url": "https://huggingface.co/papers/2601.04171",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
      "abstract": "Agentic Rubrics for verifying SWE agent patches WITHOUT running tests! An agent explores the codebase to generate context-grounded checklists, then scores patches execution-free. Rubrics provide dense, interpretable reward signals that could scale RL training for coding agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04171",
      "pdf_url": "https://arxiv.org/pdf/2601.04171",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04171",
      "scraped_at": "2026-01-09T01:51:22.502781"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
    "paper_url": "https://huggingface.co/papers/2601.02075",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "abstract": "project: https://github.com/FredericVAN/PKU_MDAgent2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02075",
      "pdf_url": "https://arxiv.org/pdf/2601.02075",
      "github_links": [
        "https://github.com/FredericVAN/PKU_MDAgent2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02075",
      "scraped_at": "2026-01-09T01:51:24.349527"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
    "paper_url": "https://huggingface.co/papers/2601.00423",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
      "abstract": "We propose an entropy aware Group Relative Policy Optimization (E-GRPO) to increase the entropy of SDE sampling steps. We have integrated a variety of current GRPO-based reinforcement learning methods as well as different image reward models. Code: https://github.com/shengjun-zhang/VisualGRPO Model: https://huggingface.co/studyOverflow/E-GRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00423",
      "pdf_url": "https://arxiv.org/pdf/2601.00423",
      "github_links": [
        "https://github.com/shengjun-zhang/VisualGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00423",
      "scraped_at": "2026-01-09T01:51:26.319389"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.03471",
    "authors": [
      "Guanchen Wu",
      "Yuzhang Xie",
      "Zewen Liu",
      "Dehai Min",
      "Mingyang Wei"
    ],
    "stars": "0",
    "details": {
      "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
      "abstract": "EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03471",
      "pdf_url": "https://arxiv.org/pdf/2601.03471",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03471",
      "scraped_at": "2026-01-09T01:51:28.218342"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.03699",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "abstract": "RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03699",
      "pdf_url": "https://arxiv.org/pdf/2601.03699",
      "github_links": [
        "https://github.com/knoveleng/redeval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03699",
      "scraped_at": "2026-01-09T01:51:30.046104"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
    "paper_url": "https://huggingface.co/papers/2601.03315",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
      "abstract": "We find that LLMs aren't scientists yet.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03315",
      "pdf_url": "https://arxiv.org/pdf/2601.03315",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03315",
      "scraped_at": "2026-01-09T01:51:31.842661"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.03467",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) Unified Thinker: A General Reasoning Modular Core for Image Generation (2026) ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning (2025) ThinkGen: Generalized Thinking for Visual Generation (2025) MIRA: Multimodal Iterative Reasoning Agent for Image Editing (2025) EditThinker: Unlocking Iterative Reasoning for Any Image Editor (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03467",
      "pdf_url": "https://arxiv.org/pdf/2601.03467",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03467",
      "scraped_at": "2026-01-09T01:51:33.683864"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
    "paper_url": "https://huggingface.co/papers/2601.03448",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
      "abstract": "We propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. L2T establishes the structural scaffolding required for linguistic competence, complementing world knowledge acquired through standard CLM. The code is available on GitHub: https://github.com/gucci-j/l2t",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03448",
      "pdf_url": "https://arxiv.org/pdf/2601.03448",
      "github_links": [
        "https://github.com/gucci-j/l2t"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03448",
      "scraped_at": "2026-01-09T01:51:35.563270"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Pearmut: Human Evaluation of Translation Made Trivial",
    "paper_url": "https://huggingface.co/papers/2601.02933",
    "authors": [
      "Tom Kocmi",
      "Vil√©m Zouhar"
    ],
    "stars": "7",
    "details": {
      "title": "Pearmut: Human Evaluation of Translation Made Trivial",
      "abstract": "Happy to discuss how people human-evaluate multilingual tasks! üôÇ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02933",
      "pdf_url": "https://arxiv.org/pdf/2601.02933",
      "github_links": [
        "https://github.com/zouharvi/pearmut"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02933",
      "scraped_at": "2026-01-09T01:51:37.483381"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03955",
    "authors": [
      "Ming Lu",
      "Kun Gai",
      "Huan Yang",
      "Cheng Da",
      "Xu Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03955",
      "pdf_url": "https://arxiv.org/pdf/2601.03955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03955",
      "scraped_at": "2026-01-09T01:51:39.413532"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
    "paper_url": "https://huggingface.co/papers/2601.03236",
    "authors": [
      "Bingzhe Li",
      "Guanpeng Li",
      "Yi Li",
      "Dongming Jiang"
    ],
    "stars": "0",
    "details": {
      "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
      "abstract": "This ia giid paper",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03236",
      "pdf_url": "https://arxiv.org/pdf/2601.03236",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03236",
      "scraped_at": "2026-01-09T01:51:41.306990"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.04090",
    "authors": [
      "Yuewen Ma",
      "Lin Ma",
      "Bangbang Yang",
      "Yuanbo Yang",
      "Jiaxin Huang"
    ],
    "stars": "34",
    "details": {
      "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
      "abstract": "We Introduce Gen3R ‚Äî create multi-quantity geometry with RGB from images. üì∑ Photorealistic Video üöÄ Accurate 3D Scene Geometry Arxiv: https://arxiv.org/abs/2601.04090 Project page: https://xdimlab.github.io/Gen3R/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04090",
      "pdf_url": "https://arxiv.org/pdf/2601.04090",
      "github_links": [
        "https://github.com/JaceyHuang/Gen3R"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04090",
      "scraped_at": "2026-01-09T01:51:43.101762"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "paper_url": "https://huggingface.co/papers/2601.00705",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
      "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00705",
      "pdf_url": "https://arxiv.org/pdf/2601.00705",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00705",
      "scraped_at": "2026-01-09T01:51:44.961750"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "64",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-10T01:47:38.996866"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "98",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-10T01:47:40.904445"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "12",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-10T01:47:42.783069"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-10T01:47:44.648079"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "8",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-10T01:47:46.604457"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "6",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-10T01:47:48.511757"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-10T01:47:50.402121"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-10T01:47:52.299582"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-10T01:47:54.355619"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-10T01:47:56.251754"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-10T01:47:58.153470"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-10T01:48:00.071087"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [
      "Meng Liu",
      "Qiancheng Xu",
      "Caiqi Zhang",
      "HongruCai",
      "dd101bb"
    ],
    "stars": "9",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-10T01:48:01.910045"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-10T01:48:03.769181"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-10T01:48:05.666518"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-10T01:48:07.549671"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [
      "Jing Ma",
      "Yuxuan Gu",
      "Shidong Cao",
      "Ziyang",
      "danielhzlin"
    ],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-10T01:48:09.387444"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-10T01:48:11.254706"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-10T01:48:13.102178"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-10T01:48:14.948337"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-10T01:48:16.843733"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-10T01:48:18.728989"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-10T01:48:20.557489"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-10T01:48:22.412221"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-10T01:48:24.291348"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-10T01:48:26.242615"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-10T01:48:28.093828"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "0",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-10T01:48:29.955705"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-10T01:48:31.814190"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-10T01:48:33.663145"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-10T01:48:35.486191"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-10T01:48:37.365709"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "1",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-10T01:48:39.220990"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "101",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-11T01:59:47.730652"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "99",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-11T01:59:49.925001"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "16",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-11T01:59:51.961752"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-11T01:59:54.099175"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "11",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-11T01:59:56.325159"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "11",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-11T01:59:58.427921"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-11T02:00:00.482812"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-11T02:00:02.805129"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-11T02:00:04.923362"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-11T02:00:06.975071"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-11T02:00:09.054428"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [
      "Meng Liu",
      "Qiancheng Xu",
      "Caiqi Zhang",
      "HongruCai",
      "dd101bb"
    ],
    "stars": "17",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-11T02:00:11.178154"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-11T02:00:13.255536"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-11T02:00:15.509738"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-11T02:00:17.713650"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-11T02:00:19.910578"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-11T02:00:21.943611"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-11T02:00:24.141593"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-11T02:00:26.124601"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-11T02:00:28.219747"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-11T02:00:30.411079"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-11T02:00:32.600123"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-11T02:00:34.631059"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-11T02:00:36.809912"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-11T02:00:38.871696"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-11T02:00:41.069566"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-11T02:00:43.140462"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-11T02:00:45.185323"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "3",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [
        "https://github.com/mbar0075/lupi-for-object-detection"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-11T02:00:47.504931"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-11T02:00:49.457446"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "1",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-11T02:00:51.730026"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-11T02:00:54.067066"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-11T02:00:56.329467"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "126",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-12T01:56:15.369905"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "99",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-12T01:56:17.350077"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "17",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-12T01:56:19.261694"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-12T01:56:21.261190"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "23",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-12T01:56:23.204161"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "16",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-12T01:56:25.164574"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-12T01:56:27.080831"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "13",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-12T01:56:29.109143"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-12T01:56:31.065656"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-12T01:56:33.013335"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-12T01:56:34.977612"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-12T01:56:36.914048"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-12T01:56:38.788826"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-12T01:56:40.809081"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-12T01:56:42.666912"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-12T01:56:44.585372"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-12T01:56:46.426451"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-12T01:56:48.265902"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-12T01:56:50.213336"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-12T01:56:52.201583"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-12T01:56:54.063156"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-12T01:56:55.989473"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-12T01:56:57.907638"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-12T01:56:59.771628"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-12T01:57:01.869712"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "2",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-12T01:57:03.736777"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-12T01:57:05.671962"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-12T01:57:07.412767"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-12T01:57:09.256703"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "4",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [
        "https://github.com/mbar0075/lupi-for-object-detection"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-12T01:57:11.141085"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-12T01:57:13.156228"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-12T01:57:15.060473"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-12T01:57:17.088547"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
    "paper_url": "https://huggingface.co/papers/2601.05432",
    "authors": [],
    "stars": "107",
    "details": {
      "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
      "abstract": "Demo video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05432",
      "pdf_url": "https://arxiv.org/pdf/2601.05432",
      "github_links": [
        "https://github.com/AMAP-ML/Thinking-with-Map"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05432",
      "scraped_at": "2026-01-13T01:48:04.298623"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
    "paper_url": "https://huggingface.co/papers/2601.03017",
    "authors": [
      "Huajian Xin",
      "Hui Shen",
      "Yunta Hsieh",
      "Qi Han",
      "Jing Xiong"
    ],
    "stars": "0",
    "details": {
      "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
      "abstract": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03017",
      "pdf_url": "https://arxiv.org/pdf/2601.03017",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03017",
      "scraped_at": "2026-01-13T01:48:06.184101"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "paper_url": "https://huggingface.co/papers/2601.03319",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
      "abstract": "Project Page: https://c4ricaturegs.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03319",
      "pdf_url": "https://arxiv.org/pdf/2601.03319",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03319",
      "scraped_at": "2026-01-13T01:48:08.069844"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06002",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "abstract": "Glad to share our recent exploratory project: üß™ Title: The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning üåê arXiv: 2601.06002 ‚Äã üßê Why revisit Long CoT? Recent work often focuses on ‚Äúmaking CoT longer,‚Äù but longer traces are more likely to derail‚Äîe.g., drifting off-track, breaking logical continuity, or amplifying hallucinations‚Äîespecially when attempting to cold-start genuine long-horizon reasoning from a standard instruction-tuned model. ‚Äã A key observation is that many trajectories that merely look like long reasoning (e.g., distilling from randomly sampled ICL demonstrations, or using human-written long step-by-step solutions) are not behaviorally stable, and models frequently fail to learn robustly from them. ‚Äã üò≠  Why imitation often fails ‚ÄúLong‚Äù human CoT is not necessarily effective: Fine-tuning on human-written long CoT does not reliably reproduce the gains achieved by distilling from a strong reasoning model. Distill from Weak instruct model + random ICL demonstrations largely fails: Using randomly chosen 1-shot ICL examples to ‚Äúfake‚Äù long reasoning for distillation leads to significant degradation, suggesting that superficial formatting is insufficient. ‚Äã- Keywords are not the driver: Replacing surface tokens (e.g., ‚Äúwait‚Äù) while preserving the underlying reasoning trajectory and behavioral pattern yields similar performance, indicating that SFT primarily learns structure/behavior rather than prompt-specific keywords. ‚Äã üîç  Early evidence: Long CoT has stable ‚Äústructural fingerprints‚Äù We observe a stable behavioral transfer graph: across different strong reasoning models and tasks, the induced distributional characteristics appear highly consistent. ‚Äã- In semantic space, we see ‚Äúlinking‚Äìfolding‚Äù patterns: deep reasoning tends to form locally dense structures; self-reflection tends to create backward links for validation/correction; and exploration tends to form weaker cross-cluster connections. ‚Äã üí° Core hypothesis: effective Long CoT as a ‚Äúmolecular structure‚Äù High-quality Long CoT is not merely a linear chain; it is stabilized by three interaction types‚Äîanalogous to chemical bonds‚Äîthat organize and constrain reasoning trajectories: ‚Äã- Deep Reasoning (covalent-bond-like): forms the main reasoning backbone; if it breaks, the solution collapses. ‚Äã- Self-Reflection (hydrogen-bond-like): folds later steps back to earlier ones to verify assumptions, detect errors, and correct the path. ‚Äã- Self-Exploration (van der Waals-like): weak but important cross-domain probing that broadens coverage and discovers alternative routes. ‚Äã An additional observation is that the Gibbs‚ÄìBoltzmann energy formulation is closely aligned with the attention formulation; hence, the ‚Äúenergy distributions‚Äù of different bonds can be estimated directly from attention, exhibiting a stable ordering reminiscent of real chemical bond energies. ‚Äã üçé ‚ÄúSemantic isomers‚Äù of Long CoT For the same problem, trajectories can be semantically close yet differ in the distribution and transitions of bonds, yielding distinct ‚Äúisomers‚Äù with dramatically different trainability and downstream performance. ‚Äã- Two isomers that appear similar may still be incompatible: mixing them during training can trigger structural conflicts and degrade performance. ‚Äã- ICL is not inherently ineffective; it helps when demonstrations are selected such that their structural distribution aligns with the target high-quality isomer. ‚Äã üîß Solution: MOLE-SYN We propose MOLE-SYN: first estimate a behavioral transfer graph from a strong reasoning model, then use it to guide a pure instruct LLM to synthesize Long CoT trajectories. ‚Äã- Empirically, distilling Qwen-2.5 with MOLE-SYN‚Äìgenerated trajectories can approach the effectiveness of distillation from QwQ. ‚Äã- This initialization also exhibits strong RL potential: it yields more stable RL training curves and sustained improvement headroom. Finally, different behaviors have distinct global effects: deep reasoning makes the core logic more compact, self-reflection increases overall ‚Äúfolding‚Äù tightness, and self-exploration expands the reachable search space. ‚Äã üëÄ A practical implication is that when CoT is heavily summarized or compressed, the ‚Äúmolecular structure‚Äù distribution can be destroyed, and distilled models may underperform even the original teacher. ‚Äã If a prior viewpoint treated CoT behaviors as nodes, this work reframes them as edges that link logical states: the training target may not be ‚Äúlonger answers,‚Äù but a more stable reasoning skeleton controlled by structured reasoning behaviors. ‚Äã",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06002",
      "pdf_url": "https://arxiv.org/pdf/2601.06002",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06002",
      "scraped_at": "2026-01-13T01:48:09.969605"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "paper_url": "https://huggingface.co/papers/2601.06021",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
      "abstract": "Code: https://github.com/THUDM/CaRR Data: https://huggingface.co/datasets/THU-KEG/CaRR-DeepDive",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06021",
      "pdf_url": "https://arxiv.org/pdf/2601.06021",
      "github_links": [
        "https://github.com/THUDM/CaRR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06021",
      "scraped_at": "2026-01-13T01:48:11.869324"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
    "paper_url": "https://huggingface.co/papers/2601.05808",
    "authors": [
      "Zhicheng Dou",
      "Yutao Zhu",
      "Haofei Chang",
      "Xiaoshuai Song",
      "dongguanting"
    ],
    "stars": "0",
    "details": {
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "abstract": "Code: https://github.com/RUC-NLPIR/EnvScaler Data & Model: https://huggingface.co/collections/XXHStudyHard/envscaler",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05808",
      "pdf_url": "https://arxiv.org/pdf/2601.05808",
      "github_links": [
        "https://github.com/RUC-NLPIR/EnvScaler"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05808",
      "scraped_at": "2026-01-13T01:48:13.888446"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
    "paper_url": "https://huggingface.co/papers/2601.04720",
    "authors": [],
    "stars": "620",
    "details": {
      "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
      "abstract": "üöÄ Introducing Qwen3-VL-Embedding and Qwen3-VL-Reranker ‚Äì advancing the state of the art in multimodal retrieval and cross-modal understanding! ‚ú® Highlights: ‚úÖ Built upon the robust Qwen3-VL foundation model ‚úÖ Processes text, images, screenshots, videos, and mixed modality inputs ‚úÖ Supports 30+ languages ‚úÖ Achieves state-of-the-art performance on multimodal retrieval benchmarks ‚úÖ Open source and available on Hugging Face, GitHub, and ModelScope ‚úÖ API deployment on Alibaba Cloud coming soon! üéØ Two-stage retrieval architecture: üìä Embedding Model ‚Äì generates semantically rich vector representations in a unified embedding space üéØ Reranker Model ‚Äì computes fine-grained relevance scores for enhanced retrieval accuracy üîç Key application scenarios: Image-text retrieval, video search, multimodal RAG, visual question answering, multimodal content clustering, multilingual visual search, and more! üåü Developer-friendly capabilities: ‚Ä¢ Configurable embedding dimensions ‚Ä¢ Task-specific instruction customization ‚Ä¢ Embedding quantization support for efficient and cost-effective downstream deployment Hugging FaceÔºö https://huggingface.co/collections/Qwen/qwen3-vl-embedding https://huggingface.co/collections/Qwen/qwen3-vl-reranker Github: https://github.com/QwenLM/Qwen3-VL-Embedding Blog: https://qwen.ai/blog?id=qwen3-vl-embedding Tech Report: https://www.arxiv.org/abs/2601.04720",
      "arxiv_page_url": "https://www.arxiv.org/abs/2601.04720",
      "pdf_url": "https://arxiv.org/pdf/2601.04720",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-VL-Embedding"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04720",
      "scraped_at": "2026-01-13T01:48:15.778962"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "paper_url": "https://huggingface.co/papers/2601.05930",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Can We Predict Before Executing Machine Learning Agents?",
      "abstract": "We replace slow trial-and-error in scientific agents with learned execution prediction, enabling FOREAGENT to think before it runs and achieve 6√ó faster and better scientific discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05930",
      "pdf_url": "https://arxiv.org/pdf/2601.05930",
      "github_links": [
        "https://github.com/zjunlp/predict-before-execute"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05930",
      "scraped_at": "2026-01-13T01:48:17.633216"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
    "paper_url": "https://huggingface.co/papers/2601.05882",
    "authors": [
      "Nikolaos Aletras",
      "Constantinos Karouzos",
      "XingweiT"
    ],
    "stars": "0",
    "details": {
      "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
      "abstract": "Our paper presents a systematic study of preference-optimization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. We found: The adaptation strategy is more influential than the alignment objective. We identify that synthetic supervision is a double-edged sword. While pseudo-labeling yields the highest target-domain win rates, it induces severe mode collapse. This diversity tax results in models that are highly reliable but linguistically monotonous, mirroring the latent templates of the teacher model. Our findings suggest a deployment recommendation: use pseudo-labeling for high-stakes and constrained tasks where reliability is paramount, but favor mixed-domain SFT and online RL for applications requiring creative or varied linguistic expression.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05882",
      "pdf_url": "https://arxiv.org/pdf/2601.05882",
      "github_links": [
        "https://github.com/ckarouzos/prefadap"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05882",
      "scraped_at": "2026-01-13T01:48:19.521997"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "paper_url": "https://huggingface.co/papers/2601.04786",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
      "abstract": "We‚Äôre introducing AgentOCR, a new way to scale LLM agents by reimagining long interaction histories as compact rendered images, leveraging the higher information density of visual tokens to curb exploding context costs. To make long-horizon rollouts practical, we add segment optical caching, splitting history into hashable segments and caching the visuals, so agents avoid redundant re-rendering as trajectories grow.  We go beyond fixed compression with agentic self-compression: the agent actively emits a compression rate and is trained with a compression-aware reward to balance task success against token efficiency. Across ALFWorld and search-based QA, AgentOCR keeps >95% of text-agent performance while cutting token use by >50% average and ~80% in peak, and our analysis shows up to a 20√ó rendering speedup thanks to our segment optical caching",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04786",
      "pdf_url": "https://arxiv.org/pdf/2601.04786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04786",
      "scraped_at": "2026-01-13T01:48:21.395496"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "paper_url": "https://huggingface.co/papers/2601.05966",
    "authors": [
      "Yu Sun",
      "Shuohuan Wang",
      "Xiaoxiong Liu",
      "Longbin Ji",
      "sjy1203"
    ],
    "stars": "0",
    "details": {
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "abstract": "VideoAR presents a scalable autoregressive video-generation framework that combines next-frame scale prediction with a 3D multi-scale tokenizer to improve temporal coherence and efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05966",
      "pdf_url": "https://arxiv.org/pdf/2601.05966",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05966",
      "scraped_at": "2026-01-13T01:48:23.249628"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "paper_url": "https://huggingface.co/papers/2601.05905",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "abstract": "We show that many LLM ‚Äúbeliefs‚Äù that look confident collapse under small context changes, and propose Neighbor-Consistency Belief (NCB) and Structure-Aware Training to measure and train models to keep their knowledge stable and robust under such interference.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05905",
      "pdf_url": "https://arxiv.org/pdf/2601.05905",
      "github_links": [
        "https://github.com/zjunlp/belief"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05905",
      "scraped_at": "2026-01-13T01:48:25.096155"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "paper_url": "https://huggingface.co/papers/2601.05848",
    "authors": [
      "Evan Luo",
      "Zitian Tang",
      "Yinghua Zhou",
      "dakshces",
      "nate-gillman"
    ],
    "stars": "0",
    "details": {
      "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
      "abstract": "Goal Force trains a physics-grounded video model to follow explicit force-directed goals, achieving zero-shot planning in real-world tasks by implicit neural physics simulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05848",
      "pdf_url": "https://arxiv.org/pdf/2601.05848",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05848",
      "scraped_at": "2026-01-13T01:48:27.018745"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05573",
    "authors": [
      "Tianyu Pang",
      "Jialei Wang",
      "Jiayang Xu",
      "Zehan Wang",
      "Viglong"
    ],
    "stars": "82",
    "details": {
      "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
      "abstract": "Code: https://github.com/SpatialVision/Orient-Anything-V2 Demo Space: https://huggingface.co/spaces/Viglong/Orient-Anything-V2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05573",
      "pdf_url": "https://arxiv.org/pdf/2601.05573",
      "github_links": [
        "https://github.com/SpatialVision/Orient-Anything-V2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05573",
      "scraped_at": "2026-01-13T01:48:28.874972"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
    "paper_url": "https://huggingface.co/papers/2601.05403",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
      "abstract": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (MFMD). In this work, we propose MFMD-Scen, a comprehensive benchmark for evaluating behavioral biases of LLMs in MFMD across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, MFMD-Scen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05403",
      "pdf_url": "https://arxiv.org/pdf/2601.05403",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05403",
      "scraped_at": "2026-01-13T01:48:30.700256"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "AnyDepth: Depth Estimation Made Easy",
    "paper_url": "https://huggingface.co/papers/2601.02760",
    "authors": [],
    "stars": "63",
    "details": {
      "title": "AnyDepth: Depth Estimation Made Easy",
      "abstract": "https://aigeeksgroup.github.io/AnyDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02760",
      "pdf_url": "https://arxiv.org/pdf/2601.02760",
      "github_links": [
        "https://github.com/AIGeeksGroup/AnyDepth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02760",
      "scraped_at": "2026-01-13T01:48:32.573488"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "paper_url": "https://huggingface.co/papers/2601.04888",
    "authors": [
      "Guanting Dong",
      "douzc",
      "vvv111222"
    ],
    "stars": "11",
    "details": {
      "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
      "abstract": "Some of the observations founded are :- i. Dual Level Credit Assessment This mechanism provides a comprehensive evaluation of query quality through both rule-based and model-based assessments. It allows for fine-grained supervision, helping to identify not just redundancy but also the usefulness of each query in the context of the search process. ii. Process Reward Mechanism The introduction of process rewards as a guiding signal for training search agents is a novel approach. It shifts the focus from solely final outcomes to the quality of intermediate queries, addressing a significant gap in existing methods that often overlook this aspect. iii. Query Refinement Strategy The framework employs a systematic query refinement process that identifies low quality queries and generates improved versions. This iterative refinement enhances the effectiveness of search trajectories, allowing agents to adaptively improve their queries based on feedback. iv. Three Stage Curriculum Learning Framework SmartSearch introduces a structured curriculum learning approach that progresses from imitation to alignment and finally to generalization. This staged learning process enables search agents to internalize query quality improvement progressively, enhancing their overall performance. v. Empirical Validation Across Diverse Benchmarks The paper presents extensive experimental results demonstrating SmartSearch's superior performance across multiple challenging knowledge-intensive tasks and web exploration scenarios. This empirical validation highlights the framework's robustness and effectiveness in real-world applications, showcasing its potential impact on future research in search agents and information retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04888",
      "pdf_url": "https://arxiv.org/pdf/2601.04888",
      "github_links": [
        "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04888",
      "scraped_at": "2026-01-13T01:48:34.423674"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Over-Searching in Search-Augmented Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.05503",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "abstract": "Systematically analyzes over-search in search-augmented LLMs, showing when retrieval helps or hurts, introducing Tokens Per Correctness and mitigation strategies.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05503",
      "pdf_url": "https://arxiv.org/pdf/2601.05503",
      "github_links": [
        "https://github.com/ruoyuxie/OversearchQA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05503",
      "scraped_at": "2026-01-13T01:48:36.245536"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.04823",
    "authors": [
      "Linqi Song",
      "Huacan Wang",
      "Ronghao Chen",
      "Guanzhi Deng",
      "liboaccn"
    ],
    "stars": "0",
    "details": {
      "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
      "abstract": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04823",
      "pdf_url": "https://arxiv.org/pdf/2601.04823",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04823",
      "scraped_at": "2026-01-13T01:48:38.107896"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.04726",
    "authors": [
      "Zhicheng Dou",
      "Yutao Zhu",
      "Jiejun Tan",
      "Jiongnan Liu",
      "namespace-ERI"
    ],
    "stars": "0",
    "details": {
      "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
      "abstract": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04726",
      "pdf_url": "https://arxiv.org/pdf/2601.04726",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04726",
      "scraped_at": "2026-01-13T01:48:39.978598"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
    "paper_url": "https://huggingface.co/papers/2601.05637",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A Reason-then-Describe Instruction Interpreter for Controllable Video Generation (2025) EVE: A Generator-Verifier System for Generative Policies (2025) Eliciting Behaviors in Multi-Turn Conversations (2025) SkillWrapper: Generative Predicate Invention for Skill Abstraction (2025) From Word to World: Can Large Language Models be Implicit Text-based World Models? (2025) SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models (2025) Propose, Solve, Verify: Self-Play Through Formal Verification (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05637",
      "pdf_url": "https://arxiv.org/pdf/2601.05637",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05637",
      "scraped_at": "2026-01-13T01:48:41.784741"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.04544",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
      "abstract": "Code: https://github.com/Tencent/TCAndon-Router",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04544",
      "pdf_url": "https://arxiv.org/pdf/2601.04544",
      "github_links": [
        "https://github.com/kyegomez/awesome-multi-agent-papers",
        "https://github.com/Tencent/TCAndon-Router"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04544",
      "scraped_at": "2026-01-13T01:48:43.634411"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "paper_url": "https://huggingface.co/papers/2601.05960",
    "authors": [
      "vicgalle"
    ],
    "stars": "1",
    "details": {
      "title": "Distilling Feedback into Memory-as-a-Tool",
      "abstract": "Code: https://github.com/vicgalle/feedback-memory-as-a-tool Data: https://huggingface.co/datasets/vicgalle/rubric-feedback-bench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05960",
      "pdf_url": "https://arxiv.org/pdf/2601.05960",
      "github_links": [
        "https://github.com/vicgalle/feedback-memory-as-a-tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05960",
      "scraped_at": "2026-01-13T01:48:45.433282"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "paper_url": "https://huggingface.co/papers/2601.05899",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
      "abstract": "Some of the observations are :- i. TowerMind is a lightweight RTS-style benchmark for LLM agents It introduces a tower defense based environment that preserves long term planning and decision making challenges of RTS games, while requiring very low computational resources compared to StarCraft II based benchmarks. ii. Multimodal observations enable broader LLM evaluation TowerMind supports pixel-based, textual (JSON), and structured state observations, making it suitable for evaluating language-only and vision-language models under the same environment. iii. Hallucination is explicitly measured via action validity Beyond performance score, the benchmark introduces valid action rate to quantify hallucinations. i.e. actions that violate game rules or state constraints allowing simultaneous evaluation of capability and reliability. iv. LLMs significantly underperform human experts Even the best-performing models (e.g. GPT-4.1, Claude 3.7 Sonnet) show a large gap from human experts, especially on harder levels, revealing weaknesses in planning validation, multifinality, and efficient action use. v. TowerMind is challenging for both LLMs and RL agents Classic RL algorithms (Ape-X DQN, PPO) also fail to reach human level performance, confirming TowerMind as a non-trivial benchmark that complements existing LLM and RL evaluation environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05899",
      "pdf_url": "https://arxiv.org/pdf/2601.05899",
      "github_links": [
        "https://github.com/tb6147877/TowerMind"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05899",
      "scraped_at": "2026-01-13T01:48:47.327914"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
    "paper_url": "https://huggingface.co/papers/2601.05851",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
      "abstract": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05851",
      "pdf_url": "https://arxiv.org/pdf/2601.05851",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05851",
      "scraped_at": "2026-01-13T01:48:49.142532"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
    "paper_url": "https://huggingface.co/papers/2601.05741",
    "authors": [
      "Marco Huber",
      "Jan Niklas Kolf",
      "Tahar Chettaoui",
      "Eduarda Caldeira",
      "gurayozgur"
    ],
    "stars": "3",
    "details": {
      "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
      "abstract": "https://github.com/gurayozgur/ViTNT-FIQA",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05741",
      "pdf_url": "https://arxiv.org/pdf/2601.05741",
      "github_links": [
        "https://github.com/gurayozgur/ViTNT-FIQA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05741",
      "scraped_at": "2026-01-13T01:48:51.076876"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "paper_url": "https://huggingface.co/papers/2601.05870",
    "authors": [
      "Zhuoyue Chen",
      "Long Li",
      "Yue Zhu",
      "Hongchen Luo",
      "Huilin Deng"
    ],
    "stars": "0",
    "details": {
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReLaX: Reasoning with Latent Exploration for Large Reasoning Models (2025) Multi-Path Collaborative Reasoning via Reinforcement Learning (2025) Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies (2025) ESPO: Entropy Importance Sampling Policy Optimization (2025) Diversity or Precision? A Deep Dive into Next Token Prediction (2025) SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization (2025) Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05870",
      "pdf_url": "https://arxiv.org/pdf/2601.05870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05870",
      "scraped_at": "2026-01-13T01:48:52.895587"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
    "paper_url": "https://huggingface.co/papers/2601.05699",
    "authors": [
      "Jesujoba Oluwadara Alabi",
      "Israel Abebe Azime",
      "Emilio Villa-Cueva",
      "Srija Anand",
      "Atnafu"
    ],
    "stars": "0",
    "details": {
      "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG (2025) Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries (2025) HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples (2025) Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles (2025) IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages (2025) See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models (2025) Multilingual VLM Training: Adapting an English-Trained VLM to French (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05699",
      "pdf_url": "https://arxiv.org/pdf/2601.05699",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05699",
      "scraped_at": "2026-01-13T01:48:54.752545"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
    "paper_url": "https://huggingface.co/papers/2601.05376",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "abstract": "This paper investigates how \"persona conditioning\" (e.g., instructing an LLM to act as a specific medical professional) impacts clinical decision-making. The authors challenge the assumption that assigning a medical persona consistently improves accuracy or safety, labeling this inconsistency the \"Persona Paradox.\" Key Insights: Non-Monotonic Effects: Assigning a medical persona (like an Emergency Department physician) does not always improve performance. It acts as a behavioral prior that can help in some contexts but hurt in others. The Context Gap: Medical personas improved accuracy and calibration by up to 20% in critical-care tasks (triage) but degraded performance by a similar margin in primary-care settings. Interaction Styles: Adding styles such as \"bold\" or \"cautious\" changes the model‚Äôs risk propensity, but these effects vary widely across base models. The Alignment Gap: While \"LLM judges\" preferred medical personas for safety-critical cases, human clinicians were much more skeptical. Human experts showed low confidence in the AI's reasoning quality in 95.9% of cases, despite moderate agreement on safety compliance. Conclusion The study concludes that personas are not \"expertise switches\" but rather priors that introduce context-dependent trade-offs. Relying on personas for clinical safety is risky because they do not provide a universal guarantee of better judgment. Personas should be used with caution in high-stakes medicine, as they can inadvertently trigger biases or performance drops depending on the specific clinical task.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05376",
      "pdf_url": "https://arxiv.org/pdf/2601.05376",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05376",
      "scraped_at": "2026-01-13T01:48:56.593118"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Legal Alignment for Safe and Ethical AI",
    "paper_url": "https://huggingface.co/papers/2601.04175",
    "authors": [
      "Rishi Bommasani",
      "Cullen O'Keefe",
      "Jack Boeglin",
      "Nicholas Caputo",
      "Noam Kolt"
    ],
    "stars": "0",
    "details": {
      "title": "Legal Alignment for Safe and Ethical AI",
      "abstract": "Field-defining paper by researchers from Stanford, MIT, Harvard, Oxford, Princeton, and other leading institutions. More details at: https://www.legal-alignment.ai/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04175",
      "pdf_url": "https://arxiv.org/pdf/2601.04175",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04175",
      "scraped_at": "2026-01-13T01:48:58.401803"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06943",
    "authors": [
      "Zhe Huang",
      "Zhuoyue Chang",
      "HJH2CMD",
      "Yu2020",
      "POTATO66"
    ],
    "stars": "51",
    "details": {
      "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
      "abstract": "First video deep research benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06943",
      "pdf_url": "https://arxiv.org/pdf/2601.06943",
      "github_links": [
        "https://github.com/QuantaAlpha/VideoDR-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06943",
      "scraped_at": "2026-01-14T01:55:05.388627"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "paper_url": "https://huggingface.co/papers/2601.06521",
    "authors": [
      "Liang Chen",
      "Liuff23",
      "Ziqi",
      "ssz1111",
      "chenxz"
    ],
    "stars": "81",
    "details": {
      "title": "BabyVision: Visual Reasoning Beyond Language",
      "abstract": "Feel free to follow our GitHub repo: https://github.com/UniPat-AI/BabyVision",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06521",
      "pdf_url": "https://arxiv.org/pdf/2601.06521",
      "github_links": [
        "https://github.com/UniPat-AI/BabyVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06521",
      "scraped_at": "2026-01-14T01:55:07.338360"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05593",
    "authors": [],
    "stars": "261",
    "details": {
      "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
      "abstract": "üéâ Introducing Parallel Coordinated Reasoning (PaCoRe) üìà An 8B model beats GPT-5 on HMMT25 by unlocking parallel thinking for test-time scaling! üìÇ Open-source deep think: data + model + inference code! üÜì MIT-licensed ‚Äî use it however you want üîçKey findings: Message Passing Unlocks Scaling Without compaction, performance flatlines at the context limit. PaCoRe breaks the memory barrier and lets reasoning scale freely. Breadth > Depth All compute is not equal. Coordinated parallel reasoning delivers far higher returns than extending a single chain. Data as a Force Multiplier The PaCoRe corpus provides exceptionally valuable supervision‚Äî even baseline models see substantial gains when trained on it. üîó Links: GitHub: https://github.com/stepfun-ai/PaCoRe Data: https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k Model: https://huggingface.co/stepfun-ai/PaCoRe-8B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05593",
      "pdf_url": "https://arxiv.org/pdf/2601.05593",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05593",
      "scraped_at": "2026-01-14T01:55:09.273922"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "paper_url": "https://huggingface.co/papers/2601.07832",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2601.07832",
      "pdf_url": "https://arxiv.org/pdf/2601.07832",
      "github_links": [
        "https://github.com/DAGroup-PKU/MHLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07832",
      "scraped_at": "2026-01-14T01:55:11.365101"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
    "paper_url": "https://huggingface.co/papers/2601.06953",
    "authors": [
      "Jane Luo",
      "Jiani Guo",
      "Xin Zhang",
      "Jie Wu",
      "Ringo1110"
    ],
    "stars": "52",
    "details": {
      "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Tailored Primitive Initialization is the Secret Key to Reinforcement Learning (2025) Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling (2026) PerfCoder: Large Language Models for Interpretable Code Performance Optimization (2025) Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization (2026) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06953",
      "pdf_url": "https://arxiv.org/pdf/2601.06953",
      "github_links": [
        "https://github.com/JieWu02/X-Coder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06953",
      "scraped_at": "2026-01-14T01:55:13.303655"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
    "paper_url": "https://huggingface.co/papers/2601.05110",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
      "abstract": "LLM + SLM > LLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05110",
      "pdf_url": "https://arxiv.org/pdf/2601.05110",
      "github_links": [
        "https://github.com/Zengwh02/GlimpRouter"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05110",
      "scraped_at": "2026-01-14T01:55:16.049503"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "paper_url": "https://huggingface.co/papers/2601.07226",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
      "abstract": "The code and dataset will be released publicly.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07226",
      "pdf_url": "https://arxiv.org/pdf/2601.07226",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07226",
      "scraped_at": "2026-01-14T01:55:18.008918"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "paper_url": "https://huggingface.co/papers/2601.07779",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
      "abstract": "Despite VLM advances, current CUA frameworks remain brittle in long-horizon workflows and weak in novel domains due to coarse historical visual context management and missing visual-aware tutorial retrieval, so we propose OS-SYMPHONY, an orchestrated framework combining milestone-driven reflection memory for trajectory-level self-correction with a SeeAct-style multimodal searcher that synthesizes visually aligned live tutorials, achieving new SOTA across three online benchmarks (65.84% on OSWorld).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07779",
      "pdf_url": "https://arxiv.org/pdf/2601.07779",
      "github_links": [
        "https://github.com/OS-Copilot/OS-Symphony"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07779",
      "scraped_at": "2026-01-14T01:55:19.919265"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.07351",
    "authors": [
      "Chenchen Jing",
      "Tianjian Feng",
      "Bozhen Fang",
      "Linyu Wu",
      "zhongzero"
    ],
    "stars": "16",
    "details": {
      "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
      "abstract": "GitHub repo: https://github.com/aim-uofa/EvoTokenDLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07351",
      "pdf_url": "https://arxiv.org/pdf/2601.07351",
      "github_links": [
        "https://github.com/aim-uofa/EvoTokenDLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07351",
      "scraped_at": "2026-01-14T01:55:21.893924"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "paper_url": "https://huggingface.co/papers/2601.05107",
    "authors": [
      "Zhengkang Guo",
      "Jingwen Xu",
      "Xiaohua Wang",
      "Muzhao Tian",
      "zisuh"
    ],
    "stars": "0",
    "details": {
      "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
      "abstract": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05107",
      "pdf_url": "https://arxiv.org/pdf/2601.05107",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05107",
      "scraped_at": "2026-01-14T01:55:23.799775"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "paper_url": "https://huggingface.co/papers/2601.01528",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
      "abstract": "DrivingGen is a comprehensive benchmark for generative world models in the driving domain with a diverse data distribution and novel evaluation metrics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01528",
      "pdf_url": "https://arxiv.org/pdf/2601.01528",
      "github_links": [
        "https://github.com/youngzhou1999/DrivingGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01528",
      "scraped_at": "2026-01-14T01:55:25.767787"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "paper_url": "https://huggingface.co/papers/2601.07526",
    "authors": [
      "Jiawei Chen",
      "Ruisheng Cao",
      "Mouxiang Chen",
      "zjj1233",
      "Lemoncoke"
    ],
    "stars": "0",
    "details": {
      "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
      "abstract": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07526",
      "pdf_url": "https://arxiv.org/pdf/2601.07526",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07526",
      "scraped_at": "2026-01-14T01:55:27.656092"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2601.05823",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
      "abstract": "arXiv link: Boosting Latent Diffusion Models via Disentangled Representation Alignment Code (Coming Soon): https://github.com/Kwai-Kolors/Send-VAE",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05823",
      "pdf_url": "https://arxiv.org/pdf/2601.05823",
      "github_links": [
        "https://github.com/Kwai-Kolors/Send-VAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05823",
      "scraped_at": "2026-01-14T01:55:29.594871"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06165",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
      "abstract": "Users often ask VLMs under-specified, informal visual questions, which current clean-prompt benchmarks fail to capture. We introduce HAERAE-Vision (653 real Korean community queries + explicit rewrites) and show that making queries explicit boosts accuracy by 8‚Äì22 points, while web search cannot fully offset what users leave unsaid.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06165",
      "pdf_url": "https://arxiv.org/pdf/2601.06165",
      "github_links": [
        "https://github.com/HAE-RAE/HAERAE-VISION"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06165",
      "scraped_at": "2026-01-14T01:55:31.500284"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
    "paper_url": "https://huggingface.co/papers/2601.06860",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
      "abstract": "Most current TIR work only focuses on the accuracy of agents in downstream tasks, while lacking calibration of the agents' behavioral patterns in TIR tasks. To address this issue, we first quantitatively analyze several possible erroneous behavioral patterns in current TIR tasks, and classify them into two categories: \"improper tool use\" and \"flawed reasoning logic\". Based on this, we propose ET-Agent, a framework that fully calibrates the behavioral patterns of agents when performing TIR tasks from both data and algorithm levels. On the data side, we propose a self-evolving data flywheel, which enhances the training data by leveraging the agent's own reflective exploration capabilities. On the algorithm side, we propose a behavioral calibration training framework. It performs rejection sampling fine-tuning on the basis of enhanced training data to broaden the agent's exploration of the action space. Subsequently, we implement iterative behavioral calibration reinforcement learning to calibrate the actions of the fine-tuned agent to the optimal behavioral pattern. Our contributions are listed as follows: We provide a comprehensive quantitative analysis of erroneous behavioral patterns in TIR. Inspired by this, we propose ET-Agent, a framework for optimizing TIR's behavioral patterns. We introduce a self-evolving data flywheel, an iterative loop where the model continuously refines its previous trajectories. This mechanism effectively unfolds the model's action space coverage beyond its initial exploration. Based on the flywheel, we present a behavior calibration training framework with two phases, aiming to calibrate the model's exploration in tool-use action space to optimal trajectories. Extensive experiments demonstrate that ET-Agent substantially improves behavioral efficiency, reasoning conciseness, and execution success rates while maintaining high accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06860",
      "pdf_url": "https://arxiv.org/pdf/2601.06860",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06860",
      "scraped_at": "2026-01-14T01:55:33.429929"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
    "paper_url": "https://huggingface.co/papers/2601.07055",
    "authors": [
      "Shaoliang Nie",
      "Suyu Ge",
      "Xianjun Yang",
      "Kartikeya Upasani",
      "Zhenrui Yue"
    ],
    "stars": "74",
    "details": {
      "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
      "abstract": "Dr. Zero enables data-free self-evolving search agents through a self-evolution loop with HRPO, achieving strong multi-step reasoning while reducing compute.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07055",
      "pdf_url": "https://arxiv.org/pdf/2601.07055",
      "github_links": [
        "https://github.com/facebookresearch/drzero"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07055",
      "scraped_at": "2026-01-14T01:55:35.318742"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06803",
    "authors": [
      "Yankai Lin",
      "Yichen Wu",
      "Yubo Wang",
      "Yuhan",
      "ZION121"
    ],
    "stars": "0",
    "details": {
      "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
      "abstract": "We hope this work encourages a paradigm shift from explicit next-token prediction to latent visual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06803",
      "pdf_url": "https://arxiv.org/pdf/2601.06803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06803",
      "scraped_at": "2026-01-14T01:55:37.211022"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
    "paper_url": "https://huggingface.co/papers/2601.04698",
    "authors": [
      "Hao Wang",
      "Xiaoxi Li",
      "Wenxiang Jiao",
      "Mining Tan",
      "Yinuo Wang"
    ],
    "stars": "0",
    "details": {
      "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
      "abstract": "We propose TourPlanner , a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04698",
      "pdf_url": "https://arxiv.org/pdf/2601.04698",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04698",
      "scraped_at": "2026-01-14T01:55:39.117410"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2601.07376",
    "authors": [
      "Jiaxuan You",
      "zsqzz"
    ],
    "stars": "568",
    "details": {
      "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
      "abstract": "üéâ Introducing OpenTinker üöÄ A scalable RL infrastructure for LLM agents that separates what you build (agents + environments) from how it runs (training + inference)! üß© Composable RL-as-a-Service No more monolithic RL pipelines. OpenTinker decomposes agentic learning into lightweight, modular components with clean abstraction boundaries. Plug in new agents, environments, and interaction protocols with minimal friction. ‚öôÔ∏è Unified Runtime for Training + Inference A centralized scheduler manages shared compute across workloads like RL (LoRA / full-parameter), SFT, and high-throughput inference. Built for multi-tenant scaling and real-world iteration speed. ü§ñ Multi-Agent Ready by Design OpenTinker supports coordinator-driven multi-agent interaction. Each agent can optimize independently while coordination emerges through environment dynamics. This keeps MARL scalable, flexible, and system-friendly. üîó Links: üìÑ Paper (arXiv): https://arxiv.org/pdf/2601.07376 üíª GitHub: https://github.com/open-tinker/OpenTinker",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07376",
      "pdf_url": "https://arxiv.org/pdf/2601.07376",
      "github_links": [
        "https://github.com/open-tinker/OpenTinker?tab=readme-ov-file",
        "https://github.com/open-tinker/OpenTinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07376",
      "scraped_at": "2026-01-14T01:55:41.067699"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Are LLM Decisions Faithful to Verbal Confidence?",
    "paper_url": "https://huggingface.co/papers/2601.07767",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Are LLM Decisions Faithful to Verbal Confidence?",
      "abstract": "While LLMs can express their confidence levels, their actual decisions do not demonstrate risk sensitivity. Even with high error penalties, they rarely abstain from making choices, often leading to utility collapse.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07767",
      "pdf_url": "https://arxiv.org/pdf/2601.07767",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07767",
      "scraped_at": "2026-01-14T01:55:42.902306"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Structured Episodic Event Memory",
    "paper_url": "https://huggingface.co/papers/2601.06411",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Structured Episodic Event Memory",
      "abstract": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06411",
      "pdf_url": "https://arxiv.org/pdf/2601.06411",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06411",
      "scraped_at": "2026-01-14T01:55:44.777676"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "paper_url": "https://huggingface.co/papers/2601.03666",
    "authors": [
      "Zhicheng Dou",
      "Tetsuya Sakai",
      "Radu Timofte",
      "Sicheng Gao",
      "Haon-Chen"
    ],
    "stars": "0",
    "details": {
      "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
      "abstract": "A lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. Checkpoints: https://huggingface.co/Haon-Chen/e5-omni-3B https://huggingface.co/Haon-Chen/e5-omni-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03666",
      "pdf_url": "https://arxiv.org/pdf/2601.03666",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03666",
      "scraped_at": "2026-01-14T01:55:46.645891"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "paper_url": "https://huggingface.co/papers/2601.07786",
    "authors": [
      "Mia Mohammad Imran",
      "Abdullah Al Mujahid"
    ],
    "stars": "0",
    "details": {
      "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
      "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07786",
      "pdf_url": "https://arxiv.org/pdf/2601.07786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07786",
      "scraped_at": "2026-01-14T01:55:48.489091"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "ShowUI-Aloha: Human-Taught GUI Agent",
    "paper_url": "https://huggingface.co/papers/2601.07181",
    "authors": [
      "Zhiheng Chen",
      "Jessica Hu",
      "Yauhong Goh",
      "Xiangwu Guo",
      "Yichun Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "ShowUI-Aloha: Human-Taught GUI Agent",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2601.07181",
      "pdf_url": "https://arxiv.org/pdf/2601.07181",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07181",
      "scraped_at": "2026-01-14T01:55:50.358460"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Codified Foreshadowing-Payoff Text Generation",
    "paper_url": "https://huggingface.co/papers/2601.07033",
    "authors": [
      "Jingbo Shang",
      "Letian Peng",
      "Kun Zhou",
      "Longfei Yun",
      "hyp1231"
    ],
    "stars": "0",
    "details": {
      "title": "Codified Foreshadowing-Payoff Text Generation",
      "abstract": "Codified Foreshadowing-Payoff Text Generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07033",
      "pdf_url": "https://arxiv.org/pdf/2601.07033",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07033",
      "scraped_at": "2026-01-14T01:55:52.291930"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
    "paper_url": "https://huggingface.co/papers/2601.04577",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
      "abstract": "While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04577",
      "pdf_url": "https://arxiv.org/pdf/2601.04577",
      "github_links": [
        "https://github.com/AmberLJC/Sci-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04577",
      "scraped_at": "2026-01-14T01:55:54.190567"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "paper_url": "https://huggingface.co/papers/2601.03570",
    "authors": [
      "Zaishuo Xia",
      "Minqian Liu",
      "Yunzhi Yao",
      "Sha Li",
      "Barry Menglong Yao"
    ],
    "stars": "0",
    "details": {
      "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
      "abstract": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03570",
      "pdf_url": "https://arxiv.org/pdf/2601.03570",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03570",
      "scraped_at": "2026-01-14T01:55:56.059301"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
    "paper_url": "https://huggingface.co/papers/2601.07389",
    "authors": [
      "Weixi Zhang",
      "Wei Han",
      "Bo Bai",
      "Xueyan Niu"
    ],
    "stars": "0",
    "details": {
      "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
      "abstract": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07389",
      "pdf_url": "https://arxiv.org/pdf/2601.07389",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07389",
      "scraped_at": "2026-01-14T01:55:57.904888"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
    "paper_url": "https://huggingface.co/papers/2601.06993",
    "authors": [
      "Xiaoming Liu",
      "Yiyang Su",
      "Paipile"
    ],
    "stars": "1",
    "details": {
      "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
      "abstract": "In this work, we investigate the impact of CoT on Fine-Grained Visual Classification (FGVC), revealing a paradox: the degradation in FGVC performance due to CoT is primarily driven by reasoning length, with longer textual reasoning consistently reducing classification accuracy. We introduce the concept of the \"Cost of Thinking\" to describe this phenomenon. Building on this insight, we propose two key contributions: (1) MRN, a normalization method for multi-reward optimization that balances heterogeneous reward signals; and (2) ReFine-RFT, a framework that integrates ensemble rewards with MRN to constrain reasoning length while providing dense, accuracy-oriented feedback. Our extensive experiments across multiple FGVC benchmarks demonstrate the effectiveness of our approach, achieving state-of-the-art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06993",
      "pdf_url": "https://arxiv.org/pdf/2601.06993",
      "github_links": [
        "https://github.com/jiezhu23/ReFine-RFT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06993",
      "scraped_at": "2026-01-14T01:55:59.761826"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "paper_url": "https://huggingface.co/papers/2601.06966",
    "authors": [
      "Shaolei Zhang",
      "Zishan Xu",
      "Sen Hu",
      "Zhiyuan Yao",
      "Haonan-Bian"
    ],
    "stars": "0",
    "details": {
      "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory (2026) Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents (2026) Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI (2025) MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards (2026) KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions (2026) MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents (2026) Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06966",
      "pdf_url": "https://arxiv.org/pdf/2601.06966",
      "github_links": [
        "https://github.com/AvatarMemory/RealMemBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06966",
      "scraped_at": "2026-01-14T01:56:01.621095"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06944",
    "authors": [
      "Shixing Li",
      "Guozhang Li",
      "Yaoyao Zhong",
      "Mei Wang",
      "Yuhang Su"
    ],
    "stars": "0",
    "details": {
      "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ViRectify: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models (2025) PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding (2025) MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models (2026) AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding (2026) CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution (2025) PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models (2025) Evaluating large language models on multimodal chemistry olympiad exams (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06944",
      "pdf_url": "https://arxiv.org/pdf/2601.06944",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06944",
      "scraped_at": "2026-01-14T01:56:03.488898"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06788",
    "authors": [
      "Manling Li",
      "Zeguan Wu",
      "Canyu Chen",
      "Zihan Wang",
      "Min Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06788",
      "pdf_url": "https://arxiv.org/pdf/2601.06788",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06788",
      "scraped_at": "2026-01-14T01:56:05.320924"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "paper_url": "https://huggingface.co/papers/2601.06747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
      "abstract": "This paper introduces FinForge, a novel framework designed to address the scarcity of high-quality, domain-specific datasets for evaluating Large Language Models (LLMs) in finance. The authors propose a scalable, semi-synthetic pipeline that combines expert-guided data curation from authoritative sources with controlled question generation and validation using Gemini 2.5 Flash. Key Contributions: FinForge Framework: A hybrid pipeline integrating manual/programmatic corpus construction with rigorous LM-based synthesis. FinForge-5k Dataset: A new snapshot benchmark comprising over 5,000 human-validated Q&A pairs across 11 financial subdomains, derived from a curated corpus of 100,000 verified documents (143M tokens). Benchmarking Results: Evaluation of state-of-the-art open and closed-source models reveals significant variance in financial reasoning capabilities, with leading models achieving approximately 80% accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06747",
      "pdf_url": "https://arxiv.org/pdf/2601.06747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06747",
      "scraped_at": "2026-01-14T01:56:07.161587"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "paper_url": "https://huggingface.co/papers/2601.06463",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06463",
      "pdf_url": "https://arxiv.org/pdf/2601.06463",
      "github_links": [
        "https://github.com/XuezheMax/gecko-llm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06463",
      "scraped_at": "2026-01-14T01:56:09.049268"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "paper_url": "https://huggingface.co/papers/2601.06423",
    "authors": [
      "Deep Mehta"
    ],
    "stars": "0",
    "details": {
      "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
      "abstract": "We ask a question that hasn't been studied before: does inference scaling improve reasoning faithfulness or just accuracy? Self-consistency (majority voting over multiple reasoning paths) reliably boosts LLM accuracy on reasoning tasks. But does getting the right answer more often mean the model is actually reasoning better? We test 4 frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K problems and find a surprising tradeoff. Accuracy gains from self-consistency don't necessarily translate to more faithful reasoning. This discovery has important implications for AI safety. We may be building systems that appear smarter without actually reasoning more reliably.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06423",
      "pdf_url": "https://arxiv.org/pdf/2601.06423",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06423",
      "scraped_at": "2026-01-14T01:56:10.861313"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
    "paper_url": "https://huggingface.co/papers/2601.05747",
    "authors": [
      "Peter St\\√ºtz",
      "Marvin Brenner",
      "farooqhassaan"
    ],
    "stars": "0",
    "details": {
      "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
      "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasible models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ‚àº20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05747",
      "pdf_url": "https://arxiv.org/pdf/2601.05747",
      "github_links": [
        "https://github.com/farooqhassaan/FlyPose"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05747",
      "scraped_at": "2026-01-14T01:56:12.800773"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "paper_url": "https://huggingface.co/papers/2601.07790",
    "authors": [
      "Chaowei Yang",
      "Joseph Rogers",
      "Zifu Wang",
      "Emily Ma",
      "ymasri"
    ],
    "stars": "1",
    "details": {
      "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
      "abstract": "We evaluate 9 open-source models under zero-shot, few-shot, and RAG (FAISS) and measure both accuracy + per-log latency. Main takeaway: RAG can massively help small models (Qwen3-4B: 95.64%, Gemma3-1B: 85.28%), but some reasoning-focused models degrade with retrieval, showing that retrieval integration isn‚Äôt uniform across architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07790",
      "pdf_url": "https://arxiv.org/pdf/2601.07790",
      "github_links": [
        "https://github.com/stccenter/Benchmarking-SLMs-and-SRLMs-on-System-Log-Severity-Classification"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07790",
      "scraped_at": "2026-01-14T01:56:14.646904"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "paper_url": "https://huggingface.co/papers/2601.07239",
    "authors": [
      "Shreyash Dhoot",
      "Aadi Pandey",
      "Anusa Saha",
      "Shourya Aggarwal",
      "Tanmay Joshi"
    ],
    "stars": "0",
    "details": {
      "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "abstract": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07239",
      "pdf_url": "https://arxiv.org/pdf/2601.07239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07239",
      "scraped_at": "2026-01-14T01:56:16.492225"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.06496",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
      "abstract": "https://github.com/AIGeeksGroup/3DCoCav2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06496",
      "pdf_url": "https://arxiv.org/pdf/2601.06496",
      "github_links": [
        "https://github.com/AIGeeksGroup/3DCoCav2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06496",
      "scraped_at": "2026-01-14T01:56:18.356557"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.06329",
    "authors": [
      "Ju-Chieh Chou",
      "Yen-Chun Kuo",
      "Yi-Cheng Lin",
      "Liang-Hsuan Tseng",
      "Jeff Chan-Jan Sju"
    ],
    "stars": "0",
    "details": {
      "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
      "abstract": "Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ‚Äúglobal token perplexity‚Äù, which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06329",
      "pdf_url": "https://arxiv.org/pdf/2601.06329",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06329",
      "scraped_at": "2026-01-14T01:56:20.214536"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
    "paper_url": "https://huggingface.co/papers/2601.06307",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Dhruva Patil",
      "Zhenlin He",
      "Ishika Agarwal"
    ],
    "stars": "0",
    "details": {
      "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
      "abstract": "https://huggingface.co/collections/ishikaa/a-rising-tide-lifts-all-boats-mtqe-rewards-for-idioms",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06307",
      "pdf_url": "https://arxiv.org/pdf/2601.06307",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06307",
      "scraped_at": "2026-01-14T01:56:22.211331"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
    "paper_url": "https://huggingface.co/papers/2601.06238",
    "authors": [
      "Aman Chadha",
      "Vinija Jain",
      "Amit Dhanda",
      "Partha Pratim Saha",
      "Arion Das"
    ],
    "stars": "0",
    "details": {
      "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
      "abstract": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06238",
      "pdf_url": "https://arxiv.org/pdf/2601.06238",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06238",
      "scraped_at": "2026-01-14T01:56:24.053442"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "paper_url": "https://huggingface.co/papers/2601.06789",
    "authors": [
      "Yu2020",
      "KunyiWang",
      "shuozhang",
      "cadche",
      "jimson991"
    ],
    "stars": "19",
    "details": {
      "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
      "abstract": "code agent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06789",
      "pdf_url": "https://arxiv.org/pdf/2601.06789",
      "github_links": [
        "https://github.com/QuantaAlpha/MemGovern"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06789",
      "scraped_at": "2026-01-15T01:50:09.498721"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Solar Open Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.07022",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Solar Open Technical Report",
      "abstract": "huggingface model: https://huggingface.co/upstage/Solar-Open-100B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07022",
      "pdf_url": "https://arxiv.org/pdf/2601.07022",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07022",
      "scraped_at": "2026-01-15T01:50:11.630863"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
    "paper_url": "https://huggingface.co/papers/2601.04745",
    "authors": [
      "lanqz7766",
      "ChenglongLi",
      "Super-shuhe-v2",
      "Zhisheng888",
      "realty2333"
    ],
    "stars": "83",
    "details": {
      "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
      "abstract": "know me",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04745",
      "pdf_url": "https://arxiv.org/pdf/2601.04745",
      "github_links": [
        "https://github.com/QuantaAlpha/KnowMeBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04745",
      "scraped_at": "2026-01-15T01:50:13.446669"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "paper_url": "https://huggingface.co/papers/2601.08225",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
      "abstract": "While large language models have shown remarkable progress in tool use, maintaining high-quality, user-centric multi-turn conversations at scale remains a significant challenge. Our work focuses on: (1) Generating high-fidelity multi-turn dialogue datasets designed for practical tool-use scenarios. (2) Enhancing model performance in complex, user-oriented interactions. (3) Providing insights into scaling dialogue generation without compromising on user experience. Check out the full paper here: https://arxiv.org/abs/2601.08225",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08225",
      "pdf_url": "https://arxiv.org/pdf/2601.08225",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08225",
      "scraped_at": "2026-01-15T01:50:15.282705"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands",
    "paper_url": "https://huggingface.co/papers/2512.24965",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands",
      "abstract": "TL;DR: ShowUI-œÄ is a 450M flow-based vision-language-action model that treats GUI actions as continuous trajectories, generating smooth clicks and drags directly from screen observations. It unifies discrete and continuous actions, enabling precise drawing, rotation, sorting, and captcha solving without tokenized coordinates. arXiv: https://arxiv.org/abs/2512.24965 Website: https://showlab.github.io/showui-pi/ Github: https://github.com/showlab/showui-pi",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24965",
      "pdf_url": "https://arxiv.org/pdf/2512.24965",
      "github_links": [
        "https://github.com/showlab/showui-pi"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24965",
      "scraped_at": "2026-01-15T01:50:17.137473"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.08079",
    "authors": [
      "Zhao Cao",
      "lz1001",
      "TommyChien"
    ],
    "stars": "39",
    "details": {
      "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
      "abstract": "Project Repo: https://github.com/qhjqhj00/MemoBrain",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08079",
      "pdf_url": "https://arxiv.org/pdf/2601.08079",
      "github_links": [
        "https://github.com/qhjqhj00/MemoBrain"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08079",
      "scraped_at": "2026-01-15T01:50:18.963578"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "paper_url": "https://huggingface.co/papers/2601.06487",
    "authors": [],
    "stars": "49",
    "details": {
      "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
      "abstract": "As a key exploration of open-domain agents, our method has been validated within Amap's (Gaode Map) real-world business scenarios. Demonstrating significant practical value, we believe this paradigm represents one of the most important direction of AI agents in the future. Project Resources: Github: https://github.com/Alibaba-NLP/qqr Paper: https://arxiv.org/abs/2601.06487 Hugging Face: https://huggingface.co/collections/Alibaba-NLP/arenarl",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06487",
      "pdf_url": "https://arxiv.org/pdf/2601.06487",
      "github_links": [
        "https://github.com/Alibaba-NLP/qqr"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06487",
      "scraped_at": "2026-01-15T01:50:20.840380"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Ministral 3",
    "paper_url": "https://huggingface.co/papers/2601.08584",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Ministral 3",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T5Gemma 2: Seeing, Reading, and Understanding Longer (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs (2026) SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation (2025) Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08584",
      "pdf_url": "https://arxiv.org/pdf/2601.08584",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08584",
      "scraped_at": "2026-01-15T01:50:22.698526"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "paper_url": "https://huggingface.co/papers/2601.08831",
    "authors": [
      "Min-Hung Chen",
      "Fu-En Yang",
      "Chin-Yang Lin",
      "Cheng Sun",
      "Yang-Che Sun"
    ],
    "stars": "0",
    "details": {
      "title": "3AM: Segment Anything with Geometric Consistency in Videos",
      "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08831",
      "pdf_url": "https://arxiv.org/pdf/2601.08831",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08831",
      "scraped_at": "2026-01-15T01:50:24.602177"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.07264",
    "authors": [
      "Qingcheng Zeng",
      "Naotoyokoyama",
      "junjuewang",
      "lrzneedresearch",
      "weihao1115"
    ],
    "stars": "0",
    "details": {
      "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
      "abstract": "We reveal a \"confidence dichotomy\" in tool-use LLM agents, finding that evidence tools like web search systematically induce overconfidence due to noisy retrieval, while verification tools like code interpreters help ground reasoning and reduce miscalibration. To address this, we propose Calibration Agentic RL (CAR), a reinforcement learning framework that jointly optimizes task accuracy and calibration through a novel reward design, demonstrating significant reductions in calibration error while maintaining competitive performance across different domains and environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07264",
      "pdf_url": "https://arxiv.org/pdf/2601.07264",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07264",
      "scraped_at": "2026-01-15T01:50:26.533802"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "paper_url": "https://huggingface.co/papers/2601.08670",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
      "abstract": "Parallel Context-of-Experts Decoding (Pced) speeds up RAG by decoding in parallel from per-document KV-cache ‚Äúexperts‚Äù and selecting retrieval-supported tokens to recover cross-document reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08670",
      "pdf_url": "https://arxiv.org/pdf/2601.08670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08670",
      "scraped_at": "2026-01-15T01:50:28.367501"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Motion Attribution for Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.08828",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Motion Attribution for Video Generation",
      "abstract": "TL;DR: We propose MOTIVE, a scalable, motion-centric data attribution framework for video generation to identify which training clips improve or degrade motion dynamics, enabling curation and more.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08828",
      "pdf_url": "https://arxiv.org/pdf/2601.08828",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08828",
      "scraped_at": "2026-01-15T01:50:30.225015"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.08620",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
      "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://huggingface.co/vidore .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08620",
      "pdf_url": "https://arxiv.org/pdf/2601.08620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08620",
      "scraped_at": "2026-01-15T01:50:32.048862"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "paper_url": "https://huggingface.co/papers/2601.08303",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
      "abstract": "Proposes efficient diffusion transformers for edge devices via sparse attention, elastic training, and knowledge-guided distillation to achieve high-fidelity, fast on-device image generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08303",
      "pdf_url": "https://arxiv.org/pdf/2601.08303",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08303",
      "scraped_at": "2026-01-15T01:50:33.857107"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "paper_url": "https://huggingface.co/papers/2601.08665",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation (2025) MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots (2025) CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving (2025) NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction (2025) ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination (2025) Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation (2025) EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08665",
      "pdf_url": "https://arxiv.org/pdf/2601.08665",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08665",
      "scraped_at": "2026-01-15T01:50:35.696313"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "paper_url": "https://huggingface.co/papers/2601.08587",
    "authors": [],
    "stars": "550",
    "details": {
      "title": "End-to-End Video Character Replacement without Structural Guidance",
      "abstract": "End-to-End Video Character Replacement without Structural Guidance",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08587",
      "pdf_url": "https://arxiv.org/pdf/2601.08587",
      "github_links": [
        "https://github.com/Orange-3DV-Team/MoCha"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08587",
      "scraped_at": "2026-01-15T01:50:37.523465"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.08468",
    "authors": [
      "Sujian Li",
      "Yudong Wang",
      "Hailin Zhang",
      "Hanyu Li",
      "Jiangshan Duo"
    ],
    "stars": "0",
    "details": {
      "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Structured Reasoning for Large Language Models (2026) Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning (2026) Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning (2026) Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards (2025) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) ORION: Teaching Language Models to Reason Efficiently in the Language of Thought (2025) ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08468",
      "pdf_url": "https://arxiv.org/pdf/2601.08468",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08468",
      "scraped_at": "2026-01-15T01:50:39.376659"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "paper_url": "https://huggingface.co/papers/2601.07290",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
      "abstract": "Joint temporal understanding and spatial perception within a single framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07290",
      "pdf_url": "https://arxiv.org/pdf/2601.07290",
      "github_links": [
        "https://github.com/JPShi12/VideoLoom"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07290",
      "scraped_at": "2026-01-15T01:50:41.246788"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.06786",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
      "abstract": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemicallycalibrated reasoning (EPICAR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Paretosuperiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3√ó reduction in inference compute, matching the K = 30 performance of STaR with only K = 10 samples in capable models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06786",
      "pdf_url": "https://arxiv.org/pdf/2601.06786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06786",
      "scraped_at": "2026-01-15T01:50:43.224423"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "paper_url": "https://huggingface.co/papers/2601.08321",
    "authors": [
      "Ting Zhu",
      "Zipeng Guo",
      "Gaojing Zhou",
      "Xiaolong Fu",
      "Lichen Ma"
    ],
    "stars": "0",
    "details": {
      "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08321",
      "pdf_url": "https://arxiv.org/pdf/2601.08321",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08321",
      "scraped_at": "2026-01-15T01:50:45.007426"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
    "paper_url": "https://huggingface.co/papers/2601.04582",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
      "abstract": "Generating working visualization code is not enough. Charts must be semantically correct and visually meaningful. We introduce RL-Text2Vis, the first reinforcement-learning framework for Text-to-Visualization, using post-execution feedback to jointly optimize: ‚úîÔ∏è textual accuracy ‚úîÔ∏è code executability ‚úîÔ∏è visualization quality üìà Results: ‚Ä¢ +22% relative improvement in chart quality over GPT-4o ‚Ä¢ Code execution success boosted from 78% ‚Üí 97% ‚Ä¢ Strong generalization to out-of-domain benchmarks This work demonstrates the power of multi-objective RL for structured, multimodal reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04582",
      "pdf_url": "https://arxiv.org/pdf/2601.04582",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04582",
      "scraped_at": "2026-01-15T01:50:46.788335"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "paper_url": "https://huggingface.co/papers/2601.02669",
    "authors": [
      "Zhen Ye",
      "Ziyang Luo",
      "Zhiqi Shen",
      "Zixin Chen",
      "Hongzhan Lin"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
      "abstract": "Current automatic LLM fact-checking tests are too narrow, they only check if a model can verify a claim, ignoring the hard parts like finding evidence and decomposing check-worthy claims. FactArena is built to evaluate the full fact-checking pipeline. Testing 16 state-of-the-art models reveals that the whole process ranking can disagree with simple accuracy. The system also revealed fragility in LLM fact-checking through subtle claim modifications (\"claim flipping\"), tanked average accuracy to 68%, proving that trustworthy auditing of every stage in the process matters.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02669",
      "pdf_url": "https://arxiv.org/pdf/2601.02669",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02669",
      "scraped_at": "2026-01-15T01:50:48.582670"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.08173",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
      "abstract": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08173",
      "pdf_url": "https://arxiv.org/pdf/2601.08173",
      "github_links": [
        "https://github.com/KnowledgeXLab/EvoEnv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08173",
      "scraped_at": "2026-01-15T01:50:50.375117"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.07632",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07632",
      "pdf_url": "https://arxiv.org/pdf/2601.07632",
      "github_links": [
        "https://github.com/JYe16/GeoMotionGPT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07632",
      "scraped_at": "2026-01-15T01:50:52.181113"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "paper_url": "https://huggingface.co/papers/2601.07348",
    "authors": [],
    "stars": "79",
    "details": {
      "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
      "abstract": "arXiv explained breakdown of this paper üëâ https://arxivexplained.com/papers/controlled-self-evolution-for-algorithmic-code-optimization",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07348",
      "pdf_url": "https://arxiv.org/pdf/2601.07348",
      "github_links": [
        "https://github.com/QuantaAlpha/EvoControl"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07348",
      "scraped_at": "2026-01-16T01:52:19.823061"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.09688",
    "authors": [],
    "stars": "67",
    "details": {
      "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
      "abstract": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09688",
      "pdf_url": "https://arxiv.org/pdf/2601.09688",
      "github_links": [
        "https://github.com/Infinity-AILab/DeepResearchEval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09688",
      "scraped_at": "2026-01-16T01:52:21.780207"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.09259",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
      "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09259",
      "pdf_url": "https://arxiv.org/pdf/2601.09259",
      "github_links": [
        "https://github.com/exoskeletonzj/MAXS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09259",
      "scraped_at": "2026-01-16T01:52:23.826039"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
    "paper_url": "https://huggingface.co/papers/2601.09274",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
      "abstract": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A3-Bench, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09274",
      "pdf_url": "https://arxiv.org/pdf/2601.09274",
      "github_links": [
        "https://github.com/exoskeletonzj/A3-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09274",
      "scraped_at": "2026-01-16T01:52:25.818622"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09088",
    "authors": [],
    "stars": "16",
    "details": {
      "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
      "abstract": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09088",
      "pdf_url": "https://arxiv.org/pdf/2601.09088",
      "github_links": [
        "https://github.com/D2I-ai/dasd-thinking"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09088",
      "scraped_at": "2026-01-16T01:52:28.085648"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "paper_url": "https://huggingface.co/papers/2601.09708",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "abstract": "Project page: https://jasper0314-huang.github.io/fast-thinkact/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09708",
      "pdf_url": "https://arxiv.org/pdf/2601.09708",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09708",
      "scraped_at": "2026-01-16T01:52:30.028323"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "paper_url": "https://huggingface.co/papers/2601.09136",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
      "abstract": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09136",
      "pdf_url": "https://arxiv.org/pdf/2601.09136",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09136",
      "scraped_at": "2026-01-16T01:52:32.025174"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
    "paper_url": "https://huggingface.co/papers/2601.09575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
      "abstract": "OpenVoxel provides training-free grouping and captioning of sparse voxels for open-vocabulary 3D scene understanding using VLMs/MLLMs and text search, enabling RES and OVS without CLIP embeddings.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09575",
      "pdf_url": "https://arxiv.org/pdf/2601.09575",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09575",
      "scraped_at": "2026-01-16T01:52:34.005779"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
    "paper_url": "https://huggingface.co/papers/2601.09028",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
      "abstract": "OpenDecoder is a novel framework that directly 'opens' the LLM to modify its decoding process within RAG scenarios by leveraging relevance signals from retrieved documents. Through a robustness-oriented training algorithm, the model learns to perform answer decoding guided by explicit indicators, rather than relying solely on prompt engineering or internal attention scores. This approach significantly enhances the system's controllability, accuracy, and robustness across various noisy environments. Take Away: Opening LLM rather than solely relying on prompt engineering is important to improve the system‚Äôs robustness, since we cannot expect LLMs‚Äô implicit identification to be always correct. The external indicators, e.g., relevance score, confidence feature, faithful factors, are useful to incorporate with LLMs‚Äô internal information processing mechanism, e.g., attention, for output decoding, where the key problem is to obtain and integrate these indicators into LLMs with a sophisticated training algorithm (during post-training). Experimental results show the robustness enhancement in different levels of noisy environments of our initial investigation of OpenDecoder. An ideal situation is that the LLM can understand to what extent to rely on external retrieved knowledge and internal parametric knowledge during answer decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09028",
      "pdf_url": "https://arxiv.org/pdf/2601.09028",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09028",
      "scraped_at": "2026-01-16T01:52:36.082937"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
    "paper_url": "https://huggingface.co/papers/2601.08605",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
      "abstract": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08605",
      "pdf_url": "https://arxiv.org/pdf/2601.08605",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08605",
      "scraped_at": "2026-01-16T01:52:37.987916"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "paper_url": "https://huggingface.co/papers/2601.09465",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
      "abstract": "EvoFSM presents a controllable self-evolution framework using a finite state machine to guide adaptive problem-solving, separating macroscopic flow and microscopic skills with critic-guided updates and reusable priors.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09465",
      "pdf_url": "https://arxiv.org/pdf/2601.09465",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09465",
      "scraped_at": "2026-01-16T01:52:39.900524"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
    "paper_url": "https://huggingface.co/papers/2601.03928",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
      "abstract": "TL;DR: High-res UI screenshots (2K/4K) force VLMs to process thousands of visual tokens. Inspired by human vision, which selects only instruction-relevant image patches, FocusUI teaches VLMs where to look in UI screenshots smartly üîç üìÑ Paper: arXiv:2601.03928 üåê Project Page: showlab.github.io/FocusUI üíª Code: github.com/showlab/FocusUI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03928",
      "pdf_url": "https://arxiv.org/pdf/2601.03928",
      "github_links": [
        "https://github.com/showlab/FocusUI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03928",
      "scraped_at": "2026-01-16T01:52:41.892931"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
    "paper_url": "https://huggingface.co/papers/2601.06596",
    "authors": [
      "Chi Zhang",
      "Jiawei Shao",
      "Jiangan Chen",
      "Yiliang Song",
      "Hongjun An"
    ],
    "stars": "0",
    "details": {
      "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
      "abstract": "This paper treats preference undermining as an experimental object, not a vibe. A clean factorial design isolates manipulation factors and quantifies when truth yields to compliance. Conclusion, stated politely: yes, a large model can be PUA-ed, and it may rationalize the outcome as alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06596",
      "pdf_url": "https://arxiv.org/pdf/2601.06596",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06596",
      "scraped_at": "2026-01-16T01:52:43.833419"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "TranslateGemma Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09012",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TranslateGemma Technical Report",
      "abstract": "TranslateGemma extends Gemma 3 with two-stage fine-tuning (supervised then RL) for multilingual translation, achieving strong WMT performance and multimodal capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09012",
      "pdf_url": "https://arxiv.org/pdf/2601.09012",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09012",
      "scraped_at": "2026-01-16T01:52:45.793509"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "paper_url": "https://huggingface.co/papers/2601.08955",
    "authors": [
      "Wenjie Li",
      "Beichen Guo",
      "Hanlin Wang",
      "Youwei Liu",
      "jwanglvy"
    ],
    "stars": "0",
    "details": {
      "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
      "abstract": "TL;DR: An agent learning framework via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step \"imagined\" trajectories. This imagination is conducted via a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08955",
      "pdf_url": "https://arxiv.org/pdf/2601.08955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08955",
      "scraped_at": "2026-01-16T01:52:47.807412"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
    "paper_url": "https://huggingface.co/papers/2601.09697",
    "authors": [
      "Ayush Tewari",
      "Joan Lasenby",
      "Jeffrey Hu",
      "Jieying Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
      "abstract": "Proposes SRENDER: generate sparse diffusion keyframes for static scenes and render 3D views to produce long videos fast and consistently.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09697",
      "pdf_url": "https://arxiv.org/pdf/2601.09697",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09697",
      "scraped_at": "2026-01-16T01:52:49.731039"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Geometric Stability: The Missing Axis of Representations",
    "paper_url": "https://huggingface.co/papers/2601.09173",
    "authors": [
      "pcr2120"
    ],
    "stars": "0",
    "details": {
      "title": "Geometric Stability: The Missing Axis of Representations",
      "abstract": "DeepSeek got it half right with their mHC paper: stability matters for scaling. But they only measure stability DURING training. What about the stability of what models LEARN? I built Shesha to measure this - a geometric stability metric with SOTA results across AI Safety , Constitutional AI , Model selection , and CRISPR perturbation analysis. The core insight: Most evals check external similarity (does output X match Y?). But imagine a massive library where someone reshuffled all the books. A content-based audit would say that nothing's wrong since the inventory is identical. But the library is useless since nothing can be found. That's the gap Shesha fills. The implications are broad with SOTA results across 4 domains : AI Safety - Shesha is the best canary in the coal mine . Shesha outperforms CKA and Procrustes on drift detection. Detects 2x more drift than CKA, triggers earlier 73% of the time, catches subtle LoRA shifts at 90% sensitivity (5% FPR) - with only 7% false alarms vs Procrustes' 44%. Constitutional AI - Shesha provides the best steering prediction . Constitutional AI needs models you can actually steer. Most metrics ask: \"Are classes separable?\" Wrong question! Shesha asks: \"Is that separation STABLE under perturbation?\" Tested on 35-69 embedding models across 3 experiments. Shesha outperforms Fisher discriminant, silhouette score, Procrustes, and anisotropy. The correlations with intervention success are rho=0.89-0.96, and the partial correlations after controlling for separability are rho=0.67-0.76. Stability ‚â† separability. Model selection - Shesha exposes what LogME misses . The DINO Paradox - best transfer scores, worst geometric stability. Tested 94 vision models on 6 datasets: DINOv2 ranked #1 in LogME on 4/6 datasets but last or near last in stability on 5/6. SOTA transfer incurs a \"geometric tax.\" CRISPR perturbations - Shesha serves as a new filter for target selection . CRISPR screens find hits by magnitude, but magnitude alone can't distinguish clean lineage drivers from promiscuous regulators. Shesha adds precision. Tested on 811 perturbations, Shesha showed uniformly positive magnitude-stability correlations ranging from rho=0.746 in high-variance screens to rho=0.963 in cleaner activation settings. Notably, in discordant cases, Shesha separates KLF1 (stable, specific) from CEBPA (strong but messy) purely from geometry. Additional validation in neuroscience: Geometric stability predicted neural-behavioral coupling (rho=0.18, p=0.005) in Neuropixels data. Centroid drift showed no relationship (rho=0.00). Stability ‚â† consistency. Try it yourself: PyPI: pip install shesha-geometry Tutorials: https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials Preprint: https://arxiv.org/abs/2601.09173 Code: https://github.com/prashantcraju/geometric-stability",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09173",
      "pdf_url": "https://arxiv.org/pdf/2601.09173",
      "github_links": [
        "https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials",
        "https://github.com/prashantcraju/geometric-stability"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09173",
      "scraped_at": "2026-01-16T01:52:51.691110"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "The AI Hippocampus: How Far are We From Human Memory?",
    "paper_url": "https://huggingface.co/papers/2601.09113",
    "authors": [
      "Tong Wu",
      "Yuxuan Wang",
      "Yipeng Kang",
      "Jiaqi Li",
      "Zixia Jia"
    ],
    "stars": "0",
    "details": {
      "title": "The AI Hippocampus: How Far are We From Human Memory?",
      "abstract": "Survey of memory in LLMs and multimodal models, detailing implicit, explicit, and agentic memory, architectures, benchmarks, and challenges in persistence, alignment, and cross-modal retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09113",
      "pdf_url": "https://arxiv.org/pdf/2601.09113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09113",
      "scraped_at": "2026-01-16T01:52:53.593035"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.01075",
    "authors": [
      "Thomas Anderson Keller",
      "Yilun Du",
      "Fangneng Zhan",
      "Benhao Huang",
      "Hansen Jin Lillemark"
    ],
    "stars": "5",
    "details": {
      "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01075",
      "pdf_url": "https://arxiv.org/pdf/2601.01075",
      "github_links": [
        "https://github.com/hlillemark/flowm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01075",
      "scraped_at": "2026-01-16T01:52:55.619837"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
    "paper_url": "https://huggingface.co/papers/2601.09609",
    "authors": [
      "Ruihua Song",
      "Yi Zhao",
      "Wei Bi",
      "Yahui Liu",
      "Qian Cao"
    ],
    "stars": "0",
    "details": {
      "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
      "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09609",
      "pdf_url": "https://arxiv.org/pdf/2601.09609",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09609",
      "scraped_at": "2026-01-16T01:52:57.577292"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09536",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
      "abstract": "This paper proposes a unified generative multimodal reasoning paradigm, using a two-stage SFT+RL framework with perception alignment loss and perception reward, and explores bootstrapping step-wise visualizations from text-only reasoning data when multimodal annotation availability is extremely limited.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09536",
      "pdf_url": "https://arxiv.org/pdf/2601.09536",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09536",
      "scraped_at": "2026-01-16T01:52:59.576152"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2601.07287",
    "authors": [
      "Xiao Yang",
      "Kaipeng Zhang",
      "Shenghai Yuan",
      "Yuanyang Yin",
      "yfdeng10"
    ],
    "stars": "0",
    "details": {
      "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision (2025) Plan-X: Instruct Video Generation via Semantic Planning (2025) AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation (2025) Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models (2025) InstanceV: Instance-Level Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07287",
      "pdf_url": "https://arxiv.org/pdf/2601.07287",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07287",
      "scraped_at": "2026-01-16T01:53:02.313236"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
    "paper_url": "https://huggingface.co/papers/2601.06794",
    "authors": [
      "Yixia Li",
      "Xingchen Zeng",
      "Yulan Hu",
      "Lingjie Jiang",
      "Zhicong Li"
    ],
    "stars": "0",
    "details": {
      "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
      "abstract": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06794",
      "pdf_url": "https://arxiv.org/pdf/2601.06794",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06794",
      "scraped_at": "2026-01-16T01:53:04.140516"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.04809",
    "authors": [
      "Yixin Cao",
      "Xinrun Wang",
      "Zhongyuan Peng",
      "Changyi Xiao",
      "SII-Molu"
    ],
    "stars": "6",
    "details": {
      "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
      "abstract": "Scalable Environment Synthesis Given a programming problem (statement + reference solution), SCALER synthesizes a reasoning environment with: Verifiability: deterministic oracle / unit tests provide correctness signals. Difficulty control: explicit scale parameters discretized into difficulty levels. Unbounded instance generation: randomized testcase generation yields unlimited training instances. Adaptive Multi-Environment RL SCALER sustains learning signals at two levels: In-environment difficulty controller: keeps sampling near a target success regime. Environment curation: maintains an active set and replaces saturated/uninformative environments to preserve diversity and long-horizon improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04809",
      "pdf_url": "https://arxiv.org/pdf/2601.04809",
      "github_links": [
        "https://github.com/molumolua/SCALER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04809",
      "scraped_at": "2026-01-16T01:53:05.993611"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "paper_url": "https://huggingface.co/papers/2601.09282",
    "authors": [
      "Jolanta Mizeria-Pietraszko",
      "lsliwko"
    ],
    "stars": "0",
    "details": {
      "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
      "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09282",
      "pdf_url": "https://arxiv.org/pdf/2601.09282",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09282",
      "scraped_at": "2026-01-16T01:53:07.875759"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
    "paper_url": "https://huggingface.co/papers/2601.08472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08472",
      "pdf_url": "https://arxiv.org/pdf/2601.08472",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08472",
      "scraped_at": "2026-01-16T01:53:09.794070"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
    "paper_url": "https://huggingface.co/papers/2601.04469",
    "authors": [
      "Aleksey Komissarov",
      "Ekaterina Chelombitko",
      "Iaroslav Chelombitko"
    ],
    "stars": "0",
    "details": {
      "title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
      "abstract": "The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04469",
      "pdf_url": "https://arxiv.org/pdf/2601.04469",
      "github_links": [
        "https://github.com/AragonerUA/SampoNLP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04469",
      "scraped_at": "2026-01-16T01:53:11.668045"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "125",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-17T01:46:10.410290"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "152",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-17T01:46:12.281061"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-17T01:46:14.129704"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-17T01:46:15.994820"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "22",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-17T01:46:17.812994"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "34",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-17T01:46:19.609904"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "12",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-17T01:46:21.464351"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "332",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-17T01:46:23.272686"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-17T01:46:25.131294"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-17T01:46:26.958024"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-17T01:46:28.768874"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-17T01:46:30.627023"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-17T01:46:32.424107"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "20",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-17T01:46:34.429955"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-17T01:46:36.227451"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-17T01:46:38.096548"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-17T01:46:39.969567"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-17T01:46:41.766508"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "75",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-17T01:46:43.586501"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-17T01:46:45.366327"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "0",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-17T01:46:47.168503"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-17T01:46:48.943667"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-17T01:46:50.737198"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-17T01:46:52.610283"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "4",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-17T01:46:54.410994"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-17T01:46:56.176236"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-17T01:46:57.933744"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-17T01:46:59.691457"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-17T01:47:01.464358"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-17T01:47:03.230799"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "45",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-17T01:47:04.980548"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-17T01:47:06.760677"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-17T01:47:08.580845"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-17T01:47:10.393786"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-17T01:47:12.142262"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-17T01:47:13.912097"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-17T01:47:15.696403"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "20",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-17T01:47:17.436915"
    },
    "scraped_date": "2026-01-17"
  }
]