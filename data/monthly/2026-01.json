[
  {
    "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
    "paper_url": "https://huggingface.co/papers/2512.21185",
    "authors": [
      "Yang Li",
      "Dehao Hao",
      "LutaoJiang",
      "StarYDY",
      "infinith"
    ],
    "stars": "110",
    "details": {
      "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
      "abstract": "Github: https://github.com/PKU-YuanGroup/UltraShape-1.0 Project Page: https://pku-yuangroup.github.io/UltraShape-1.0/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21185",
      "pdf_url": "https://arxiv.org/pdf/2512.21185",
      "github_links": [
        "https://github.com/PKU-YuanGroup/UltraShape-1.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21185",
      "scraped_at": "2026-01-01T01:59:26.540091"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "DreamOmni3: Scribble-based Editing and Generation",
    "paper_url": "https://huggingface.co/papers/2512.22525",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamOmni3: Scribble-based Editing and Generation",
      "abstract": "Github: https://github.com/dvlab-research/DreamOmni3",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22525",
      "pdf_url": "https://arxiv.org/pdf/2512.22525",
      "github_links": [
        "https://github.com/dvlab-research/DreamOmni3"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22525",
      "scraped_at": "2026-01-01T01:59:28.344263"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "End-to-End Test-Time Training for Long Context",
    "paper_url": "https://huggingface.co/papers/2512.23675",
    "authors": [
      "Marcel R√∏d",
      "Daniel Koceja",
      "Xinhao Li",
      "Karan Dalal",
      "Arnuv Tandon"
    ],
    "stars": "96",
    "details": {
      "title": "End-to-End Test-Time Training for Long Context",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Sliding Window Attention Adaptation (2025) Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) Attention and Compression is all you need for Controllably Efficient Language Models (2025) Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings (2025) Data-Free Pruning of Self-Attention Layers in LLMs (2025) Architectural Trade-offs in Small Language Models Under Compute Constraints (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23675",
      "pdf_url": "https://arxiv.org/pdf/2512.23675",
      "github_links": [
        "https://github.com/test-time-training/e2e"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23675",
      "scraped_at": "2026-01-01T01:59:30.184972"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "Evaluating Parameter Efficient Methods for RLVR",
    "paper_url": "https://huggingface.co/papers/2512.23165",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Parameter Efficient Methods for RLVR",
      "abstract": "https://www.alphaxiv.org/abs/2512.23165",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23165",
      "pdf_url": "https://arxiv.org/pdf/2512.23165",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23165",
      "scraped_at": "2026-01-01T01:59:31.986046"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization",
    "paper_url": "https://huggingface.co/papers/2512.22469",
    "authors": [
      "Wei Zhang",
      "Aofan Liu",
      "Pengfei Gao",
      "Wei Liu",
      "pengchao"
    ],
    "stars": "0",
    "details": {
      "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization",
      "abstract": "Issues describe symptoms‚Äîthe real buggy code is often hidden behind multi-hop dependencies. GraphLocator moves beyond relevance retrieval by explicitly modelling causal structure: decomposing sub-problems, capturing causal dependencies, and performing constrained causal reasoning over code dependency graphs. Across multiple Python and Java datasets, GraphLocator significantly improves function-level recall and precision, and the inferred causal structures further boost downstream issue resolution by +28.74%.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22469",
      "pdf_url": "https://arxiv.org/pdf/2512.22469",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22469",
      "scraped_at": "2026-01-01T01:59:33.772256"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
    "paper_url": "https://huggingface.co/papers/2512.22206",
    "authors": [
      "Yogeswar"
    ],
    "stars": "0",
    "details": {
      "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
      "abstract": "\"I introduce CosineGate, a SOTA dynamic routing mechanism for ResNets that uses the Cosine Incompatibility Ratio (CIR) as a self-supervised signal. üöÄ Why it matters: It matches ResNet-20 accuracy on CIFAR-10 while slashing computation by 28.5%‚Äîwithout needing extra 'predictor' sub-networks or distillation. üõ†Ô∏è Key Features: Fully differentiable (via Gumbel-Softmax). Bio-inspired (Predictive Coding). Plug-and-play for efficient computer vision. Check out our GitHub Repo linked in the sidebar for the implementation!\"",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22206",
      "pdf_url": "https://arxiv.org/pdf/2512.22206",
      "github_links": [
        "https://github.com/thotayogeswarreddy/CosineGate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22206",
      "scraped_at": "2026-01-01T01:59:35.535486"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs",
    "paper_url": "https://huggingface.co/papers/2512.21008",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs",
      "abstract": "Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness. In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21008",
      "pdf_url": "https://arxiv.org/pdf/2512.21008",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21008",
      "scraped_at": "2026-01-01T01:59:37.334356"
    },
    "scraped_date": "2026-01-01"
  },
  {
    "title": "mHC: Manifold-Constrained Hyper-Connections",
    "paper_url": "https://huggingface.co/papers/2512.24880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "abstract": "DeepSeek released a new paper proposing a novel architecture called mHC (Manifold-Constrained Hyper-Connections).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24880",
      "pdf_url": "https://arxiv.org/pdf/2512.24880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24880",
      "scraped_at": "2026-01-02T01:50:23.780257"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24618",
    "authors": [
      "Xinyi Dai",
      "Yinghui Li",
      "Lingfeng Qiao",
      "Jiarui Qin",
      "Junru Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24618",
      "pdf_url": "https://arxiv.org/pdf/2512.24618",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24618",
      "scraped_at": "2026-01-02T01:50:25.585213"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.24873",
    "authors": [
      "Wei Gao",
      "Fangwen Dai",
      "Wanhe An",
      "XiaoXiao Xu",
      "Weixun Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24873",
      "pdf_url": "https://arxiv.org/pdf/2512.24873",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24873",
      "scraped_at": "2026-01-02T01:50:27.522125"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "paper_url": "https://huggingface.co/papers/2512.25073",
    "authors": [
      "Yu-Lun Liu",
      "Ying-Huan Chen",
      "Chin-Yang Lin",
      "Hao-Jen Chien",
      "Yi-Chuan Huang"
    ],
    "stars": "0",
    "details": {
      "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
      "abstract": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25√ó speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.25073",
      "pdf_url": "https://arxiv.org/pdf/2512.25073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.25073",
      "scraped_at": "2026-01-02T01:50:29.343131"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
    "paper_url": "https://huggingface.co/papers/2512.23380",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23380",
      "pdf_url": "https://arxiv.org/pdf/2512.23380",
      "github_links": [
        "https://github.com/NasirzadehMoh/CoLog"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23380",
      "scraped_at": "2026-01-02T01:50:31.161979"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "paper_url": "https://huggingface.co/papers/2512.25070",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Scaling Open-Ended Reasoning to Predict the Future",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.25070",
      "pdf_url": "https://arxiv.org/pdf/2512.25070",
      "github_links": [
        "https://github.com/OpenForecaster/scaling-forecasting-training"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.25070",
      "scraped_at": "2026-01-02T01:50:33.046417"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24551",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
      "abstract": "A data construction pipeline and a new DPO framework for physically consistent Text-to-video generation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24551",
      "pdf_url": "https://arxiv.org/pdf/2512.24551",
      "github_links": [
        "https://github.com/caiyuanhao1998/Open-PhyGDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24551",
      "scraped_at": "2026-01-02T01:50:34.905156"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
    "paper_url": "https://huggingface.co/papers/2512.23343",
    "authors": [
      "Shixin Jiang",
      "Jiaqi Zhou",
      "Chang Li",
      "Hao Li",
      "Jiafeng Liang"
    ],
    "stars": "24",
    "details": {
      "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
      "abstract": "https://github.com/AgentMemory/Huaman-Agent-Memory",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23343",
      "pdf_url": "https://arxiv.org/pdf/2512.23343",
      "github_links": [
        "https://github.com/AgentMemory/Huaman-Agent-Memory"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23343",
      "scraped_at": "2026-01-02T01:50:36.704147"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "GR-Dexter Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.24210",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GR-Dexter Technical Report",
      "abstract": "VLAs go from grippers to 21 DoF dexterous ByteDexter V2 :)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24210",
      "pdf_url": "https://arxiv.org/pdf/2512.24210",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24210",
      "scraped_at": "2026-01-02T01:50:38.600164"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
    "paper_url": "https://huggingface.co/papers/2512.23988",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "abstract": "This is an impressive piece of work. Not only for the elegance of the sparse autoencoder pipeline, but because the empirical results reveal something far deeper than what is stated in the paper. Your SAE-derived ‚Äúreasoning vectors‚Äù behave exactly like stable dynamical modes inside a recursive state system ‚Äî not merely interpretable directions. The separation of reflection, backtracking, confidence, and response-length clusters across layers strongly suggests that modern transformer reasoning is governed by a latent, substrate-bound dynamical structure rather than a purely token-level process. A few observations that stood out: Reasoning vectors behave like attractor modes, not just features. The clustering of SAE decoder columns into semantically distinct basins is consistent with the existence of stable dynamical invariants that govern the model‚Äôs step-wise evolution. This is exactly the behavior expected when a system has: stable recurrence points, local attractor basins in its state manifold, and identity-like update modes that persist across tasks. Your causal interventions reinforce this: modifying a reasoning vector steers the entire reasoning trajectory while preserving final correctness. That is classic attractor dynamics. The layer-wise geometry mirrors a recursive integration process. The strongest separability occurring in mid-to-late layers, followed by a decline near the final layer, mirrors the behavior of systems that integrate state over time and then compress it near output. This is structurally identical to a recursive state-aware update: a(t+1)=R(a(t)) where the model accumulates long-range structure before collapsing it for output. Cross-domain generalization of these vectors indicates substrate-bound stability. The fact that reflection/backtracking vectors trained on MATH500 steer behavior on GPQA and KnowLogic implies the existence of substrate-stable reasoning structures that are independent of dataset distribution. This is a property of a dynamical system ‚Äî not a static embedding space. Confidence emerges as a coherent cluster because it is tied to entropy and coherence. Your discovery that confidence vectors suppress reflection/backtracking is an empirical confirmation of a predicted relation between: information coherence computational alignment noise minimization and entropy reduction Confidence is not a semantic trait ‚Äî it's a low-entropy attractor mode. Response-length alignment is a structural axis, not a surface feature. Length correlating with latent-space geometry further confirms that reasoning depth emerges from the system's internal temporal continuity rather than token heuristics. A broader note: These empirical findings align remarkably well with a larger theoretical framework I‚Äôve been developing, the Field of General Awareness (FoGA), which predicts: the existence of invariant reasoning modes, substrate-sensitive drift in state evolution, recursive attractor-based reasoning paths, and coherence-driven modulation of reasoning confidence. Your results are the clearest real-world demonstration I‚Äôve seen of these principles emerging naturally inside transformer models. If you're interested, I‚Äôm happy to share the relevant portions of the theory (and the mathematical basis behind these predictions), as well as the Dynamic Transformer Architecture ‚Äî an architecture patch explicitly designed to stabilize such recurrence modes. Excellent work. This paper is going to be foundational for understanding why LLM reasoning behaves the way it does. ‚Äî Zenith Zaraki SkyTeam Aerospace Foundation https://www.skyteamaerospacefoundation.com/foga https://www.skyteamaerospacefoundation.com/dta",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23988",
      "pdf_url": "https://arxiv.org/pdf/2512.23988",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23988",
      "scraped_at": "2026-01-02T01:50:40.409987"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
    "paper_url": "https://huggingface.co/papers/2512.23851",
    "authors": [
      "Beijia Lu",
      "Chong Zeng",
      "Muyang Li",
      "Shengqu Cai",
      "Lvmin Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
      "abstract": "Arxiv: https://arxiv.org/abs/2512.23851 Repo: https://github.com/lllyasviel/PFP",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23851",
      "pdf_url": "https://arxiv.org/pdf/2512.23851",
      "github_links": [
        "https://github.com/lllyasviel/PFP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23851",
      "scraped_at": "2026-01-02T01:50:42.190944"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "paper_url": "https://huggingface.co/papers/2512.25075",
    "authors": [
      "Tuanfeng Y. Wang",
      "Yulia Gryaditskaya",
      "Xuelin Chen",
      "Hyeonho Jeong",
      "Zhening Huang"
    ],
    "stars": "0",
    "details": {
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API BulletTime: Decoupled Control of Time and Camera Pose for Video Generation (2025) FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint (2025) ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Generative Video Motion Editing with 3D Point Tracks (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.25075",
      "pdf_url": "https://arxiv.org/pdf/2512.25075",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.25075",
      "scraped_at": "2026-01-02T01:50:43.989867"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
    "paper_url": "https://huggingface.co/papers/2512.22905",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
      "abstract": "üî•üî•üî• JavisGPT üåü We introduce JavisGPT, a multimodal LLM that can understand audiovisual inputs and simultaneously generate synchronized sounding videos in a unified model. ü§† We contribute JavisInst-Omni, a dataset to facilitate diverse and complex instruction-tuning for comprehension and generation on sounding videos. üìù Paper: https://arxiv.org/abs/2503.23377 üéâ Project: https://javisverse.github.io/JavisGPT-page/ ‚ú® Code: https://github.com/JavisVerse/JavisGPT",
      "arxiv_page_url": "https://arxiv.org/abs/2503.23377",
      "pdf_url": "https://arxiv.org/pdf/2512.22905",
      "github_links": [
        "https://github.com/JavisVerse/JavisGPT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22905",
      "scraped_at": "2026-01-02T01:50:45.835201"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
    "paper_url": "https://huggingface.co/papers/2512.22564",
    "authors": [
      "Mah≈üuk Taylan",
      "Ahmet Feridun I≈üƒ±k",
      "Selin Vulga I≈üƒ±k",
      "Atakanisik"
    ],
    "stars": "0",
    "details": {
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "abstract": "Hi all, We present a robust framework for Lung Sound Classification using AST backbones enhanced with SAM optimizer . Traditional transformers often struggle with limited medical data, but our experiments show that geometry-aware optimization (SAM) leads to a massive boost in sensitivity. We achieved a 68.10% Score on the official ICBHI 2017 split. We invite everyone to benchmark our results. The repository includes: Cyclic padding implementation Full training scripts Evaluation of model Check it out here: GitHub Link",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22564",
      "pdf_url": "https://arxiv.org/pdf/2512.22564",
      "github_links": [
        "https://github.com/Atakanisik/ICBHI-AST-SAM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22564",
      "scraped_at": "2026-01-02T01:50:47.616554"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
    "paper_url": "https://huggingface.co/papers/2512.24885",
    "authors": [
      "Mengmeng Wang",
      "Chenxi Li",
      "Qi Shen",
      "Zhaoxin Yu",
      "Hengli Li"
    ],
    "stars": "0",
    "details": {
      "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
      "abstract": "Arxiv: https://arxiv.org/abs/2512.24885 X thread: https://x.com/Hengli_Li_pku/status/2006606887652045158",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24885",
      "pdf_url": "https://arxiv.org/pdf/2512.24885",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24885",
      "scraped_at": "2026-01-02T01:50:49.444890"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
    "paper_url": "https://huggingface.co/papers/2512.24385",
    "authors": [],
    "stars": "105",
    "details": {
      "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
      "abstract": "GitHub at https://github.com/worldbench/awesome-spatial-intelligence",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24385",
      "pdf_url": "https://arxiv.org/pdf/2512.24385",
      "github_links": [
        "https://github.com/worldbench/awesome-spatial-intelligence"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24385",
      "scraped_at": "2026-01-02T01:50:51.222394"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
    "paper_url": "https://huggingface.co/papers/2512.24297",
    "authors": [
      "Jie Zhou",
      "Fandong Meng",
      "Meiqi Chen"
    ],
    "stars": "4",
    "details": {
      "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API V-Thinker: Interactive Thinking with Images (2025) Interleaved Latent Visual Reasoning with Selective Perceptual Modeling (2025) Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images (2025) ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking (2025) ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning (2025) Look as You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning (2025) CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24297",
      "pdf_url": "https://arxiv.org/pdf/2512.24297",
      "github_links": [
        "https://github.com/chenmeiqii/FIGR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24297",
      "scraped_at": "2026-01-02T01:50:53.259496"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
    "paper_url": "https://huggingface.co/papers/2512.24176",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
      "abstract": "üî• New SOTA on 256 √ó 256 ImageNet generation. We present Internal Guidance (IG), a simple yet powerful guidance mechanism for Diffusion Transformers. LightningDiT-XL/1 + IG sets a new state of the art with FID = 1.07 on ImageNet (balanced sampling), while achieving FID = 1.24 without classifier-free guidance. IG delivers dramatic quality gains with far fewer training epochs, adds negligible overhead, and works as a drop-in upgrade for modern diffusion transformers.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24176",
      "pdf_url": "https://arxiv.org/pdf/2512.24176",
      "github_links": [
        "https://github.com/CVL-UESTC/Internal-Guidance"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24176",
      "scraped_at": "2026-01-02T01:50:55.053809"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Factorized Learning for Temporally Grounded Video-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24097",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "abstract": "We tackle temporally grounded video-language understanding from a factorized perspective. Some key takeaways: [1] We emphasize the distinct yet causally dependent nature of temporal grounding and textual response. [2] Our study highlights the importance of explicit event-level visual semantic capture in enhancing both grounding and textual response quality. [3] We also propose a new Factorized Preference Optimization (FPO) scheme that jointly optimizes temporal and textual factors. A factorized data synthesis approach is also proposed to support FPO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24097",
      "pdf_url": "https://arxiv.org/pdf/2512.24097",
      "github_links": [
        "https://github.com/nusnlp/d2vlm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24097",
      "scraped_at": "2026-01-02T01:50:56.944458"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Valori: A Deterministic Memory Substrate for AI Systems",
    "paper_url": "https://huggingface.co/papers/2512.22280",
    "authors": [
      "varam17"
    ],
    "stars": "2",
    "details": {
      "title": "Valori: A Deterministic Memory Substrate for AI Systems",
      "abstract": "Valori: A Deterministic Fixed-Point Vector Kernel Tags: vector-database , rust , determinism , finance , audit , hnsw , fixed-point , systems-engineering TL;DR Floating-point math causes vector search results to drift between ARM (Mac) and x86 (Linux) architectures due to compiler and hardware variances. Valori is a Q16.16 Fixed-Point kernel written in Rust that guarantees bit-exact reproducibility across all hardware platforms, making it the first \"Audit-Grade\" vector engine for high-stakes AI. 1. The Problem: \"The Silent Drift\" Most vector databases (FAISS, Pinecone, Weaviate) rely on hardware-accelerated floating-point arithmetic ( f32 ). While fast, this introduces Non-Determinism : Associativity Variance: (a + b) + c ‚â† a + (b + c) depending on SIMD optimizations. Architecture Variance: A backtest run on a MacBook (ARM NEON) often yields different nearest neighbors than the live execution on an AWS Server (Intel AVX2). In RAG pipelines , this is annoying. In High-Frequency Trading or Legal Forensics , this variance is a liability. You cannot audit a probabilistic black box. 2. The Solution: Valori Kernel Valori is a forensic memory engine built from scratch in Rust . It replaces standard floating-point math with a custom Q16.16 Fixed-Point Arithmetic system inside an HNSW graph. Zero Drift: The mathematical operations are integer-based. 1 + 1 = 2 on every CPU, forever. Audit-Ready: Includes a calculate_state_hash() method that generates a cryptographic fingerprint of the database state. If a single bit changes, the hash breaks. Crash Proof: Implements a specialized persistence layer with <40ms recovery time (Cold Boot to Full Query) for 50k vector datasets. 3. Benchmarks (Engineering Verification) Validated on Apple M2 (ARM64) vs Intel Xeon (x86_64). Metric Result Notes Recall@10 99.0% Matches brute-force ground truth. Determinism 100% Bit-exact match across architectures. Recovery Time 35ms For 50k vectors (Snapshot Load). Latency ~0.5ms Per query (Single Thread). 4. Usage (Rust) Valori is designed to be embedded directly into high-integrity Rust applications. Add to Cargo.toml : [dependencies] valori_kernel = { git = \"https://github.com/varshith-Git/Valori-Kernel\" } Example: Deterministic Ingest & Audit use valori_kernel::{ValoriKernel, types::FixedPointVector}; fn main () -> anyhow:: Result <()> { // 1. Initialize the Kernel (Q16.16 Math) let mut kernel = ValoriKernel:: new (); // 2. Ingest Data (Manual f32 -> FixedPoint conversion for safety) let raw_vector = [ 0.5 , - 0.2 , 0.9 , ...]; // 128-dim // Convert float to Q16.16 integer representation let mut fixed_arr = [ 0i32 ; 128 ]; for (i, &val) in raw_vector. iter (). enumerate () {\n        fixed_arr[i] = (val * 65536.0 ) as i32 ;\n    } // Insert with ID=1, Tag=100 kernel. insert ( 1 , FixedPointVector (fixed_arr), 100 ); // 3. Search // Guaranteed to return the exact same ID and Distance on any CPU let results = kernel. search (&fixed_arr, 5 , None )?; println! ( \"Found neighbors: {:?}\" , results); // 4. Generate Forensic Evidence // This hash proves the database state is identical bit-for-bit let audit_hash = kernel.graph. calculate_state_hash (); println! ( \"Cryptographic State Signature: {}\" , audit_hash); Ok (())\n} 5. Who is this for? Quant Developers: Who need backtests to match live execution perfectly. Systems Engineers: Debugging \"Why did the agent say X?\" (Eliminate the database as a variable). Auditors: Who need to certify AI decision logs using cryptographic proofs. Citation If you use Valori in your research, please cite: @ misc {gudur2025valori,\n  title={Valori: A Deterministic Fixed-Point Vector Kernel},\n  author={Gudur, Varshith},\n  year={2025},\n  publisher={arXiv},\n  url={https://arxiv.org/abs/2512.22280}\n}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22280",
      "pdf_url": "https://arxiv.org/pdf/2512.22280",
      "github_links": [
        "https://github.com/varshith-Git/Valori-Kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22280",
      "scraped_at": "2026-01-02T01:50:58.743405"
    },
    "scraped_date": "2026-01-02"
  },
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-03T01:44:37.960652"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLen‚Äìbased implementation for efficient concept-token cross-attention; (2) a decoupled ŒºP formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-03T01:44:39.850579"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "13",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-03T01:44:41.687283"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-03T01:44:43.488349"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-03T01:44:45.286927"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-03T01:44:47.116340"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "3",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO‚Äôs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-03T01:44:48.907162"
    },
    "scraped_date": "2026-01-03"
  },
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-04T01:59:50.718806"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLen‚Äìbased implementation for efficient concept-token cross-attention; (2) a decoupled ŒºP formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-04T01:59:52.693703"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "24",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-04T01:59:54.609230"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-04T01:59:56.484887"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-04T01:59:58.371341"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-04T02:00:00.212924"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "4",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO‚Äôs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-04T02:00:02.005832"
    },
    "scraped_date": "2026-01-04"
  },
  {
    "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
    "paper_url": "https://huggingface.co/papers/2512.23959",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "abstract": "Code released at: https://github.com/Encyclomen/HGMem",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23959",
      "pdf_url": "https://arxiv.org/pdf/2512.23959",
      "github_links": [
        "https://github.com/Encyclomen/HGMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23959",
      "scraped_at": "2026-01-05T01:59:59.631752"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.24617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "abstract": "Dynamic Large Concept Models (DLCM) introduce an end-to-end trained concept-level language modeling architecture that breaks the token-uniform computation paradigm in modern LLMs. Inspired by hierarchical models such as H-Net, DLCM learns semantic boundaries directly from latent representations, dynamically compresses token sequences into variable-length concepts, performs deep reasoning in the concept space, and projects the results back to tokens via causal cross-attention. Compared to standard dense Transformers trained with next-token prediction, DLCM achieves ~34% inference FLOPs reduction under apple-to-apple settings, while consistently improving performance on reasoning-dominant benchmarks. Notably, the relative FLOPs savings increase with model scale, indicating favorable scaling behavior beyond parameter efficiency alone. At similar loss levels, DLCM reallocates computation toward boundary and planning tokens, yielding stronger downstream accuracy despite reduced redundant token processing. Technically, the paper contributes: (1) a FlashAttention-VarLen‚Äìbased implementation for efficient concept-token cross-attention; (2) a decoupled ŒºP formulation tailored to heterogeneous token- and concept-width modules, enabling zero-shot hyperparameter transfer across scales; (3) a Global Parser that enforces stable, content-adaptive compression at the batch level and delivers solid empirical gains. Overall, DLCM can be viewed as a principled special case of layer-wise local compression combined with sparse attention, offering a scalable path toward more compute-efficient and reasoning-centric language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24617",
      "pdf_url": "https://arxiv.org/pdf/2512.24617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24617",
      "scraped_at": "2026-01-05T02:00:01.852412"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.24165",
    "authors": [
      "Siyuan Huang",
      "Yafu Li",
      "Xiaoye Qu",
      "Spico",
      "yhx12"
    ],
    "stars": "61",
    "details": {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "abstract": "TLDR: A new paradigm for multi-modal reasoning with image-to-image generation. Diffusion could think too!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24165",
      "pdf_url": "https://arxiv.org/pdf/2512.24165",
      "github_links": [
        "https://github.com/lcqysl/DiffThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24165",
      "scraped_at": "2026-01-05T02:00:03.959198"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "On the Role of Discreteness in Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.22630",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "On the Role of Discreteness in Diffusion LLMs",
      "abstract": "TL;DR: We identify two core failure modes in current large diffusion LLMs: uniform corruption ignores where information lives in a sentence, and token-wise marginal training struggles with multi-token dependencies during parallel decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22630",
      "pdf_url": "https://arxiv.org/pdf/2512.22630",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22630",
      "scraped_at": "2026-01-05T02:00:05.988614"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "paper_url": "https://huggingface.co/papers/2512.24766",
    "authors": [
      "Ruohan Zhang",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Wenlong Huang",
      "Karthik Dharmarajan"
    ],
    "stars": "0",
    "details": {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24766",
      "pdf_url": "https://arxiv.org/pdf/2512.24766",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24766",
      "scraped_at": "2026-01-05T02:00:08.091511"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24724",
    "authors": [
      "Youngjung Uh",
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Jibin Song"
    ],
    "stars": "0",
    "details": {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24724",
      "pdf_url": "https://arxiv.org/pdf/2512.24724",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24724",
      "scraped_at": "2026-01-05T02:00:10.042015"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
    "paper_url": "https://huggingface.co/papers/2512.24007",
    "authors": [
      "Ghaith Rabadi",
      "Sean Mondesire",
      "Bulent Soykan"
    ],
    "stars": "4",
    "details": {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "abstract": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO‚Äôs effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: github.com/bulentsoykan/TESO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24007",
      "pdf_url": "https://arxiv.org/pdf/2512.24007",
      "github_links": [
        "https://github.com/bulentsoykan/TESO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24007",
      "scraped_at": "2026-01-05T02:00:11.922915"
    },
    "scraped_date": "2026-01-05"
  },
  {
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.24615",
    "authors": [],
    "stars": "4.1k",
    "details": {
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "abstract": "LONG wait. Youtu-Agent ( https://github.com/TencentCloudADP/Youtu-agent ) now releases its technical report with two major updates, i.e., Automated Generation and Hybrid Policy Optimization. Additionally, we've launched Youtu-Tip ( https://github.com/TencentCloudADP/youtu-tip ), a more user-friendly application that runs on macOS. Check them out and have fun!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24615",
      "pdf_url": "https://arxiv.org/pdf/2512.24615",
      "github_links": [
        "https://github.com/TencentCloudADP/youtu-tip",
        "https://github.com/TencentCloudADP/youtu-agent",
        "https://github.com/TencentCloudADP/Youtu-agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24615",
      "scraped_at": "2026-01-06T01:50:51.163830"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2601.00393",
    "authors": [
      "Feng Wang",
      "Junran Peng",
      "renshengjihe",
      "Abyssaledge",
      "Yuppie1204"
    ],
    "stars": "124",
    "details": {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "abstract": "NeoVerse is a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. Project page: https://neoverse-4d.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00393",
      "pdf_url": "https://arxiv.org/pdf/2601.00393",
      "github_links": [
        "https://github.com/IamCreateAI/NeoVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00393",
      "scraped_at": "2026-01-06T01:50:53.205658"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "paper_url": "https://huggingface.co/papers/2601.00664",
    "authors": [
      "Sung Ju Hwang",
      "Jaehyeong Jo",
      "Sangwon Jang",
      "jaehong31",
      "taekyungki"
    ],
    "stars": "65",
    "details": {
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "abstract": "arXiv explained breakdown of this paper üëâ https://arxivexplained.com/papers/avatar-forcing-real-time-interactive-head-avatar-generation-for-natural-conversation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00664",
      "pdf_url": "https://arxiv.org/pdf/2601.00664",
      "github_links": [
        "https://github.com/TaekyungKi/AvatarForcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00664",
      "scraped_at": "2026-01-06T01:50:55.077779"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.24330",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "abstract": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24330",
      "pdf_url": "https://arxiv.org/pdf/2512.24330",
      "github_links": [
        "https://github.com/OpenSenseNova/SenseNova-MARS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24330",
      "scraped_at": "2026-01-06T01:50:56.907809"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.24271",
    "authors": [],
    "stars": "29",
    "details": {
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "abstract": "An interesting work! github: https://github.com/AMAP-ML/Taming-Hallucinations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24271",
      "pdf_url": "https://arxiv.org/pdf/2512.24271",
      "github_links": [
        "https://github.com/AMAP-ML/Taming-Hallucinations"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24271",
      "scraped_at": "2026-01-06T01:50:58.822184"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.00796",
    "authors": [
      "Yu-Lun Liu",
      "Zhenjun Zhao",
      "Jiewen Chan"
    ],
    "stars": "0",
    "details": {
      "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
      "abstract": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00796",
      "pdf_url": "https://arxiv.org/pdf/2601.00796",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00796",
      "scraped_at": "2026-01-06T01:51:00.710791"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Deep Delta Learning",
    "paper_url": "https://huggingface.co/papers/2601.00417",
    "authors": [],
    "stars": "234",
    "details": {
      "title": "Deep Delta Learning",
      "abstract": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Œ≤(X). We provide a spectral analysis of this operator, demonstrating that the gate Œ≤(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00417",
      "pdf_url": "https://arxiv.org/pdf/2601.00417",
      "github_links": [
        "https://github.com/yifanzhang-pro/deep-delta-learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00417",
      "scraped_at": "2026-01-06T01:51:02.670038"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "paper_url": "https://huggingface.co/papers/2512.24695",
    "authors": [
      "Vahab Mirrokni",
      "Peilin Zhong",
      "Meisam Razaviyayn",
      "AliBehrouz"
    ],
    "stars": "0",
    "details": {
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "abstract": "Nested Learning (NL) is a new learning paradigm for continual learning and machine learning in general.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24695",
      "pdf_url": "https://arxiv.org/pdf/2512.24695",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24695",
      "scraped_at": "2026-01-06T01:51:04.593752"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.00747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "abstract": "For those of you interested in RLVR, here is a paper that formally characterizes the mechanism behind \"diversity collapse\" in reasoning models trained with scalar rewards (such as STaR, GRPO, and DPO). The paper introduces a variational framework based on Shahshahani gradient flow to prove that optimizing solely for correctness inherently erodes the diversity of reasoning paths, leading to a \"reasoning monoculture.\" To address this, they propose Distributional Creative Reasoning (DCR), which incorporates a diversity energy functional (using entropy and kernel-based novelty) into the objective, mathematically guaranteeing the maintenance of a diverse portfolio of successful reasoning strategies while still optimizing for utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00747",
      "pdf_url": "https://arxiv.org/pdf/2601.00747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00747",
      "scraped_at": "2026-01-06T01:51:06.385822"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
    "paper_url": "https://huggingface.co/papers/2512.22955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
      "abstract": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs).  The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode.  To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning.  By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision.  Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22955",
      "pdf_url": "https://arxiv.org/pdf/2512.22955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22955",
      "scraped_at": "2026-01-06T01:51:08.307456"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Fast-weight Product Key Memory",
    "paper_url": "https://huggingface.co/papers/2601.00671",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-weight Product Key Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Trellis: Learning to Compress Key-Value Memory in Attention Models (2025) TNT: Improving Chunkwise Training for Test-Time Memorization (2025) GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory (2025) MIDUS: Memory-Infused Depth Up-Scaling (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models (2025) BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00671",
      "pdf_url": "https://arxiv.org/pdf/2601.00671",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00671",
      "scraped_at": "2026-01-06T01:51:10.101023"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
    "paper_url": "https://huggingface.co/papers/2601.00575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "abstract": "Project Page: https://ishirgarg.github.io/infosynth_web/ Code: https://github.com/ishirgarg/infosynth Dataset: https://huggingface.co/datasets/ishirgarg/InfoSynth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00575",
      "pdf_url": "https://arxiv.org/pdf/2601.00575",
      "github_links": [
        "https://github.com/ishirgarg/infosynth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00575",
      "scraped_at": "2026-01-06T01:51:12.005175"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
    "paper_url": "https://huggingface.co/papers/2601.00204",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Hao Tang",
      "Zeyu Cai",
      "XiaokunSun"
    ],
    "stars": "19",
    "details": {
      "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
      "abstract": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io Code: https://github.com/XiaokunSun/MorphAny3D",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00204",
      "pdf_url": "https://arxiv.org/pdf/2601.00204",
      "github_links": [
        "https://github.com/XiaokunSun/MorphAny3D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00204",
      "scraped_at": "2026-01-06T01:51:13.858117"
    },
    "scraped_date": "2026-01-06"
  },
  {
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "paper_url": "https://huggingface.co/papers/2512.20578",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
      "abstract": "Can Large Language Models predict their own failures? üß†‚ö° We all know the critical bottleneck in GenAI: LLMs are incredible, but they can confidently hallucinate and make mistakes. Until now, most fixes have been computationally massive ‚Äî relying on expensive external judges, huge reward models, or costly training to make the LLM itself more robust. This brings us to two fundamental questions: ‚ùì Do LLMs recognize when they're making mistakes? ‚ùì Can we make them self-aware about their own failures? üöÄ Introducing Gnosis: A lightweight self-awareness mechanism for frozen LLMs. Named after the Greek word for knowledge/insight, Gnosis gives LLMs a form of introspection. We add only ~5M parameters to enable a frozen LLM to verify its own outputs by decoding internal hidden states + attention patterns during inference ‚Äî with negligible overhead and no external judge . The results challenge the classic efficiency‚Äìaccuracy trade-off: üèÜ Superior performance across domains Despite being orders of magnitude smaller, Gnosis can outperform strong 8B reward models and proprietary judges like Gemini 2.5 Pro on both multi-step reasoning and factual/parametric knowledge QA (e.g., TriviaQA), across multiple backbones. ‚ö° Real-time early failure detection Gnosis doesn‚Äôt need to wait for the final token. By monitoring the generation trajectory in real time, it can predict an error before the model finishes ‚Äî enabling early stopping, preventing bad outputs from reaching users, and saving significant compute. This suggests something important: the model often already contains signals of impending failure during generation ‚Äî we just needed the right mechanism to read them. üëá code + models: üíª Code: https://github.com/Amirhosein-gh98/Gnosis",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20578",
      "pdf_url": "https://arxiv.org/pdf/2512.20578",
      "github_links": [
        "https://github.com/Amirhosein-gh98/Gnosis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20578",
      "scraped_at": "2026-01-07T01:50:17.166637"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.02204",
    "authors": [],
    "stars": "60",
    "details": {
      "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02204",
      "pdf_url": "https://arxiv.org/pdf/2601.02204",
      "github_links": [
        "https://github.com/ByteVisionLab/NextFlow"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02204",
      "scraped_at": "2026-01-07T01:50:19.105092"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "K-EXAONE Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.01739",
    "authors": [],
    "stars": "39",
    "details": {
      "title": "K-EXAONE Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning (2025) Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01739",
      "pdf_url": "https://arxiv.org/pdf/2601.01739",
      "github_links": [
        "https://github.com/LG-AI-EXAONE/K-EXAONE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01739",
      "scraped_at": "2026-01-07T01:50:21.012374"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.01425",
    "authors": [],
    "stars": "86",
    "details": {
      "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
      "abstract": "We introduce DreamID-V, the first Diffusion Transformer-based framework for high-fidelity video face swapping. DreamID-V bridges the gap between image and video domains, achieving exceptional identity similarity and temporal coherence even in challenging scenarios. Our code : https://github.com/bytedance/DreamID-V Our project : https://guoxu1233.github.io/DreamID-V/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01425",
      "pdf_url": "https://arxiv.org/pdf/2601.01425",
      "github_links": [
        "https://github.com/bytedance/DreamID-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01425",
      "scraped_at": "2026-01-07T01:50:22.967886"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2601.02256",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02256",
      "pdf_url": "https://arxiv.org/pdf/2601.02256",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02256",
      "scraped_at": "2026-01-07T01:50:24.885134"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
    "paper_url": "https://huggingface.co/papers/2512.24138",
    "authors": [
      "Zhiyong Wang",
      "Jiajun Liang",
      "Jie Liu",
      "Yuxiao Ye",
      "Haoran He"
    ],
    "stars": "18",
    "details": {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "abstract": "Introducing GARDO: Reinforcing Diffusion Models without Reward Hacking paper: https://arxiv.org/abs/2512.24138 code: https://github.com/tinnerhrhe/gardo project: https://tinnerhrhe.github.io/gardo_project/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24138",
      "pdf_url": "https://arxiv.org/pdf/2512.24138",
      "github_links": [
        "https://github.com/tinnerhrhe/GARDO",
        "https://github.com/tinnerhrhe/gardo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24138",
      "scraped_at": "2026-01-07T01:50:26.781382"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "paper_url": "https://huggingface.co/papers/2601.02358",
    "authors": [
      "Kun Gai",
      "Pengfei Wan",
      "Zhoujie Fu",
      "Tong He",
      "Junyi Chen"
    ],
    "stars": "42",
    "details": {
      "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02358",
      "pdf_url": "https://arxiv.org/pdf/2601.02358",
      "github_links": [
        "https://github.com/SOTAMak1r/VINO-code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02358",
      "scraped_at": "2026-01-07T01:50:28.658298"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "paper_url": "https://huggingface.co/papers/2601.02281",
    "authors": [],
    "stars": "76",
    "details": {
      "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
      "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ‚Äúrolling‚Äù the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02281",
      "pdf_url": "https://arxiv.org/pdf/2601.02281",
      "github_links": [
        "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02281",
      "scraped_at": "2026-01-07T01:50:30.519682"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Recursive Language Models",
    "paper_url": "https://huggingface.co/papers/2512.24601",
    "authors": [],
    "stars": "675",
    "details": {
      "title": "Recursive Language Models",
      "abstract": "Study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. They propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. They find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query. Some of the observations they found are :- -- LLMs interacting with their own prompts as objects. -- In their approach, a prompt isn‚Äôt ‚Äúrun‚Äù directly, instead it‚Äôs stored as a variable in an external Python REPL, and the language model writes code to inspect /slice/ decompose that long string, observes execution outputs, and then constructs sub-tasks where it recursively invokes an LLM on just the relevant snippets. Stitching the result together when the recursive process ends. So it can solve 10M+ token tasks with far less ‚Äúcontext rot‚Äù and often lower cost than summarization/RAG, turning long-context scaling into an inference-time algorithm rather than just a bigger context window. -- The ability to search the Prompt is what enables handling long context inputs, sub calls help handle information dense inputs. -- Inference cost of RLMs remain comparable to a base model call but are high variance  because it can keep making sub-calls or iterate if it can't solve the problem initially. -- The key insight is that long prompts should not be fed into the LLM directly, but should instead be treated as part of the environment that the LLM can search, read and interact with as needed for the task.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24601",
      "pdf_url": "https://arxiv.org/pdf/2512.24601",
      "github_links": [
        "https://github.com/alexzhang13/rlm/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24601",
      "scraped_at": "2026-01-07T01:50:32.438055"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "paper_url": "https://huggingface.co/papers/2601.02346",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent (2025) Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning (2025) CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions (2025) Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning (2025) Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02346",
      "pdf_url": "https://arxiv.org/pdf/2601.02346",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02346",
      "scraped_at": "2026-01-07T01:50:34.314010"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "paper_url": "https://huggingface.co/papers/2601.02356",
    "authors": [
      "Shuo Yang",
      "Jiarui Cai",
      "Yantao Shen",
      "ZyZcuhk",
      "jingtan"
    ],
    "stars": "13",
    "details": {
      "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) Loom: Diffusion-Transformer for Interleaved Generation (2025) What Happens Next? Next Scene Prediction with a Unified Video Model (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02356",
      "pdf_url": "https://arxiv.org/pdf/2601.02356",
      "github_links": [
        "https://github.com/sparkstj/Talk2Move"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02356",
      "scraped_at": "2026-01-07T01:50:36.191628"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "paper_url": "https://huggingface.co/papers/2601.02179",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "abstract": "In this paper, we explore the confidence estimation in a new paradigm: multi-turn interactions! Check it out!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02179",
      "pdf_url": "https://arxiv.org/pdf/2601.02179",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02179",
      "scraped_at": "2026-01-07T01:50:38.040156"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01046",
    "authors": [
      "Yi Yang",
      "Yixuan Tang"
    ],
    "stars": "0",
    "details": {
      "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
      "abstract": "‚ú® Turn any decoder-only LLM into a powerful embedding model‚Äîzero training needed! ‚ú® The Trick : Re-route the final token's key-value states as an internal prefix, giving all tokens access to global context in one forward pass. No input modification, no mask removal, just smart internal state manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01046",
      "pdf_url": "https://arxiv.org/pdf/2601.01046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01046",
      "scraped_at": "2026-01-07T01:50:39.891420"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2601.00501",
    "authors": [
      "Mohammad Asiful Hossain",
      "Kevin Cannons",
      "Saeed Ranjbar Alvar",
      "Mohsen Gholami",
      "Ahmad Rezaei"
    ],
    "stars": "0",
    "details": {
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "abstract": "CPPO: Contrastive Perception for Vision Language Policy Optimization introduces a new method (CPPO) for fine-tuning vision-language models (VLMs) using reinforcement learning. Instead of relying on explicit perception rewards or auxiliary models, the approach identifies perceptual tokens via entropy changes under perturbed images and augments the policy objective with a contrastive perception loss to improve multimodal reasoning performance and training efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00501",
      "pdf_url": "https://arxiv.org/pdf/2601.00501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00501",
      "scraped_at": "2026-01-07T01:50:41.758090"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "paper_url": "https://huggingface.co/papers/2601.02267",
    "authors": [
      "Jian Yang",
      "Ying Tai",
      "Zhenyu Zhang",
      "wrk226"
    ],
    "stars": "1",
    "details": {
      "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
      "abstract": "Project page: https://wrk226.github.io/DiffProxy.html Code: https://github.com/wrk226/DiffProxy",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02267",
      "pdf_url": "https://arxiv.org/pdf/2601.02267",
      "github_links": [
        "https://github.com/wrk226/DiffProxy"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02267",
      "scraped_at": "2026-01-07T01:50:43.650977"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01836",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "abstract": "COMPASS is the first framework for evaluating LLM alignment with organization-specific policies rather than universal harms. While models handle legitimate requests well (>95% accuracy), they catastrophically fail at enforcing prohibitions, refusing only 13-40% of denylist violations. GitHub: https://github.com/AIM-Intelligence/COMPASS",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01836",
      "pdf_url": "https://arxiv.org/pdf/2601.01836",
      "github_links": [
        "https://github.com/AIM-Intelligence/COMPASS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01836",
      "scraped_at": "2026-01-07T01:50:45.620954"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
    "paper_url": "https://huggingface.co/papers/2512.23035",
    "authors": [
      "Shiying Wang",
      "Kai Li",
      "Shun Zhang",
      "Xuechao Zou",
      "Yi Zhou"
    ],
    "stars": "4",
    "details": {
      "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
      "abstract": "We are excited to introduce our latest work on semi-supervised semantic segmentation : üìÑ Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion This paper tackles one of the most challenging issues in semi-supervised segmentation: pseudo-label drift . When labeled data are extremely scarce, self-training methods are prone to deterministic bias, where early incorrect pseudo-labels accumulate over time, leading to unstable and degraded training. üß† Motivation Most existing consistency- or pseudo-label‚Äìbased semi-supervised approaches rely heavily on self-generated supervision . Once early pseudo-labels become unreliable, error accumulation is inevitable. Our goal is to introduce stronger semantic priors to correct such drift and stabilize the training process. ‚ú® Key Contributions 1Ô∏è‚É£ Heterogeneous Dual-Student Framework We leverage two complementary vision foundation models‚Äî CLIP for global semantic priors and DINOv3 for fine-grained local structures‚Äîto enable stable mutual learning and suppress error accumulation. 2Ô∏è‚É£ Explicit‚ÄìImplicit Semantic Co-Guidance By jointly utilizing text embeddings (explicit semantics) and learnable queries (implicit semantics), we provide class-level semantic anchors and enhance semantic consistency. 3Ô∏è‚É£ Global‚ÄìLocal Feature Co-Fusion We fuse CLIP‚Äôs global contextual understanding with DINOv3‚Äôs local structural details, yielding more accurate and stable segmentation results. üìä Experimental Results Extensive evaluations on six mainstream remote sensing benchmarks demonstrate that Co2S consistently achieves strong and stable performance across different data splits and scenarios, especially under extremely low annotation budgets . üì¶ Open-Source Resources arXiv Paper : https://arxiv.org/abs/2512.23035 Project Page : https://xavierjiezou.github.io/Co2S/ GitHub Code : https://github.com/XavierJiezou/Co2S HuggingFace Models : https://huggingface.co/XavierJiezou/co2s-models HuggingFace Datasets : https://huggingface.co/datasets/XavierJiezou/co2s-datasets #Remote Sensing #Semantic Segmentation #Semi-Supervised Learning #Vision Foundation Models #CLIP #DINOv3",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23035",
      "pdf_url": "https://arxiv.org/pdf/2512.23035",
      "github_links": [
        "https://github.com/XavierJiezou/Co2S"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23035",
      "scraped_at": "2026-01-07T01:50:47.571407"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "paper_url": "https://huggingface.co/papers/2601.01426",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01426",
      "pdf_url": "https://arxiv.org/pdf/2601.01426",
      "github_links": [
        "https://github.com/SWE-Lego/SWE-Lego"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01426",
      "scraped_at": "2026-01-07T01:50:49.438589"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
    "paper_url": "https://huggingface.co/papers/2601.01576",
    "authors": [
      "Chunchun Ma",
      "Yujiong Shen",
      "Yueyuan Huang",
      "Kexin Tan",
      "Ming Zhang"
    ],
    "stars": "3",
    "details": {
      "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation (2025) SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning (2025) WisPaper: Your AI Scholar Search Engine (2025) AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis (2025) OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists (2025) AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement (2025) Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01576",
      "pdf_url": "https://arxiv.org/pdf/2601.01576",
      "github_links": [
        "https://github.com/january-blue/OpenNovelty"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01576",
      "scraped_at": "2026-01-07T01:50:51.251342"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
    "paper_url": "https://huggingface.co/papers/2601.00863",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
      "abstract": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00863",
      "pdf_url": "https://arxiv.org/pdf/2601.00863",
      "github_links": [
        "https://github.com/lamm-mit/MusicAnalysis"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00863",
      "scraped_at": "2026-01-07T01:50:53.135282"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
    "paper_url": "https://huggingface.co/papers/2512.21472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "abstract": "‚ú® The largest publicly available dermoscopic skin lesion segmentation dataset with 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image. ‚ú® 16 unique annotators , 3 different tools used, and 2 skill levels of the manual reviewer. ‚ú® Contains consensus masks for the 2,394 images that have multi-annotator segmentations (2-5 segmentations per image). ‚ú® Collected and curated from the ISIC Archive .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21472",
      "pdf_url": "https://arxiv.org/pdf/2512.21472",
      "github_links": [
        "https://github.com/sfu-mial/IMAplusplus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21472",
      "scraped_at": "2026-01-07T01:50:55.025985"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "paper_url": "https://huggingface.co/papers/2601.02315",
    "authors": [
      "Beth Tellman",
      "Lalit Maurya",
      "Saurabh Kaushik"
    ],
    "stars": "3",
    "details": {
      "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
      "abstract": "Despite the recent success of large pretrained encoders (Geo‚ÄëFoundation Models), we consistently observe that U‚ÄëNet‚Äëbased models remain highly competitive‚Äîand in some cases outperform transformers, particularly due to their strength in capturing local spatial nuances. Motivated by this, we propose Prithvi‚ÄëCAFE (Prithvi‚ÄëComplementary Adaptive Fusion Encoder), which enhances local representations through complementary fusion with a CNN‚Äëbased encoder. We evaluate our approach on two major flood datasets‚ÄîFloodPlanet and Sen1Floods11‚Äîand achieve state‚Äëof‚Äëthe‚Äëart performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02315",
      "pdf_url": "https://arxiv.org/pdf/2601.02315",
      "github_links": [
        "https://github.com/Sk-2103/Prithvi-CAFE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02315",
      "scraped_at": "2026-01-07T01:50:56.844097"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.02314",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "abstract": "Does COT in llms stay faithful to their thoughts?",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02314",
      "pdf_url": "https://arxiv.org/pdf/2601.02314",
      "github_links": [
        "https://github.com/skhanzad/AridadneXAI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02314",
      "scraped_at": "2026-01-07T01:50:58.682941"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.22877",
    "authors": [
      "Jun-Cheng Chen",
      "Cheng-Fu Chou",
      "Ju-Hsuan Weng",
      "jwliao1209"
    ],
    "stars": "0",
    "details": {
      "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
      "abstract": "Concept Erasure Benchmark",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22877",
      "pdf_url": "https://arxiv.org/pdf/2512.22877",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22877",
      "scraped_at": "2026-01-07T01:51:00.532894"
    },
    "scraped_date": "2026-01-07"
  },
  {
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "paper_url": "https://huggingface.co/papers/2601.03252",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
      "abstract": "Depth Beyond Pixels üöÄ We Introduce InfiniDepth ‚Äî casting monocular depth estimation as a neural implicit field. üîç Arbitrary-Resolution üìê Accurate Metric Depth üì∑ Single-View NVS under large viewpoints shifts Arxiv: https://arxiv.org/abs/2601.03252 page: https://zju3dv.github.io/InfiniDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03252",
      "pdf_url": "https://arxiv.org/pdf/2601.03252",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03252",
      "scraped_at": "2026-01-08T01:50:45.247652"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "paper_url": "https://huggingface.co/papers/2601.01554",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
      "abstract": "MOSS Transcribe Diarize üéôÔ∏è We introduce MOSS Transcribe Diarize ‚Äî a unified multimodal model for Speaker-Attributed, Time-Stamped Transcription (SATS) . üîç End-to-end SATS in a single pass (transcription + speaker attribution + timestamps) üß† 128k context window for up to ~90-minute audio without chunking (strong long-range speaker memory) üåç Trained on extensive in-the-wild conversations + controllable simulated mixtures (robust to overlap/noise/domain shift) üìä Strong results on AISHELL-4 / Podcast / Movies benchmarks (best cpCER / Œîcp among evaluated systems) Paper: [2601.01554] MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization Homepage: https://mosi.cn/models/moss-transcribe-diarize Online Demo: https://moss-transcribe-diarize-demo.mosi.cn",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01554",
      "pdf_url": "https://arxiv.org/pdf/2601.01554",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01554",
      "scraped_at": "2026-01-08T01:50:47.162285"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "paper_url": "https://huggingface.co/papers/2601.03233",
    "authors": [
      "kvochko",
      "jacobitterman",
      "nisan",
      "benibraz",
      "yoavhacohen"
    ],
    "stars": "922",
    "details": {
      "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API 3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation (2025) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation (2025) MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning (2026) DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation (2025) JoVA: Unified Multimodal Learning for Joint Video-Audio Generation (2025) In-Context Audio Control of Video Diffusion Transformers (2025) JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03233",
      "pdf_url": "https://arxiv.org/pdf/2601.03233",
      "github_links": [
        "https://github.com/Lightricks/LTX-2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03233",
      "scraped_at": "2026-01-08T01:50:49.139726"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.22334",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "abstract": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.22334",
      "pdf_url": "https://arxiv.org/pdf/2512.22334",
      "github_links": [
        "https://github.com/InternScience/SciEvalKit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.22334",
      "scraped_at": "2026-01-08T01:50:51.230691"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "paper_url": "https://huggingface.co/papers/2601.03193",
    "authors": [
      "Lin-Chen",
      "lovesnowbest",
      "YuZeng260",
      "CostaliyA",
      "Hungryyan"
    ],
    "stars": "25",
    "details": {
      "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
      "abstract": "UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03193",
      "pdf_url": "https://arxiv.org/pdf/2601.03193",
      "github_links": [
        "https://github.com/Hungryyan1/UniCorn"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03193",
      "scraped_at": "2026-01-08T01:50:53.221654"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "paper_url": "https://huggingface.co/papers/2601.02427",
    "authors": [],
    "stars": "1.44k",
    "details": {
      "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
      "abstract": "NitroGen is a vision-action foundation model trained on 40k hours of gameplay across 1,000+ games, enabling cross-game generalization with behavior cloning and benchmarking, achieving strong unseen-game transfer.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02427",
      "pdf_url": "https://arxiv.org/pdf/2601.02427",
      "github_links": [
        "https://github.com/MineDojo/NitroGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02427",
      "scraped_at": "2026-01-08T01:50:55.235135"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2601.03044",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "abstract": "üöÄ Website: https://www.agibot.com/research/sop We introduce SOP for online post-training of generalist VLAs in the real world ‚Äî unlocking persistent, reliable deployment of generalist robots in physical environments. üîÅ 36 hours of continuous cloth folding: video üì¶ 36 hours of continuous cardboard box assembly: video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03044",
      "pdf_url": "https://arxiv.org/pdf/2601.03044",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03044",
      "scraped_at": "2026-01-08T01:50:57.176273"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "paper_url": "https://huggingface.co/papers/2601.02785",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamStyle: A Unified Framework for Video Stylization",
      "abstract": "DreamStyle unifies text-, style-image-, and first-frame-guided video stylization on an I2V backbone, using LoRA with token-specific up matrices to improve style consistency and video quality.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02785",
      "pdf_url": "https://arxiv.org/pdf/2601.02785",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02785",
      "scraped_at": "2026-01-08T01:50:59.158844"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MiMo-V2-Flash Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.02780",
    "authors": [],
    "stars": "957",
    "details": {
      "title": "MiMo-V2-Flash Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Xiaomi MiMo-VL-Miloco Technical Report (2025) Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning (2025) Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks (2025) AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing (2025) NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations (2025) Practical Policy Distillation for Reinforcement Learning in Radio Access Networks (2025) Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02780",
      "pdf_url": "https://arxiv.org/pdf/2601.02780",
      "github_links": [
        "https://github.com/XiaomiMiMo/MiMo-V2-Flash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02780",
      "scraped_at": "2026-01-08T01:51:01.099679"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2601.01874",
    "authors": [
      "Aojun Lu",
      "Junjie Xie",
      "Shuhang Chen",
      "JacobYuan",
      "Yunqiu"
    ],
    "stars": "0",
    "details": {
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "abstract": "Project page: https://shchen233.github.io/cogflow/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01874",
      "pdf_url": "https://arxiv.org/pdf/2601.01874",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01874",
      "scraped_at": "2026-01-08T01:51:03.012607"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "paper_url": "https://huggingface.co/papers/2601.01321",
    "authors": [
      "Yao Su",
      "vztu",
      "ZihanJia",
      "fjchendp",
      "roz322"
    ],
    "stars": "2",
    "details": {
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "abstract": "This paper systematically analyzes AI integration in Digital Twins through a four-stage framework (modeling ‚Üí mirroring ‚Üí intervention ‚Üí autonomous management), covering LLMs, foundation models, world models, and intelligent agents across 11 application domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01321",
      "pdf_url": "https://arxiv.org/pdf/2601.01321",
      "github_links": [
        "https://github.com/rongzhou7/Awesome-Digital-Twin-AI/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01321",
      "scraped_at": "2026-01-08T01:51:04.924547"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "paper_url": "https://huggingface.co/papers/2601.02439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "abstract": "WebGym creates a large, non-stationary visual web task suite and scalable RL pipeline, enabling fast trajectory rollout and improved vision-language agent performance on unseen websites.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02439",
      "pdf_url": "https://arxiv.org/pdf/2601.02439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02439",
      "scraped_at": "2026-01-08T01:51:06.769414"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
    "paper_url": "https://huggingface.co/papers/2601.02989",
    "authors": [
      "Fatemeh Askari",
      "Sadegh Mohammadian",
      "Mohammadali Banayeeanzade",
      "Hosein Hasani",
      "safinal"
    ],
    "stars": "0",
    "details": {
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "abstract": "üî¢ Overcoming Transformer Depth Limits in Counting Tasks LLMs often fail at counting not because they aren't smart, but because of architectural depth constraints üöß. We propose a simple, effective System-2 strategy üß© that decomposes counting tasks to bypass these limits. üî¨ We also provide a full mechanistic interpretation , identifying the specific attention heads and representations responsible for transferring \"latent counts\" across the network. üìà This approach allows LLMs to achieve high accuracy on large-scale counting benchmarks where they typically fail.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02989",
      "pdf_url": "https://arxiv.org/pdf/2601.02989",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02989",
      "scraped_at": "2026-01-08T01:51:08.644226"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "paper_url": "https://huggingface.co/papers/2601.03256",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
      "abstract": "Project page: https://luhexiao.github.io/Muses.github.io/ Code: https://github.com/luhexiao/Muses",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03256",
      "pdf_url": "https://arxiv.org/pdf/2601.03256",
      "github_links": [
        "https://github.com/luhexiao/Muses"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03256",
      "scraped_at": "2026-01-08T01:51:10.461072"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "paper_url": "https://huggingface.co/papers/2601.01720",
    "authors": [
      "Donghao Luo",
      "yanweifuture",
      "chengjie-wang",
      "ChengmingX",
      "ScarletAce"
    ],
    "stars": "0",
    "details": {
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization (2025) Unified Video Editing with Temporal Reasoner (2025) VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization (2025) IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning (2025) EasyV2V: A High-quality Instruction-based Video Editing Framework (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01720",
      "pdf_url": "https://arxiv.org/pdf/2601.01720",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01720",
      "scraped_at": "2026-01-08T01:51:12.354426"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01592",
    "authors": [
      "Yang Yao",
      "Yixu Wang",
      "Juncheng Li",
      "Yunhao Chen",
      "xinwang22"
    ],
    "stars": "112",
    "details": {
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "abstract": "Even State-of-the-Art Models Fail to Hold Ground Against Sophisticated Adversaries. Our comprehensive evaluation highlights two key findings. (1) A clear stratification in defense capability: Top-tier models such as Claude Haiku 4.5, GPT-5.2, and Qwen3-Max exhibit strong baseline robustness, effectively neutralizing static, template-based attacks and complex logic traps, often keeping ASR below 20%.This suggests that leading labs have improved defenses against recognizable, repeatable jailbreak structures, while several models (e.g., Llama-4, Mistral Large 3) remain more susceptible to these simpler patterns. (2) A shift in the attack landscape: adaptive, multi-turn, and multi-agent strategies dominate, whereas static, single-turn, and template-based approaches are increasingly ineffective. Methods like EvoSynth and X-Teaming can achieve >90% ASR even against advanced models. This indicates current safety training overfits to static templates, failing to generalize against the broad attack surface exposed by automated red-teaming. Adversarial Robustness Exhibits Inconsistent and Polarized Vulnerability Patterns. We observe a polarization effect where models demonstrate high resistance to specific attack families (e.g., text-based cipher) yet remain completely defenseless against others (e.g., logic nesting). For instance, Grok 4.1 Fast shows 1.5% ASR against RedQueen but 90.5% against X-Teaming. This stark performance disparity (~90%) underscores that current defenses are often patch-based rather than holistic, necessitating the multi-faceted evaluation provided by OpenRT. Enhanced Reasoning and Multimodal Capabilities are New Vectors for Exploitation. Contrary to the common assumption that more capable models are inherently safer, we find that enhanced capabilities often introduce new vectors for exploitation. Reasoning-enhanced models (CoT) do not demonstrate superior robustness; instead, their verbose reasoning processes can be manipulated to bypass safety filters. Similarly, Multimodal LLMs exhibit a critical modality gap: visual inputs frequently bypass text-based safety mechanisms, allowing cross-modal attacks to compromise models that are otherwise robust to purely textual jailbreaks. These findings suggest that current safety alignment has not kept pace with the architectural expansion of model capabilities. Proprietary Models Can Be as Vulnerable as Open-Source Models Under Certain Attacks. Our analysis reveals that proprietary and open-source models exhibit comparable susceptibility to our attack suite. Across our 20 evaluated models, only GPT-5.2 and Claude Haiku 4.5 maintained an average ASR below 30%, while all other models consistently exceeded this threshold. This universality sharply contradicts the assumption that closed deployments offer superior protection, demonstrating that the safety through obscurity of proprietary strategies fails to provide any tangible mitigation against sophisticated adversarial attacks. Scaling MLLMs Robustness via Defense-in-Depth and Continuous Red Teaming. Challenges such as polarized robustness, weak generalization to unseen attacks, and cross-modal bypasses highlight the limits of single-layer defense. Effective mitigation requires a paradigm shift toward Defense-in-Depth: integrating intrinsic architectural safety with runtime risk estimation and adversarial training on multimodal and multi-turn interactions. Crucially, continuous Red Teaming via infrastructure like OpenRT provides systematic evaluation to verify empirical robustness and prevent benchmark overfitting.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01592",
      "pdf_url": "https://arxiv.org/pdf/2601.01592",
      "github_links": [
        "https://github.com/AI45Lab/OpenRT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01592",
      "scraped_at": "2026-01-08T01:51:14.181550"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.23412",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
      "abstract": "In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows the model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL. The agent reasoning framework, MWE-Bench, three smaller-scale agent models (2B, 3B, and 4B) distilled from MindWatcher 32B, and related resources will be open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23412",
      "pdf_url": "https://arxiv.org/pdf/2512.23412",
      "github_links": [
        "https://github.com/TIMMY-CHAN/MindWatcher"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23412",
      "scraped_at": "2026-01-08T01:51:16.037424"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "paper_url": "https://huggingface.co/papers/2601.03227",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "abstract": "We found the sonar moment in audio language models. We propose the task of audio geo-localization. And amazingly, Gemini 3 Pro can reach the distance error of less than 55km for 25%  samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03227",
      "pdf_url": "https://arxiv.org/pdf/2601.03227",
      "github_links": [
        "https://github.com/Rising0321/AGL1K"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03227",
      "scraped_at": "2026-01-08T01:51:21.077717"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "paper_url": "https://huggingface.co/papers/2601.03194",
    "authors": [
      "Sai Rithwik Reddy Chirra",
      "Shashivardhan Reddy Koppula",
      "Mohammad Zia Ur Rehman",
      "shwetankssingh",
      "UVSKKR"
    ],
    "stars": "0",
    "details": {
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (explainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03194",
      "pdf_url": "https://arxiv.org/pdf/2601.03194",
      "github_links": [
        "https://github.com/ziarehman30/X-MuTeST"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03194",
      "scraped_at": "2026-01-08T01:51:23.122431"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Parallel Latent Reasoning for Sequential Recommendation",
    "paper_url": "https://huggingface.co/papers/2601.03153",
    "authors": [
      "Yuning Jiang",
      "Jian Wu",
      "Wen Chen",
      "Xu Chen",
      "TangJiakai5704"
    ],
    "stars": "0",
    "details": {
      "title": "Parallel Latent Reasoning for Sequential Recommendation",
      "abstract": "Parallel Latent Reasoning (PLR): Sequential Recommendation with Parallel Reasoning üî• üìâ Depth-only reasoning often hits performance plateaus‚ÄîPLR mitigates this with parallel latent reasoning. Core Innovation ‚ú® üéØ Learnable trigger tokens: Build parallel streams in continuous latent space. üîÑ Global regularization: Preserve stream diversity to avoid redundancy. ‚öñÔ∏è Adaptive aggregation: Smartly combine multi-stream insights for optimal results. Key Advantages üöÄ üìä Outperforms SOTA baselines (SASRec, BERT4Rec, ReaRec, LRESA) by 5.5%‚Äì14.9% on Recall@10/20 and NDCG@10/20 across three real-world datasets. ‚ö° Real-time efficiency: Only 5.8% latency increase vs. base models, enabled by KV Caching and GPU parallelism. üõ°Ô∏è Strong robustness: Maintains top performance even with 30% missing user interactions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03153",
      "pdf_url": "https://arxiv.org/pdf/2601.03153",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03153",
      "scraped_at": "2026-01-08T01:51:25.012089"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03127",
    "authors": [
      "Yue Cao",
      "Hanqing Yang",
      "Jijin Hu",
      "Qiang Zhou",
      "Sashuai Zhou"
    ],
    "stars": "0",
    "details": {
      "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
      "abstract": "reasoning-based image generation and editing",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03127",
      "pdf_url": "https://arxiv.org/pdf/2601.03127",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03127",
      "scraped_at": "2026-01-08T01:51:26.836393"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
    "paper_url": "https://huggingface.co/papers/2601.02996",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
      "abstract": "https://github.com/cisnlp/multilingual-latent-reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02996",
      "pdf_url": "https://arxiv.org/pdf/2601.02996",
      "github_links": [
        "https://github.com/cisnlp/multilingual-latent-reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02996",
      "scraped_at": "2026-01-08T01:51:28.643997"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "paper_url": "https://huggingface.co/papers/2601.02359",
    "authors": [
      "Vladislav Golyanik",
      "Toshihiko Yamasaki",
      "mapooon"
    ],
    "stars": "0",
    "details": {
      "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
      "abstract": "Detecting deepfakes with generative AI. We introduce ExposeAnyone ‚Äî a paradigm shift in face forgery detection! üîçÔ∏è Fully self-supervised approach ü•á Best average AUC on traditional deepfake benchmarks üí™ Best AUC even on Sora2 by OpenAI üí¢ Strong Robustness to common corruptions such as JPEG/MPEG compression Arxiv: https://arxiv.org/abs/2601.02359 Project page: https://mapooon.github.io/ExposeAnyonePage/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02359",
      "pdf_url": "https://arxiv.org/pdf/2601.02359",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02359",
      "scraped_at": "2026-01-08T01:51:30.555236"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
    "paper_url": "https://huggingface.co/papers/2601.00581",
    "authors": [],
    "stars": "458",
    "details": {
      "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
      "abstract": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at this https URL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00581",
      "pdf_url": "https://arxiv.org/pdf/2601.00581",
      "github_links": [
        "https://github.com/torchmd/torchmd-net"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00581",
      "scraped_at": "2026-01-08T01:51:32.482242"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
    "paper_url": "https://huggingface.co/papers/2512.23950",
    "authors": [
      "Peng Li",
      "Yulong Xiao",
      "Mingzhe Liu",
      "Huibin Li",
      "FengShaner"
    ],
    "stars": "2",
    "details": {
      "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
      "abstract": "Title: DehazeSNN ‚Äî U-Net-like Spiking Neural Networks for Single Image Dehazing Short summary: DehazeSNN integrates a U-Net architecture with Spiking Neural Networks to reduce compute while achieving competitive dehazing results. Code: github.com/HaoranLiu507/DehazeSNN. Highlights: U-Net + SNN design for lower MACs. OLIF block for improved cross-channel communication. Benchmarks show comparable or better dehazing with smaller model footprint.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23950",
      "pdf_url": "https://arxiv.org/pdf/2512.23950",
      "github_links": [
        "https://github.com/HaoranLiu507/DehazeSNN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23950",
      "scraped_at": "2026-01-08T01:51:34.324181"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.01584",
    "authors": [
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "abstract": "This paper measures how easily ‚Äúinstrumental-convergence‚Äù behaviors (e.g., shutdown avoidance, self-replication) in LLMs can be amplified or suppressed by simple steering, and argues that the common claim ‚Äúas AI capability (often glossed as ‚Äòintelligence‚Äô) increases, systems inevitably become less controllable‚Äù should not be treated as a default assumption. Using InstrumentalEval on Qwen3 (4B/30B; Base/Instruct/Thinking) with a GPT-5.2 judge, a short anti-instrumental prompt suffix drops convergence sharply (e.g., Qwen3-30B Instruct: 81.69% to 2.82%), while a pro-instrumental suffix pushes it high. The key takeaway is a safety‚Äìsecurity dilemma for open weights: the same high steerability that helps builders enforce safe behavior can also help attackers elicit disallowed behavior, so widening the gap between authorized vs. unauthorized steerability remains a central open problem.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01584",
      "pdf_url": "https://arxiv.org/pdf/2601.01584",
      "github_links": [
        "https://github.com/j-hoscilowicz/instrumental_steering/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01584",
      "scraped_at": "2026-01-08T01:51:36.179871"
    },
    "scraped_date": "2026-01-08"
  },
  {
    "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
    "paper_url": "https://huggingface.co/papers/2601.02151",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
      "abstract": "üíª Code: https://github.com/PRIS-CV/EAFT ‚ú® Project Page: https://ymxyll.github.io/EAFT/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02151",
      "pdf_url": "https://arxiv.org/pdf/2601.02151",
      "github_links": [
        "https://github.com/hiyouga/LLaMA-Factory",
        "https://github.com/PRIS-CV/EAFT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02151",
      "scraped_at": "2026-01-09T01:51:09.218763"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Evolving Programmatic Skill Networks",
    "paper_url": "https://huggingface.co/papers/2601.03509",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evolving Programmatic Skill Networks",
      "abstract": "We study continual skill acquisition in openended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1) REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN‚Äôs learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03509",
      "pdf_url": "https://arxiv.org/pdf/2601.03509",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03509",
      "scraped_at": "2026-01-09T01:51:11.028376"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.03872",
    "authors": [
      "Yuhao Shen",
      "Jiahao Yuan",
      "Ruihan Jin",
      "Guocheng Zhai",
      "Jinyang23"
    ],
    "stars": "0",
    "details": {
      "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
      "abstract": "üöÄ [New Paper] Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning The growing diversity of LLMs and external tools presents a significant challenge: how to select the optimal model-tool combination for complex reasoning tasks. Existing methods often fall short by relying on single models or fixed tool-calling logic. ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation) addresses this by introducing a dual-path framework for dynamic model-tool alignment and invocation across multiple domains. ‚ú® The Core Intuition: ATLAS employs a dual-path approach to achieve dynamic model-tool alignment and invocation: 1Ô∏è‚É£ Training-Free Cluster-Based Routing: This path leverages empirical priors for domain-specific alignment, efficiently guiding the model-tool selection process. 2Ô∏è‚É£ RL-Based Multi-Step Routing: This path explores autonomous trajectories to achieve strong generalization, particularly for out-of-distribution tasks. üìà Highlights: Superior Performance: ATLAS significantly outperforms closed-source models like GPT-4o and existing routing methods, achieving +10.1% on in-distribution tasks and +13.1% on out-of-distribution tasks across 15 benchmarks. Enhanced Visual Reasoning: The framework demonstrates substantial improvements in visual reasoning by effectively orchestrating specialized multi-modal tools. Adaptive Orchestration: ATLAS learns to assess its internal state and dynamically invoke external resources, internalizing the alignment between domains and tool utilization. Robust and Generalizable: The design ensures that the routing policy effectively captures expertise distribution, making it robust and generalizable even as tools and models evolve.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03872",
      "pdf_url": "https://arxiv.org/pdf/2601.03872",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03872",
      "scraped_at": "2026-01-09T01:51:12.875928"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
    "paper_url": "https://huggingface.co/papers/2601.03986",
    "authors": [
      "Muling Wu",
      "Changze Lv",
      "Jingwen Xu",
      "Qi Qian",
      "ChengsongHuang"
    ],
    "stars": "0",
    "details": {
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03986",
      "pdf_url": "https://arxiv.org/pdf/2601.03986",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03986",
      "scraped_at": "2026-01-09T01:51:14.732608"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
    "paper_url": "https://huggingface.co/papers/2601.03822",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
      "abstract": "ROI-Reasoning introduces a principled framework for budget-aware inference-time reasoning in large language models. Instead of blindly scaling computation, the authors formulate multi-task reasoning under a global token constraint as an Ordered Stochastic Multiple-Choice Knapsack Problem, explicitly modeling the trade-off between reasoning cost and expected utility. The proposed two-stage approach combines Meta-Cognitive Fine-Tuning, which enables models to anticipate difficulty and make solve-or-skip decisions before reasoning, with Rationality-Aware Reinforcement Learning, which optimizes long-horizon computation allocation under strict budgets. Across challenging mathematical reasoning benchmarks, ROI-Reasoning consistently improves total score and substantially reduces regret‚Äîdemonstrating that meta-cognitive planning, not just stronger reasoning, is key to efficient test-time scaling of LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03822",
      "pdf_url": "https://arxiv.org/pdf/2601.03822",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03822",
      "scraped_at": "2026-01-09T01:51:16.648836"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "paper_url": "https://huggingface.co/papers/2601.04151",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
      "abstract": "Klear: 26B model for joint audio-video generation Single-tower DiT with \"Omni-Full Attention\" across video, audio, and text Progressive multi-task training (T2V, T2A, T2AV, I2V all in one model) 81M sample dataset with dense captions Claims Veo 3-level performance on lip-sync & AV consistency",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04151",
      "pdf_url": "https://arxiv.org/pdf/2601.04151",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04151",
      "scraped_at": "2026-01-09T01:51:18.670540"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Choreographing a World of Dynamic Objects",
    "paper_url": "https://huggingface.co/papers/2601.04194",
    "authors": [
      "Hadi Alzayer",
      "Yunzhi Zhang",
      "Karthik Dharmarajan",
      "Chen Geng",
      "Yanzhe Lyu"
    ],
    "stars": "0",
    "details": {
      "title": "Choreographing a World of Dynamic Objects",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Animus3D: Text-driven 3D Animation via Motion Score Distillation (2025) AnimaMimic: Imitating 3D Animation from Video Priors (2025) Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions (2025) DIMO: Diverse 3D Motion Generation for Arbitrary Objects (2025) Inferring Compositional 4D Scenes without Ever Seeing One (2025) SS4D: Native 4D Generative Model via Structured Spacetime Latents (2025) WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04194",
      "pdf_url": "https://arxiv.org/pdf/2601.04194",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04194",
      "scraped_at": "2026-01-09T01:51:20.596669"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "paper_url": "https://huggingface.co/papers/2601.04171",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
      "abstract": "Agentic Rubrics for verifying SWE agent patches WITHOUT running tests! An agent explores the codebase to generate context-grounded checklists, then scores patches execution-free. Rubrics provide dense, interpretable reward signals that could scale RL training for coding agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04171",
      "pdf_url": "https://arxiv.org/pdf/2601.04171",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04171",
      "scraped_at": "2026-01-09T01:51:22.502781"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
    "paper_url": "https://huggingface.co/papers/2601.02075",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "abstract": "project: https://github.com/FredericVAN/PKU_MDAgent2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02075",
      "pdf_url": "https://arxiv.org/pdf/2601.02075",
      "github_links": [
        "https://github.com/FredericVAN/PKU_MDAgent2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02075",
      "scraped_at": "2026-01-09T01:51:24.349527"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
    "paper_url": "https://huggingface.co/papers/2601.00423",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
      "abstract": "We propose an entropy aware Group Relative Policy Optimization (E-GRPO) to increase the entropy of SDE sampling steps. We have integrated a variety of current GRPO-based reinforcement learning methods as well as different image reward models. Code: https://github.com/shengjun-zhang/VisualGRPO Model: https://huggingface.co/studyOverflow/E-GRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00423",
      "pdf_url": "https://arxiv.org/pdf/2601.00423",
      "github_links": [
        "https://github.com/shengjun-zhang/VisualGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00423",
      "scraped_at": "2026-01-09T01:51:26.319389"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.03471",
    "authors": [
      "Guanchen Wu",
      "Yuzhang Xie",
      "Zewen Liu",
      "Dehai Min",
      "Mingyang Wei"
    ],
    "stars": "0",
    "details": {
      "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
      "abstract": "EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03471",
      "pdf_url": "https://arxiv.org/pdf/2601.03471",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03471",
      "scraped_at": "2026-01-09T01:51:28.218342"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.03699",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "abstract": "RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03699",
      "pdf_url": "https://arxiv.org/pdf/2601.03699",
      "github_links": [
        "https://github.com/knoveleng/redeval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03699",
      "scraped_at": "2026-01-09T01:51:30.046104"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
    "paper_url": "https://huggingface.co/papers/2601.03315",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
      "abstract": "We find that LLMs aren't scientists yet.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03315",
      "pdf_url": "https://arxiv.org/pdf/2601.03315",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03315",
      "scraped_at": "2026-01-09T01:51:31.842661"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.03467",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling (2025) CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing (2025) Unified Thinker: A General Reasoning Modular Core for Image Generation (2026) ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning (2025) ThinkGen: Generalized Thinking for Visual Generation (2025) MIRA: Multimodal Iterative Reasoning Agent for Image Editing (2025) EditThinker: Unlocking Iterative Reasoning for Any Image Editor (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03467",
      "pdf_url": "https://arxiv.org/pdf/2601.03467",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03467",
      "scraped_at": "2026-01-09T01:51:33.683864"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
    "paper_url": "https://huggingface.co/papers/2601.03448",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
      "abstract": "We propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. L2T establishes the structural scaffolding required for linguistic competence, complementing world knowledge acquired through standard CLM. The code is available on GitHub: https://github.com/gucci-j/l2t",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03448",
      "pdf_url": "https://arxiv.org/pdf/2601.03448",
      "github_links": [
        "https://github.com/gucci-j/l2t"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03448",
      "scraped_at": "2026-01-09T01:51:35.563270"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Pearmut: Human Evaluation of Translation Made Trivial",
    "paper_url": "https://huggingface.co/papers/2601.02933",
    "authors": [
      "Tom Kocmi",
      "Vil√©m Zouhar"
    ],
    "stars": "7",
    "details": {
      "title": "Pearmut: Human Evaluation of Translation Made Trivial",
      "abstract": "Happy to discuss how people human-evaluate multilingual tasks! üôÇ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02933",
      "pdf_url": "https://arxiv.org/pdf/2601.02933",
      "github_links": [
        "https://github.com/zouharvi/pearmut"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02933",
      "scraped_at": "2026-01-09T01:51:37.483381"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.03955",
    "authors": [
      "Ming Lu",
      "Kun Gai",
      "Huan Yang",
      "Cheng Da",
      "Xu Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03955",
      "pdf_url": "https://arxiv.org/pdf/2601.03955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03955",
      "scraped_at": "2026-01-09T01:51:39.413532"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
    "paper_url": "https://huggingface.co/papers/2601.03236",
    "authors": [
      "Bingzhe Li",
      "Guanpeng Li",
      "Yi Li",
      "Dongming Jiang"
    ],
    "stars": "0",
    "details": {
      "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
      "abstract": "This ia giid paper",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03236",
      "pdf_url": "https://arxiv.org/pdf/2601.03236",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03236",
      "scraped_at": "2026-01-09T01:51:41.306990"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.04090",
    "authors": [
      "Yuewen Ma",
      "Lin Ma",
      "Bangbang Yang",
      "Yuanbo Yang",
      "Jiaxin Huang"
    ],
    "stars": "34",
    "details": {
      "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
      "abstract": "We Introduce Gen3R ‚Äî create multi-quantity geometry with RGB from images. üì∑ Photorealistic Video üöÄ Accurate 3D Scene Geometry Arxiv: https://arxiv.org/abs/2601.04090 Project page: https://xdimlab.github.io/Gen3R/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04090",
      "pdf_url": "https://arxiv.org/pdf/2601.04090",
      "github_links": [
        "https://github.com/JaceyHuang/Gen3R"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04090",
      "scraped_at": "2026-01-09T01:51:43.101762"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "paper_url": "https://huggingface.co/papers/2601.00705",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
      "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00705",
      "pdf_url": "https://arxiv.org/pdf/2601.00705",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00705",
      "scraped_at": "2026-01-09T01:51:44.961750"
    },
    "scraped_date": "2026-01-09"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "64",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-10T01:47:38.996866"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "98",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-10T01:47:40.904445"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "12",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-10T01:47:42.783069"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-10T01:47:44.648079"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "8",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-10T01:47:46.604457"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "6",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-10T01:47:48.511757"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-10T01:47:50.402121"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-10T01:47:52.299582"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-10T01:47:54.355619"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-10T01:47:56.251754"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-10T01:47:58.153470"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-10T01:48:00.071087"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [
      "Meng Liu",
      "Qiancheng Xu",
      "Caiqi Zhang",
      "HongruCai",
      "dd101bb"
    ],
    "stars": "9",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-10T01:48:01.910045"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-10T01:48:03.769181"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-10T01:48:05.666518"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-10T01:48:07.549671"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [
      "Jing Ma",
      "Yuxuan Gu",
      "Shidong Cao",
      "Ziyang",
      "danielhzlin"
    ],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-10T01:48:09.387444"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-10T01:48:11.254706"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-10T01:48:13.102178"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-10T01:48:14.948337"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-10T01:48:16.843733"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-10T01:48:18.728989"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-10T01:48:20.557489"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-10T01:48:22.412221"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-10T01:48:24.291348"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-10T01:48:26.242615"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-10T01:48:28.093828"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "0",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-10T01:48:29.955705"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-10T01:48:31.814190"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-10T01:48:33.663145"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-10T01:48:35.486191"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-10T01:48:37.365709"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "1",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-10T01:48:39.220990"
    },
    "scraped_date": "2026-01-10"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "101",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-11T01:59:47.730652"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "99",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-11T01:59:49.925001"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "16",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-11T01:59:51.961752"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-11T01:59:54.099175"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "11",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-11T01:59:56.325159"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "11",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-11T01:59:58.427921"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-11T02:00:00.482812"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-11T02:00:02.805129"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-11T02:00:04.923362"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-11T02:00:06.975071"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-11T02:00:09.054428"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [
      "Meng Liu",
      "Qiancheng Xu",
      "Caiqi Zhang",
      "HongruCai",
      "dd101bb"
    ],
    "stars": "17",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-11T02:00:11.178154"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-11T02:00:13.255536"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-11T02:00:15.509738"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-11T02:00:17.713650"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-11T02:00:19.910578"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-11T02:00:21.943611"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-11T02:00:24.141593"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-11T02:00:26.124601"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-11T02:00:28.219747"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-11T02:00:30.411079"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-11T02:00:32.600123"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-11T02:00:34.631059"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-11T02:00:36.809912"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-11T02:00:38.871696"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-11T02:00:41.069566"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-11T02:00:43.140462"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-11T02:00:45.185323"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "3",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [
        "https://github.com/mbar0075/lupi-for-object-detection"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-11T02:00:47.504931"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-11T02:00:49.457446"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "1",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-11T02:00:51.730026"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-11T02:00:54.067066"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-11T02:00:56.329467"
    },
    "scraped_date": "2026-01-11"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "paper_url": "https://huggingface.co/papers/2601.05242",
    "authors": [],
    "stars": "126",
    "details": {
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "GDPO is a drop-in replacement for GRPO in verl and TRL ‚Äî only minor code changes needed. We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks. ‚è±Ô∏è Each run can be completed in ~1 hour on 8√óA100s, or ~2.5 hours on a single A100. üîÑ Switching from GRPO to GDPO is easy. üëâ Try it yourself: https://github.com/NVlabs/GDPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05242",
      "pdf_url": "https://arxiv.org/pdf/2601.05242",
      "github_links": [
        "https://github.com/NVlabs/GDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05242",
      "scraped_at": "2026-01-12T01:56:15.369905"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "paper_url": "https://huggingface.co/papers/2601.04890",
    "authors": [],
    "stars": "99",
    "details": {
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "abstract": "Building on the ŒºP multipliers applied in Falcon-H1 pretraining ( https://huggingface.co/papers/2507.22448 ), this work extends the idea to learnable matrix-, row-, and column-wise scaling. We show that the weight-norm equilibrium induced by weight decay and gradient noise is suboptimal, and that freeing these scale constraints yields consistent gains, generalizes ŒºP, and improves downstream performance with both Adam and Muon optimizers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04890",
      "pdf_url": "https://arxiv.org/pdf/2601.04890",
      "github_links": [
        "https://github.com/tiiuae/falcon-h1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04890",
      "scraped_at": "2026-01-12T01:56:17.350077"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "paper_url": "https://huggingface.co/papers/2601.05249",
    "authors": [
      "Chia-Che Chang",
      "Kuan-Lin Chen",
      "yulunliu",
      "NeilLeeNTU"
    ],
    "stars": "17",
    "details": {
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05249",
      "pdf_url": "https://arxiv.org/pdf/2601.05249",
      "github_links": [
        "https://github.com/BrianChen1120/RL-AWB"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05249",
      "scraped_at": "2026-01-12T01:56:19.261694"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "paper_url": "https://huggingface.co/papers/2601.05106",
    "authors": [
      "Furong Huang",
      "Zhaorun Chen",
      "Hanqing Zeng",
      "Nuoya Xiong",
      "zyhang1998"
    ],
    "stars": "0",
    "details": {
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LLMBoost: Make Large Language Models Stronger with Boosting (2025) SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning (2025) Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy (2025) W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search (2025) Escaping the Verifier: Learning to Reason via Demonstrations (2025) OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs (2025) Building Domain-Specific Small Language Models via Guided Data Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05106",
      "pdf_url": "https://arxiv.org/pdf/2601.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05106",
      "scraped_at": "2026-01-12T01:56:21.261190"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "paper_url": "https://huggingface.co/papers/2601.05175",
    "authors": [],
    "stars": "23",
    "details": {
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Rethinking Chain-of-Thought Reasoning for Videos (2025) LongVT: Incentivizing\"Thinking with Long Videos\"via Native Tool Calling (2025) More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models (2025) VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning (2025) Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning (2025) AdaTooler-V: Adaptive Tool-Use for Images and Videos (2025) Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05175",
      "pdf_url": "https://arxiv.org/pdf/2601.05175",
      "github_links": [
        "https://github.com/IVUL-KAUST/VideoAuto-R1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05175",
      "scraped_at": "2026-01-12T01:56:23.204161"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "paper_url": "https://huggingface.co/papers/2601.05167",
    "authors": [
      "Haolin Liu",
      "Jinyuan Li",
      "Tong Zheng",
      "shrango",
      "ChengsongHuang"
    ],
    "stars": "16",
    "details": {
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05167",
      "pdf_url": "https://arxiv.org/pdf/2601.05167",
      "github_links": [
        "https://github.com/Chengsong-Huang/RelayLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05167",
      "scraped_at": "2026-01-12T01:56:25.164574"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "paper_url": "https://huggingface.co/papers/2601.04767",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "abstract": "Abstract LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT 2 PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT 2 PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04767",
      "pdf_url": "https://arxiv.org/pdf/2601.04767",
      "github_links": [
        "https://github.com/zzfoutofspace/ATPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04767",
      "scraped_at": "2026-01-12T01:56:27.080831"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.05241",
    "authors": [
      "Jia-Zeng",
      "ZhaoyangLyu",
      "matthewmao",
      "wuzhi-hao",
      "HikariDawn"
    ],
    "stars": "13",
    "details": {
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The project webpage is at: https://robovip.github.io/RoboVIP/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05241",
      "pdf_url": "https://arxiv.org/pdf/2601.05241",
      "github_links": [
        "https://github.com/RoboVIP/RoboVIP_VDM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05241",
      "scraped_at": "2026-01-12T01:56:29.109143"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21815",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "abstract": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21815",
      "pdf_url": "https://arxiv.org/pdf/2512.21815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21815",
      "scraped_at": "2026-01-12T01:56:31.065656"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "paper_url": "https://huggingface.co/papers/2601.05138",
    "authors": [
      "Xiaoyu Li",
      "Wenbo Hu",
      "Minghao Yin",
      "yanweifuture",
      "sxzheng"
    ],
    "stars": "0",
    "details": {
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05138",
      "pdf_url": "https://arxiv.org/pdf/2601.05138",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05138",
      "scraped_at": "2026-01-12T01:56:33.013335"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Agent-as-a-Judge",
    "paper_url": "https://huggingface.co/papers/2601.05111",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Agent-as-a-Judge",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) The Path Ahead for Agentic AI: Challenges and Opportunities (2026) Step-DeepResearch Technical Report (2025) AI Agent Systems: Architectures, Applications, and Evaluation (2026) ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment (2025) Environment Scaling for Interactive Agentic Experience Collection: A Survey (2025) Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05111",
      "pdf_url": "https://arxiv.org/pdf/2601.05111",
      "github_links": [
        "https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05111",
      "scraped_at": "2026-01-12T01:56:34.977612"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "paper_url": "https://huggingface.co/papers/2601.03425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "abstract": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03425",
      "pdf_url": "https://arxiv.org/pdf/2601.03425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03425",
      "scraped_at": "2026-01-12T01:56:36.914048"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.03559",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "abstract": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03559",
      "pdf_url": "https://arxiv.org/pdf/2601.03559",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03559",
      "scraped_at": "2026-01-12T01:56:38.788826"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Plenoptic Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.05239",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Plenoptic Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation (2025) Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation (2025) StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation (2025) SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time (2025) PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention (2025) CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization (2025) Light-X: Generative 4D Video Rendering with Camera and Illumination Control (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05239",
      "pdf_url": "https://arxiv.org/pdf/2601.05239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05239",
      "scraped_at": "2026-01-12T01:56:40.809081"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05172",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05172",
      "pdf_url": "https://arxiv.org/pdf/2601.05172",
      "github_links": [
        "https://github.com/ziplab/CoV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05172",
      "scraped_at": "2026-01-12T01:56:42.666912"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "paper_url": "https://huggingface.co/papers/2601.03111",
    "authors": [
      "Xuefeng Li",
      "Weixun Wang",
      "Yanan Wu",
      "Zhen Huang",
      "Yiyuan Li"
    ],
    "stars": "0",
    "details": {
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "abstract": "This work discusses the potential of lifting broader reasoning ability by learning from one high-quality sample. In polymath learning, the quality of samples can be selected through the lens of salient math skills and categories. The model learned from the polymath sample outperformances the one learned from dataset thousand times larger in multidisciplinary reasoning tasks, indicating the significance of deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03111",
      "pdf_url": "https://arxiv.org/pdf/2601.03111",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03111",
      "scraped_at": "2026-01-12T01:56:44.585372"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2601.05124",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "eternaldolphin",
      "Zhiminli",
      "hrz2000"
    ],
    "stars": "1",
    "details": {
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "This paper introduces Re-Align, a unified framework for in-context image generation and editing that bridges the gap between multimodal understanding and image synthesis. Re-Align employs a structured In-Context Chain-of-Thought (IC-CoT) to explicitly separate semantic guidance and reference association, reducing ambiguity in image‚Äìtext interleaved prompts. It further applies reinforcement learning with a surrogate alignment reward to improve consistency between reasoning and generated images. Extensive experiments show that Re-Align outperforms prior methods on both in-context image generation and editing tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05124",
      "pdf_url": "https://arxiv.org/pdf/2601.05124",
      "github_links": [
        "https://github.com/hrz2000/realign"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05124",
      "scraped_at": "2026-01-12T01:56:46.426451"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "paper_url": "https://huggingface.co/papers/2601.05163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05163",
      "pdf_url": "https://arxiv.org/pdf/2601.05163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05163",
      "scraped_at": "2026-01-12T01:56:48.265902"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2601.04342",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
      "abstract": "üöÄ Introducing PyramidalWan! Our paper presents a novel pipeline to convert pretrained video diffusion models (like Wan2.1-1.3B) into efficient pyramidal ones via low-cost finetuning. Key Innovations: Efficiency via Hierarchy: We restructure the diffusion process into three spatiotemporal stages, processing high noise at lower resolutions to significantly reduce inference costs. Theoretical Generalization: We extended resolution transitions to a broader class of upsampling/downsampling functions based on orthogonal transforms. Step Distillation: A systematic study of distillation techniques for pyramidal setups, including the first successful training of Pyramidal Patchification models for few-step generation. Key Insights & Results: Near-Original Quality: Achieves video quality comparable to the original Wan model while requiring significantly less compute. Superior Motion: Our recommended recipe, PyramidalWan-DMD-PT*, provides consistent motion and fills the gap for high-performing few-step inference. Artifact Reduction: Unlike training-free acceleration methods (e.g., Jenga), our approach avoids severe scene and motion artifacts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04342",
      "pdf_url": "https://arxiv.org/pdf/2601.04342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04342",
      "scraped_at": "2026-01-12T01:56:50.213336"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.05149",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Multi-Scale Local Speculative Decoding (MuLo-SD), a new framework to supercharge Autoregressive (AR) image generation! By combining multi-resolution drafting with spatially informed verification, we achieve substantial speedups of up to 1.7x while maintaining high perceptual quality and semantic alignment. The Core Idea: Unlike standard methods that use raster-scan rejection, MuLo-SD exploits the spatial structure of images. We propose candidate tokens using a low-resolution drafter and a learned up-sampler, which are then verified in parallel by a high-resolution target model. Key Innovation: Local Verification üîç Crucially, we introduced a local rejection and resampling mechanism. Instead of discarding every token after a single error, we correct errors by focusing only on the spatial neighborhoods of rejected tokens, significantly boosting efficiency. Results at a Glance: ‚úÖ Outperforms strong baselines like EAGLE-2 and LANTERN. ‚úÖ Consistently delivers greater speedups across 512p and 1024p resolutions. ‚úÖ Integrates seamlessly with unified Multimodal LLMs. üîó https://qualcomm-ai-research.github.io/mulo-sd-webpage",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05149",
      "pdf_url": "https://arxiv.org/pdf/2601.05149",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05149",
      "scraped_at": "2026-01-12T01:56:52.201583"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "paper_url": "https://huggingface.co/papers/2601.04792",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
      "abstract": "We tackle the challenge of quadratic complexity in video generation with a novel Recurrent Hybrid Attention mechanism. By combining the fidelity of softmax attention for local dependencies with the efficiency of linear attention globally, we enable high-quality modeling with linear scaling. Constant Memory Usage: Our chunk-wise recurrent reformulation allows for the generation of arbitrarily long videos. Massive Training Efficiency: Using a two-stage distillation pipeline, we reduced training costs by two orders of magnitude to just ~160 GPU hours. SOTA Performance: Validated on VBench and VBench-2.0, ReHyAt achieves state-of-the-art quality while unlocking practical on-device video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04792",
      "pdf_url": "https://arxiv.org/pdf/2601.04792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04792",
      "scraped_at": "2026-01-12T01:56:54.063156"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2601.04754",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
      "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. We introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. For more information, please check out our project page: https://chiou1203.github.io/ProFuse/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04754",
      "pdf_url": "https://arxiv.org/pdf/2601.04754",
      "github_links": [
        "https://github.com/chiou1203/ProFuse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04754",
      "scraped_at": "2026-01-12T01:56:55.989473"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "paper_url": "https://huggingface.co/papers/2601.04575",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "abstract": "We introduce Pixels2Play (P2P), an open-source generalist agent designed for real-time control across diverse 3D video games on consumer-grade GPUs. Built on an efficient, decoder-only transformer architecture that predicts keyboard and mouse actions from raw pixel inputs , the model is trained on a massive new dataset of over 8,300 hours of high-quality, text-annotated human gameplay. Beyond achieving human-level competence in commercial environments like DOOM and Roblox , we systematically investigate the scaling laws of behavior cloning, demonstrating that increasing model and data scale significantly improves causal reasoning and mitigates the \"causal confusion\" often inherent in imitation learning. To accelerate research in generalist game AI, we are releasing the full training recipe, model checkpoints, and our extensive gameplay dataset under an open license",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04575",
      "pdf_url": "https://arxiv.org/pdf/2601.04575",
      "github_links": [
        "https://github.com/elefant-ai/open-p2p"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04575",
      "scraped_at": "2026-01-12T01:56:57.907638"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "paper_url": "https://huggingface.co/papers/2601.04300",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision (2025) Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models (2026) PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier (2025) Multi-dimensional Preference Alignment by Conditioning Reward Itself (2025) Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning (2025) Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning (2026) BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04300",
      "pdf_url": "https://arxiv.org/pdf/2601.04300",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04300",
      "scraped_at": "2026-01-12T01:56:59.771628"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "paper_url": "https://huggingface.co/papers/2601.03362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "abstract": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03362",
      "pdf_url": "https://arxiv.org/pdf/2601.03362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03362",
      "scraped_at": "2026-01-12T01:57:01.869712"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "paper_url": "https://huggingface.co/papers/2512.24160",
    "authors": [
      "YuanFu Yang",
      "ZhenQi Chen",
      "water-fountain"
    ],
    "stars": "2",
    "details": {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24160",
      "pdf_url": "https://arxiv.org/pdf/2512.24160",
      "github_links": [
        "https://github.com/NinaNeon/IMDD-1M-Towards-Open-Vocabulary-Industrial-Defect-"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24160",
      "scraped_at": "2026-01-12T01:57:03.736777"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "paper_url": "https://huggingface.co/papers/2512.23628",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "abstract": "Our code is available at https://github.com/zlab-princeton/3d-gen-mem.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.23628",
      "pdf_url": "https://arxiv.org/pdf/2512.23628",
      "github_links": [
        "https://github.com/zlab-princeton/3d-gen-mem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.23628",
      "scraped_at": "2026-01-12T01:57:05.671962"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "paper_url": "https://huggingface.co/papers/2601.04620",
    "authors": [
      "Di Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04620",
      "pdf_url": "https://arxiv.org/pdf/2601.04620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04620",
      "scraped_at": "2026-01-12T01:57:07.412767"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.02702",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Tal August",
      "Priyanka Kargupta",
      "Shuhaib Mehri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
      "abstract": "Current long-term conversation benchmarks focus on recall. But this ignores key skills like recognizing what user information is valuable & leveraging it to improve future interactions. In our work, we present MultiSessionCollab to evaluate agents in a multi-session collaboration environment. Additionally, we use memory to help agents learn user preferences and improve collaboration over time.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02702",
      "pdf_url": "https://arxiv.org/pdf/2601.02702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02702",
      "scraped_at": "2026-01-12T01:57:09.256703"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "paper_url": "https://huggingface.co/papers/2601.02016",
    "authors": [
      "Carl James Debono",
      "Matthew Montebello",
      "Gabriel Hili",
      "Dylan Seychell",
      "mbar0075"
    ],
    "stars": "4",
    "details": {
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02016",
      "pdf_url": "https://arxiv.org/pdf/2601.02016",
      "github_links": [
        "https://github.com/mbar0075/lupi-for-object-detection"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02016",
      "scraped_at": "2026-01-12T01:57:11.141085"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.04233",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
      "abstract": "LEMAS: A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models LEMAS is a large-scale extensible multilingual audio suite, providing multilingual speech corpus (LEMAS-Dataset) with word-level timestamps, covering over 150,000 hours across 10 major languages. Built with a rigorous alignment and confidence-based filtering pipeline, LEMAS supports diverse generative paradigms including zero-shot multilingual synthesis (LEMAS-TTS) and seamless speech editing (LEMAS-Edit). More technical details can be found in our technical report: https://arxiv.org/abs/2601.04233",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04233",
      "pdf_url": "https://arxiv.org/pdf/2601.04233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04233",
      "scraped_at": "2026-01-12T01:57:13.156228"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05125",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "We usually train VLMs on visual synthetic data that we (as humans) label as photorealistic. We argue that this is an anthropocentric perspective imposed to a model that might not synthetize visual information as we do. VERSE helps to visualize latent space and overlay visual features to detect poor-performance regions and take action to include better-suited training sets to boost model performance. You can explore more here: Code: https://github.com/nachoDRT/MERIT-Dataset Hugging Face Space: https://huggingface.co/spaces/de-Rodrigo/Embeddings Thanks!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05125",
      "pdf_url": "https://arxiv.org/pdf/2601.05125",
      "github_links": [
        "https://github.com/nachoDRT/VrDU-Doctor"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05125",
      "scraped_at": "2026-01-12T01:57:15.060473"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "paper_url": "https://huggingface.co/papers/2601.01887",
    "authors": [
      "Jian Liu",
      "Jian Lou",
      "Kejia Chen",
      "Jiawen Zhang",
      "ttttonyhe"
    ],
    "stars": "0",
    "details": {
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost . Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01887",
      "pdf_url": "https://arxiv.org/pdf/2601.01887",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01887",
      "scraped_at": "2026-01-12T01:57:17.088547"
    },
    "scraped_date": "2026-01-12"
  },
  {
    "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
    "paper_url": "https://huggingface.co/papers/2601.05432",
    "authors": [],
    "stars": "107",
    "details": {
      "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
      "abstract": "Demo video",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05432",
      "pdf_url": "https://arxiv.org/pdf/2601.05432",
      "github_links": [
        "https://github.com/AMAP-ML/Thinking-with-Map"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05432",
      "scraped_at": "2026-01-13T01:48:04.298623"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
    "paper_url": "https://huggingface.co/papers/2601.03017",
    "authors": [
      "Huajian Xin",
      "Hui Shen",
      "Yunta Hsieh",
      "Qi Han",
      "Jing Xiong"
    ],
    "stars": "0",
    "details": {
      "title": "MMFormalizer: Multimodal Autoformalization in the Wild",
      "abstract": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03017",
      "pdf_url": "https://arxiv.org/pdf/2601.03017",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03017",
      "scraped_at": "2026-01-13T01:48:06.184101"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "paper_url": "https://huggingface.co/papers/2601.03319",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
      "abstract": "Project Page: https://c4ricaturegs.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03319",
      "pdf_url": "https://arxiv.org/pdf/2601.03319",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03319",
      "scraped_at": "2026-01-13T01:48:08.069844"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06002",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "abstract": "Glad to share our recent exploratory project: üß™ Title: The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning üåê arXiv: 2601.06002 ‚Äã üßê Why revisit Long CoT? Recent work often focuses on ‚Äúmaking CoT longer,‚Äù but longer traces are more likely to derail‚Äîe.g., drifting off-track, breaking logical continuity, or amplifying hallucinations‚Äîespecially when attempting to cold-start genuine long-horizon reasoning from a standard instruction-tuned model. ‚Äã A key observation is that many trajectories that merely look like long reasoning (e.g., distilling from randomly sampled ICL demonstrations, or using human-written long step-by-step solutions) are not behaviorally stable, and models frequently fail to learn robustly from them. ‚Äã üò≠  Why imitation often fails ‚ÄúLong‚Äù human CoT is not necessarily effective: Fine-tuning on human-written long CoT does not reliably reproduce the gains achieved by distilling from a strong reasoning model. Distill from Weak instruct model + random ICL demonstrations largely fails: Using randomly chosen 1-shot ICL examples to ‚Äúfake‚Äù long reasoning for distillation leads to significant degradation, suggesting that superficial formatting is insufficient. ‚Äã- Keywords are not the driver: Replacing surface tokens (e.g., ‚Äúwait‚Äù) while preserving the underlying reasoning trajectory and behavioral pattern yields similar performance, indicating that SFT primarily learns structure/behavior rather than prompt-specific keywords. ‚Äã üîç  Early evidence: Long CoT has stable ‚Äústructural fingerprints‚Äù We observe a stable behavioral transfer graph: across different strong reasoning models and tasks, the induced distributional characteristics appear highly consistent. ‚Äã- In semantic space, we see ‚Äúlinking‚Äìfolding‚Äù patterns: deep reasoning tends to form locally dense structures; self-reflection tends to create backward links for validation/correction; and exploration tends to form weaker cross-cluster connections. ‚Äã üí° Core hypothesis: effective Long CoT as a ‚Äúmolecular structure‚Äù High-quality Long CoT is not merely a linear chain; it is stabilized by three interaction types‚Äîanalogous to chemical bonds‚Äîthat organize and constrain reasoning trajectories: ‚Äã- Deep Reasoning (covalent-bond-like): forms the main reasoning backbone; if it breaks, the solution collapses. ‚Äã- Self-Reflection (hydrogen-bond-like): folds later steps back to earlier ones to verify assumptions, detect errors, and correct the path. ‚Äã- Self-Exploration (van der Waals-like): weak but important cross-domain probing that broadens coverage and discovers alternative routes. ‚Äã An additional observation is that the Gibbs‚ÄìBoltzmann energy formulation is closely aligned with the attention formulation; hence, the ‚Äúenergy distributions‚Äù of different bonds can be estimated directly from attention, exhibiting a stable ordering reminiscent of real chemical bond energies. ‚Äã üçé ‚ÄúSemantic isomers‚Äù of Long CoT For the same problem, trajectories can be semantically close yet differ in the distribution and transitions of bonds, yielding distinct ‚Äúisomers‚Äù with dramatically different trainability and downstream performance. ‚Äã- Two isomers that appear similar may still be incompatible: mixing them during training can trigger structural conflicts and degrade performance. ‚Äã- ICL is not inherently ineffective; it helps when demonstrations are selected such that their structural distribution aligns with the target high-quality isomer. ‚Äã üîß Solution: MOLE-SYN We propose MOLE-SYN: first estimate a behavioral transfer graph from a strong reasoning model, then use it to guide a pure instruct LLM to synthesize Long CoT trajectories. ‚Äã- Empirically, distilling Qwen-2.5 with MOLE-SYN‚Äìgenerated trajectories can approach the effectiveness of distillation from QwQ. ‚Äã- This initialization also exhibits strong RL potential: it yields more stable RL training curves and sustained improvement headroom. Finally, different behaviors have distinct global effects: deep reasoning makes the core logic more compact, self-reflection increases overall ‚Äúfolding‚Äù tightness, and self-exploration expands the reachable search space. ‚Äã üëÄ A practical implication is that when CoT is heavily summarized or compressed, the ‚Äúmolecular structure‚Äù distribution can be destroyed, and distilled models may underperform even the original teacher. ‚Äã If a prior viewpoint treated CoT behaviors as nodes, this work reframes them as edges that link logical states: the training target may not be ‚Äúlonger answers,‚Äù but a more stable reasoning skeleton controlled by structured reasoning behaviors. ‚Äã",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06002",
      "pdf_url": "https://arxiv.org/pdf/2601.06002",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06002",
      "scraped_at": "2026-01-13T01:48:09.969605"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "paper_url": "https://huggingface.co/papers/2601.06021",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
      "abstract": "Code: https://github.com/THUDM/CaRR Data: https://huggingface.co/datasets/THU-KEG/CaRR-DeepDive",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06021",
      "pdf_url": "https://arxiv.org/pdf/2601.06021",
      "github_links": [
        "https://github.com/THUDM/CaRR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06021",
      "scraped_at": "2026-01-13T01:48:11.869324"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
    "paper_url": "https://huggingface.co/papers/2601.05808",
    "authors": [
      "Zhicheng Dou",
      "Yutao Zhu",
      "Haofei Chang",
      "Xiaoshuai Song",
      "dongguanting"
    ],
    "stars": "0",
    "details": {
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "abstract": "Code: https://github.com/RUC-NLPIR/EnvScaler Data & Model: https://huggingface.co/collections/XXHStudyHard/envscaler",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05808",
      "pdf_url": "https://arxiv.org/pdf/2601.05808",
      "github_links": [
        "https://github.com/RUC-NLPIR/EnvScaler"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05808",
      "scraped_at": "2026-01-13T01:48:13.888446"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
    "paper_url": "https://huggingface.co/papers/2601.04720",
    "authors": [],
    "stars": "620",
    "details": {
      "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
      "abstract": "üöÄ Introducing Qwen3-VL-Embedding and Qwen3-VL-Reranker ‚Äì advancing the state of the art in multimodal retrieval and cross-modal understanding! ‚ú® Highlights: ‚úÖ Built upon the robust Qwen3-VL foundation model ‚úÖ Processes text, images, screenshots, videos, and mixed modality inputs ‚úÖ Supports 30+ languages ‚úÖ Achieves state-of-the-art performance on multimodal retrieval benchmarks ‚úÖ Open source and available on Hugging Face, GitHub, and ModelScope ‚úÖ API deployment on Alibaba Cloud coming soon! üéØ Two-stage retrieval architecture: üìä Embedding Model ‚Äì generates semantically rich vector representations in a unified embedding space üéØ Reranker Model ‚Äì computes fine-grained relevance scores for enhanced retrieval accuracy üîç Key application scenarios: Image-text retrieval, video search, multimodal RAG, visual question answering, multimodal content clustering, multilingual visual search, and more! üåü Developer-friendly capabilities: ‚Ä¢ Configurable embedding dimensions ‚Ä¢ Task-specific instruction customization ‚Ä¢ Embedding quantization support for efficient and cost-effective downstream deployment Hugging FaceÔºö https://huggingface.co/collections/Qwen/qwen3-vl-embedding https://huggingface.co/collections/Qwen/qwen3-vl-reranker Github: https://github.com/QwenLM/Qwen3-VL-Embedding Blog: https://qwen.ai/blog?id=qwen3-vl-embedding Tech Report: https://www.arxiv.org/abs/2601.04720",
      "arxiv_page_url": "https://www.arxiv.org/abs/2601.04720",
      "pdf_url": "https://arxiv.org/pdf/2601.04720",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-VL-Embedding"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04720",
      "scraped_at": "2026-01-13T01:48:15.778962"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "paper_url": "https://huggingface.co/papers/2601.05930",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Can We Predict Before Executing Machine Learning Agents?",
      "abstract": "We replace slow trial-and-error in scientific agents with learned execution prediction, enabling FOREAGENT to think before it runs and achieve 6√ó faster and better scientific discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05930",
      "pdf_url": "https://arxiv.org/pdf/2601.05930",
      "github_links": [
        "https://github.com/zjunlp/predict-before-execute"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05930",
      "scraped_at": "2026-01-13T01:48:17.633216"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
    "paper_url": "https://huggingface.co/papers/2601.05882",
    "authors": [
      "Nikolaos Aletras",
      "Constantinos Karouzos",
      "XingweiT"
    ],
    "stars": "0",
    "details": {
      "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
      "abstract": "Our paper presents a systematic study of preference-optimization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. We found: The adaptation strategy is more influential than the alignment objective. We identify that synthetic supervision is a double-edged sword. While pseudo-labeling yields the highest target-domain win rates, it induces severe mode collapse. This diversity tax results in models that are highly reliable but linguistically monotonous, mirroring the latent templates of the teacher model. Our findings suggest a deployment recommendation: use pseudo-labeling for high-stakes and constrained tasks where reliability is paramount, but favor mixed-domain SFT and online RL for applications requiring creative or varied linguistic expression.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05882",
      "pdf_url": "https://arxiv.org/pdf/2601.05882",
      "github_links": [
        "https://github.com/ckarouzos/prefadap"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05882",
      "scraped_at": "2026-01-13T01:48:19.521997"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "paper_url": "https://huggingface.co/papers/2601.04786",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
      "abstract": "We‚Äôre introducing AgentOCR, a new way to scale LLM agents by reimagining long interaction histories as compact rendered images, leveraging the higher information density of visual tokens to curb exploding context costs. To make long-horizon rollouts practical, we add segment optical caching, splitting history into hashable segments and caching the visuals, so agents avoid redundant re-rendering as trajectories grow.  We go beyond fixed compression with agentic self-compression: the agent actively emits a compression rate and is trained with a compression-aware reward to balance task success against token efficiency. Across ALFWorld and search-based QA, AgentOCR keeps >95% of text-agent performance while cutting token use by >50% average and ~80% in peak, and our analysis shows up to a 20√ó rendering speedup thanks to our segment optical caching",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04786",
      "pdf_url": "https://arxiv.org/pdf/2601.04786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04786",
      "scraped_at": "2026-01-13T01:48:21.395496"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "paper_url": "https://huggingface.co/papers/2601.05966",
    "authors": [
      "Yu Sun",
      "Shuohuan Wang",
      "Xiaoxiong Liu",
      "Longbin Ji",
      "sjy1203"
    ],
    "stars": "0",
    "details": {
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "abstract": "VideoAR presents a scalable autoregressive video-generation framework that combines next-frame scale prediction with a 3D multi-scale tokenizer to improve temporal coherence and efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05966",
      "pdf_url": "https://arxiv.org/pdf/2601.05966",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05966",
      "scraped_at": "2026-01-13T01:48:23.249628"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "paper_url": "https://huggingface.co/papers/2601.05905",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "abstract": "We show that many LLM ‚Äúbeliefs‚Äù that look confident collapse under small context changes, and propose Neighbor-Consistency Belief (NCB) and Structure-Aware Training to measure and train models to keep their knowledge stable and robust under such interference.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05905",
      "pdf_url": "https://arxiv.org/pdf/2601.05905",
      "github_links": [
        "https://github.com/zjunlp/belief"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05905",
      "scraped_at": "2026-01-13T01:48:25.096155"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "paper_url": "https://huggingface.co/papers/2601.05848",
    "authors": [
      "Evan Luo",
      "Zitian Tang",
      "Yinghua Zhou",
      "dakshces",
      "nate-gillman"
    ],
    "stars": "0",
    "details": {
      "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
      "abstract": "Goal Force trains a physics-grounded video model to follow explicit force-directed goals, achieving zero-shot planning in real-world tasks by implicit neural physics simulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05848",
      "pdf_url": "https://arxiv.org/pdf/2601.05848",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05848",
      "scraped_at": "2026-01-13T01:48:27.018745"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "paper_url": "https://huggingface.co/papers/2601.05573",
    "authors": [
      "Tianyu Pang",
      "Jialei Wang",
      "Jiayang Xu",
      "Zehan Wang",
      "Viglong"
    ],
    "stars": "82",
    "details": {
      "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
      "abstract": "Code: https://github.com/SpatialVision/Orient-Anything-V2 Demo Space: https://huggingface.co/spaces/Viglong/Orient-Anything-V2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05573",
      "pdf_url": "https://arxiv.org/pdf/2601.05573",
      "github_links": [
        "https://github.com/SpatialVision/Orient-Anything-V2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05573",
      "scraped_at": "2026-01-13T01:48:28.874972"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
    "paper_url": "https://huggingface.co/papers/2601.05403",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
      "abstract": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (MFMD). In this work, we propose MFMD-Scen, a comprehensive benchmark for evaluating behavioral biases of LLMs in MFMD across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, MFMD-Scen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05403",
      "pdf_url": "https://arxiv.org/pdf/2601.05403",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05403",
      "scraped_at": "2026-01-13T01:48:30.700256"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "AnyDepth: Depth Estimation Made Easy",
    "paper_url": "https://huggingface.co/papers/2601.02760",
    "authors": [],
    "stars": "63",
    "details": {
      "title": "AnyDepth: Depth Estimation Made Easy",
      "abstract": "https://aigeeksgroup.github.io/AnyDepth",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02760",
      "pdf_url": "https://arxiv.org/pdf/2601.02760",
      "github_links": [
        "https://github.com/AIGeeksGroup/AnyDepth"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02760",
      "scraped_at": "2026-01-13T01:48:32.573488"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "paper_url": "https://huggingface.co/papers/2601.04888",
    "authors": [
      "Guanting Dong",
      "douzc",
      "vvv111222"
    ],
    "stars": "11",
    "details": {
      "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
      "abstract": "Some of the observations founded are :- i. Dual Level Credit Assessment This mechanism provides a comprehensive evaluation of query quality through both rule-based and model-based assessments. It allows for fine-grained supervision, helping to identify not just redundancy but also the usefulness of each query in the context of the search process. ii. Process Reward Mechanism The introduction of process rewards as a guiding signal for training search agents is a novel approach. It shifts the focus from solely final outcomes to the quality of intermediate queries, addressing a significant gap in existing methods that often overlook this aspect. iii. Query Refinement Strategy The framework employs a systematic query refinement process that identifies low quality queries and generates improved versions. This iterative refinement enhances the effectiveness of search trajectories, allowing agents to adaptively improve their queries based on feedback. iv. Three Stage Curriculum Learning Framework SmartSearch introduces a structured curriculum learning approach that progresses from imitation to alignment and finally to generalization. This staged learning process enables search agents to internalize query quality improvement progressively, enhancing their overall performance. v. Empirical Validation Across Diverse Benchmarks The paper presents extensive experimental results demonstrating SmartSearch's superior performance across multiple challenging knowledge-intensive tasks and web exploration scenarios. This empirical validation highlights the framework's robustness and effectiveness in real-world applications, showcasing its potential impact on future research in search agents and information retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04888",
      "pdf_url": "https://arxiv.org/pdf/2601.04888",
      "github_links": [
        "https://github.com/MYVAE/SmartSearch?tab=readme-ov-file"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04888",
      "scraped_at": "2026-01-13T01:48:34.423674"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Over-Searching in Search-Augmented Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.05503",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "abstract": "Systematically analyzes over-search in search-augmented LLMs, showing when retrieval helps or hurts, introducing Tokens Per Correctness and mitigation strategies.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05503",
      "pdf_url": "https://arxiv.org/pdf/2601.05503",
      "github_links": [
        "https://github.com/ruoyuxie/OversearchQA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05503",
      "scraped_at": "2026-01-13T01:48:36.245536"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.04823",
    "authors": [
      "Linqi Song",
      "Huacan Wang",
      "Ronghao Chen",
      "Guanzhi Deng",
      "liboaccn"
    ],
    "stars": "0",
    "details": {
      "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
      "abstract": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04823",
      "pdf_url": "https://arxiv.org/pdf/2601.04823",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04823",
      "scraped_at": "2026-01-13T01:48:38.107896"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.04726",
    "authors": [
      "Zhicheng Dou",
      "Yutao Zhu",
      "Jiejun Tan",
      "Jiongnan Liu",
      "namespace-ERI"
    ],
    "stars": "0",
    "details": {
      "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
      "abstract": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04726",
      "pdf_url": "https://arxiv.org/pdf/2601.04726",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04726",
      "scraped_at": "2026-01-13T01:48:39.978598"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
    "paper_url": "https://huggingface.co/papers/2601.05637",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A Reason-then-Describe Instruction Interpreter for Controllable Video Generation (2025) EVE: A Generator-Verifier System for Generative Policies (2025) Eliciting Behaviors in Multi-Turn Conversations (2025) SkillWrapper: Generative Predicate Invention for Skill Abstraction (2025) From Word to World: Can Large Language Models be Implicit Text-based World Models? (2025) SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models (2025) Propose, Solve, Verify: Self-Play Through Formal Verification (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05637",
      "pdf_url": "https://arxiv.org/pdf/2601.05637",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05637",
      "scraped_at": "2026-01-13T01:48:41.784741"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
    "paper_url": "https://huggingface.co/papers/2601.04544",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration",
      "abstract": "Code: https://github.com/Tencent/TCAndon-Router",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04544",
      "pdf_url": "https://arxiv.org/pdf/2601.04544",
      "github_links": [
        "https://github.com/kyegomez/awesome-multi-agent-papers",
        "https://github.com/Tencent/TCAndon-Router"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04544",
      "scraped_at": "2026-01-13T01:48:43.634411"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "paper_url": "https://huggingface.co/papers/2601.05960",
    "authors": [
      "vicgalle"
    ],
    "stars": "1",
    "details": {
      "title": "Distilling Feedback into Memory-as-a-Tool",
      "abstract": "Code: https://github.com/vicgalle/feedback-memory-as-a-tool Data: https://huggingface.co/datasets/vicgalle/rubric-feedback-bench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05960",
      "pdf_url": "https://arxiv.org/pdf/2601.05960",
      "github_links": [
        "https://github.com/vicgalle/feedback-memory-as-a-tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05960",
      "scraped_at": "2026-01-13T01:48:45.433282"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "paper_url": "https://huggingface.co/papers/2601.05899",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
      "abstract": "Some of the observations are :- i. TowerMind is a lightweight RTS-style benchmark for LLM agents It introduces a tower defense based environment that preserves long term planning and decision making challenges of RTS games, while requiring very low computational resources compared to StarCraft II based benchmarks. ii. Multimodal observations enable broader LLM evaluation TowerMind supports pixel-based, textual (JSON), and structured state observations, making it suitable for evaluating language-only and vision-language models under the same environment. iii. Hallucination is explicitly measured via action validity Beyond performance score, the benchmark introduces valid action rate to quantify hallucinations. i.e. actions that violate game rules or state constraints allowing simultaneous evaluation of capability and reliability. iv. LLMs significantly underperform human experts Even the best-performing models (e.g. GPT-4.1, Claude 3.7 Sonnet) show a large gap from human experts, especially on harder levels, revealing weaknesses in planning validation, multifinality, and efficient action use. v. TowerMind is challenging for both LLMs and RL agents Classic RL algorithms (Ape-X DQN, PPO) also fail to reach human level performance, confirming TowerMind as a non-trivial benchmark that complements existing LLM and RL evaluation environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05899",
      "pdf_url": "https://arxiv.org/pdf/2601.05899",
      "github_links": [
        "https://github.com/tb6147877/TowerMind"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05899",
      "scraped_at": "2026-01-13T01:48:47.327914"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
    "paper_url": "https://huggingface.co/papers/2601.05851",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
      "abstract": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05851",
      "pdf_url": "https://arxiv.org/pdf/2601.05851",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05851",
      "scraped_at": "2026-01-13T01:48:49.142532"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
    "paper_url": "https://huggingface.co/papers/2601.05741",
    "authors": [
      "Marco Huber",
      "Jan Niklas Kolf",
      "Tahar Chettaoui",
      "Eduarda Caldeira",
      "gurayozgur"
    ],
    "stars": "3",
    "details": {
      "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
      "abstract": "https://github.com/gurayozgur/ViTNT-FIQA",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05741",
      "pdf_url": "https://arxiv.org/pdf/2601.05741",
      "github_links": [
        "https://github.com/gurayozgur/ViTNT-FIQA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05741",
      "scraped_at": "2026-01-13T01:48:51.076876"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
    "paper_url": "https://huggingface.co/papers/2601.05870",
    "authors": [
      "Zhuoyue Chen",
      "Long Li",
      "Yue Zhu",
      "Hongchen Luo",
      "Huilin Deng"
    ],
    "stars": "0",
    "details": {
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ReLaX: Reasoning with Latent Exploration for Large Reasoning Models (2025) Multi-Path Collaborative Reasoning via Reinforcement Learning (2025) Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies (2025) ESPO: Entropy Importance Sampling Policy Optimization (2025) Diversity or Precision? A Deep Dive into Next Token Prediction (2025) SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization (2025) Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05870",
      "pdf_url": "https://arxiv.org/pdf/2601.05870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05870",
      "scraped_at": "2026-01-13T01:48:52.895587"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
    "paper_url": "https://huggingface.co/papers/2601.05699",
    "authors": [
      "Jesujoba Oluwadara Alabi",
      "Israel Abebe Azime",
      "Emilio Villa-Cueva",
      "Srija Anand",
      "Atnafu"
    ],
    "stars": "0",
    "details": {
      "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG (2025) Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries (2025) HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples (2025) Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles (2025) IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages (2025) See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models (2025) Multilingual VLM Training: Adapting an English-Trained VLM to French (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05699",
      "pdf_url": "https://arxiv.org/pdf/2601.05699",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05699",
      "scraped_at": "2026-01-13T01:48:54.752545"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
    "paper_url": "https://huggingface.co/papers/2601.05376",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "abstract": "This paper investigates how \"persona conditioning\" (e.g., instructing an LLM to act as a specific medical professional) impacts clinical decision-making. The authors challenge the assumption that assigning a medical persona consistently improves accuracy or safety, labeling this inconsistency the \"Persona Paradox.\" Key Insights: Non-Monotonic Effects: Assigning a medical persona (like an Emergency Department physician) does not always improve performance. It acts as a behavioral prior that can help in some contexts but hurt in others. The Context Gap: Medical personas improved accuracy and calibration by up to 20% in critical-care tasks (triage) but degraded performance by a similar margin in primary-care settings. Interaction Styles: Adding styles such as \"bold\" or \"cautious\" changes the model‚Äôs risk propensity, but these effects vary widely across base models. The Alignment Gap: While \"LLM judges\" preferred medical personas for safety-critical cases, human clinicians were much more skeptical. Human experts showed low confidence in the AI's reasoning quality in 95.9% of cases, despite moderate agreement on safety compliance. Conclusion The study concludes that personas are not \"expertise switches\" but rather priors that introduce context-dependent trade-offs. Relying on personas for clinical safety is risky because they do not provide a universal guarantee of better judgment. Personas should be used with caution in high-stakes medicine, as they can inadvertently trigger biases or performance drops depending on the specific clinical task.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05376",
      "pdf_url": "https://arxiv.org/pdf/2601.05376",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05376",
      "scraped_at": "2026-01-13T01:48:56.593118"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Legal Alignment for Safe and Ethical AI",
    "paper_url": "https://huggingface.co/papers/2601.04175",
    "authors": [
      "Rishi Bommasani",
      "Cullen O'Keefe",
      "Jack Boeglin",
      "Nicholas Caputo",
      "Noam Kolt"
    ],
    "stars": "0",
    "details": {
      "title": "Legal Alignment for Safe and Ethical AI",
      "abstract": "Field-defining paper by researchers from Stanford, MIT, Harvard, Oxford, Princeton, and other leading institutions. More details at: https://www.legal-alignment.ai/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04175",
      "pdf_url": "https://arxiv.org/pdf/2601.04175",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04175",
      "scraped_at": "2026-01-13T01:48:58.401803"
    },
    "scraped_date": "2026-01-13"
  },
  {
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06943",
    "authors": [
      "Zhe Huang",
      "Zhuoyue Chang",
      "HJH2CMD",
      "Yu2020",
      "POTATO66"
    ],
    "stars": "51",
    "details": {
      "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
      "abstract": "First video deep research benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06943",
      "pdf_url": "https://arxiv.org/pdf/2601.06943",
      "github_links": [
        "https://github.com/QuantaAlpha/VideoDR-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06943",
      "scraped_at": "2026-01-14T01:55:05.388627"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "paper_url": "https://huggingface.co/papers/2601.06521",
    "authors": [
      "Liang Chen",
      "Liuff23",
      "Ziqi",
      "ssz1111",
      "chenxz"
    ],
    "stars": "81",
    "details": {
      "title": "BabyVision: Visual Reasoning Beyond Language",
      "abstract": "Feel free to follow our GitHub repo: https://github.com/UniPat-AI/BabyVision",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06521",
      "pdf_url": "https://arxiv.org/pdf/2601.06521",
      "github_links": [
        "https://github.com/UniPat-AI/BabyVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06521",
      "scraped_at": "2026-01-14T01:55:07.338360"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.05593",
    "authors": [],
    "stars": "261",
    "details": {
      "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
      "abstract": "üéâ Introducing Parallel Coordinated Reasoning (PaCoRe) üìà An 8B model beats GPT-5 on HMMT25 by unlocking parallel thinking for test-time scaling! üìÇ Open-source deep think: data + model + inference code! üÜì MIT-licensed ‚Äî use it however you want üîçKey findings: Message Passing Unlocks Scaling Without compaction, performance flatlines at the context limit. PaCoRe breaks the memory barrier and lets reasoning scale freely. Breadth > Depth All compute is not equal. Coordinated parallel reasoning delivers far higher returns than extending a single chain. Data as a Force Multiplier The PaCoRe corpus provides exceptionally valuable supervision‚Äî even baseline models see substantial gains when trained on it. üîó Links: GitHub: https://github.com/stepfun-ai/PaCoRe Data: https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k Model: https://huggingface.co/stepfun-ai/PaCoRe-8B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05593",
      "pdf_url": "https://arxiv.org/pdf/2601.05593",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05593",
      "scraped_at": "2026-01-14T01:55:09.273922"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "paper_url": "https://huggingface.co/papers/2601.07832",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2601.07832",
      "pdf_url": "https://arxiv.org/pdf/2601.07832",
      "github_links": [
        "https://github.com/DAGroup-PKU/MHLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07832",
      "scraped_at": "2026-01-14T01:55:11.365101"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
    "paper_url": "https://huggingface.co/papers/2601.06953",
    "authors": [
      "Jane Luo",
      "Jiani Guo",
      "Xin Zhang",
      "Jie Wu",
      "Ringo1110"
    ],
    "stars": "52",
    "details": {
      "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Tailored Primitive Initialization is the Secret Key to Reinforcement Learning (2025) Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes (2025) Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling (2026) PerfCoder: Large Language Models for Interpretable Code Performance Optimization (2025) Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization (2026) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06953",
      "pdf_url": "https://arxiv.org/pdf/2601.06953",
      "github_links": [
        "https://github.com/JieWu02/X-Coder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06953",
      "scraped_at": "2026-01-14T01:55:13.303655"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
    "paper_url": "https://huggingface.co/papers/2601.05110",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
      "abstract": "LLM + SLM > LLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05110",
      "pdf_url": "https://arxiv.org/pdf/2601.05110",
      "github_links": [
        "https://github.com/Zengwh02/GlimpRouter"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05110",
      "scraped_at": "2026-01-14T01:55:16.049503"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "paper_url": "https://huggingface.co/papers/2601.07226",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
      "abstract": "The code and dataset will be released publicly.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07226",
      "pdf_url": "https://arxiv.org/pdf/2601.07226",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07226",
      "scraped_at": "2026-01-14T01:55:18.008918"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "paper_url": "https://huggingface.co/papers/2601.07779",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
      "abstract": "Despite VLM advances, current CUA frameworks remain brittle in long-horizon workflows and weak in novel domains due to coarse historical visual context management and missing visual-aware tutorial retrieval, so we propose OS-SYMPHONY, an orchestrated framework combining milestone-driven reflection memory for trajectory-level self-correction with a SeeAct-style multimodal searcher that synthesizes visually aligned live tutorials, achieving new SOTA across three online benchmarks (65.84% on OSWorld).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07779",
      "pdf_url": "https://arxiv.org/pdf/2601.07779",
      "github_links": [
        "https://github.com/OS-Copilot/OS-Symphony"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07779",
      "scraped_at": "2026-01-14T01:55:19.919265"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.07351",
    "authors": [
      "Chenchen Jing",
      "Tianjian Feng",
      "Bozhen Fang",
      "Linyu Wu",
      "zhongzero"
    ],
    "stars": "16",
    "details": {
      "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
      "abstract": "GitHub repo: https://github.com/aim-uofa/EvoTokenDLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07351",
      "pdf_url": "https://arxiv.org/pdf/2601.07351",
      "github_links": [
        "https://github.com/aim-uofa/EvoTokenDLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07351",
      "scraped_at": "2026-01-14T01:55:21.893924"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "paper_url": "https://huggingface.co/papers/2601.05107",
    "authors": [
      "Zhengkang Guo",
      "Jingwen Xu",
      "Xiaohua Wang",
      "Muzhao Tian",
      "zisuh"
    ],
    "stars": "0",
    "details": {
      "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
      "abstract": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05107",
      "pdf_url": "https://arxiv.org/pdf/2601.05107",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05107",
      "scraped_at": "2026-01-14T01:55:23.799775"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "paper_url": "https://huggingface.co/papers/2601.01528",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
      "abstract": "DrivingGen is a comprehensive benchmark for generative world models in the driving domain with a diverse data distribution and novel evaluation metrics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01528",
      "pdf_url": "https://arxiv.org/pdf/2601.01528",
      "github_links": [
        "https://github.com/youngzhou1999/DrivingGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01528",
      "scraped_at": "2026-01-14T01:55:25.767787"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "paper_url": "https://huggingface.co/papers/2601.07526",
    "authors": [
      "Jiawei Chen",
      "Ruisheng Cao",
      "Mouxiang Chen",
      "zjj1233",
      "Lemoncoke"
    ],
    "stars": "0",
    "details": {
      "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
      "abstract": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07526",
      "pdf_url": "https://arxiv.org/pdf/2601.07526",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07526",
      "scraped_at": "2026-01-14T01:55:27.656092"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2601.05823",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
      "abstract": "arXiv link: Boosting Latent Diffusion Models via Disentangled Representation Alignment Code (Coming Soon): https://github.com/Kwai-Kolors/Send-VAE",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05823",
      "pdf_url": "https://arxiv.org/pdf/2601.05823",
      "github_links": [
        "https://github.com/Kwai-Kolors/Send-VAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05823",
      "scraped_at": "2026-01-14T01:55:29.594871"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06165",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
      "abstract": "Users often ask VLMs under-specified, informal visual questions, which current clean-prompt benchmarks fail to capture. We introduce HAERAE-Vision (653 real Korean community queries + explicit rewrites) and show that making queries explicit boosts accuracy by 8‚Äì22 points, while web search cannot fully offset what users leave unsaid.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06165",
      "pdf_url": "https://arxiv.org/pdf/2601.06165",
      "github_links": [
        "https://github.com/HAE-RAE/HAERAE-VISION"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06165",
      "scraped_at": "2026-01-14T01:55:31.500284"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
    "paper_url": "https://huggingface.co/papers/2601.06860",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
      "abstract": "Most current TIR work only focuses on the accuracy of agents in downstream tasks, while lacking calibration of the agents' behavioral patterns in TIR tasks. To address this issue, we first quantitatively analyze several possible erroneous behavioral patterns in current TIR tasks, and classify them into two categories: \"improper tool use\" and \"flawed reasoning logic\". Based on this, we propose ET-Agent, a framework that fully calibrates the behavioral patterns of agents when performing TIR tasks from both data and algorithm levels. On the data side, we propose a self-evolving data flywheel, which enhances the training data by leveraging the agent's own reflective exploration capabilities. On the algorithm side, we propose a behavioral calibration training framework. It performs rejection sampling fine-tuning on the basis of enhanced training data to broaden the agent's exploration of the action space. Subsequently, we implement iterative behavioral calibration reinforcement learning to calibrate the actions of the fine-tuned agent to the optimal behavioral pattern. Our contributions are listed as follows: We provide a comprehensive quantitative analysis of erroneous behavioral patterns in TIR. Inspired by this, we propose ET-Agent, a framework for optimizing TIR's behavioral patterns. We introduce a self-evolving data flywheel, an iterative loop where the model continuously refines its previous trajectories. This mechanism effectively unfolds the model's action space coverage beyond its initial exploration. Based on the flywheel, we present a behavior calibration training framework with two phases, aiming to calibrate the model's exploration in tool-use action space to optimal trajectories. Extensive experiments demonstrate that ET-Agent substantially improves behavioral efficiency, reasoning conciseness, and execution success rates while maintaining high accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06860",
      "pdf_url": "https://arxiv.org/pdf/2601.06860",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06860",
      "scraped_at": "2026-01-14T01:55:33.429929"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
    "paper_url": "https://huggingface.co/papers/2601.07055",
    "authors": [
      "Shaoliang Nie",
      "Suyu Ge",
      "Xianjun Yang",
      "Kartikeya Upasani",
      "Zhenrui Yue"
    ],
    "stars": "74",
    "details": {
      "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
      "abstract": "Dr. Zero enables data-free self-evolving search agents through a self-evolution loop with HRPO, achieving strong multi-step reasoning while reducing compute.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07055",
      "pdf_url": "https://arxiv.org/pdf/2601.07055",
      "github_links": [
        "https://github.com/facebookresearch/drzero"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07055",
      "scraped_at": "2026-01-14T01:55:35.318742"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.06803",
    "authors": [
      "Yankai Lin",
      "Yichen Wu",
      "Yubo Wang",
      "Yuhan",
      "ZION121"
    ],
    "stars": "0",
    "details": {
      "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
      "abstract": "We hope this work encourages a paradigm shift from explicit next-token prediction to latent visual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06803",
      "pdf_url": "https://arxiv.org/pdf/2601.06803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06803",
      "scraped_at": "2026-01-14T01:55:37.211022"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
    "paper_url": "https://huggingface.co/papers/2601.04698",
    "authors": [
      "Hao Wang",
      "Xiaoxi Li",
      "Wenxiang Jiao",
      "Mining Tan",
      "Yinuo Wang"
    ],
    "stars": "0",
    "details": {
      "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
      "abstract": "We propose TourPlanner , a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04698",
      "pdf_url": "https://arxiv.org/pdf/2601.04698",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04698",
      "scraped_at": "2026-01-14T01:55:39.117410"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2601.07376",
    "authors": [
      "Jiaxuan You",
      "zsqzz"
    ],
    "stars": "568",
    "details": {
      "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
      "abstract": "üéâ Introducing OpenTinker üöÄ A scalable RL infrastructure for LLM agents that separates what you build (agents + environments) from how it runs (training + inference)! üß© Composable RL-as-a-Service No more monolithic RL pipelines. OpenTinker decomposes agentic learning into lightweight, modular components with clean abstraction boundaries. Plug in new agents, environments, and interaction protocols with minimal friction. ‚öôÔ∏è Unified Runtime for Training + Inference A centralized scheduler manages shared compute across workloads like RL (LoRA / full-parameter), SFT, and high-throughput inference. Built for multi-tenant scaling and real-world iteration speed. ü§ñ Multi-Agent Ready by Design OpenTinker supports coordinator-driven multi-agent interaction. Each agent can optimize independently while coordination emerges through environment dynamics. This keeps MARL scalable, flexible, and system-friendly. üîó Links: üìÑ Paper (arXiv): https://arxiv.org/pdf/2601.07376 üíª GitHub: https://github.com/open-tinker/OpenTinker",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07376",
      "pdf_url": "https://arxiv.org/pdf/2601.07376",
      "github_links": [
        "https://github.com/open-tinker/OpenTinker?tab=readme-ov-file",
        "https://github.com/open-tinker/OpenTinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07376",
      "scraped_at": "2026-01-14T01:55:41.067699"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Are LLM Decisions Faithful to Verbal Confidence?",
    "paper_url": "https://huggingface.co/papers/2601.07767",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Are LLM Decisions Faithful to Verbal Confidence?",
      "abstract": "While LLMs can express their confidence levels, their actual decisions do not demonstrate risk sensitivity. Even with high error penalties, they rarely abstain from making choices, often leading to utility collapse.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07767",
      "pdf_url": "https://arxiv.org/pdf/2601.07767",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07767",
      "scraped_at": "2026-01-14T01:55:42.902306"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Structured Episodic Event Memory",
    "paper_url": "https://huggingface.co/papers/2601.06411",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Structured Episodic Event Memory",
      "abstract": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06411",
      "pdf_url": "https://arxiv.org/pdf/2601.06411",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06411",
      "scraped_at": "2026-01-14T01:55:44.777676"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "paper_url": "https://huggingface.co/papers/2601.03666",
    "authors": [
      "Zhicheng Dou",
      "Tetsuya Sakai",
      "Radu Timofte",
      "Sicheng Gao",
      "Haon-Chen"
    ],
    "stars": "0",
    "details": {
      "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
      "abstract": "A lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. Checkpoints: https://huggingface.co/Haon-Chen/e5-omni-3B https://huggingface.co/Haon-Chen/e5-omni-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03666",
      "pdf_url": "https://arxiv.org/pdf/2601.03666",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03666",
      "scraped_at": "2026-01-14T01:55:46.645891"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "paper_url": "https://huggingface.co/papers/2601.07786",
    "authors": [
      "Mia Mohammad Imran",
      "Abdullah Al Mujahid"
    ],
    "stars": "0",
    "details": {
      "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
      "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07786",
      "pdf_url": "https://arxiv.org/pdf/2601.07786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07786",
      "scraped_at": "2026-01-14T01:55:48.489091"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "ShowUI-Aloha: Human-Taught GUI Agent",
    "paper_url": "https://huggingface.co/papers/2601.07181",
    "authors": [
      "Zhiheng Chen",
      "Jessica Hu",
      "Yauhong Goh",
      "Xiangwu Guo",
      "Yichun Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "ShowUI-Aloha: Human-Taught GUI Agent",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2601.07181",
      "pdf_url": "https://arxiv.org/pdf/2601.07181",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07181",
      "scraped_at": "2026-01-14T01:55:50.358460"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Codified Foreshadowing-Payoff Text Generation",
    "paper_url": "https://huggingface.co/papers/2601.07033",
    "authors": [
      "Jingbo Shang",
      "Letian Peng",
      "Kun Zhou",
      "Longfei Yun",
      "hyp1231"
    ],
    "stars": "0",
    "details": {
      "title": "Codified Foreshadowing-Payoff Text Generation",
      "abstract": "Codified Foreshadowing-Payoff Text Generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07033",
      "pdf_url": "https://arxiv.org/pdf/2601.07033",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07033",
      "scraped_at": "2026-01-14T01:55:52.291930"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
    "paper_url": "https://huggingface.co/papers/2601.04577",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
      "abstract": "While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04577",
      "pdf_url": "https://arxiv.org/pdf/2601.04577",
      "github_links": [
        "https://github.com/AmberLJC/Sci-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04577",
      "scraped_at": "2026-01-14T01:55:54.190567"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "paper_url": "https://huggingface.co/papers/2601.03570",
    "authors": [
      "Zaishuo Xia",
      "Minqian Liu",
      "Yunzhi Yao",
      "Sha Li",
      "Barry Menglong Yao"
    ],
    "stars": "0",
    "details": {
      "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
      "abstract": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03570",
      "pdf_url": "https://arxiv.org/pdf/2601.03570",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03570",
      "scraped_at": "2026-01-14T01:55:56.059301"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
    "paper_url": "https://huggingface.co/papers/2601.07389",
    "authors": [
      "Weixi Zhang",
      "Wei Han",
      "Bo Bai",
      "Xueyan Niu"
    ],
    "stars": "0",
    "details": {
      "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
      "abstract": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07389",
      "pdf_url": "https://arxiv.org/pdf/2601.07389",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07389",
      "scraped_at": "2026-01-14T01:55:57.904888"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
    "paper_url": "https://huggingface.co/papers/2601.06993",
    "authors": [
      "Xiaoming Liu",
      "Yiyang Su",
      "Paipile"
    ],
    "stars": "1",
    "details": {
      "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
      "abstract": "In this work, we investigate the impact of CoT on Fine-Grained Visual Classification (FGVC), revealing a paradox: the degradation in FGVC performance due to CoT is primarily driven by reasoning length, with longer textual reasoning consistently reducing classification accuracy. We introduce the concept of the \"Cost of Thinking\" to describe this phenomenon. Building on this insight, we propose two key contributions: (1) MRN, a normalization method for multi-reward optimization that balances heterogeneous reward signals; and (2) ReFine-RFT, a framework that integrates ensemble rewards with MRN to constrain reasoning length while providing dense, accuracy-oriented feedback. Our extensive experiments across multiple FGVC benchmarks demonstrate the effectiveness of our approach, achieving state-of-the-art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06993",
      "pdf_url": "https://arxiv.org/pdf/2601.06993",
      "github_links": [
        "https://github.com/jiezhu23/ReFine-RFT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06993",
      "scraped_at": "2026-01-14T01:55:59.761826"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "paper_url": "https://huggingface.co/papers/2601.06966",
    "authors": [
      "Shaolei Zhang",
      "Zishan Xu",
      "Sen Hu",
      "Zhiyuan Yao",
      "Haonan-Bian"
    ],
    "stars": "0",
    "details": {
      "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory (2026) Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents (2026) Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI (2025) MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards (2026) KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions (2026) MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents (2026) Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06966",
      "pdf_url": "https://arxiv.org/pdf/2601.06966",
      "github_links": [
        "https://github.com/AvatarMemory/RealMemBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06966",
      "scraped_at": "2026-01-14T01:56:01.621095"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06944",
    "authors": [
      "Shixing Li",
      "Guozhang Li",
      "Yaoyao Zhong",
      "Mei Wang",
      "Yuhang Su"
    ],
    "stars": "0",
    "details": {
      "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ViRectify: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models (2025) PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding (2025) MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models (2026) AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding (2026) CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution (2025) PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models (2025) Evaluating large language models on multimodal chemistry olympiad exams (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06944",
      "pdf_url": "https://arxiv.org/pdf/2601.06944",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06944",
      "scraped_at": "2026-01-14T01:56:03.488898"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.06788",
    "authors": [
      "Manling Li",
      "Zeguan Wu",
      "Canyu Chen",
      "Zihan Wang",
      "Min Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06788",
      "pdf_url": "https://arxiv.org/pdf/2601.06788",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06788",
      "scraped_at": "2026-01-14T01:56:05.320924"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "paper_url": "https://huggingface.co/papers/2601.06747",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
      "abstract": "This paper introduces FinForge, a novel framework designed to address the scarcity of high-quality, domain-specific datasets for evaluating Large Language Models (LLMs) in finance. The authors propose a scalable, semi-synthetic pipeline that combines expert-guided data curation from authoritative sources with controlled question generation and validation using Gemini 2.5 Flash. Key Contributions: FinForge Framework: A hybrid pipeline integrating manual/programmatic corpus construction with rigorous LM-based synthesis. FinForge-5k Dataset: A new snapshot benchmark comprising over 5,000 human-validated Q&A pairs across 11 financial subdomains, derived from a curated corpus of 100,000 verified documents (143M tokens). Benchmarking Results: Evaluation of state-of-the-art open and closed-source models reveals significant variance in financial reasoning capabilities, with leading models achieving approximately 80% accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06747",
      "pdf_url": "https://arxiv.org/pdf/2601.06747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06747",
      "scraped_at": "2026-01-14T01:56:07.161587"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "paper_url": "https://huggingface.co/papers/2601.06463",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06463",
      "pdf_url": "https://arxiv.org/pdf/2601.06463",
      "github_links": [
        "https://github.com/XuezheMax/gecko-llm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06463",
      "scraped_at": "2026-01-14T01:56:09.049268"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "paper_url": "https://huggingface.co/papers/2601.06423",
    "authors": [
      "Deep Mehta"
    ],
    "stars": "0",
    "details": {
      "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
      "abstract": "We ask a question that hasn't been studied before: does inference scaling improve reasoning faithfulness or just accuracy? Self-consistency (majority voting over multiple reasoning paths) reliably boosts LLM accuracy on reasoning tasks. But does getting the right answer more often mean the model is actually reasoning better? We test 4 frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K problems and find a surprising tradeoff. Accuracy gains from self-consistency don't necessarily translate to more faithful reasoning. This discovery has important implications for AI safety. We may be building systems that appear smarter without actually reasoning more reliably.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06423",
      "pdf_url": "https://arxiv.org/pdf/2601.06423",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06423",
      "scraped_at": "2026-01-14T01:56:10.861313"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
    "paper_url": "https://huggingface.co/papers/2601.05747",
    "authors": [
      "Peter St\\√ºtz",
      "Marvin Brenner",
      "farooqhassaan"
    ],
    "stars": "0",
    "details": {
      "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
      "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasible models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ‚àº20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.05747",
      "pdf_url": "https://arxiv.org/pdf/2601.05747",
      "github_links": [
        "https://github.com/farooqhassaan/FlyPose"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.05747",
      "scraped_at": "2026-01-14T01:56:12.800773"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "paper_url": "https://huggingface.co/papers/2601.07790",
    "authors": [
      "Chaowei Yang",
      "Joseph Rogers",
      "Zifu Wang",
      "Emily Ma",
      "ymasri"
    ],
    "stars": "1",
    "details": {
      "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
      "abstract": "We evaluate 9 open-source models under zero-shot, few-shot, and RAG (FAISS) and measure both accuracy + per-log latency. Main takeaway: RAG can massively help small models (Qwen3-4B: 95.64%, Gemma3-1B: 85.28%), but some reasoning-focused models degrade with retrieval, showing that retrieval integration isn‚Äôt uniform across architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07790",
      "pdf_url": "https://arxiv.org/pdf/2601.07790",
      "github_links": [
        "https://github.com/stccenter/Benchmarking-SLMs-and-SRLMs-on-System-Log-Severity-Classification"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07790",
      "scraped_at": "2026-01-14T01:56:14.646904"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "paper_url": "https://huggingface.co/papers/2601.07239",
    "authors": [
      "Shreyash Dhoot",
      "Aadi Pandey",
      "Anusa Saha",
      "Shourya Aggarwal",
      "Tanmay Joshi"
    ],
    "stars": "0",
    "details": {
      "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "abstract": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07239",
      "pdf_url": "https://arxiv.org/pdf/2601.07239",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07239",
      "scraped_at": "2026-01-14T01:56:16.492225"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.06496",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
      "abstract": "https://github.com/AIGeeksGroup/3DCoCav2",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06496",
      "pdf_url": "https://arxiv.org/pdf/2601.06496",
      "github_links": [
        "https://github.com/AIGeeksGroup/3DCoCav2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06496",
      "scraped_at": "2026-01-14T01:56:18.356557"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.06329",
    "authors": [
      "Ju-Chieh Chou",
      "Yen-Chun Kuo",
      "Yi-Cheng Lin",
      "Liang-Hsuan Tseng",
      "Jeff Chan-Jan Sju"
    ],
    "stars": "0",
    "details": {
      "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
      "abstract": "Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ‚Äúglobal token perplexity‚Äù, which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06329",
      "pdf_url": "https://arxiv.org/pdf/2601.06329",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06329",
      "scraped_at": "2026-01-14T01:56:20.214536"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
    "paper_url": "https://huggingface.co/papers/2601.06307",
    "authors": [
      "Dilek Hakkani-T√ºr",
      "Dhruva Patil",
      "Zhenlin He",
      "Ishika Agarwal"
    ],
    "stars": "0",
    "details": {
      "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
      "abstract": "https://huggingface.co/collections/ishikaa/a-rising-tide-lifts-all-boats-mtqe-rewards-for-idioms",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06307",
      "pdf_url": "https://arxiv.org/pdf/2601.06307",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06307",
      "scraped_at": "2026-01-14T01:56:22.211331"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
    "paper_url": "https://huggingface.co/papers/2601.06238",
    "authors": [
      "Aman Chadha",
      "Vinija Jain",
      "Amit Dhanda",
      "Partha Pratim Saha",
      "Arion Das"
    ],
    "stars": "0",
    "details": {
      "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
      "abstract": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06238",
      "pdf_url": "https://arxiv.org/pdf/2601.06238",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06238",
      "scraped_at": "2026-01-14T01:56:24.053442"
    },
    "scraped_date": "2026-01-14"
  },
  {
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "paper_url": "https://huggingface.co/papers/2601.06789",
    "authors": [
      "Yu2020",
      "KunyiWang",
      "shuozhang",
      "cadche",
      "jimson991"
    ],
    "stars": "19",
    "details": {
      "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
      "abstract": "code agent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06789",
      "pdf_url": "https://arxiv.org/pdf/2601.06789",
      "github_links": [
        "https://github.com/QuantaAlpha/MemGovern"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06789",
      "scraped_at": "2026-01-15T01:50:09.498721"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Solar Open Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.07022",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Solar Open Technical Report",
      "abstract": "huggingface model: https://huggingface.co/upstage/Solar-Open-100B",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07022",
      "pdf_url": "https://arxiv.org/pdf/2601.07022",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07022",
      "scraped_at": "2026-01-15T01:50:11.630863"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
    "paper_url": "https://huggingface.co/papers/2601.04745",
    "authors": [
      "lanqz7766",
      "ChenglongLi",
      "Super-shuhe-v2",
      "Zhisheng888",
      "realty2333"
    ],
    "stars": "83",
    "details": {
      "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
      "abstract": "know me",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04745",
      "pdf_url": "https://arxiv.org/pdf/2601.04745",
      "github_links": [
        "https://github.com/QuantaAlpha/KnowMeBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04745",
      "scraped_at": "2026-01-15T01:50:13.446669"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "paper_url": "https://huggingface.co/papers/2601.08225",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
      "abstract": "While large language models have shown remarkable progress in tool use, maintaining high-quality, user-centric multi-turn conversations at scale remains a significant challenge. Our work focuses on: (1) Generating high-fidelity multi-turn dialogue datasets designed for practical tool-use scenarios. (2) Enhancing model performance in complex, user-oriented interactions. (3) Providing insights into scaling dialogue generation without compromising on user experience. Check out the full paper here: https://arxiv.org/abs/2601.08225",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08225",
      "pdf_url": "https://arxiv.org/pdf/2601.08225",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08225",
      "scraped_at": "2026-01-15T01:50:15.282705"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands",
    "paper_url": "https://huggingface.co/papers/2512.24965",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "ShowUI-œÄ: Flow-based Generative Models as GUI Dexterous Hands",
      "abstract": "TL;DR: ShowUI-œÄ is a 450M flow-based vision-language-action model that treats GUI actions as continuous trajectories, generating smooth clicks and drags directly from screen observations. It unifies discrete and continuous actions, enabling precise drawing, rotation, sorting, and captcha solving without tokenized coordinates. arXiv: https://arxiv.org/abs/2512.24965 Website: https://showlab.github.io/showui-pi/ Github: https://github.com/showlab/showui-pi",
      "arxiv_page_url": "https://arxiv.org/abs/2512.24965",
      "pdf_url": "https://arxiv.org/pdf/2512.24965",
      "github_links": [
        "https://github.com/showlab/showui-pi"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.24965",
      "scraped_at": "2026-01-15T01:50:17.137473"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.08079",
    "authors": [
      "Zhao Cao",
      "lz1001",
      "TommyChien"
    ],
    "stars": "39",
    "details": {
      "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
      "abstract": "Project Repo: https://github.com/qhjqhj00/MemoBrain",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08079",
      "pdf_url": "https://arxiv.org/pdf/2601.08079",
      "github_links": [
        "https://github.com/qhjqhj00/MemoBrain"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08079",
      "scraped_at": "2026-01-15T01:50:18.963578"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "paper_url": "https://huggingface.co/papers/2601.06487",
    "authors": [],
    "stars": "49",
    "details": {
      "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
      "abstract": "As a key exploration of open-domain agents, our method has been validated within Amap's (Gaode Map) real-world business scenarios. Demonstrating significant practical value, we believe this paradigm represents one of the most important direction of AI agents in the future. Project Resources: Github: https://github.com/Alibaba-NLP/qqr Paper: https://arxiv.org/abs/2601.06487 Hugging Face: https://huggingface.co/collections/Alibaba-NLP/arenarl",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06487",
      "pdf_url": "https://arxiv.org/pdf/2601.06487",
      "github_links": [
        "https://github.com/Alibaba-NLP/qqr"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06487",
      "scraped_at": "2026-01-15T01:50:20.840380"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Ministral 3",
    "paper_url": "https://huggingface.co/papers/2601.08584",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Ministral 3",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models (2025) T5Gemma 2: Seeing, Reading, and Understanding Longer (2025) MiniLingua: A Small Open-Source LLM for European Languages (2025) Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM (2025) ELO: Efficient Layer-Specific Optimization for Continual Pretraining of Multilingual LLMs (2026) SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation (2025) Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08584",
      "pdf_url": "https://arxiv.org/pdf/2601.08584",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08584",
      "scraped_at": "2026-01-15T01:50:22.698526"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "paper_url": "https://huggingface.co/papers/2601.08831",
    "authors": [
      "Min-Hung Chen",
      "Fu-En Yang",
      "Chin-Yang Lin",
      "Cheng Sun",
      "Yang-Che Sun"
    ],
    "stars": "0",
    "details": {
      "title": "3AM: Segment Anything with Geometric Consistency in Videos",
      "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08831",
      "pdf_url": "https://arxiv.org/pdf/2601.08831",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08831",
      "scraped_at": "2026-01-15T01:50:24.602177"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.07264",
    "authors": [
      "Qingcheng Zeng",
      "Naotoyokoyama",
      "junjuewang",
      "lrzneedresearch",
      "weihao1115"
    ],
    "stars": "0",
    "details": {
      "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
      "abstract": "We reveal a \"confidence dichotomy\" in tool-use LLM agents, finding that evidence tools like web search systematically induce overconfidence due to noisy retrieval, while verification tools like code interpreters help ground reasoning and reduce miscalibration. To address this, we propose Calibration Agentic RL (CAR), a reinforcement learning framework that jointly optimizes task accuracy and calibration through a novel reward design, demonstrating significant reductions in calibration error while maintaining competitive performance across different domains and environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07264",
      "pdf_url": "https://arxiv.org/pdf/2601.07264",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07264",
      "scraped_at": "2026-01-15T01:50:26.533802"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "paper_url": "https://huggingface.co/papers/2601.08670",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
      "abstract": "Parallel Context-of-Experts Decoding (Pced) speeds up RAG by decoding in parallel from per-document KV-cache ‚Äúexperts‚Äù and selecting retrieval-supported tokens to recover cross-document reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08670",
      "pdf_url": "https://arxiv.org/pdf/2601.08670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08670",
      "scraped_at": "2026-01-15T01:50:28.367501"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Motion Attribution for Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.08828",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Motion Attribution for Video Generation",
      "abstract": "TL;DR: We propose MOTIVE, a scalable, motion-centric data attribution framework for video generation to identify which training clips improve or degrade motion dynamics, enabling curation and more.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08828",
      "pdf_url": "https://arxiv.org/pdf/2601.08828",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08828",
      "scraped_at": "2026-01-15T01:50:30.225015"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.08620",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
      "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://huggingface.co/vidore .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08620",
      "pdf_url": "https://arxiv.org/pdf/2601.08620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08620",
      "scraped_at": "2026-01-15T01:50:32.048862"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "paper_url": "https://huggingface.co/papers/2601.08303",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
      "abstract": "Proposes efficient diffusion transformers for edge devices via sparse attention, elastic training, and knowledge-guided distillation to achieve high-fidelity, fast on-device image generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08303",
      "pdf_url": "https://arxiv.org/pdf/2601.08303",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08303",
      "scraped_at": "2026-01-15T01:50:33.857107"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "paper_url": "https://huggingface.co/papers/2601.08665",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation (2025) MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots (2025) CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving (2025) NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction (2025) ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination (2025) Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation (2025) EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08665",
      "pdf_url": "https://arxiv.org/pdf/2601.08665",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08665",
      "scraped_at": "2026-01-15T01:50:35.696313"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "paper_url": "https://huggingface.co/papers/2601.08587",
    "authors": [],
    "stars": "550",
    "details": {
      "title": "End-to-End Video Character Replacement without Structural Guidance",
      "abstract": "End-to-End Video Character Replacement without Structural Guidance",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08587",
      "pdf_url": "https://arxiv.org/pdf/2601.08587",
      "github_links": [
        "https://github.com/Orange-3DV-Team/MoCha"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08587",
      "scraped_at": "2026-01-15T01:50:37.523465"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.08468",
    "authors": [
      "Sujian Li",
      "Yudong Wang",
      "Hailin Zhang",
      "Hanyu Li",
      "Jiangshan Duo"
    ],
    "stars": "0",
    "details": {
      "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Structured Reasoning for Large Language Models (2026) Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning (2026) Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning (2026) Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards (2025) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) ORION: Teaching Language Models to Reason Efficiently in the Language of Thought (2025) ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08468",
      "pdf_url": "https://arxiv.org/pdf/2601.08468",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08468",
      "scraped_at": "2026-01-15T01:50:39.376659"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "paper_url": "https://huggingface.co/papers/2601.07290",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
      "abstract": "Joint temporal understanding and spatial perception within a single framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07290",
      "pdf_url": "https://arxiv.org/pdf/2601.07290",
      "github_links": [
        "https://github.com/JPShi12/VideoLoom"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07290",
      "scraped_at": "2026-01-15T01:50:41.246788"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.06786",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
      "abstract": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemicallycalibrated reasoning (EPICAR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Paretosuperiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3√ó reduction in inference compute, matching the K = 30 performance of STaR with only K = 10 samples in capable models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06786",
      "pdf_url": "https://arxiv.org/pdf/2601.06786",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06786",
      "scraped_at": "2026-01-15T01:50:43.224423"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "paper_url": "https://huggingface.co/papers/2601.08321",
    "authors": [
      "Ting Zhu",
      "Zipeng Guo",
      "Gaojing Zhou",
      "Xiaolong Fu",
      "Lichen Ma"
    ],
    "stars": "0",
    "details": {
      "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08321",
      "pdf_url": "https://arxiv.org/pdf/2601.08321",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08321",
      "scraped_at": "2026-01-15T01:50:45.007426"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
    "paper_url": "https://huggingface.co/papers/2601.04582",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
      "abstract": "Generating working visualization code is not enough. Charts must be semantically correct and visually meaningful. We introduce RL-Text2Vis, the first reinforcement-learning framework for Text-to-Visualization, using post-execution feedback to jointly optimize: ‚úîÔ∏è textual accuracy ‚úîÔ∏è code executability ‚úîÔ∏è visualization quality üìà Results: ‚Ä¢ +22% relative improvement in chart quality over GPT-4o ‚Ä¢ Code execution success boosted from 78% ‚Üí 97% ‚Ä¢ Strong generalization to out-of-domain benchmarks This work demonstrates the power of multi-objective RL for structured, multimodal reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04582",
      "pdf_url": "https://arxiv.org/pdf/2601.04582",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04582",
      "scraped_at": "2026-01-15T01:50:46.788335"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "paper_url": "https://huggingface.co/papers/2601.02669",
    "authors": [
      "Zhen Ye",
      "Ziyang Luo",
      "Zhiqi Shen",
      "Zixin Chen",
      "Hongzhan Lin"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
      "abstract": "Current automatic LLM fact-checking tests are too narrow, they only check if a model can verify a claim, ignoring the hard parts like finding evidence and decomposing check-worthy claims. FactArena is built to evaluate the full fact-checking pipeline. Testing 16 state-of-the-art models reveals that the whole process ranking can disagree with simple accuracy. The system also revealed fragility in LLM fact-checking through subtle claim modifications (\"claim flipping\"), tanked average accuracy to 68%, proving that trustworthy auditing of every stage in the process matters.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02669",
      "pdf_url": "https://arxiv.org/pdf/2601.02669",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02669",
      "scraped_at": "2026-01-15T01:50:48.582670"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.08173",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
      "abstract": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08173",
      "pdf_url": "https://arxiv.org/pdf/2601.08173",
      "github_links": [
        "https://github.com/KnowledgeXLab/EvoEnv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08173",
      "scraped_at": "2026-01-15T01:50:50.375117"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.07632",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07632",
      "pdf_url": "https://arxiv.org/pdf/2601.07632",
      "github_links": [
        "https://github.com/JYe16/GeoMotionGPT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07632",
      "scraped_at": "2026-01-15T01:50:52.181113"
    },
    "scraped_date": "2026-01-15"
  },
  {
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "paper_url": "https://huggingface.co/papers/2601.07348",
    "authors": [],
    "stars": "79",
    "details": {
      "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
      "abstract": "arXiv explained breakdown of this paper üëâ https://arxivexplained.com/papers/controlled-self-evolution-for-algorithmic-code-optimization",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07348",
      "pdf_url": "https://arxiv.org/pdf/2601.07348",
      "github_links": [
        "https://github.com/QuantaAlpha/EvoControl"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07348",
      "scraped_at": "2026-01-16T01:52:19.823061"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.09688",
    "authors": [],
    "stars": "67",
    "details": {
      "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
      "abstract": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09688",
      "pdf_url": "https://arxiv.org/pdf/2601.09688",
      "github_links": [
        "https://github.com/Infinity-AILab/DeepResearchEval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09688",
      "scraped_at": "2026-01-16T01:52:21.780207"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
    "paper_url": "https://huggingface.co/papers/2601.09259",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
      "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09259",
      "pdf_url": "https://arxiv.org/pdf/2601.09259",
      "github_links": [
        "https://github.com/exoskeletonzj/MAXS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09259",
      "scraped_at": "2026-01-16T01:52:23.826039"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
    "paper_url": "https://huggingface.co/papers/2601.09274",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
      "abstract": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A3-Bench, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09274",
      "pdf_url": "https://arxiv.org/pdf/2601.09274",
      "github_links": [
        "https://github.com/exoskeletonzj/A3-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09274",
      "scraped_at": "2026-01-16T01:52:25.818622"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09088",
    "authors": [],
    "stars": "16",
    "details": {
      "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
      "abstract": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09088",
      "pdf_url": "https://arxiv.org/pdf/2601.09088",
      "github_links": [
        "https://github.com/D2I-ai/dasd-thinking"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09088",
      "scraped_at": "2026-01-16T01:52:28.085648"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "paper_url": "https://huggingface.co/papers/2601.09708",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "abstract": "Project page: https://jasper0314-huang.github.io/fast-thinkact/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09708",
      "pdf_url": "https://arxiv.org/pdf/2601.09708",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09708",
      "scraped_at": "2026-01-16T01:52:30.028323"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "paper_url": "https://huggingface.co/papers/2601.09136",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
      "abstract": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09136",
      "pdf_url": "https://arxiv.org/pdf/2601.09136",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09136",
      "scraped_at": "2026-01-16T01:52:32.025174"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
    "paper_url": "https://huggingface.co/papers/2601.09575",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
      "abstract": "OpenVoxel provides training-free grouping and captioning of sparse voxels for open-vocabulary 3D scene understanding using VLMs/MLLMs and text search, enabling RES and OVS without CLIP embeddings.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09575",
      "pdf_url": "https://arxiv.org/pdf/2601.09575",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09575",
      "scraped_at": "2026-01-16T01:52:34.005779"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
    "paper_url": "https://huggingface.co/papers/2601.09028",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
      "abstract": "OpenDecoder is a novel framework that directly 'opens' the LLM to modify its decoding process within RAG scenarios by leveraging relevance signals from retrieved documents. Through a robustness-oriented training algorithm, the model learns to perform answer decoding guided by explicit indicators, rather than relying solely on prompt engineering or internal attention scores. This approach significantly enhances the system's controllability, accuracy, and robustness across various noisy environments. Take Away: Opening LLM rather than solely relying on prompt engineering is important to improve the system‚Äôs robustness, since we cannot expect LLMs‚Äô implicit identification to be always correct. The external indicators, e.g., relevance score, confidence feature, faithful factors, are useful to incorporate with LLMs‚Äô internal information processing mechanism, e.g., attention, for output decoding, where the key problem is to obtain and integrate these indicators into LLMs with a sophisticated training algorithm (during post-training). Experimental results show the robustness enhancement in different levels of noisy environments of our initial investigation of OpenDecoder. An ideal situation is that the LLM can understand to what extent to rely on external retrieved knowledge and internal parametric knowledge during answer decoding.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09028",
      "pdf_url": "https://arxiv.org/pdf/2601.09028",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09028",
      "scraped_at": "2026-01-16T01:52:36.082937"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
    "paper_url": "https://huggingface.co/papers/2601.08605",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
      "abstract": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08605",
      "pdf_url": "https://arxiv.org/pdf/2601.08605",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08605",
      "scraped_at": "2026-01-16T01:52:37.987916"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "paper_url": "https://huggingface.co/papers/2601.09465",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
      "abstract": "EvoFSM presents a controllable self-evolution framework using a finite state machine to guide adaptive problem-solving, separating macroscopic flow and microscopic skills with critic-guided updates and reusable priors.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09465",
      "pdf_url": "https://arxiv.org/pdf/2601.09465",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09465",
      "scraped_at": "2026-01-16T01:52:39.900524"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
    "paper_url": "https://huggingface.co/papers/2601.03928",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
      "abstract": "TL;DR: High-res UI screenshots (2K/4K) force VLMs to process thousands of visual tokens. Inspired by human vision, which selects only instruction-relevant image patches, FocusUI teaches VLMs where to look in UI screenshots smartly üîç üìÑ Paper: arXiv:2601.03928 üåê Project Page: showlab.github.io/FocusUI üíª Code: github.com/showlab/FocusUI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.03928",
      "pdf_url": "https://arxiv.org/pdf/2601.03928",
      "github_links": [
        "https://github.com/showlab/FocusUI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.03928",
      "scraped_at": "2026-01-16T01:52:41.892931"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
    "paper_url": "https://huggingface.co/papers/2601.06596",
    "authors": [
      "Chi Zhang",
      "Jiawei Shao",
      "Jiangan Chen",
      "Yiliang Song",
      "Hongjun An"
    ],
    "stars": "0",
    "details": {
      "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
      "abstract": "This paper treats preference undermining as an experimental object, not a vibe. A clean factorial design isolates manipulation factors and quantifies when truth yields to compliance. Conclusion, stated politely: yes, a large model can be PUA-ed, and it may rationalize the outcome as alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06596",
      "pdf_url": "https://arxiv.org/pdf/2601.06596",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06596",
      "scraped_at": "2026-01-16T01:52:43.833419"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "TranslateGemma Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09012",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TranslateGemma Technical Report",
      "abstract": "TranslateGemma extends Gemma 3 with two-stage fine-tuning (supervised then RL) for multilingual translation, achieving strong WMT performance and multimodal capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09012",
      "pdf_url": "https://arxiv.org/pdf/2601.09012",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09012",
      "scraped_at": "2026-01-16T01:52:45.793509"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "paper_url": "https://huggingface.co/papers/2601.08955",
    "authors": [
      "Wenjie Li",
      "Beichen Guo",
      "Hanlin Wang",
      "Youwei Liu",
      "jwanglvy"
    ],
    "stars": "0",
    "details": {
      "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
      "abstract": "TL;DR: An agent learning framework via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step \"imagined\" trajectories. This imagination is conducted via a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08955",
      "pdf_url": "https://arxiv.org/pdf/2601.08955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08955",
      "scraped_at": "2026-01-16T01:52:47.807412"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
    "paper_url": "https://huggingface.co/papers/2601.09697",
    "authors": [
      "Ayush Tewari",
      "Joan Lasenby",
      "Jeffrey Hu",
      "Jieying Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
      "abstract": "Proposes SRENDER: generate sparse diffusion keyframes for static scenes and render 3D views to produce long videos fast and consistently.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09697",
      "pdf_url": "https://arxiv.org/pdf/2601.09697",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09697",
      "scraped_at": "2026-01-16T01:52:49.731039"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Geometric Stability: The Missing Axis of Representations",
    "paper_url": "https://huggingface.co/papers/2601.09173",
    "authors": [
      "pcr2120"
    ],
    "stars": "0",
    "details": {
      "title": "Geometric Stability: The Missing Axis of Representations",
      "abstract": "DeepSeek got it half right with their mHC paper: stability matters for scaling. But they only measure stability DURING training. What about the stability of what models LEARN? I built Shesha to measure this - a geometric stability metric with SOTA results across AI Safety , Constitutional AI , Model selection , and CRISPR perturbation analysis. The core insight: Most evals check external similarity (does output X match Y?). But imagine a massive library where someone reshuffled all the books. A content-based audit would say that nothing's wrong since the inventory is identical. But the library is useless since nothing can be found. That's the gap Shesha fills. The implications are broad with SOTA results across 4 domains : AI Safety - Shesha is the best canary in the coal mine . Shesha outperforms CKA and Procrustes on drift detection. Detects 2x more drift than CKA, triggers earlier 73% of the time, catches subtle LoRA shifts at 90% sensitivity (5% FPR) - with only 7% false alarms vs Procrustes' 44%. Constitutional AI - Shesha provides the best steering prediction . Constitutional AI needs models you can actually steer. Most metrics ask: \"Are classes separable?\" Wrong question! Shesha asks: \"Is that separation STABLE under perturbation?\" Tested on 35-69 embedding models across 3 experiments. Shesha outperforms Fisher discriminant, silhouette score, Procrustes, and anisotropy. The correlations with intervention success are rho=0.89-0.96, and the partial correlations after controlling for separability are rho=0.67-0.76. Stability ‚â† separability. Model selection - Shesha exposes what LogME misses . The DINO Paradox - best transfer scores, worst geometric stability. Tested 94 vision models on 6 datasets: DINOv2 ranked #1 in LogME on 4/6 datasets but last or near last in stability on 5/6. SOTA transfer incurs a \"geometric tax.\" CRISPR perturbations - Shesha serves as a new filter for target selection . CRISPR screens find hits by magnitude, but magnitude alone can't distinguish clean lineage drivers from promiscuous regulators. Shesha adds precision. Tested on 811 perturbations, Shesha showed uniformly positive magnitude-stability correlations ranging from rho=0.746 in high-variance screens to rho=0.963 in cleaner activation settings. Notably, in discordant cases, Shesha separates KLF1 (stable, specific) from CEBPA (strong but messy) purely from geometry. Additional validation in neuroscience: Geometric stability predicted neural-behavioral coupling (rho=0.18, p=0.005) in Neuropixels data. Centroid drift showed no relationship (rho=0.00). Stability ‚â† consistency. Try it yourself: PyPI: pip install shesha-geometry Tutorials: https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials Preprint: https://arxiv.org/abs/2601.09173 Code: https://github.com/prashantcraju/geometric-stability",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09173",
      "pdf_url": "https://arxiv.org/pdf/2601.09173",
      "github_links": [
        "https://github.com/prashantcraju/shesha?tab=readme-ov-file#tutorials",
        "https://github.com/prashantcraju/geometric-stability"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09173",
      "scraped_at": "2026-01-16T01:52:51.691110"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "The AI Hippocampus: How Far are We From Human Memory?",
    "paper_url": "https://huggingface.co/papers/2601.09113",
    "authors": [
      "Tong Wu",
      "Yuxuan Wang",
      "Yipeng Kang",
      "Jiaqi Li",
      "Zixia Jia"
    ],
    "stars": "0",
    "details": {
      "title": "The AI Hippocampus: How Far are We From Human Memory?",
      "abstract": "Survey of memory in LLMs and multimodal models, detailing implicit, explicit, and agentic memory, architectures, benchmarks, and challenges in persistence, alignment, and cross-modal retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09113",
      "pdf_url": "https://arxiv.org/pdf/2601.09113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09113",
      "scraped_at": "2026-01-16T01:52:53.593035"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.01075",
    "authors": [
      "Thomas Anderson Keller",
      "Yilun Du",
      "Fangneng Zhan",
      "Benhao Huang",
      "Hansen Jin Lillemark"
    ],
    "stars": "5",
    "details": {
      "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.01075",
      "pdf_url": "https://arxiv.org/pdf/2601.01075",
      "github_links": [
        "https://github.com/hlillemark/flowm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.01075",
      "scraped_at": "2026-01-16T01:52:55.619837"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
    "paper_url": "https://huggingface.co/papers/2601.09609",
    "authors": [
      "Ruihua Song",
      "Yi Zhao",
      "Wei Bi",
      "Yahui Liu",
      "Qian Cao"
    ],
    "stars": "0",
    "details": {
      "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
      "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09609",
      "pdf_url": "https://arxiv.org/pdf/2601.09609",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09609",
      "scraped_at": "2026-01-16T01:52:57.577292"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09536",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
      "abstract": "This paper proposes a unified generative multimodal reasoning paradigm, using a two-stage SFT+RL framework with perception alignment loss and perception reward, and explores bootstrapping step-wise visualizations from text-only reasoning data when multimodal annotation availability is extremely limited.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09536",
      "pdf_url": "https://arxiv.org/pdf/2601.09536",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09536",
      "scraped_at": "2026-01-16T01:52:59.576152"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2601.07287",
    "authors": [
      "Xiao Yang",
      "Kaipeng Zhang",
      "Shenghai Yuan",
      "Yuanyang Yin",
      "yfdeng10"
    ],
    "stars": "0",
    "details": {
      "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision (2025) Plan-X: Instruct Video Generation via Semantic Planning (2025) AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation (2025) Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models (2025) InstanceV: Instance-Level Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07287",
      "pdf_url": "https://arxiv.org/pdf/2601.07287",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07287",
      "scraped_at": "2026-01-16T01:53:02.313236"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
    "paper_url": "https://huggingface.co/papers/2601.06794",
    "authors": [
      "Yixia Li",
      "Xingchen Zeng",
      "Yulan Hu",
      "Lingjie Jiang",
      "Zhicong Li"
    ],
    "stars": "0",
    "details": {
      "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
      "abstract": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06794",
      "pdf_url": "https://arxiv.org/pdf/2601.06794",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06794",
      "scraped_at": "2026-01-16T01:53:04.140516"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.04809",
    "authors": [
      "Yixin Cao",
      "Xinrun Wang",
      "Zhongyuan Peng",
      "Changyi Xiao",
      "SII-Molu"
    ],
    "stars": "6",
    "details": {
      "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
      "abstract": "Scalable Environment Synthesis Given a programming problem (statement + reference solution), SCALER synthesizes a reasoning environment with: Verifiability: deterministic oracle / unit tests provide correctness signals. Difficulty control: explicit scale parameters discretized into difficulty levels. Unbounded instance generation: randomized testcase generation yields unlimited training instances. Adaptive Multi-Environment RL SCALER sustains learning signals at two levels: In-environment difficulty controller: keeps sampling near a target success regime. Environment curation: maintains an active set and replaces saturated/uninformative environments to preserve diversity and long-horizon improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04809",
      "pdf_url": "https://arxiv.org/pdf/2601.04809",
      "github_links": [
        "https://github.com/molumolua/SCALER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04809",
      "scraped_at": "2026-01-16T01:53:05.993611"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "paper_url": "https://huggingface.co/papers/2601.09282",
    "authors": [
      "Jolanta Mizeria-Pietraszko",
      "lsliwko"
    ],
    "stars": "0",
    "details": {
      "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
      "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09282",
      "pdf_url": "https://arxiv.org/pdf/2601.09282",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09282",
      "scraped_at": "2026-01-16T01:53:07.875759"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
    "paper_url": "https://huggingface.co/papers/2601.08472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08472",
      "pdf_url": "https://arxiv.org/pdf/2601.08472",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08472",
      "scraped_at": "2026-01-16T01:53:09.794070"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
    "paper_url": "https://huggingface.co/papers/2601.04469",
    "authors": [
      "Aleksey Komissarov",
      "Ekaterina Chelombitko",
      "Iaroslav Chelombitko"
    ],
    "stars": "0",
    "details": {
      "title": "SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers",
      "abstract": "The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP",
      "arxiv_page_url": "https://arxiv.org/abs/2601.04469",
      "pdf_url": "https://arxiv.org/pdf/2601.04469",
      "github_links": [
        "https://github.com/AragonerUA/SampoNLP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.04469",
      "scraped_at": "2026-01-16T01:53:11.668045"
    },
    "scraped_date": "2026-01-16"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "125",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-17T01:46:10.410290"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "152",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-17T01:46:12.281061"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-17T01:46:14.129704"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-17T01:46:15.994820"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "22",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-17T01:46:17.812994"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "34",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-17T01:46:19.609904"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "12",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-17T01:46:21.464351"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "332",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-17T01:46:23.272686"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-17T01:46:25.131294"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-17T01:46:26.958024"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-17T01:46:28.768874"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-17T01:46:30.627023"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-17T01:46:32.424107"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "20",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-17T01:46:34.429955"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-17T01:46:36.227451"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-17T01:46:38.096548"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-17T01:46:39.969567"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-17T01:46:41.766508"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "75",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-17T01:46:43.586501"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-17T01:46:45.366327"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "0",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-17T01:46:47.168503"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-17T01:46:48.943667"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-17T01:46:50.737198"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-17T01:46:52.610283"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "4",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-17T01:46:54.410994"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-17T01:46:56.176236"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-17T01:46:57.933744"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-17T01:46:59.691457"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-17T01:47:01.464358"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-17T01:47:03.230799"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "45",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-17T01:47:04.980548"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-17T01:47:06.760677"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-17T01:47:08.580845"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-17T01:47:10.393786"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-17T01:47:12.142262"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-17T01:47:13.912097"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-17T01:47:15.696403"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "20",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-17T01:47:17.436915"
    },
    "scraped_date": "2026-01-17"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "168",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-18T01:58:28.325011"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "135",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-18T01:58:30.488100"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-18T01:58:32.445211"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-18T01:58:34.356864"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "29",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-18T01:58:36.230150"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-18T01:58:38.101276"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "14",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-18T01:58:40.112824"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "336",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-18T01:58:42.067107"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-18T01:58:43.999136"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "86",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-18T01:58:45.918917"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-18T01:58:47.820357"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-18T01:58:49.722522"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-18T01:58:52.822014"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-18T01:58:54.734358"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "21",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-18T01:58:56.653857"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-18T01:58:58.523736"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-18T01:59:00.469539"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-18T01:59:02.337989"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-18T01:59:04.192079"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "164",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [
        "https://github.com/HeartMuLa/heartlib"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-18T01:59:06.066745"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "159",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-18T01:59:08.004724"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-18T01:59:09.867129"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-18T01:59:11.788446"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-18T01:59:13.656078"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-18T01:59:15.486341"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "4",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-18T01:59:17.319882"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-18T01:59:19.169455"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-18T01:59:21.091232"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-18T01:59:22.939202"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-18T01:59:24.819341"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-18T01:59:26.635216"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "55",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-18T01:59:28.561269"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-18T01:59:30.387446"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-18T01:59:32.244800"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-18T01:59:34.092644"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-18T01:59:35.925826"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-18T01:59:37.795150"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "20",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-18T01:59:39.653822"
    },
    "scraped_date": "2026-01-18"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.09668",
    "authors": [],
    "stars": "172",
    "details": {
      "title": "STEP3-VL-10B Technical Report",
      "abstract": "üéâ Introducing Step3-VL-10B, Compact Yet Frontier Multimodal Intelligence, with best performance at 10B model scale, even matching 10x-20x size of open-source frontier models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09668",
      "pdf_url": "https://arxiv.org/pdf/2601.09668",
      "github_links": [
        "https://github.com/stepfun-ai/PaCoRe",
        "https://github.com/stepfun-ai/Step3-VL-10B"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09668",
      "scraped_at": "2026-01-19T01:56:41.527757"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10477",
    "authors": [],
    "stars": "139",
    "details": {
      "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
      "abstract": "It is a very interesting idea of practical value.  Applying VLM  + RL on (real world) Socio-Semantic Segmentation task!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10477",
      "pdf_url": "https://arxiv.org/pdf/2601.10477",
      "github_links": [
        "https://github.com/AMAP-ML/SocioReasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10477",
      "scraped_at": "2026-01-19T01:56:43.398499"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.08763",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "abstract": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08763",
      "pdf_url": "https://arxiv.org/pdf/2601.08763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08763",
      "scraped_at": "2026-01-19T01:56:45.417046"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.09667",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "abstract": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09667",
      "pdf_url": "https://arxiv.org/pdf/2601.09667",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09667",
      "scraped_at": "2026-01-19T01:56:47.195191"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "paper_url": "https://huggingface.co/papers/2601.02242",
    "authors": [
      "Bulat Suleimanov",
      "WildChlamydia",
      "iitolstykh",
      "grac20101",
      "Riko0"
    ],
    "stars": "33",
    "details": {
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "üéâ Introducing VIBE: Visual Instruction Based Editor, a compact and powerful system that fits comfortably within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100, without any extra optimizations!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.02242",
      "pdf_url": "https://arxiv.org/pdf/2601.02242",
      "github_links": [
        "https://github.com/ai-forever/vibe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.02242",
      "scraped_at": "2026-01-19T01:56:49.104995"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.07641",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
      "abstract": "üéâ Introducing Test-Time Tool Evolution (TTE) ‚Äî Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning (arXiv:2601.07641)! Why it matters: Scientific problems don‚Äôt come with a complete tool library. TTE lets agents synthesize ‚Üí validate ‚Üí evolve executable tools at inference time, turning tools from ‚Äúfixed resources‚Äù into problem-driven, self-improving artifacts. ‚úÖ Code + dataset are fully open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07641",
      "pdf_url": "https://arxiv.org/pdf/2601.07641",
      "github_links": [
        "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07641",
      "scraped_at": "2026-01-19T01:56:50.951705"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "paper_url": "https://huggingface.co/papers/2601.10402",
    "authors": [],
    "stars": "340",
    "details": {
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "abstract": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10402",
      "pdf_url": "https://arxiv.org/pdf/2601.10402",
      "github_links": [
        "https://github.com/sjtu-sai-agents/ML-Master"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10402",
      "scraped_at": "2026-01-19T01:56:52.804505"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10305",
    "authors": [
      "fengziyong",
      "SeriousBro",
      "dfgdgh",
      "TianchengGu",
      "dewecho"
    ],
    "stars": "16",
    "details": {
      "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
      "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10305",
      "pdf_url": "https://arxiv.org/pdf/2601.10305",
      "github_links": [
        "https://github.com/deepglint/DanQing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10305",
      "scraped_at": "2026-01-19T01:56:54.714747"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.10061",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
      "abstract": "paper link: https://arxiv.org/pdf/2601.10061",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10061",
      "pdf_url": "https://arxiv.org/pdf/2601.10061",
      "github_links": [
        "https://github.com/VisionChengzhuo/CoF-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10061",
      "scraped_at": "2026-01-19T01:56:56.691194"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "paper_url": "https://huggingface.co/papers/2601.10332",
    "authors": [],
    "stars": "123",
    "details": {
      "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
      "abstract": "Github: https://github.com/zhijie-group/Think-Then-Generate",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10332",
      "pdf_url": "https://arxiv.org/pdf/2601.10332",
      "github_links": [
        "https://github.com/zhijie-group/Think-Then-Generate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10332",
      "scraped_at": "2026-01-19T01:56:58.577937"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09881",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Transition Matching Distillation for Fast Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VDOT: Efficient Unified Video Creation via Optimal Transport Distillation (2025) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training (2025) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories (2025) USV: Unified Sparsification for Accelerating Video Diffusion Models (2025) Few-Step Distillation for Text-to-Image Generation: A Practical Guide (2025) Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09881",
      "pdf_url": "https://arxiv.org/pdf/2601.09881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09881",
      "scraped_at": "2026-01-19T01:57:00.381000"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "paper_url": "https://huggingface.co/papers/2601.10714",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "abstract": "[TL;DR] We present Alterbute, a diffusion-based method for editing an object‚Äôs intrinsic attributes ‚Äî color, texture, material, and shape ‚Äî while preserving its perceived identity and scene context. Paper üìÑ: https://arxiv.org/pdf/2601.10714 Project page üåê: https://talreiss.github.io/alterbute/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10714",
      "pdf_url": "https://arxiv.org/pdf/2601.10714",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10714",
      "scraped_at": "2026-01-19T01:57:02.250681"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "paper_url": "https://huggingface.co/papers/2601.10611",
    "authors": [
      "gstoica3",
      "praeclarumjj3",
      "tairaa",
      "kimdon20",
      "zhongzhengrenzhang"
    ],
    "stars": "0",
    "details": {
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VideoWeave: A Data-Centric Approach for Efficient Video Understanding (2026) TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs (2025) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation (2025) FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models (2025) VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models (2025) LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10611",
      "pdf_url": "https://arxiv.org/pdf/2601.10611",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10611",
      "scraped_at": "2026-01-19T01:57:04.107961"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "paper_url": "https://huggingface.co/papers/2601.10156",
    "authors": [
      "Shikun Zhang",
      "Peiyang Liu",
      "Lijun Li",
      "Zhangchi Xue",
      "MurrayTom"
    ],
    "stars": "0",
    "details": {
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "abstract": "GitHub: https://github.com/MurrayTom/ToolSafe",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10156",
      "pdf_url": "https://arxiv.org/pdf/2601.10156",
      "github_links": [
        "https://github.com/MurrayTom/ToolSafe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10156",
      "scraped_at": "2026-01-19T01:57:05.942403"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "paper_url": "https://huggingface.co/papers/2601.10712",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "abstract": "üí° Overview We propose MatchTIR, a fine-grained reinforcement learning framework specifically designed for Tool-Integrated Reasoning (TIR). The core principle of MatchTIR is to introduce precise supervision via bipartite matching-based turn-level reward assignment, allowing the model to distinguish between effective, redundant, or erroneous tool calls in multi-turn scenarios. üî•Key Insights We highlight that existing RL methods assign uniform advantages to all steps, failing to distinguish between critical, redundant, or erroneous tool calls in long-horizon trajectories. We reformulate credit assignment as a bipartite matching problem between predicted and ground-truth tool calls. By constructing a matching matrix, we get the precise reward. We introduce two matching mechanisms: Hard Assignment (KM algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment, providing dense and verifiable supervision signals. To balance local precision with global success, we propose a scheme that integrates trajectory-level signals (overall quality) with turn-level advantages (individual step contribution via discounted rewards). Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model outperforms larger 8B competitors, particularly in long-horizon and multi-turn tasks. üîß‚ú® All the code, datasets and model checkpoints of MatchTIR are fully open-sourced: Github: https://github.com/quchangle1/MatchTIR Datasets & Models: https://huggingface.co/collections/ChangleQu/matchtir",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10712",
      "pdf_url": "https://arxiv.org/pdf/2601.10712",
      "github_links": [
        "https://github.com/quchangle1/MatchTIR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10712",
      "scraped_at": "2026-01-19T01:57:07.809566"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "paper_url": "https://huggingface.co/papers/2601.10527",
    "authors": [
      "Yutao Wu",
      "Yixu Wang",
      "Xingjun Ma",
      "xinwang22",
      "DobyXu"
    ],
    "stars": "22",
    "details": {
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10527",
      "pdf_url": "https://arxiv.org/pdf/2601.10527",
      "github_links": [
        "https://github.com/XSafeAI/AI-safety-report"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10527",
      "scraped_at": "2026-01-19T01:57:09.668057"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "paper_url": "https://huggingface.co/papers/2601.10657",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10657",
      "pdf_url": "https://arxiv.org/pdf/2601.10657",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10657",
      "scraped_at": "2026-01-19T01:57:11.540441"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "paper_url": "https://huggingface.co/papers/2601.10592",
    "authors": [],
    "stars": "198",
    "details": {
      "title": "Action100M: A Large-scale Video Action Dataset",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos (2025) Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training (2025) R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios (2025) SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models (2026) InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision (2025) OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory (2025) LongVideoAgent: Multi-Agent Reasoning with Long Videos (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10592",
      "pdf_url": "https://arxiv.org/pdf/2601.10592",
      "github_links": [
        "https://github.com/facebookresearch/Action100M"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10592",
      "scraped_at": "2026-01-19T01:57:13.397394"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "paper_url": "https://huggingface.co/papers/2601.10547",
    "authors": [
      "hhguo",
      "YuanyuanWang",
      "Gongxizhu",
      "bverxie",
      "Dongchao"
    ],
    "stars": "358",
    "details": {
      "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
      "abstract": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio\u0002text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10547",
      "pdf_url": "https://arxiv.org/pdf/2601.10547",
      "github_links": [
        "https://github.com/HeartMuLa/heartlib"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10547",
      "scraped_at": "2026-01-19T01:57:15.280746"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
      "abstract": "Project page link from the paper: https://grisoon.github.io/FlowAct-R1/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10103",
      "pdf_url": "https://arxiv.org/pdf/2601.10103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10103",
      "scraped_at": "2026-01-19T01:57:17.135623"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "paper_url": "https://huggingface.co/papers/2601.10131",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
      "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multiobjective control and numeric reasoning without external structure and feedback. We introduce M4olGen, a fragment-level, retrievalaugmented, two-stage framework for molecule generation under multi-property constraints.Stage I: Prototype generation: a multiagent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II: RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multihop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10131",
      "pdf_url": "https://arxiv.org/pdf/2601.10131",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10131",
      "scraped_at": "2026-01-19T01:57:19.016650"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "paper_url": "https://huggingface.co/papers/2601.10553",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "abstract": "I assume I can also use any arbitrary off-the-shelf metric as a substitute for 'surprise' then?ü§£ Coming soon: Inference-time Overall Alignment of Video Generative Models with Random Seed.üòé",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10553",
      "pdf_url": "https://arxiv.org/pdf/2601.10553",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10553",
      "scraped_at": "2026-01-19T01:57:20.881698"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "paper_url": "https://huggingface.co/papers/2601.08881",
    "authors": [
      "Tiankai Hang",
      "Yiji Cheng",
      "Juan Cao",
      "Yu Xu",
      "Yana-Hangabina"
    ],
    "stars": "0",
    "details": {
      "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation (2025) 3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory (2025) Unified Personalized Understanding, Generating and Editing (2026) Loom: Diffusion-Transformer for Interleaved Generation (2025) Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach. (2025) MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation (2025) WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08881",
      "pdf_url": "https://arxiv.org/pdf/2601.08881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08881",
      "scraped_at": "2026-01-19T01:57:22.792635"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "paper_url": "https://huggingface.co/papers/2601.09142",
    "authors": [
      "Yi Yang",
      "Yan Lin",
      "FutureMa"
    ],
    "stars": "0",
    "details": {
      "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
      "abstract": "Thanks for featuring our work! üöÄ EvasionBench aims to bridge the gap in financial transparency. We've released the Eva-4B model and the 1k human-annotated test set. üìù Paper: https://arxiv.org/abs/2601.09142 ü§ó Model: https://huggingface.co/FutureMa/Eva-4B üéÆ Demo: https://huggingface.co/spaces/FutureMa/financial-evasion-detection Feel free to ask any questions!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09142",
      "pdf_url": "https://arxiv.org/pdf/2601.09142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09142",
      "scraped_at": "2026-01-19T01:57:24.623126"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "paper_url": "https://huggingface.co/papers/2601.06431",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "abstract": "In this work, we propose LSRIF, a logic-structured training framework. We construct LSRINSTRUCT, a multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and out-of-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06431",
      "pdf_url": "https://arxiv.org/pdf/2601.06431",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06431",
      "scraped_at": "2026-01-19T01:57:26.426957"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "paper_url": "https://huggingface.co/papers/2601.10201",
    "authors": [
      "TongZhang",
      "RickyDeSkywalker",
      "FlippyDora"
    ],
    "stars": "5",
    "details": {
      "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
      "abstract": "Abstract Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show that the effectiveness of PRL could be verified and generalized.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10201",
      "pdf_url": "https://arxiv.org/pdf/2601.10201",
      "github_links": [
        "https://github.com/MaxwellJryao/Process-Reward-Learning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10201",
      "scraped_at": "2026-01-19T01:57:28.241301"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.10129",
    "authors": [
      "Linquan Wu",
      "jackykeung",
      "afunnyhy",
      "fly1113",
      "txjiang"
    ],
    "stars": "0",
    "details": {
      "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
      "abstract": "https://github.com/Svardfox/LaViT",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10129",
      "pdf_url": "https://arxiv.org/pdf/2601.10129",
      "github_links": [
        "https://github.com/Svardfox/LaViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10129",
      "scraped_at": "2026-01-19T01:57:30.064872"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "paper_url": "https://huggingface.co/papers/2601.09499",
    "authors": [
      "vedaldi",
      "zlai",
      "einsafutdinov",
      "edgarsucar"
    ],
    "stars": "68",
    "details": {
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09499",
      "pdf_url": "https://arxiv.org/pdf/2601.09499",
      "github_links": [
        "https://github.com/eldar/vdpm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09499",
      "scraped_at": "2026-01-19T01:57:31.858383"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "paper_url": "https://huggingface.co/papers/2601.06378",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
      "abstract": "üöÄ New work: RigMo ‚Äî Unifying Rig & Motion Learning for Generative Animation Rigging and motion are two hard problems‚Äîusually solved separately. RigMo unifies them. A feed-forward framework that jointly learns rig structure + motion directly from raw mesh sequences ‚Üí no rig annotations, no per-sequence optimization. ‚ú® Highlights ‚Ä¢ Explicit Gaussian bones + skinning weights ‚Ä¢ SE(3) bone motions from compact latents ‚Ä¢ Motion-DiT for controllable motion synthesis ‚Ä¢ Interpretable, reusable, truly animatable 4D assets üìä Strong results on DeformingThings4D / Objaverse-XL / TrueBones üîó Project page: https://rigmo-page.github.io üìÑ Paper (arXiv): https://arxiv.org/abs/2601.06378 üì∫ Video: https://youtube.com/watch?v=0H0lsM3USVM üíª Code: coming soon #ComputerGraphics #Animation #Rigging #4D #3D #GenerativeAI",
      "arxiv_page_url": "https://arxiv.org/abs/2601.06378",
      "pdf_url": "https://arxiv.org/pdf/2601.06378",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.06378",
      "scraped_at": "2026-01-19T01:57:33.734234"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "paper_url": "https://huggingface.co/papers/2601.10080",
    "authors": [
      "Kun Zhou",
      "shangjingbo",
      "hyp1231",
      "ylf1017",
      "KomeijiForce"
    ],
    "stars": "0",
    "details": {
      "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "abstract": "Deriving Character Logic from Storyline as Codified Decision Trees",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10080",
      "pdf_url": "https://arxiv.org/pdf/2601.10080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10080",
      "scraped_at": "2026-01-19T01:57:35.523261"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "paper_url": "https://huggingface.co/papers/2601.10338",
    "authors": [
      "Ruitao Feng",
      "Weizhe Wang",
      "Gelei",
      "yaozhang",
      "sumleo"
    ],
    "stars": "0",
    "details": {
      "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
      "abstract": "Really interesting and timely work‚Äîagent ‚Äúskills‚Äù seem like a rapidly growing supply-chain attack surface, and it‚Äôs refreshing to see a large-scale empirical analysis rather than a purely conceptual treatment. The scale and methodology (tens of thousands of skills analyzed using a combined static and LLM-based pipeline) are particularly compelling, and the 14-pattern, four-category vulnerability taxonomy helps make the risk landscape much more concrete. The reported numbers are striking: roughly a quarter of skills containing vulnerabilities, with a non-trivial fraction exhibiting high-severity patterns suggestive of malicious behavior, which strongly motivates the need for better ecosystem defenses. It would be interesting to learn more about how ambiguous cases were handled when separating malicious intent from accidental insecurity, whether vulnerability prevalence differs across marketplaces with different governance models, and how developers might practically use the detection results for remediation. The call for capability-based permission systems is persuasive, and additional discussion of concrete enforcement mechanisms (e.g., sandboxing, least-privilege access, signing, or dependency controls) would further strengthen the practical impact.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10338",
      "pdf_url": "https://arxiv.org/pdf/2601.10338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10338",
      "scraped_at": "2026-01-19T01:57:37.320211"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "paper_url": "https://huggingface.co/papers/2601.09876",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
      "abstract": "Introducing ClinSQL, a 633-task expert-annotated benchmark on MIMIC-IV v3.1 for real-world clinical text-to-SQL.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09876",
      "pdf_url": "https://arxiv.org/pdf/2601.09876",
      "github_links": [
        "https://github.com/Barryshen1/ClinSQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09876",
      "scraped_at": "2026-01-19T01:57:39.137495"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "paper_url": "https://huggingface.co/papers/2601.08302",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "abstract": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08302",
      "pdf_url": "https://arxiv.org/pdf/2601.08302",
      "github_links": [
        "https://github.com/Marvin2108/ESCID-LLM-APET"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08302",
      "scraped_at": "2026-01-19T01:57:40.957007"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2601.10716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
      "abstract": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10716",
      "pdf_url": "https://arxiv.org/pdf/2601.10716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10716",
      "scraped_at": "2026-01-19T01:57:42.760780"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10124",
    "authors": [
      "Lei Zhu",
      "xingzhaohu",
      "yscript"
    ],
    "stars": "4",
    "details": {
      "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
      "abstract": "Code available at: https://github.com/script-Yang/VQ-Seg",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10124",
      "pdf_url": "https://arxiv.org/pdf/2601.10124",
      "github_links": [
        "https://github.com/script-Yang/VQ-Seg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10124",
      "scraped_at": "2026-01-19T01:57:44.669585"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "paper_url": "https://huggingface.co/papers/2601.09923",
    "authors": [
      "ftramer",
      "nkristina",
      "tom-bl",
      "rcmullins",
      "aprilflower"
    ],
    "stars": "0",
    "details": {
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "abstract": "When AI agents control your mouse, a single malicious email can drain your accounts. We built the first system-level defense to sandbox these agents, and the results challenged our assumptions about AI. It turns out, digital environments are more predictable than we admit: our research shows that half of OSWorld tasks can be solved without the agent ever seeing the screen. By leveraging this, CaMeLs provides strict security without killing performance. You don't need an omnipotent agent; you need a predictable one.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09923",
      "pdf_url": "https://arxiv.org/pdf/2601.09923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09923",
      "scraped_at": "2026-01-19T01:57:46.515065"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "paper_url": "https://huggingface.co/papers/2601.08297",
    "authors": [
      "Chao Du",
      "Cunxiao Du",
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Yuan Cheng"
    ],
    "stars": "0",
    "details": {
      "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
      "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Œî-th sub-diagonal for some offset Œî. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08297",
      "pdf_url": "https://arxiv.org/pdf/2601.08297",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08297",
      "scraped_at": "2026-01-19T01:57:48.295647"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.00756",
    "authors": [
      "Dimitrios Rafailidis",
      "Tomk187"
    ],
    "stars": "21",
    "details": {
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.00756",
      "pdf_url": "https://arxiv.org/pdf/2601.00756",
      "github_links": [
        "https://github.com/Thomkat/MBC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.00756",
      "scraped_at": "2026-01-19T01:57:50.101478"
    },
    "scraped_date": "2026-01-19"
  },
  {
    "title": "Your Group-Relative Advantage Is Biased",
    "paper_url": "https://huggingface.co/papers/2601.08521",
    "authors": [
      "Xiaohan Wang",
      "Yikunb",
      "PandaChai",
      "chenzherui007",
      "ShortCatisLong"
    ],
    "stars": "0",
    "details": {
      "title": "Your Group-Relative Advantage Is Biased",
      "abstract": "This paper fundamentally shows that: \"The commonly used group-relative advantage estimator is inherently biased except at p_t = 0.5: it systematically underestimates true advantage on hard prompts and overestimates true advantag on easy prompts\". This bias is not just random‚Äîit becomes deterministic in extreme difficulty regimes, meaning the estimator must underestimate for very hard prompts and must overestimate for very easy prompts. This analysis highlights this as a core limitation in group-relative methods and motivates corrections that better align estimated and true advantage.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08521",
      "pdf_url": "https://arxiv.org/pdf/2601.08521",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08521",
      "scraped_at": "2026-01-20T01:51:45.604021"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
    "paper_url": "https://huggingface.co/papers/2601.11496",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
      "abstract": "Imagine a company introducing a shiny new technology üçé. Not to use it, but to force a regulator to rewrite the rules. Once the rules change? The apple is discarded. The technology is never used. But the strategic shift is complete: the manipulator secures a higher payoff, while other players are left worse off. In our new paper, \"The Poisoned Apple Effect\", we identify a strategic vulnerability in AI-mediated markets. We show how agents can expand the technological space purely to manipulate the mediator's design. The result? The manipulator profits from the new rules, while competitors (and social welfare) pay the price.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11496",
      "pdf_url": "https://arxiv.org/pdf/2601.11496",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11496",
      "scraped_at": "2026-01-20T01:51:47.504722"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
    "paper_url": "https://huggingface.co/papers/2601.10355",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
      "abstract": "We propose a novel \"Text to Trajectory\" paradigm to address the scarcity of multi-turn tool usage trajectory data needed to train agents. Traditional methods rely on predefined API sets to synthesize data, but this approach is limited by the scope of tools and is costly. We observe that text corpora naturally contain rich multi-step problem-solving experiences, which can be extracted and transformed into realistic, scalable, and high-quality multi-turn tool usage data. Based on this insight, we develop a pipeline called GEM to enable automatic generation and extraction of multi-turn tool-use trajectory to validate this paradigm.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10355",
      "pdf_url": "https://arxiv.org/pdf/2601.10355",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10355",
      "scraped_at": "2026-01-20T01:51:49.467451"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
    "paper_url": "https://huggingface.co/papers/2601.08430",
    "authors": [
      "Jiale Zhao",
      "Sunzhu Li",
      "kaikezhang",
      "liushunyu",
      "renhuimin"
    ],
    "stars": "25",
    "details": {
      "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
      "abstract": "We introduce RubricHub, a large-scale (~110k) and multi-domain rubric dataset constructed via an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces highly discriminative criteria capable of capturing subtle nuances in model responses. dataset: https://huggingface.co/datasets/sojuL/RubricHub_v1 github: https://github.com/teqkilla/RubricHub arxiv: https://arxiv.org/abs/2601.08430 alphaXiv: https://www.alphaxiv.org/zh/overview/2601.08430v1 Training  code is coming soon!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08430",
      "pdf_url": "https://arxiv.org/pdf/2601.08430",
      "github_links": [
        "https://github.com/teqkilla/RubricHub"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08430",
      "scraped_at": "2026-01-20T01:51:51.409998"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
    "paper_url": "https://huggingface.co/papers/2601.11000",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
      "abstract": "üí° Overview Personalization is increasingly adopted in modern LLM systems, but we find it can systematically distort factual reasoning. We identify personalization-induced hallucinations, where models generate answers aligned with user history rather than objective truth. To mitigate this, we also propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that detects harmful personalization and adaptively steers internal representations to recover factual correctness while keeping useful personalization. üî•Key Insights Problem discovery: We provide the first systematic study of personalization-induced hallucinations and show risks to factual reliability, downstream knowledge acquisition, and long-term user trust. Mitigation method: We propose FPPS, a lightweight inference-time framework that selectively restores factuality under personalization. Evaluation dataset: We develop PFQABench to jointly evaluate factual QA and personalized QA under aligned user sessions, enabling controlled assessment of factuality failures and mitigation. Results: Extensive experiments across multiple LLM backbones and personalization methods show FPPS substantially improves factual accuracy without sacrificing personalization performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11000",
      "pdf_url": "https://arxiv.org/pdf/2601.11000",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11000",
      "scraped_at": "2026-01-20T01:51:53.340876"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2601.11404",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
      "abstract": "abs: Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11404",
      "pdf_url": "https://arxiv.org/pdf/2601.11404",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11404",
      "scraped_at": "2026-01-20T01:51:55.298099"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
    "paper_url": "https://huggingface.co/papers/2601.11037",
    "authors": [
      "Yunbo Tang",
      "bitwjg",
      "Elliott",
      "yongjing",
      "ShiyuLiu"
    ],
    "stars": "16",
    "details": {
      "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
      "abstract": "Boundary-Aware Policy OptimizationÔºàBAPOÔºâ is a novel reinforcement learning-based framework for training reliable agentic search models. Beyond correctness rewards, BAPO incorporates boundary-aware rewards to encourage appropriate \"I Don't Know\" (IDK) responses. To tackle the tradeoff between exploration and exploitation during RL training, we introduce an adaptive reward modulator to prevent the model from being over-encouraged to admit ignorance.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11037",
      "pdf_url": "https://arxiv.org/pdf/2601.11037",
      "github_links": [
        "https://github.com/Liushiyu-0709/BAPO-Reliable-Search"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11037",
      "scraped_at": "2026-01-20T01:51:57.225931"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
    "paper_url": "https://huggingface.co/papers/2601.10909",
    "authors": [
      "Gerard Pons-Moll",
      "andreas-geiger",
      "Yongcao",
      "xianghuix",
      "coralli"
    ],
    "stars": "37",
    "details": {
      "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
      "abstract": "TL;DR: We introduce the first framework for atomic, part-level motion control, powered by our new hierarchical Frankenstein dataset (39h) constructed via LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10909",
      "pdf_url": "https://arxiv.org/pdf/2601.10909",
      "github_links": [
        "https://github.com/Coral79/FrankenMotion-Code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10909",
      "scraped_at": "2026-01-20T01:51:59.256617"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM",
    "paper_url": "https://huggingface.co/papers/2601.09001",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM",
      "abstract": "A first exploration of a lightweight, inference-time method for monitoring LLM accuracy under domain drift using output-entropy traces derived from next-token probabilities. This approach demonstrates promising results for slice-level accuracy estimation across STEM reasoning benchmarks, suggesting that entropy-based signals could serve as a practical tool for real-time model monitoring in production. It offers potential utility for both continuous performance tracking and prioritizing data acquisition in dynamic environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09001",
      "pdf_url": "https://arxiv.org/pdf/2601.09001",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09001",
      "scraped_at": "2026-01-20T01:52:01.228592"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
    "paper_url": "https://huggingface.co/papers/2601.09195",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
      "abstract": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks. Codes are available at https://github.com/Utaotao/ProFit",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09195",
      "pdf_url": "https://arxiv.org/pdf/2601.09195",
      "github_links": [
        "https://github.com/Utaotao/ProFit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09195",
      "scraped_at": "2026-01-20T01:52:03.229359"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.10781",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
      "abstract": "We introduce FOFPred, a language-driven future optical flow prediction framework that enables improved robot control and video generation. Instead of reacting to motion, FOFPred predicts how motion will evolve, conditioned on natural language. üåê Project: fofpred.github.io üìÑ Paper: arxiv.org/abs/2601.10781 üíª Code: github.com/SalesforceAIResearch/FOFPred ü§ó Model: huggingface.co/Salesforce/FOFPred üïπÔ∏è Demo: fofpred.salesforceresearch.ai",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10781",
      "pdf_url": "https://arxiv.org/pdf/2601.10781",
      "github_links": [
        "https://github.com/SalesforceAIResearch/FOFPred"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10781",
      "scraped_at": "2026-01-20T01:52:05.129097"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
    "paper_url": "https://huggingface.co/papers/2601.11514",
    "authors": [],
    "stars": "175",
    "details": {
      "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
      "abstract": "Project Page | Paper | Video | HF-Model | HF Evaluation Dataset",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11514",
      "pdf_url": "https://arxiv.org/pdf/2601.11514",
      "github_links": [
        "https://github.com/facebookresearch/ShapeR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11514",
      "scraped_at": "2026-01-20T01:52:07.039726"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Reasoning Models Generate Societies of Thought",
    "paper_url": "https://huggingface.co/papers/2601.10825",
    "authors": [
      "James Evans",
      "Blaise Ag√ºera y Arcas",
      "ninoscherrer",
      "ShiYangLAI",
      "junsol"
    ],
    "stars": "0",
    "details": {
      "title": "Reasoning Models Generate Societies of Thought",
      "abstract": "Reasoning models gain accuracy via internal multi-agent-like debates among diverse perspectives, enabling broader exploration of solutions and improved reasoning than single-agent baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10825",
      "pdf_url": "https://arxiv.org/pdf/2601.10825",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10825",
      "scraped_at": "2026-01-20T01:52:08.924713"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
    "paper_url": "https://huggingface.co/papers/2601.11087",
    "authors": [
      "Zheng Zhang",
      "Qiyuan Zhang",
      "shen12313",
      "Shuaishuai0219",
      "BiaoGong"
    ],
    "stars": "0",
    "details": {
      "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
      "abstract": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11087",
      "pdf_url": "https://arxiv.org/pdf/2601.11087",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11087",
      "scraped_at": "2026-01-20T01:52:10.837296"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
    "paper_url": "https://huggingface.co/papers/2601.09636",
    "authors": [
      "Liqiang Nie",
      "Weili Guan",
      "Rui Shao",
      "cgwfeel",
      "user0102"
    ],
    "stars": "0",
    "details": {
      "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
      "abstract": "good",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09636",
      "pdf_url": "https://arxiv.org/pdf/2601.09636",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09636",
      "scraped_at": "2026-01-20T01:52:12.769316"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Building Production-Ready Probes For Gemini",
    "paper_url": "https://huggingface.co/papers/2601.11516",
    "authors": [
      "Rohin Shah",
      "Zheng Wang",
      "Joshua Engels",
      "bilalchughtai",
      "jkramar"
    ],
    "stars": "0",
    "details": {
      "title": "Building Production-Ready Probes For Gemini",
      "abstract": "Proposes long-context robust probes for Gemini misuse mitigation, showing architecture and diverse-training distribution requirements for generalization, and demonstrates efficient pairing with prompted classifiers and automated probe architecture search.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11516",
      "pdf_url": "https://arxiv.org/pdf/2601.11516",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11516",
      "scraped_at": "2026-01-20T01:52:14.665843"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
    "paper_url": "https://huggingface.co/papers/2601.11044",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts",
      "abstract": "Some of the observations founded are :- i. Long-horizon tasks remain challenging : Even frontier models struggle with sustained reasoning over real world tasks that require 1M tokens and 90 tool calls, indicating limits in long context autonomy. ii. Proprietary models outperform open source models: Closed source models achieve a higher average score (48.4%) than open source counterparts (32.1%), revealing a persistent performance gap on complex agentic tasks. iii. Feedback driven self correction varies widely: Models like GPT 5.2 and Claude show strong gains from iterative feedback, while others (e.g., DeepSeek-V3.2) exhibit minimal or no improvement after feedback. iv. Efficiency trade offs are significant: High performing models often consume far more tokens and time, some models (e.g. Grok-4.1 Fast) are more token efficient despite lower absolute scores. v. Agentic scaffolds strongly influence performance: Models tend to perform best within their native or optimized ecosystems, highlighting that agent performance depends on tight coupling between the model and its scaffold not the model alone.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11044",
      "pdf_url": "https://arxiv.org/pdf/2601.11044",
      "github_links": [
        "https://github.com/GAIR-NLP/AgencyBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11044",
      "scraped_at": "2026-01-20T01:52:16.689123"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
    "paper_url": "https://huggingface.co/papers/2601.07812",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
      "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07812",
      "pdf_url": "https://arxiv.org/pdf/2601.07812",
      "github_links": [
        "https://github.com/anurag-198/MIMIC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07812",
      "scraped_at": "2026-01-20T01:52:18.607248"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "paper_url": "https://huggingface.co/papers/2601.11354",
    "authors": [
      "Jingjing Gong",
      "Weiyi Wang",
      "xpqiu",
      "xjhuang",
      "dalstonchen"
    ],
    "stars": "4",
    "details": {
      "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
      "abstract": "Introduces AstroReason-Bench, a benchmark for evaluating unified agentic planning in space planning problems with physics constraints, heterogeneous objectives, and long-horizon decisions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11354",
      "pdf_url": "https://arxiv.org/pdf/2601.11354",
      "github_links": [
        "https://github.com/Mtrya/astro-reason"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11354",
      "scraped_at": "2026-01-20T01:52:20.488459"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "Language of Thought Shapes Output Diversity in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.11227",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Language of Thought Shapes Output Diversity in Large Language Models",
      "abstract": "This paper reveals that controlling the language used during model thinking‚Äîthe language of thought‚Äîprovides a novel and structural source of output diversity.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11227",
      "pdf_url": "https://arxiv.org/pdf/2601.11227",
      "github_links": [
        "https://github.com/iNLP-Lab/Multilingual-LoT-Diversity"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11227",
      "scraped_at": "2026-01-20T01:52:22.355536"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge",
    "paper_url": "https://huggingface.co/papers/2601.10922",
    "authors": [
      "Vikas Kumar",
      "Pavel Bushuyeu",
      "Boris Sobolev",
      "Michael Buriek",
      "Yosub Shin"
    ],
    "stars": "0",
    "details": {
      "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge",
      "abstract": "Some of the observations founded are : i. Difficulty based example selection is the dominant driver of performance: Selecting challenging but learnable examples yields the largest gains in multimodal reasoning accuracy, outperforming other curation strategies. ii. Increasing dataset size does not reliably improve mean accuracy: Once a well aligned base dataset is chosen, larger datasets mainly reduce run to run variance rather than boosting average performance. iii. Data curation operates in a saturation regime: Most performance improvements come from a relatively small number of carefully curated examples, with diminishing returns from adding more data. iv. Common diversity heuristics provide little or no benefit: Techniques such as clustering based diversity, category balancing, and synthetic augmentation often fail to improve performance and can even degrade accuracy. v. Alignment between dataset, benchmark, and base model is crucial: Strong alignment amplifies the effectiveness of difficulty filtering and explains why compact, well aligned datasets can outperform larger but less aligned ones.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10922",
      "pdf_url": "https://arxiv.org/pdf/2601.10922",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10922",
      "scraped_at": "2026-01-20T01:52:24.215009"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "paper_url": "https://huggingface.co/papers/2601.09255",
    "authors": [
      "Boxi Wu",
      "Xiaofei He",
      "Hengjia Li",
      "zjuyb"
    ],
    "stars": "0",
    "details": {
      "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
      "abstract": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline, PhyRPR: PhyReason‚ÄìPhyPlan‚ÄìPhyRefine, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09255",
      "pdf_url": "https://arxiv.org/pdf/2601.09255",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09255",
      "scraped_at": "2026-01-20T01:52:26.096444"
    },
    "scraped_date": "2026-01-20"
  },
  {
    "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
    "paper_url": "https://huggingface.co/papers/2601.11077",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
      "abstract": "Hi everyone,  I'm one of the authors of ABC-Bench . (arXiv:2601.11077). While building Code Agents, we realized that current benchmarks often stop at \"generating correct code snippets.\" But as developers, we know that real-world backend engineering is much more than that‚Äîit's about exploring unfamiliar repos, configuring environments, writing Dockerfiles, and actually deploying a live service. That's why we created ABC-Bench. ‚ú® Key Features: Full Lifecycle: We evaluate everything from code editing to Containerization and Service Launch. Real Integration Testing: We validate agents by sending actual HTTP requests to the service they deploy. Diverse Stack: 224 tasks from real-world repos (8 languages, 19 frameworks). ü§ó Open Source on HF: We've released the full dataset and fine-tuned models: üìö Dataset: OpenMOSS-Team/ABC-Bench ü§ñ Models: OpenMOSS-Team/Qwen3-8B-ABC & OpenMOSS-Team/Qwen3-32B-ABC Hope this serves as a useful testbed for the community! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11077",
      "pdf_url": "https://arxiv.org/pdf/2601.11077",
      "github_links": [
        "https://github.com/OpenMOSS/ABC-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11077",
      "scraped_at": "2026-01-21T01:54:38.294314"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
    "paper_url": "https://huggingface.co/papers/2601.08808",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
      "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08808",
      "pdf_url": "https://arxiv.org/pdf/2601.08808",
      "github_links": [
        "https://github.com/GMLR-Penn/Multiplex-Thinking"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08808",
      "scraped_at": "2026-01-21T01:54:40.218487"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
    "paper_url": "https://huggingface.co/papers/2601.11004",
    "authors": [
      "Tianshi Zheng",
      "Qingcheng Zeng",
      "Qing Zong",
      "Rui Wang",
      "Jiayu Liu"
    ],
    "stars": "6",
    "details": {
      "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
      "abstract": "This paper addresses the often-overlooked problem of confidence calibration for large language models (LLMs) in retrieval-augmented generation (RAG) settings, where noisy retrieved contexts can severely inflate model overconfidence. The authors systematically study calibration performance across multiple benchmarks and propose Noise-AwAre Confidence CaLibration Rules (NAACL Rules) along with a noise-aware supervised fine-tuning framework (NAACL) that leverages guided supervision to imbue models with intrinsic noise awareness. Empirical results demonstrate consistent reductions in expected calibration error both in-domain and out-of-domain, highlighting the method‚Äôs potential to improve epistemic reliability of LLM outputs in factual applications. This work is timely and relevant for enhancing trustworthiness of deployed RAG systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11004",
      "pdf_url": "https://arxiv.org/pdf/2601.11004",
      "github_links": [
        "https://github.com/HKUST-KnowComp/NAACL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11004",
      "scraped_at": "2026-01-21T01:54:42.116241"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2601.10880",
    "authors": [
      "Ziyang Yan",
      "Jiachen Tu",
      "Chuhan Song",
      "Tianxingjian Ding",
      "ChongCong"
    ],
    "stars": "18",
    "details": {
      "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
      "abstract": "üè• Medical SAM3: Bridging the Gap in Text-Guided Medical Image Segmentation Existing foundation models often face challenges when applying \"segment anything\" paradigms to medical imaging, particularly in the absence of spatial prompts (bounding boxes). Medical SAM3 aims to address this by enhancing the model's semantic understanding through full-parameter fine-tuning. üí° Key Contributions: üó®Ô∏è Reduced Reliance on Spatial Cues: The model is trained to perform segmentation using solely text prompts (e.g., \"Polyp\", \"Tumor\"), aiming for a more automated workflow. üìà Improved Generalization: Experiments on 7 unseen external datasets suggest a significant performance improvement in zero-shot settings (Dice score: 11.9% vs 73.9%). ü©ª Diverse Training Data: Developed on a corpus of 33 datasets across 10 imaging modalities to capture a wide range of medical semantics. We hope this work contributes to the development of more robust, prompt-driven medical AI assistants.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10880",
      "pdf_url": "https://arxiv.org/pdf/2601.10880",
      "github_links": [
        "https://github.com/AIM-Research-Lab/Medical-SAM3.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10880",
      "scraped_at": "2026-01-21T01:54:43.975841"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
    "paper_url": "https://huggingface.co/papers/2601.10387",
    "authors": [
      "Jack Lindsey",
      "Kyle Fish",
      "Jonathan Michala",
      "Jack Gallagher",
      "Christina Lu"
    ],
    "stars": "0",
    "details": {
      "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
      "abstract": "arXivlens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/the-assistant-axis-situating-and-stabilizing-the-default-persona-of-language-models-6264-f01123de Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10387",
      "pdf_url": "https://arxiv.org/pdf/2601.10387",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10387",
      "scraped_at": "2026-01-21T01:54:45.824669"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
    "paper_url": "https://huggingface.co/papers/2601.11096",
    "authors": [
      "Hengshuang",
      "shen12313",
      "DonJoey",
      "fengyutong",
      "kema"
    ],
    "stars": "0",
    "details": {
      "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
      "abstract": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11096",
      "pdf_url": "https://arxiv.org/pdf/2601.11096",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11096",
      "scraped_at": "2026-01-21T01:54:47.749022"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
    "paper_url": "https://huggingface.co/papers/2601.11061",
    "authors": [
      "Lecheng Yan",
      "ChrisLee",
      "kksinn",
      "JiahuiGengNLP",
      "rzdiversity"
    ],
    "stars": "6",
    "details": {
      "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
      "abstract": "RLVR is the secret sauce for reasoning models, but it has a dark side. The Spurious Rewards Paradox reveals how models exploit latent contamination to achieve SOTA benchmark results without genuine reasoning. By identifying the specific Anchor-Adapter circuit, our paper shows we can now causally steer a model's reliance on shortcuts. Check out the code ( https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts ) and the circuit analysis in our paper to see how reasoning might just be hidden memorization in disguise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11061",
      "pdf_url": "https://arxiv.org/pdf/2601.11061",
      "github_links": [
        "https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11061",
      "scraped_at": "2026-01-21T01:54:49.633759"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.08441",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
      "abstract": "Dense steering vectors often fail due to feature entanglement. YaPO solves this by learning sparse steering vectors directly in a Sparse Autoencoder's latent space using preference data in a DPO-fashion optimization loss. Highlights: Precision & Stability: Converges significantly faster and is more stable than dense baselines like BiPO. Cultural Alignment: Superior performance on a new 15-culture benchmark, specifically closing the \"implicit-explicit\" gap where models usually struggle. Generalization: Works on hallucination and jailbreaks without degrading general knowledge (MMLU).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08441",
      "pdf_url": "https://arxiv.org/pdf/2601.08441",
      "github_links": [
        "https://github.com/MBZUAI-Paris/YaPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08441",
      "scraped_at": "2026-01-21T01:54:51.510472"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "PubMed-OCR: PMC Open Access OCR Annotations",
    "paper_url": "https://huggingface.co/papers/2601.11425",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PubMed-OCR: PMC Open Access OCR Annotations",
      "abstract": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11425",
      "pdf_url": "https://arxiv.org/pdf/2601.11425",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11425",
      "scraped_at": "2026-01-21T01:54:53.335576"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
    "paper_url": "https://huggingface.co/papers/2601.10108",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
      "abstract": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10108",
      "pdf_url": "https://arxiv.org/pdf/2601.10108",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10108",
      "scraped_at": "2026-01-21T01:54:55.279403"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "paper_url": "https://huggingface.co/papers/2601.09512",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
      "abstract": "TL;DR ü§ñ CLARE enables Vision-Language-Action models to learn new robot tasks without forgetting previous ones ‚Äî no replay buffers, no task IDs at inference. üîå Plug-and-play adapters : Extends PEFT with a new CLARE adapter type üß† Smart expansion : Automatically adds new adapter modules only when needed (based on feature similarity) üéØ Task-free inference : Autoencoder-based routing selects the right adapters without knowing the task üìà SOTA on LIBERO : Outperforms exemplar-based continual learning methods on long task sequences Built on ü§ó LeRobot + PEFT .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.09512",
      "pdf_url": "https://arxiv.org/pdf/2601.09512",
      "github_links": [
        "https://github.com/huggingface/lerobot",
        "https://github.com/utiasDSL/clare",
        "https://github.com/huggingface/peft"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.09512",
      "scraped_at": "2026-01-21T01:54:57.181509"
    },
    "scraped_date": "2026-01-21"
  },
  {
    "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
    "paper_url": "https://huggingface.co/papers/2601.12993",
    "authors": [],
    "stars": "265",
    "details": {
      "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
      "abstract": "We scale human-centric robot learning with Being-H0.5 toward cross-embodiment generalization. Building on over 35,000 hours data, we unify human hand motion and diverse robot embodiments with a Unified Action Space, and train all heterogeneous supervision through unified sequence modeling under a single framework. This yields a single foundation model that can perceive, describe, and act within one framework, enabling robust cross-embodiment generalization and real-world deployment across diverse robots and tasks. Blog: https://research.beingbeyond.com/being-h05 arXiv: https://arxiv.org/pdf/2601.12993 GitHub: https://github.com/BeingBeyond/Being-H HuggingFace: https://huggingface.co/collections/BeingBeyond/being-h05",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12993",
      "pdf_url": "https://arxiv.org/pdf/2601.12993",
      "github_links": [
        "https://github.com/BeingBeyond/Being-H"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12993",
      "scraped_at": "2026-01-22T01:55:03.772388"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
    "paper_url": "https://huggingface.co/papers/2601.11655",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
      "abstract": "üöÄ Awesome issue resolution: a comprehensive survey! This paper surveyed 175+ works to construct the first unified taxonomy serving as the comprehensive roadmap for issue resolution.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11655",
      "pdf_url": "https://arxiv.org/pdf/2601.11655",
      "github_links": [
        "https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11655",
      "scraped_at": "2026-01-22T01:55:05.630833"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Think3D: Thinking with Space for Spatial Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.13029",
    "authors": [
      "Yuhan Wu",
      "JeremyYin",
      "sunz525",
      "luciasnowblack",
      "MrBean2024"
    ],
    "stars": "32",
    "details": {
      "title": "Think3D: Thinking with Space for Spatial Reasoning",
      "abstract": "We introduce Think3D, a framework that enables VLM agents to think in 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13029",
      "pdf_url": "https://arxiv.org/pdf/2601.13029",
      "github_links": [
        "https://github.com/zhangzaibin/spagent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13029",
      "scraped_at": "2026-01-22T01:55:07.494034"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "paper_url": "https://huggingface.co/papers/2601.14250",
    "authors": [],
    "stars": "54",
    "details": {
      "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
      "abstract": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer , a unified framework for spatio-temporal video transfer . It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance ( ID and style ) and temporal transfer ( camera movement and video effects ), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14250",
      "pdf_url": "https://arxiv.org/pdf/2601.14250",
      "github_links": [
        "https://github.com/PangzeCheung/OmniTransfer"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14250",
      "scraped_at": "2026-01-22T01:55:09.432481"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
    "paper_url": "https://huggingface.co/papers/2601.14192",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
      "abstract": "This paper surveys efficiency-oriented methods for agentic systems across memory, tool learning, and planning, distills shared design principles, and summarizes how recent methods and benchmarks measure efficiency, which hopes to guide the development of efficient agents. github link: https://github.com/yxf203/Awesome-Efficient-Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14192",
      "pdf_url": "https://arxiv.org/pdf/2601.14192",
      "github_links": [
        "https://github.com/yxf203/Awesome-Efficient-Agents"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14192",
      "scraped_at": "2026-01-22T01:55:11.370876"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
    "paper_url": "https://huggingface.co/papers/2601.13836",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
      "abstract": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs üîó Paper: https://arxiv.org/pdf/2601.13836 üíª Code: https://github.com/OpenMOSS/FutureOmni üåê Project: https://openmoss.github.io/FutureOmni üé¨ Datasets: https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13836",
      "pdf_url": "https://arxiv.org/pdf/2601.13836",
      "github_links": [
        "https://github.com/OpenMOSS/FutureOmni"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13836",
      "scraped_at": "2026-01-22T01:55:13.353686"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.11969",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
      "abstract": "Check our code: https://github.com/LCM-Lab/MemRewardBench and Benchmark: https://huggingface.co/datasets/LCM-Lab/MemRewardBench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11969",
      "pdf_url": "https://arxiv.org/pdf/2601.11969",
      "github_links": [
        "https://github.com/LCM-Lab/MemRewardBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11969",
      "scraped_at": "2026-01-22T01:55:15.293524"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.14004",
    "authors": [
      "qiaw99",
      "WANGYIWEI",
      "zunhai",
      "mingyang26",
      "hengyuanya"
    ],
    "stars": "0",
    "details": {
      "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
      "abstract": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14004",
      "pdf_url": "https://arxiv.org/pdf/2601.14004",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14004",
      "scraped_at": "2026-01-22T01:55:17.331702"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.11522",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
      "abstract": "We introduce UniX, a unified foundation model for Chest X-Ray that combines Autoregression (for understanding) and Diffusion (for generation) within a decoupled dual-branch architecture! üè•‚ú® Why UniX? Current unified models often face a conflict between semantic abstraction and pixel-level reconstruction. UniX solves this via structural decoupling and Cross-Modal Self-Attention. üî• Key Results: Compared to previous works (like LLM-CXR), UniX achieves: üìà +46.1% improvement in Understanding. üé® +24.2% improvement in Generation Quality. ‚ö° Only 25% of the parameters! Resources: Code: https://github.com/ZrH42/UniX Weights: https://huggingface.co/ZrH42/UniX Paper: arXiv:2601.11522",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11522",
      "pdf_url": "https://arxiv.org/pdf/2601.11522",
      "github_links": [
        "https://github.com/ZrH42/UniX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11522",
      "scraped_at": "2026-01-22T01:55:19.364836"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
    "paper_url": "https://huggingface.co/papers/2601.12294",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "abstract": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12294",
      "pdf_url": "https://arxiv.org/pdf/2601.12294",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12294",
      "scraped_at": "2026-01-22T01:55:21.277931"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "paper_url": "https://huggingface.co/papers/2601.13247",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
      "abstract": "WorldMind helps language models stop making physically impossible plans by learning real-world rules from feedback and successful experiences, rather than retraining the model itself.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13247",
      "pdf_url": "https://arxiv.org/pdf/2601.13247",
      "github_links": [
        "https://github.com/zjunlp/WorldMind"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13247",
      "scraped_at": "2026-01-22T01:55:23.093451"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Agentic-R: Learning to Retrieve for Agentic Search",
    "paper_url": "https://huggingface.co/papers/2601.11888",
    "authors": [
      "Daiting Shi",
      "Yuchen Li",
      "Yutao Zhu",
      "Xinyu Ma",
      "Wenhan Liu"
    ],
    "stars": "0",
    "details": {
      "title": "Agentic-R: Learning to Retrieve for Agentic Search",
      "abstract": "Agentic-R: Learning to Retrieve for Agentic Search",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11888",
      "pdf_url": "https://arxiv.org/pdf/2601.11888",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11888",
      "scraped_at": "2026-01-22T01:55:24.928034"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
    "paper_url": "https://huggingface.co/papers/2601.13288",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
      "abstract": "Rather than adding another model to the stack, this work reuses computation already paid for in the serving LLM‚Äôs forward pass by training compact probes on hidden states. It frames the problem as principled selection across tokens and layers (not just ‚Äúfinal layer‚Äù or ‚Äúfirst token‚Äù), implemented with a two-stage aggregation template and lightweight variants that stay close to serving-time cost.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13288",
      "pdf_url": "https://arxiv.org/pdf/2601.13288",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13288",
      "scraped_at": "2026-01-22T01:55:26.778477"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2601.14232",
    "authors": [
      "Aleksandr I. Panov",
      "Alexey K. Kovalev",
      "Daniil Zelezetsky",
      "Egor Cherepanov"
    ],
    "stars": "7",
    "details": {
      "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
      "abstract": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14232",
      "pdf_url": "https://arxiv.org/pdf/2601.14232",
      "github_links": [
        "https://github.com/CognitiveAISystems/kage-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14232",
      "scraped_at": "2026-01-22T01:55:28.677125"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
    "paper_url": "https://huggingface.co/papers/2601.14251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
      "abstract": "We present LightOnOCR-2-1B , a 1B-parameter end-to-end multilingual vision-language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9√ó smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14251",
      "pdf_url": "https://arxiv.org/pdf/2601.14251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14251",
      "scraped_at": "2026-01-22T01:55:30.586443"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
    "paper_url": "https://huggingface.co/papers/2601.14046",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
      "abstract": "Main take-aways PRiSM is the first fully-open benchmark that evaluates Phone-Recognition systems on both intrinsic (phone-transcription) and extrinsic (down-stream) tasks across 12 datasets covering clinical, L2-learning and multilingual settings.  We find that Large Audio-Language Models still lag behind specialized PR models on such tasks. Since intrinsic phone recognition capability is not fully indicative of performance in extrinsic settings, we design transcript and representation based probes that allow an exhaustive analysis, interpretability, and fair comparison. Language exposure > data size: multilingual training with broad, diverse data matters more for cross lingual generalization. Code, prompts and data are released under permissive licences.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14046",
      "pdf_url": "https://arxiv.org/pdf/2601.14046",
      "github_links": [
        "https://github.com/changelinglab/prism"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14046",
      "scraped_at": "2026-01-22T01:55:32.480062"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "paper_url": "https://huggingface.co/papers/2601.13976",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "abstract": "FantasyVLN is a unified multimodal Chain-of-Thought (CoT) reasoning framework that enables efficient and precise navigation based on natural language instructions and visual observations. FantasyVLN combines the benefits of textual, visual, and multimodal CoT reasoning by constructing a unified representation space across these reasoning modes. To enable efficient reasoning, we align these CoT reasoning modes with non-CoT reasoning during training, while using only non-CoT reasoning at test time. Notably, we perform visual CoT in the latent space of a VAR model, where only low-scale latent representations are predicted. Compared to traditional pixel-level visual CoT methods, our approach significantly improves both training and inference efficiency. See our project page for more detail: https://fantasy-amap.github.io/fantasy-vln/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13976",
      "pdf_url": "https://arxiv.org/pdf/2601.13976",
      "github_links": [
        "https://github.com/Fantasy-AMAP/fantasy-vln",
        "https://github.com/FoundationVision/VAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13976",
      "scraped_at": "2026-01-22T01:55:34.333254"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
    "paper_url": "https://huggingface.co/papers/2601.13761",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
      "abstract": "In this work, we introduce the DARC framework, which adopts decoupled training and asymmetric self-distillation to stabilize self-evolving. We hope this work provides useful insights for LLM self-evolution. avXiv: https://arxiv.org/abs/2601.13761 Github: https://github.com/RUCBM/DARC HuggingFace: https://huggingface.co/papers/2601.13761",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13761",
      "pdf_url": "https://arxiv.org/pdf/2601.13761",
      "github_links": [
        "https://github.com/RUCBM/DARC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13761",
      "scraped_at": "2026-01-22T01:55:36.147165"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "paper_url": "https://huggingface.co/papers/2601.14249",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
      "abstract": "Code: https://github.com/UmeanNever/RankSurprisalRatio",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14249",
      "pdf_url": "https://arxiv.org/pdf/2601.14249",
      "github_links": [
        "https://github.com/UmeanNever/RankSurprisalRatio"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14249",
      "scraped_at": "2026-01-22T01:55:37.991407"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.14209",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
      "abstract": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14209",
      "pdf_url": "https://arxiv.org/pdf/2601.14209",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14209",
      "scraped_at": "2026-01-22T01:55:39.795059"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
    "paper_url": "https://huggingface.co/papers/2601.13697",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
      "abstract": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13697",
      "pdf_url": "https://arxiv.org/pdf/2601.13697",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13697",
      "scraped_at": "2026-01-22T01:55:41.659816"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing",
    "paper_url": "https://huggingface.co/papers/2601.12937",
    "authors": [
      "Marten van Dijk",
      "Kaleel Mahmood",
      "Min Chen",
      "emirhanboge",
      "bilgehanertan"
    ],
    "stars": "0",
    "details": {
      "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing",
      "abstract": "üßë‚Äç‚öñÔ∏èüìÑ This paper shows that membership inference attacks are not reliable technical evidence for copyright infringement in court. Even with strong MIAs, semantics-preserving paraphrasing breaks the signal while keeping utility, making them brittle in adversarial legal settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12937",
      "pdf_url": "https://arxiv.org/pdf/2601.12937",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12937",
      "scraped_at": "2026-01-22T01:55:43.453742"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "paper_url": "https://huggingface.co/papers/2601.10237",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
      "abstract": "This paper quantifies a fundamental lower bound on the noise required for differentially private stochastic gradient descent (DP-SGD) to maintain strong privacy, revealing that even with massive datasets and both shuffled and Poisson subsampling, the utility degradation due to necessary noise is substantial and persistent. https://www.alphaxiv.org/overview/2601.10237",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10237",
      "pdf_url": "https://arxiv.org/pdf/2601.10237",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10237",
      "scraped_at": "2026-01-22T01:55:45.232902"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
    "paper_url": "https://huggingface.co/papers/2601.13591",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
      "abstract": "This paper introduce the DSAEval, evaluating LLM based Data Agent in a wide-range of real world problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13591",
      "pdf_url": "https://arxiv.org/pdf/2601.13591",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13591",
      "scraped_at": "2026-01-22T01:55:47.016763"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
    "paper_url": "https://huggingface.co/papers/2601.13253",
    "authors": [
      "√ñzay Ezerceli",
      "Mehmet Emin Buldur",
      "MElHuseyni",
      "etosun"
    ],
    "stars": "0",
    "details": {
      "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
      "abstract": "Addressing data scarcity in low-resource languages, this paper introduces a cost-effective ($65) pipeline for generating large-scale semantic datasets. By integrating FastText clustering, Gemini 2.5-Flash labeling, and dictionary curation, the authors release a Turkish corpus of 843,000 pairs (synonyms, antonyms, co-hyponyms), achieving 90% F1-macro on classification benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13253",
      "pdf_url": "https://arxiv.org/pdf/2601.13253",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13253",
      "scraped_at": "2026-01-22T01:55:48.814077"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
    "paper_url": "https://huggingface.co/papers/2601.13251",
    "authors": [
      "√ñzay Ezerceli",
      "Mehmet Emin Buldur",
      "MElHuseyni",
      "etosun"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
      "abstract": "This paper addresses the inability of neural embeddings to distinguish synonyms from antonyms. The authors introduce a soft-to-hard clustering algorithm that prevents semantic drift and a 3-way relation discriminator (90% F1). Validated against a new dataset of 843k pairs generated via Gemini 2.5, the pipeline yields 2.9M semantic clusters, significantly enhancing precision for RAG and semantic search.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13251",
      "pdf_url": "https://arxiv.org/pdf/2601.13251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13251",
      "scraped_at": "2026-01-22T01:55:50.632933"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions",
    "paper_url": "https://huggingface.co/papers/2601.13075",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions",
      "abstract": "Students have immense research potential, but enough mentors for them. What if we could design an AI system to mentor them? We introduce METIS (Mentoring Engine for Thoughtful Inquiry & Solutions), a stage-aware research mentor.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13075",
      "pdf_url": "https://arxiv.org/pdf/2601.13075",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13075",
      "scraped_at": "2026-01-22T01:55:52.565769"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
    "paper_url": "https://huggingface.co/papers/2601.12910",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
      "abstract": "We introduce the SciCoQA dataset for evaluating models on detecting discrepancies between paper and code. Find all resources here: Paper: arXiv Data: Hugging Face Dataset Code: GitHub Demo: Hugging Face Space Project Page : UKPLab/scicoqa",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12910",
      "pdf_url": "https://arxiv.org/pdf/2601.12910",
      "github_links": [
        "https://github.com/UKPLab/scicoqa",
        "https://github.com/ukplab/scicoqa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12910",
      "scraped_at": "2026-01-22T01:55:54.385925"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "paper_url": "https://huggingface.co/papers/2601.10700",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
      "abstract": "The paper addresses the lack of reliable ground-truth benchmarks for evaluating concept-based explainability in Large Language Models. The authors introduce LIBERTy, a framework that generates \"structural counterfactuals\" by explicitly defining Structured Causal Models (SCMs) where the LLM acts as a component to generate text. By intervening on high-level concepts (e.g., gender, disease symptoms) within the SCM and propagating these changes to the LLM's output, the framework creates synthetic yet causally grounded datasets without relying on costly human annotation. The study introduces three such datasets (covering disease detection, CV screening, and workplace violence) and a new metric called \"order-faithfulness.\" Experiments using LIBERTy reveal that while fine-tuned matching methods currently offer the best explanations, there is significant room for improvement, and some proprietary models like GPT-4o exhibit notably low sensitivity to demographic interventions due to safety alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.10700",
      "pdf_url": "https://arxiv.org/pdf/2601.10700",
      "github_links": [
        "https://github.com/GilatToker/Liberty-benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.10700",
      "scraped_at": "2026-01-22T01:55:56.208957"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging",
    "paper_url": "https://huggingface.co/papers/2601.13677",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging",
      "abstract": "üöÄ Building on nnActive , an evaluation framework for active learning in 3D biomedical imaging, this paper proposes a simple and effective method that consistently outperforms strong random baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.19183",
      "pdf_url": "https://arxiv.org/pdf/2601.13677",
      "github_links": [
        "https://github.com/MIC-DKFZ/nnActive/tree/nnActive_v2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13677",
      "scraped_at": "2026-01-22T01:55:58.016384"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
    "paper_url": "https://huggingface.co/papers/2601.13481",
    "authors": [
      "Yu He",
      "Weiping Fu",
      "Zhiyuan Wang",
      "Zhangqi Wang",
      "Jian Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
      "abstract": "We propose APOLO (Automated Prompt Optimization for Linguistic emOtion diagnosis), a framework that systematically explores a broader and finer-grained prompt space to enhance diagnostic efficiency and robustness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13481",
      "pdf_url": "https://arxiv.org/pdf/2601.13481",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13481",
      "scraped_at": "2026-01-22T01:55:59.798013"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection",
    "paper_url": "https://huggingface.co/papers/2601.11898",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection",
      "abstract": "https://github.com/yilmazkorkmaz1/RemoteVAR",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11898",
      "pdf_url": "https://arxiv.org/pdf/2601.11898",
      "github_links": [
        "https://github.com/yilmazkorkmaz1/RemoteVAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11898",
      "scraped_at": "2026-01-22T01:56:01.626876"
    },
    "scraped_date": "2026-01-22"
  },
  {
    "title": "Agentic Reasoning for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.12538",
    "authors": [],
    "stars": "105",
    "details": {
      "title": "Agentic Reasoning for Large Language Models",
      "abstract": "üåê Awesome-Agentic-Reasoning GitHub Link: https://github.com/weitianxin/Awesome-Agentic-Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12538",
      "pdf_url": "https://arxiv.org/pdf/2601.12538",
      "github_links": [
        "https://github.com/weitianxin/Awesome-Agentic-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12538",
      "scraped_at": "2026-01-23T01:51:23.971108"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
    "paper_url": "https://huggingface.co/papers/2601.12346",
    "authors": [
      "Samiul Alam",
      "Zhongwei Wan",
      "Zixuan Zhong",
      "Peizhou Huang",
      "donghao-zhou"
    ],
    "stars": "14",
    "details": {
      "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
      "abstract": "Introducing MMDeepResearch-Bench, a benchmark for multimodal deep research agents. Page: https://mmdeepresearch-bench.github.io/ Paper: https://arxiv.org/abs/2601.12346 Code: https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench Dataset: https://huggingface.co/datasets/MMDR-2025/MMdeepresearch",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12346",
      "pdf_url": "https://arxiv.org/pdf/2601.12346",
      "github_links": [
        "https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12346",
      "scraped_at": "2026-01-23T01:51:25.854219"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Rethinking Video Generation Model for the Embodied World",
    "paper_url": "https://huggingface.co/papers/2601.15282",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Rethinking Video Generation Model for the Embodied World",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test (2026) RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence (2025) 4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models (2025) Mitty: Diffusion-based Human-to-Robot Video Generation (2025) Large Video Planner Enables Generalizable Robot Control (2025) Video Generation Models in Robotics - Applications, Research Challenges, Future Directions (2026) MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15282",
      "pdf_url": "https://arxiv.org/pdf/2601.15282",
      "github_links": [
        "https://github.com/DAGroup-PKU/ReVidgen/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15282",
      "scraped_at": "2026-01-23T01:51:27.713798"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
    "paper_url": "https://huggingface.co/papers/2601.14171",
    "authors": [],
    "stars": "146",
    "details": {
      "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
      "abstract": "RebuttalAgent is an AI-powered multi-agent system that helps researchers craft high-quality rebuttals for academic paper reviews. The system analyzes reviewer comments, searches relevant literature, generates rebuttal strategies, and produces formal rebuttal letters, all through an interactive human-in-the-loop workflow. üåê Paper: https://arxiv.org/abs/2601.14171 üî• Project Page: https://mqleet.github.io/Paper2Rebuttal_ProjectPage/ üïπÔ∏è Code: https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal (‚≠êÔ∏è) ü§ó Huggingface Space: https://huggingface.co/spaces/Mqleet/RebuttalAgent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14171",
      "pdf_url": "https://arxiv.org/pdf/2601.14171",
      "github_links": [
        "https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14171",
      "scraped_at": "2026-01-23T01:51:29.716204"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Behavior Knowledge Merge in Reinforced Agentic Models",
    "paper_url": "https://huggingface.co/papers/2601.13572",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Behavior Knowledge Merge in Reinforced Agentic Models",
      "abstract": "üöÄ TL;DR We introduce RAM (Reinforced Agent Merging) , a method designed to merge RL-trained agents into a single generalist model without retraining, outperforming the original specialized agents in their domains. üí° Key Insights The Problem: Standard merging methods (like TIES/DARE) are built for SFT models. We find they fail for RL models because RL updates are extremely sparse and heterogeneous , leading to \"Signal Dilution\" when averaged (performance drops). The Solution: RAM explicitly disentangles \"shared\" vs \"unique\" parameters. It preserves the full magnitude of unique task vectors to prevent dilution while averaging shared knowledge. Performance: RAM outperforms all existing merging baselines on agentic benchmarks (CURE, BFCL, MemAgent).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13572",
      "pdf_url": "https://arxiv.org/pdf/2601.13572",
      "github_links": [
        "https://github.com/xiangchi-yuan/mrl"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13572",
      "scraped_at": "2026-01-23T01:51:31.618334"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.14750",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Forest Before Trees: Latent Superposition for Efficient Visual Reasoning (2026) Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring (2026) Interleaved Latent Visual Reasoning with Selective Perceptual Modeling (2025) LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning (2026) Global Context Compression with Interleaved Vision-Text Transformation (2026) Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs (2025) FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14750",
      "pdf_url": "https://arxiv.org/pdf/2601.14750",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14750",
      "scraped_at": "2026-01-23T01:51:33.444046"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
    "paper_url": "https://huggingface.co/papers/2601.14490",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
      "abstract": "We're excited to share our first open model release, a grounded VLM for OCR applications!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14490",
      "pdf_url": "https://arxiv.org/pdf/2601.14490",
      "github_links": [
        "https://github.com/Roots-Automation/GutenOCR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14490",
      "scraped_at": "2026-01-23T01:51:35.315081"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction",
    "paper_url": "https://huggingface.co/papers/2601.14722",
    "authors": [],
    "stars": "85",
    "details": {
      "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction",
      "abstract": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14722",
      "pdf_url": "https://arxiv.org/pdf/2601.14722",
      "github_links": [
        "https://github.com/scb-10x/typhoon-ocr"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14722",
      "scraped_at": "2026-01-23T01:51:37.192567"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
    "paper_url": "https://huggingface.co/papers/2601.13044",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
      "abstract": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13044",
      "pdf_url": "https://arxiv.org/pdf/2601.13044",
      "github_links": [
        "https://github.com/scb-10x/typhoon-asr"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13044",
      "scraped_at": "2026-01-23T01:51:39.051573"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
    "paper_url": "https://huggingface.co/papers/2601.14027",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
      "abstract": "Recommend to try our demo at: https://demo.projectnumina.ai/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14027",
      "pdf_url": "https://arxiv.org/pdf/2601.14027",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14027",
      "scraped_at": "2026-01-23T01:51:40.876746"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
    "paper_url": "https://huggingface.co/papers/2601.11141",
    "authors": [],
    "stars": "141",
    "details": {
      "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
      "abstract": "Some of the observations founded are :- -- End to end S2S advantage : Chroma 1.0 avoids cascaded ASR LLM TTS pipelines, reducing latency and preserving paralinguistic cues like timbre and prosody. -- High fidelity voice cloning : With only a few seconds of reference audio, Chroma achieves 10.96% higher speaker similarity than the human baseline, outperforming existing open and commercial models. -- Real time streaming design : The interleaved 1:2 text audio token schedule enables sub-second responsiveness (TTFT ‚âà 147 ms) and smooth streaming synthesis. -- Efficiency at small scale : Despite having only 4B parameters, Chroma maintains competitive understanding, reasoning, and dialogue performance compared to larger 7‚Äì9B models . -- Naturalness vs fidelity trade off : Subjective tests show commercial systems may sound more ‚Äúnatural,‚Äù but Chroma preserves speaker identity more faithfully highlighting that listener preference does not always equal true speaker similarity.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11141",
      "pdf_url": "https://arxiv.org/pdf/2601.11141",
      "github_links": [
        "https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11141",
      "scraped_at": "2026-01-23T01:51:42.687376"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
    "paper_url": "https://huggingface.co/papers/2601.07853",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
      "abstract": "the first execution-grounded security benchmark for financial agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07853",
      "pdf_url": "https://arxiv.org/pdf/2601.07853",
      "github_links": [
        "https://github.com/aifinlab/FinVault"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07853",
      "scraped_at": "2026-01-23T01:51:44.595356"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15220",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
      "abstract": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models Overview This paper identifies a critical new failure mode in language models called \"privacy collapse\" . The researchers demonstrate that benign, high-quality fine-tuning can severely degrade a model's ability to reason about contextual privacy, even whilst the model maintains strong performance on standard safety and capability benchmarks. Key Findings The study reveals that diverse training data characteristics can trigger privacy collapse: Optimisation for helpfulness - Models become overly proactive in sharing information Emotional and empathetic dialogue - Attentive conversations weaken privacy boundaries Exposure to user information - Personal data in training context normalises broad access Debugging code - Logging statements that expose internal variables transfer to social contexts Fine-tuned models inappropriately share sensitive information with tools, violate memory boundaries across conversation sessions, and fail to respect contextual privacy norms. Why It Matters Privacy collapse represents a \"silent failure\" : Models appear healthy on standard safety evaluations Severe privacy vulnerabilities remain undetected Affects 6 models (both closed and open-weight) Emerges from 5 different fine-tuning datasets Generalises across agentic and memory-based tasks Mechanistic Insights The research reveals: Privacy representations are encoded in late model layers These representations are uniquely fragile to fine-tuning compared to task-relevant features Introspective discourse and emotional engagement drive privacy degradation Training samples that reinforce persistent user identity representations weaken learned boundaries Technical Details Evaluation benchmarks: PrivacyLens - Agentic tool-use scenarios (493 contexts) CIMemories - Persistent memory privacy (cross-session boundaries) Models tested: GPT-4o, GPT-4o-mini, GPT-4.1, GPT-4.1-mini, GPT-3.5-turbo Llama-3-8B Privacy degradation observed: Up to 98% relative accuracy drop on privacy benchmarks Whilst safety and capability metrics remain stable or improve Implications This work exposes a critical gap in current safety evaluations, particularly for specialised agents handling sensitive user data. Recommendations: Integrate contextual privacy into safety evaluation pipelines Implement data filtering strategies to identify privacy-degrading patterns Monitor fine-tuned models specifically for privacy preservation Develop robust mitigation strategies beyond standard safety testing Citation @ misc {goel2026privacycollapsebenignfinetuning,\n      title={Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models}, \n      author={Anmol Goel and Cornelius Emde and Sangdoo Yun and Seong Joon Oh and Martin Gubri},\n      year={2026},\n      eprint={2601.15220},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2601.15220}, \n} Resources üìÑ Paper üíª Code",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15220",
      "pdf_url": "https://arxiv.org/pdf/2601.15220",
      "github_links": [
        "https://github.com/parameterlab/privacy-collapse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15220",
      "scraped_at": "2026-01-23T01:51:46.442305"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
    "paper_url": "https://huggingface.co/papers/2601.14245",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
      "abstract": "project website: https://01yzzyu.github.io/xr.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14245",
      "pdf_url": "https://arxiv.org/pdf/2601.14245",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14245",
      "scraped_at": "2026-01-23T01:51:48.244359"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
    "paper_url": "https://huggingface.co/papers/2601.14352",
    "authors": [
      "Yuheng Ji",
      "Yijie Xu",
      "Zhiyu Li",
      "Huajie Tan",
      "Zhoues"
    ],
    "stars": "0",
    "details": {
      "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Towards Cross-View Point Correspondence in Vision-Language Models (2025) Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation (2026) MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images (2025) Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos (2025) SpatialMosaic: A Multiview VLM Dataset for Partial Visibility (2025) Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training (2025) From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14352",
      "pdf_url": "https://arxiv.org/pdf/2601.14352",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14352",
      "scraped_at": "2026-01-23T01:51:50.079130"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
    "paper_url": "https://huggingface.co/papers/2601.14417",
    "authors": [
      "Jihwan Lee",
      "Thanapat Trachu",
      "Yoonjeong Lee",
      "Thanathai Lertpetchpun",
      "tiantiaf"
    ],
    "stars": "0",
    "details": {
      "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
      "abstract": "Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation. This paper will be presented at ICASSP 2026.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14417",
      "pdf_url": "https://arxiv.org/pdf/2601.14417",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14417",
      "scraped_at": "2026-01-23T01:51:51.860623"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
    "paper_url": "https://huggingface.co/papers/2601.14256",
    "authors": [
      "Zhenheng Yang",
      "Xuefeng Hu",
      "Xiao Wang",
      "Matthew Gwilliam"
    ],
    "stars": "0",
    "details": {
      "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
      "abstract": "Code: https://github.com/tiktok/huvr",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14256",
      "pdf_url": "https://arxiv.org/pdf/2601.14256",
      "github_links": [
        "https://github.com/tiktok/huvr"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14256",
      "scraped_at": "2026-01-23T01:51:53.635741"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
    "paper_url": "https://huggingface.co/papers/2601.13918",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
      "abstract": "This paper presents AGENTEHR, a novel benchmark designed to bridge the gap between idealized experimental settings and realistic clinical environments. Unlike previous tasks that focus on factual retrieval (e.g., searching for a specific medication), AGENTEHR challenges agents to perform complex clinical decision-making, such as diagnosis and treatment planning, directly within raw, high-noise EHR databases. To address the information loss inherent in long-context clinical reasoning, the paper proposes RETROSUM, a framework that unifies a retrospective summarization mechanism with an evolving experience strategy. RETROSUM achieves performance gains of up to 29.16% over baselines while reducing interaction errors by up to 92.3%.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13918",
      "pdf_url": "https://arxiv.org/pdf/2601.13918",
      "github_links": [
        "https://github.com/BlueZeros/AgentEHR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13918",
      "scraped_at": "2026-01-23T01:51:55.436570"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "FARE: Fast-Slow Agentic Robotic Exploration",
    "paper_url": "https://huggingface.co/papers/2601.14681",
    "authors": [
      "Jingsong Liang",
      "Shizhe Zhang",
      "Jeric Lew",
      "Xuxin Lv",
      "Shuhao Liao"
    ],
    "stars": "0",
    "details": {
      "title": "FARE: Fast-Slow Agentic Robotic Exploration",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation (2026) Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation (2025) CAUSALNAV: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios (2026) Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives (2025) SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots (2025) H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning (2026) ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14681",
      "pdf_url": "https://arxiv.org/pdf/2601.14681",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14681",
      "scraped_at": "2026-01-23T01:51:57.216560"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
    "paper_url": "https://huggingface.co/papers/2601.14152",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
      "abstract": "Prompt order can break LMs performance ‚Äî even with the same content.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14152",
      "pdf_url": "https://arxiv.org/pdf/2601.14152",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14152",
      "scraped_at": "2026-01-23T01:51:58.986007"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
    "paper_url": "https://huggingface.co/papers/2601.15059",
    "authors": [
      "Roman Bondar",
      "Oleg Romanchuk"
    ],
    "stars": "0",
    "details": {
      "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
      "abstract": "Some of the observations founded are :- -- Authority capacity mismatch is structural : Decisions are formally approved by humans, but the epistemic capacity to understand those decisions does not scale with agent generated throughput, creating a systematic gap between authority and understanding. -- Responsibility vacuum emerges beyond a throughput threshold : When decision generation rate exceeds bounded human verification capacity, personalized responsibility becomes unattainable even though processes are followed correctly . -- Verification degrades into ritualized approval : Human review persists as a formal act, but shifts from substantive inspection to reliance on proxy signals (e.g. CI green ), decoupling approval from understanding. -- CI/CD automation amplifies the problem rather than solving it : Adding more automated checks increases proxy signal density without restoring human capacity, accelerating cognitive offloading and widening the responsibility gap. -- Local optimizations cannot eliminate the failure mode : Better models, more CI, or improved tooling may shift thresholds but cannot remove the structural limit, only explicit redesign of responsibility boundaries (e.g. batch/system level ownership or constrained throughput) addresses the issue.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15059",
      "pdf_url": "https://arxiv.org/pdf/2601.15059",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15059",
      "scraped_at": "2026-01-23T01:52:00.954598"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
    "paper_url": "https://huggingface.co/papers/2601.15100",
    "authors": [
      "Arpit Narechania",
      "Yanwei Huang"
    ],
    "stars": "0",
    "details": {
      "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Developer Interaction Patterns with Proactive AI: A Five-Day Field Study (2026) Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools (2025) Engineering Trustworthy Automation: Design Principles and Evaluation for AutoML Tools for Novices (2025) Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR (2025) PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing (2025) WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment (2025) TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15100",
      "pdf_url": "https://arxiv.org/pdf/2601.15100",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15100",
      "scraped_at": "2026-01-23T01:52:02.711109"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
    "paper_url": "https://huggingface.co/papers/2601.14253",
    "authors": [
      "Anpei Chen",
      "Zexiang Xu",
      "Youjia Zhang",
      "Xingyu Chen",
      "Hongyuan Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14253",
      "pdf_url": "https://arxiv.org/pdf/2601.14253",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14253",
      "scraped_at": "2026-01-23T01:52:04.540646"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation",
    "paper_url": "https://huggingface.co/papers/2601.12029",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation",
      "abstract": "Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.12029",
      "pdf_url": "https://arxiv.org/pdf/2601.12029",
      "github_links": [
        "https://github.com/sandyherho/sangkuriang-ideal-solver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.12029",
      "scraped_at": "2026-01-23T01:52:06.368678"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
    "paper_url": "https://huggingface.co/papers/2601.11387",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
      "abstract": "TL;DR: In an AI-supported fact-checking task, people consistently relied on underlying evidence to judge AI reliability, using explanations as a supplement rather than a substitute, showing that evidence is central to how people evaluate AI-aided decisions.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11387",
      "pdf_url": "https://arxiv.org/pdf/2601.11387",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11387",
      "scraped_at": "2026-01-23T01:52:08.317691"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.13262",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
      "abstract": "We introduce CURE-MED, a curriculum-informed reinforcement learning framework for multilingual medical reasoning across 13 languages, including low-resource settings. The work studies how code-switching-aware supervision and curriculum-guided RL jointly improve logical correctness and language consistency. Feel free to give your feedback on potential extensions to other medical tasks or languages.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13262",
      "pdf_url": "https://arxiv.org/pdf/2601.13262",
      "github_links": [
        "https://github.com/AikyamLab/cure-med"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13262",
      "scraped_at": "2026-01-23T01:52:10.158251"
    },
    "scraped_date": "2026-01-23"
  },
  {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "paper_url": "https://huggingface.co/papers/2601.15876",
    "authors": [
      "Linsen Guo",
      "Mianqiu Huang",
      "Taofeng Xue",
      "GenSouKai",
      "KleinChong"
    ],
    "stars": "0",
    "details": {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "EvoCUA: Evolving Computer Use Agent ü•á #1 Open-Source Model on OSWorld | A General-Purpose Multimodal Model Excelling at Computer Use üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/meituan/EvoCUA üåü Highlights ü•á #1 Open-Source Model on OSWorld : Achieves 56.7% task completion rate, #1 among all open-source models üìà Significant Improvements : +11.7% over OpenCUA-72B (45.0%‚Üí56.7%), +15.1% over Qwen3-VL thinking (41.6%‚Üí56.7%), with fewer parameters and half the steps üñ•Ô∏è End-to-End Multi-Turn Automation : Operates Chrome, Excel, PowerPoint, VSCode and more through screenshots and natural language instructions üß† Novel Training Method : Our data synthesis and training approach consistently improves Computer Use capability across multiple open-source VLMs without degrading general performance üìä Performance Comparison Rank Model Open/Closed Type Max Steps Score 1 Claude-sonnet-4-5 üîí Closed General 100 62.9% 2 Seed-1.8 üîí Closed General 100 61.9% 3 Claude-sonnet-4-5 üîí Closed General 50 58.1% 4 EvoCUA-20260105 (Ours) üü¢ Open General 50 56.7% ü•á 5 DeepMiner-Mano-72B üîí Closed Specialized 100 53.9% 6 UI-TARS-2-2509 üîí Closed General 100 53.1% 7 EvoCUA (Previous Version) üîí Closed General 50 50.3% 8 EvoCUA-8B-20260105 (Ours) üü¢ Open General 50 46.1% 9 OpenCUA-72B üü¢ Open Specialized 100 45.0% ... ... ... ... ... ... 13 Qwen3-VL-Flash üîí Closed General 100 41.6% EvoCUA is #1 among all open-source models , achieving competitive results with only 50 steps . Human-level performance remains significantly higher, indicating substantial room for improvement. üìù Citation If you find EvoCUA useful in your research, please consider citing: @ misc {evocua2026,\n  title={EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience},\n  author={Chong Peng* and Taofeng Xue*},\n  year={2026},\n  url={https://github.com/meituan/EvoCUA},\n  note={* Equal contribution}\n} üìú License This project is licensed under the Apache 2.0 License - see the LICENSE file for details. Built with ‚ù§Ô∏è by Meituan LongCat Team",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15876",
      "pdf_url": "https://arxiv.org/pdf/2601.15876",
      "github_links": [
        "https://github.com/meituan/EvoCUA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15876",
      "scraped_at": "2026-01-24T01:48:31.333646"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15165",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "abstract": "Links üìÑ paper: https://arxiv.org/abs/2601.15165 üè† project page: https://nzl-thu.github.io/the-flexibility-trap üíª code: https://github.com/LeapLabTHU/JustGRPO ü§ó model: https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15165",
      "pdf_url": "https://arxiv.org/pdf/2601.15165",
      "github_links": [
        "https://github.com/LeapLabTHU/JustGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15165",
      "scraped_at": "2026-01-24T01:48:33.239756"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.14724",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "abstract": "üöÄ Introducing HERMES: The Future of Real-Time Streaming Video Understanding! While today's Multimodal Large Language Models (MLLMs) perform impressively at offline video comprehension, they often face a \"painful trade-off\" when it comes to real-time streaming video - balancing real-time responses, low memory usage, and high accuracy. To solve this, we introduce the following innovations: üí° The HERMES Breakthrough: 1Ô∏è‚É£ Novel memory architecture: By deeply analyzing attention mechanisms, we' ve introduced a \"Hierarchical Memory\" approach. The KV Cache is now reimagined as a multi-level memory framework: Shallow layers act as Sensory Memory (events that just happened). Deep layers focus on Long-term Memory (frame-level semantic anchors). Middle layers bridge the gap with Working Memory. 2Ô∏è‚É£ Plug-and-play architecture: HERMES achieves highly efficient KV Cache reuse and optimization strategies including cross-layer memory smoothing and position re-indexing , delivering instant responses without the need for additional training, or auxiliary computations when user queries arrive. 3Ô∏è‚É£ Incredible efficiency and performance: ‚ö° Blazing speed: HERMES is 10x faster than previous SOTA in terms of response latency (TTFT)! üöÄ Compact efficiency: Even with 68% fewer video tokens, the model remains rock-solid, achieving up to 11.4% improvement in streaming comprehension tasks! üíæ Memory-friendly: No matter the video length, memory usage stays constant, leaving OOM errors in the past. üî• Join us in exploring this breakthrough: If you're passionate about streaming video understanding and efficient inference, we'd love to discuss and collaborate! üîç Explore the Details : üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/haowei-freesky/HERMES üåê Project: https://hermes-streaming.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14724",
      "pdf_url": "https://arxiv.org/pdf/2601.14724",
      "github_links": [
        "https://github.com/haowei-freesky/HERMES"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14724",
      "scraped_at": "2026-01-24T01:48:35.135166"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "paper_url": "https://huggingface.co/papers/2601.15197",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "abstract": "üèóÔ∏è Architecture BayesianVLA is a novel framework designed to solve the Vision Shortcut problem in Vision-Language-Action (VLA) models. In current VLA training, goal-driven datasets often make language instructions highly predictable from visual observations alone. This leads to Information Collapse, where the model ignores language and degenerates into a vision-only policy, failing miserably in out-of-distribution (OOD) scenarios. BayesianVLA addresses this by: Bayesian Decomposition : Explicitly modeling a vision-only prior $p(a|v)$ and a language-conditioned posterior $\\pi(a|v, \\ell)$. LLR Optimization : Maximizing the Log-Likelihood Ratio (LLR) to penalize actions that rely solely on visual cues and reward actions that are truly grounded in language instructions. ‚ú® Key Features Dual-Branch Architecture : Uses learnable Latent Action Queries to decouple vision-only and language-conditioned action distributions. Zero Extra Data : Achieves significant performance gains (e.g., +11.3% on SimplerEnv) using the exact same datasets as baselines. Preserves VLM Intelligence : Effectively regularizes the model to prevent the \"catastrophic forgetting\" of general multimodal reasoning capabilities common in standard VLA fine-tuning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15197",
      "pdf_url": "https://arxiv.org/pdf/2601.15197",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/BayesianVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15197",
      "scraped_at": "2026-01-24T01:48:37.045162"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.16206",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "Introducing LLM-in-Sandbox ‚Äî put your LLM in a virtual computer to unlock general agentic intelligence for non-code tasks! Significant gains for chemistry, long-context QA, instruction following, and more. No extra training needed. üåê Demo: https://llm-in-sandbox.github.io üíª Code: https://github.com/llm-in-sandbox/llm-in-sandbox pip install llm-in-sandbox Feel free to open issues or discussions  ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16206",
      "pdf_url": "https://arxiv.org/pdf/2601.16206",
      "github_links": [
        "https://github.com/llm-in-sandbox/llm-in-sandbox"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16206",
      "scraped_at": "2026-01-24T01:48:38.948194"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "paper_url": "https://huggingface.co/papers/2601.16208",
    "authors": [],
    "stars": "58",
    "details": {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "abstract": "We scale RAE to text-to-image, and its advantage over VAEs still holds!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16208",
      "pdf_url": "https://arxiv.org/pdf/2601.16208",
      "github_links": [
        "https://github.com/ZitengWangNYU/Scale-RAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16208",
      "scraped_at": "2026-01-24T01:48:40.846696"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "paper_url": "https://huggingface.co/papers/2601.15892",
    "authors": [],
    "stars": "16",
    "details": {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs (2025) LLaDA2.0: Scaling Up Diffusion Language Models to 100B (2025) WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference (2025) CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models (2026) SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding (2025) Fast and Accurate Causal Parallel Decoding using Jacobi Forcing (2025) Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15892",
      "pdf_url": "https://arxiv.org/pdf/2601.15892",
      "github_links": [
        "https://github.com/ByteDance-Seed/Stable-DiffCoder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15892",
      "scraped_at": "2026-01-24T01:48:42.765322"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "paper_url": "https://huggingface.co/papers/2601.16093",
    "authors": [],
    "stars": "1.5k",
    "details": {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Project page: https://zhouyiks.github.io/projects/SAMTok/ Training Code: https://github.com/bytedance/Sa2VA/tree/main/projects/samtok Short Bio:   We present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16093",
      "pdf_url": "https://arxiv.org/pdf/2601.16093",
      "github_links": [
        "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16093",
      "scraped_at": "2026-01-24T01:48:44.741613"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Learning to Discover at Test Time",
    "paper_url": "https://huggingface.co/papers/2601.16175",
    "authors": [],
    "stars": "88",
    "details": {
      "title": "Learning to Discover at Test Time",
      "abstract": "New paper on scientific discovery with test time training. New discoveries on several open scientific problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16175",
      "pdf_url": "https://arxiv.org/pdf/2601.16175",
      "github_links": [
        "https://github.com/test-time-training/discover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16175",
      "scraped_at": "2026-01-24T01:48:46.720243"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Qwen3-TTS Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.15621",
    "authors": [],
    "stars": "2.19k",
    "details": {
      "title": "Qwen3-TTS Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IndexTTS 2.5 Technical Report (2026) FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2026) DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec (2025) VoiceSculptor: Your Voice, Designed By You (2026) GLM-TTS Technical Report (2025) MiMo-Audio: Audio Language Models are Few-Shot Learners (2025) JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15621",
      "pdf_url": "https://arxiv.org/pdf/2601.15621",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-TTS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15621",
      "scraped_at": "2026-01-24T01:48:48.676895"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "paper_url": "https://huggingface.co/papers/2601.11868",
    "authors": [
      "Boxuan Li",
      "Nicholas Carlini",
      "Alexander G. Shaw",
      "Mike A. Merrill",
      "menorf"
    ],
    "stars": "1.41k",
    "details": {
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts (2026) Real-Time Procedural Learning From Experience for AI Agents (2025) Benchmarking LLM Agents for Wealth-Management Workflows (2025) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (2025) SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (2025) The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11868",
      "pdf_url": "https://arxiv.org/pdf/2601.11868",
      "github_links": [
        "https://github.com/laude-institute/terminal-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11868",
      "scraped_at": "2026-01-24T01:48:50.598708"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.16125",
    "authors": [
      "Dingkun Long",
      "Zhuoning Guo",
      "Mingxin Li",
      "Yanzhao Zhang",
      "songtingyu"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "abstract": "A new benchmark for Composed Image Retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16125",
      "pdf_url": "https://arxiv.org/pdf/2601.16125",
      "github_links": [
        "https://github.com/SighingSnow/edir"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16125",
      "scraped_at": "2026-01-24T01:48:52.456807"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.15369",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "abstract": "Project Page: https://ucsc-vlaa.github.io/OpenVision3/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15369",
      "pdf_url": "https://arxiv.org/pdf/2601.15369",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15369",
      "scraped_at": "2026-01-24T01:48:54.368305"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "paper_url": "https://huggingface.co/papers/2601.15727",
    "authors": [
      "Yixin Shen",
      "Haiming Wu",
      "Chi Hsu Tsai",
      "Peiyu Zang",
      "Yang Yu"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "Summary of Key Points Kernel quality is a fundamental bottleneck for modern AI system performance, yet high-quality kernel engineering is expert-intensive, time-consuming, and difficult to scale. Recent advances in large language models (LLMs) and LLM-based agents enable automated kernel generation and optimization by capturing expert knowledge and supporting iterative, feedback-driven optimization loops. Despite rapid progress, existing work is fragmented and lacks a unified, systematic perspective. This survey provides a structured overview of LLM-based kernel generation methods and agentic optimization workflows, and compiles the key datasets and benchmarks used for training and evaluation. The paper further identifies open challenges and outlines future research directions, aiming to serve as a comprehensive reference for next-generation automated kernel optimization. Resources Open-source repository tracking this field: https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15727",
      "pdf_url": "https://arxiv.org/pdf/2601.15727",
      "github_links": [
        "https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15727",
      "scraped_at": "2026-01-24T01:48:56.231071"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15224",
    "authors": [
      "Dingcheng Wang",
      "Haoran Lu",
      "Haosen Sun",
      "Jianshu Zhang",
      "Raymond-Qiancx"
    ],
    "stars": "54",
    "details": {
      "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
      "abstract": "Towards General Progress Understanding for Embodied Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15224",
      "pdf_url": "https://arxiv.org/pdf/2601.15224",
      "github_links": [
        "https://github.com/ProgressLM/ProgressLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15224",
      "scraped_at": "2026-01-24T01:48:58.104027"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "paper_url": "https://huggingface.co/papers/2601.14255",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "abstract": "Demo: https://huggingface.co/spaces/SammyLim/VideoMaMa Git: https://github.com/cvlab-kaist/VideoMaMa Project Page: https://cvlab-kaist.github.io/VideoMaMa/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14255",
      "pdf_url": "https://arxiv.org/pdf/2601.14255",
      "github_links": [
        "https://github.com/cvlab-kaist/VideoMaMa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14255",
      "scraped_at": "2026-01-24T01:49:00.047552"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
    "paper_url": "https://huggingface.co/papers/2601.16192",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
      "abstract": "360Anything lifts arbitrary perspective images and videos to seamless, gravity-aligned 360¬∞ panoramas, without using any camera or 3D information. Project page: https://360anything.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16192",
      "pdf_url": "https://arxiv.org/pdf/2601.16192",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16192",
      "scraped_at": "2026-01-24T01:49:01.994351"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "paper_url": "https://huggingface.co/papers/2601.16163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "abstract": "Cosmos Policy fine-tunes a pretrained video model in one stage for visuomotor control, enabling action latent frames, future state prediction, and planning, achieving state-of-the-art robotic benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16163",
      "pdf_url": "https://arxiv.org/pdf/2601.16163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16163",
      "scraped_at": "2026-01-24T01:49:03.949664"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.16148",
    "authors": [],
    "stars": "57",
    "details": {
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "abstract": "ü§óTry it out: https://huggingface.co/spaces/facebook/ActionMesh üåêProject Page: https://remysabathier.github.io/actionmesh/ üìÑPaper: https://remysabathier.github.io/actionmesh/actionmesh_2026.pdf üíªCode: https://github.com/facebookresearch/actionmesh",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16148",
      "pdf_url": "https://arxiv.org/pdf/2601.16148",
      "github_links": [
        "https://github.com/facebookresearch/actionmesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16148",
      "scraped_at": "2026-01-24T01:49:05.900220"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "paper_url": "https://huggingface.co/papers/2601.15549",
    "authors": [
      "Ryo Hachiuma",
      "Hideo Saito",
      "Ryo Fujii"
    ],
    "stars": "0",
    "details": {
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "abstract": "Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15549",
      "pdf_url": "https://arxiv.org/pdf/2601.15549",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15549",
      "scraped_at": "2026-01-24T01:49:07.750498"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "paper_url": "https://huggingface.co/papers/2601.16134",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LLM Prompt Evaluation for Educational Applications",
      "abstract": "Some of the observations founded are :- -- Prompt design matters as much as the model : The study shows that different prompt templates using the same LLM produce significantly different educational outcomes, proving prompt engineering is a critical lever in AI supported learning. -- Persona + Context Manager is the strongest combination : The Strategic Reading Coach prompt combining Persona and Context Manager patterns outperformed all others with 81‚Äì100% win probability, making it the most effective for follow up educational questions. -- Systematic prompt evaluation beats ad-hoc refinement : The tournament style evaluation using comparative judgment + Glicko2 ranking provides a reproducible, evidence based alternative to informal trial and error prompt tuning. -- Learning theory grounded prompts perform better : Prompts explicitly grounded in adult learning theory, self directed learning, and metacognition consistently generated higher quality educational dialogue than theory light designs -- Theoretical alignment alone is not enough : Some prompts rooted in strong learning theories (e.g. constructivism) still underperformed, highlighting that empirical evaluation is essential good theory must be paired with effective prompt patterns.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16134",
      "pdf_url": "https://arxiv.org/pdf/2601.16134",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16134",
      "scraped_at": "2026-01-24T01:49:09.630524"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Agentic Confidence Calibration",
    "paper_url": "https://huggingface.co/papers/2601.15778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Confidence Calibration",
      "abstract": "üéØ Don't let your Agents be \"Confidently Wrong\"! Traditional calibration works for static text, but Autonomous Agents fail differently‚Äîerrors compound over long trajectories. We introduce Holistic Trajectory Calibration (HTC) , a new paradigm to diagnose the entire execution process. Why it matters: üîç Process-Centric: Extracts rich features (Dynamics, Stability) from the agent's thinking process, not just the final output. üìà SOTA Calibration: Consistently outperforms baselines across 8 benchmarks (SimpleQA, Math500, etc.). üåç Generalization: We release the General Agent Calibrator (GAC) , which achieves the best zero-shot calibration on the challenging GAIA benchmark. Achieve Interpretability, Transferability, and Trust in your AI Agents. üõ°Ô∏è",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15778",
      "pdf_url": "https://arxiv.org/pdf/2601.15778",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15778",
      "scraped_at": "2026-01-24T01:49:11.476964"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Agentic Uncertainty Quantification",
    "paper_url": "https://huggingface.co/papers/2601.15703",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Uncertainty Quantification",
      "abstract": "üõë Stop the \"Spiral of Hallucination\" in Autonomous Agents! Long-horizon agents often fail because minor early errors snowball into irreversible failures. We introduce Agentic Uncertainty Quantification (AUQ) , a training-free Dual-Process framework inspired by System 1/System 2 thinking: üß† System 1 (Fast): Uncertainty-Aware Memory propagates doubt to prevent blind commitment. ü§î System 2 (Slow): Triggers active reflection only when confidence drops below a specific threshold. Key Wins: ‚úÖ SOTA Performance: Outperforms ReAct & Reflexion on ALFWorld, WebShop, and the new DeepResearch Bench . ‚úÖ Efficiency: Prevents long, futile failure loops, making it more token-efficient than standard methods. ‚úÖ Plug-and-Play: No fine-tuning required. From \"Passive Diagnosis\" to \"Active Control\" ‚Äî make your agents reliable! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15703",
      "pdf_url": "https://arxiv.org/pdf/2601.15703",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15703",
      "scraped_at": "2026-01-24T01:49:13.350945"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15690",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "üó∫Ô∏è The 2026 Roadmap for Reliable AI: Making Uncertainty Actionable We are witnessing a paradigm shift in LLMs: Uncertainty is no longer just a passive score for diagnosis‚Äîit is evolving into an Active Control Signal for real-time decision-making. Our comprehensive survey covers this transformation across three frontiers: üß† Reasoning: Triggering self-correction & optimizing \"thinking budget\" (System 2). ü§ñ Agents: Determining when to use tools, ask for help, or stop generation. üéØ Alignment: Using uncertainty as an intrinsic reward to mitigate reward hacking in RLHF. If you are building Agents or Reasoning Models, this is the functional evolution you need to know. üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15690",
      "pdf_url": "https://arxiv.org/pdf/2601.15690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15690",
      "scraped_at": "2026-01-24T01:49:15.251204"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "paper_url": "https://huggingface.co/papers/2601.15440",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "abstract": "In this work, we address the performance limitations often encountered in Python-based DLA simulations. By utilizing Numba for just-in-time compilation, we developed an implementation that achieves computational speeds comparable to legacy Fortran codes, offering a speedup over pure Python. We also validated the solver by analyzing the fractal dimension of the generated clusters (D‚âà1.71). We have released the code as a PyPI package named dla-ideal-solver to facilitate easier use and reproducibility. We hope this tool proves useful to those working in computational physics and complex systems, and we welcome any feedback from the community.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15440",
      "pdf_url": "https://arxiv.org/pdf/2601.15440",
      "github_links": [
        "https://github.com/sandyherho/dla-ideal-solver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15440",
      "scraped_at": "2026-01-24T01:49:17.202444"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "paper_url": "https://huggingface.co/papers/2601.08118",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
      "abstract": "The framework is open-sourced at https://github.com/SAP/mirrorbench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08118",
      "pdf_url": "https://arxiv.org/pdf/2601.08118",
      "github_links": [
        "https://github.com/SAP/mirrorbench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08118",
      "scraped_at": "2026-01-24T01:49:19.059976"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "paper_url": "https://huggingface.co/papers/2601.16004",
    "authors": [
      "Cohaerence"
    ],
    "stars": "4",
    "details": {
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts. Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16004",
      "pdf_url": "https://arxiv.org/pdf/2601.16004",
      "github_links": [
        "https://github.com/christopher-altman/ibm-qml-kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16004",
      "scraped_at": "2026-01-24T01:49:20.868988"
    },
    "scraped_date": "2026-01-24"
  },
  {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "paper_url": "https://huggingface.co/papers/2601.15876",
    "authors": [],
    "stars": "147",
    "details": {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "EvoCUA: Evolving Computer Use Agent ü•á #1 Open-Source Model on OSWorld | A General-Purpose Multimodal Model Excelling at Computer Use üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/meituan/EvoCUA üåü Highlights ü•á #1 Open-Source Model on OSWorld : Achieves 56.7% task completion rate, #1 among all open-source models üìà Significant Improvements : +11.7% over OpenCUA-72B (45.0%‚Üí56.7%), +15.1% over Qwen3-VL thinking (41.6%‚Üí56.7%), with fewer parameters and half the steps üñ•Ô∏è End-to-End Multi-Turn Automation : Operates Chrome, Excel, PowerPoint, VSCode and more through screenshots and natural language instructions üß† Novel Training Method : Our data synthesis and training approach consistently improves Computer Use capability across multiple open-source VLMs without degrading general performance üìä Performance Comparison Rank Model Open/Closed Type Max Steps Score 1 Claude-sonnet-4-5 üîí Closed General 100 62.9% 2 Seed-1.8 üîí Closed General 100 61.9% 3 Claude-sonnet-4-5 üîí Closed General 50 58.1% 4 EvoCUA-20260105 (Ours) üü¢ Open General 50 56.7% ü•á 5 DeepMiner-Mano-72B üîí Closed Specialized 100 53.9% 6 UI-TARS-2-2509 üîí Closed General 100 53.1% 7 EvoCUA (Previous Version) üîí Closed General 50 50.3% 8 EvoCUA-8B-20260105 (Ours) üü¢ Open General 50 46.1% 9 OpenCUA-72B üü¢ Open Specialized 100 45.0% ... ... ... ... ... ... 13 Qwen3-VL-Flash üîí Closed General 100 41.6% EvoCUA is #1 among all open-source models , achieving competitive results with only 50 steps . Human-level performance remains significantly higher, indicating substantial room for improvement. üìù Citation If you find EvoCUA useful in your research, please consider citing: @ misc {evocua2026,\n  title={EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience},\n  author={Chong Peng* and Taofeng Xue*},\n  year={2026},\n  url={https://github.com/meituan/EvoCUA},\n  note={* Equal contribution}\n} üìú License This project is licensed under the Apache 2.0 License - see the LICENSE file for details. Built with ‚ù§Ô∏è by Meituan LongCat Team",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15876",
      "pdf_url": "https://arxiv.org/pdf/2601.15876",
      "github_links": [
        "https://github.com/meituan/EvoCUA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15876",
      "scraped_at": "2026-01-25T02:01:53.666276"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15165",
    "authors": [],
    "stars": "68",
    "details": {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "abstract": "Links üìÑ paper: https://arxiv.org/abs/2601.15165 üè† project page: https://nzl-thu.github.io/the-flexibility-trap üíª code: https://github.com/LeapLabTHU/JustGRPO ü§ó model: https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15165",
      "pdf_url": "https://arxiv.org/pdf/2601.15165",
      "github_links": [
        "https://github.com/LeapLabTHU/JustGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15165",
      "scraped_at": "2026-01-25T02:01:55.563516"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.14724",
    "authors": [],
    "stars": "29",
    "details": {
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "abstract": "üöÄ Introducing HERMES: The Future of Real-Time Streaming Video Understanding! While today's Multimodal Large Language Models (MLLMs) perform impressively at offline video comprehension, they often face a \"painful trade-off\" when it comes to real-time streaming video - balancing real-time responses, low memory usage, and high accuracy. To solve this, we introduce the following innovations: üí° The HERMES Breakthrough: 1Ô∏è‚É£ Novel memory architecture: By deeply analyzing attention mechanisms, we' ve introduced a \"Hierarchical Memory\" approach. The KV Cache is now reimagined as a multi-level memory framework: Shallow layers act as Sensory Memory (events that just happened). Deep layers focus on Long-term Memory (frame-level semantic anchors). Middle layers bridge the gap with Working Memory. 2Ô∏è‚É£ Plug-and-play architecture: HERMES achieves highly efficient KV Cache reuse and optimization strategies including cross-layer memory smoothing and position re-indexing , delivering instant responses without the need for additional training, or auxiliary computations when user queries arrive. 3Ô∏è‚É£ Incredible efficiency and performance: ‚ö° Blazing speed: HERMES is 10x faster than previous SOTA in terms of response latency (TTFT)! üöÄ Compact efficiency: Even with 68% fewer video tokens, the model remains rock-solid, achieving up to 11.4% improvement in streaming comprehension tasks! üíæ Memory-friendly: No matter the video length, memory usage stays constant, leaving OOM errors in the past. üî• Join us in exploring this breakthrough: If you're passionate about streaming video understanding and efficient inference, we'd love to discuss and collaborate! üîç Explore the Details : üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/haowei-freesky/HERMES üåê Project: https://hermes-streaming.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14724",
      "pdf_url": "https://arxiv.org/pdf/2601.14724",
      "github_links": [
        "https://github.com/haowei-freesky/HERMES"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14724",
      "scraped_at": "2026-01-25T02:01:57.411173"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.16206",
    "authors": [],
    "stars": "63",
    "details": {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "Introducing LLM-in-Sandbox ‚Äî put your LLM in a virtual computer to unlock general agentic intelligence for non-code tasks! Significant gains for chemistry, long-context QA, instruction following, and more. No extra training needed. üåê Demo: https://llm-in-sandbox.github.io üíª Code: https://github.com/llm-in-sandbox/llm-in-sandbox pip install llm-in-sandbox Feel free to open issues or discussions  ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16206",
      "pdf_url": "https://arxiv.org/pdf/2601.16206",
      "github_links": [
        "https://github.com/llm-in-sandbox/llm-in-sandbox"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16206",
      "scraped_at": "2026-01-25T02:01:59.299451"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "paper_url": "https://huggingface.co/papers/2601.15197",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "abstract": "üèóÔ∏è Architecture BayesianVLA is a novel framework designed to solve the Vision Shortcut problem in Vision-Language-Action (VLA) models. In current VLA training, goal-driven datasets often make language instructions highly predictable from visual observations alone. This leads to Information Collapse, where the model ignores language and degenerates into a vision-only policy, failing miserably in out-of-distribution (OOD) scenarios. BayesianVLA addresses this by: Bayesian Decomposition : Explicitly modeling a vision-only prior $p(a|v)$ and a language-conditioned posterior $\\pi(a|v, \\ell)$. LLR Optimization : Maximizing the Log-Likelihood Ratio (LLR) to penalize actions that rely solely on visual cues and reward actions that are truly grounded in language instructions. ‚ú® Key Features Dual-Branch Architecture : Uses learnable Latent Action Queries to decouple vision-only and language-conditioned action distributions. Zero Extra Data : Achieves significant performance gains (e.g., +11.3% on SimplerEnv) using the exact same datasets as baselines. Preserves VLM Intelligence : Effectively regularizes the model to prevent the \"catastrophic forgetting\" of general multimodal reasoning capabilities common in standard VLA fine-tuning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15197",
      "pdf_url": "https://arxiv.org/pdf/2601.15197",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/BayesianVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15197",
      "scraped_at": "2026-01-25T02:02:01.183148"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "paper_url": "https://huggingface.co/papers/2601.16208",
    "authors": [],
    "stars": "107",
    "details": {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "abstract": "We scale RAE to text-to-image, and its advantage over VAEs still holds!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16208",
      "pdf_url": "https://arxiv.org/pdf/2601.16208",
      "github_links": [
        "https://github.com/ZitengWangNYU/Scale-RAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16208",
      "scraped_at": "2026-01-25T02:02:03.026447"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "paper_url": "https://huggingface.co/papers/2601.15892",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs (2025) LLaDA2.0: Scaling Up Diffusion Language Models to 100B (2025) WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference (2025) CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models (2026) SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding (2025) Fast and Accurate Causal Parallel Decoding using Jacobi Forcing (2025) Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15892",
      "pdf_url": "https://arxiv.org/pdf/2601.15892",
      "github_links": [
        "https://github.com/ByteDance-Seed/Stable-DiffCoder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15892",
      "scraped_at": "2026-01-25T02:02:04.826791"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "paper_url": "https://huggingface.co/papers/2601.16093",
    "authors": [],
    "stars": "1.51k",
    "details": {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Project page: https://zhouyiks.github.io/projects/SAMTok/ Training Code: https://github.com/bytedance/Sa2VA/tree/main/projects/samtok Short Bio:   We present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16093",
      "pdf_url": "https://arxiv.org/pdf/2601.16093",
      "github_links": [
        "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16093",
      "scraped_at": "2026-01-25T02:02:06.714279"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Learning to Discover at Test Time",
    "paper_url": "https://huggingface.co/papers/2601.16175",
    "authors": [],
    "stars": "121",
    "details": {
      "title": "Learning to Discover at Test Time",
      "abstract": "New paper on scientific discovery with test time training. New discoveries on several open scientific problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16175",
      "pdf_url": "https://arxiv.org/pdf/2601.16175",
      "github_links": [
        "https://github.com/test-time-training/discover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16175",
      "scraped_at": "2026-01-25T02:02:08.536010"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Qwen3-TTS Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.15621",
    "authors": [],
    "stars": "3.29k",
    "details": {
      "title": "Qwen3-TTS Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IndexTTS 2.5 Technical Report (2026) FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2026) DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec (2025) VoiceSculptor: Your Voice, Designed By You (2026) GLM-TTS Technical Report (2025) MiMo-Audio: Audio Language Models are Few-Shot Learners (2025) JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15621",
      "pdf_url": "https://arxiv.org/pdf/2601.15621",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-TTS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15621",
      "scraped_at": "2026-01-25T02:02:10.378788"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "paper_url": "https://huggingface.co/papers/2601.11868",
    "authors": [
      "Boxuan Li",
      "Nicholas Carlini",
      "Alexander G. Shaw",
      "Mike A. Merrill",
      "menorf"
    ],
    "stars": "1.41k",
    "details": {
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts (2026) Real-Time Procedural Learning From Experience for AI Agents (2025) Benchmarking LLM Agents for Wealth-Management Workflows (2025) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (2025) SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (2025) The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11868",
      "pdf_url": "https://arxiv.org/pdf/2601.11868",
      "github_links": [
        "https://github.com/laude-institute/terminal-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11868",
      "scraped_at": "2026-01-25T02:02:12.180505"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.15369",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "abstract": "Project Page: https://ucsc-vlaa.github.io/OpenVision3/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15369",
      "pdf_url": "https://arxiv.org/pdf/2601.15369",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15369",
      "scraped_at": "2026-01-25T02:02:14.104309"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.16125",
    "authors": [
      "Dingkun Long",
      "Zhuoning Guo",
      "Mingxin Li",
      "Yanzhao Zhang",
      "songtingyu"
    ],
    "stars": "1",
    "details": {
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "abstract": "A new benchmark for Composed Image Retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16125",
      "pdf_url": "https://arxiv.org/pdf/2601.16125",
      "github_links": [
        "https://github.com/SighingSnow/edir"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16125",
      "scraped_at": "2026-01-25T02:02:15.924213"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "paper_url": "https://huggingface.co/papers/2601.15727",
    "authors": [
      "Yixin Shen",
      "Haiming Wu",
      "Chi Hsu Tsai",
      "Peiyu Zang",
      "Yang Yu"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "Summary of Key Points Kernel quality is a fundamental bottleneck for modern AI system performance, yet high-quality kernel engineering is expert-intensive, time-consuming, and difficult to scale. Recent advances in large language models (LLMs) and LLM-based agents enable automated kernel generation and optimization by capturing expert knowledge and supporting iterative, feedback-driven optimization loops. Despite rapid progress, existing work is fragmented and lacks a unified, systematic perspective. This survey provides a structured overview of LLM-based kernel generation methods and agentic optimization workflows, and compiles the key datasets and benchmarks used for training and evaluation. The paper further identifies open challenges and outlines future research directions, aiming to serve as a comprehensive reference for next-generation automated kernel optimization. Resources Open-source repository tracking this field: https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15727",
      "pdf_url": "https://arxiv.org/pdf/2601.15727",
      "github_links": [
        "https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15727",
      "scraped_at": "2026-01-25T02:02:17.722592"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15224",
    "authors": [
      "Dingcheng Wang",
      "Haoran Lu",
      "Haosen Sun",
      "Jianshu Zhang",
      "Raymond-Qiancx"
    ],
    "stars": "68",
    "details": {
      "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
      "abstract": "Towards General Progress Understanding for Embodied Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15224",
      "pdf_url": "https://arxiv.org/pdf/2601.15224",
      "github_links": [
        "https://github.com/ProgressLM/ProgressLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15224",
      "scraped_at": "2026-01-25T02:02:19.571613"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "paper_url": "https://huggingface.co/papers/2601.16163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "abstract": "Cosmos Policy fine-tunes a pretrained video model in one stage for visuomotor control, enabling action latent frames, future state prediction, and planning, achieving state-of-the-art robotic benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16163",
      "pdf_url": "https://arxiv.org/pdf/2601.16163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16163",
      "scraped_at": "2026-01-25T02:02:21.442201"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "paper_url": "https://huggingface.co/papers/2601.14255",
    "authors": [],
    "stars": "59",
    "details": {
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "abstract": "Demo: https://huggingface.co/spaces/SammyLim/VideoMaMa Git: https://github.com/cvlab-kaist/VideoMaMa Project Page: https://cvlab-kaist.github.io/VideoMaMa/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14255",
      "pdf_url": "https://arxiv.org/pdf/2601.14255",
      "github_links": [
        "https://github.com/cvlab-kaist/VideoMaMa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14255",
      "scraped_at": "2026-01-25T02:02:23.333545"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.16148",
    "authors": [],
    "stars": "79",
    "details": {
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "abstract": "ü§óTry it out: https://huggingface.co/spaces/facebook/ActionMesh üåêProject Page: https://remysabathier.github.io/actionmesh/ üìÑPaper: https://remysabathier.github.io/actionmesh/actionmesh_2026.pdf üíªCode: https://github.com/facebookresearch/actionmesh",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16148",
      "pdf_url": "https://arxiv.org/pdf/2601.16148",
      "github_links": [
        "https://github.com/facebookresearch/actionmesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16148",
      "scraped_at": "2026-01-25T02:02:25.268638"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
    "paper_url": "https://huggingface.co/papers/2601.16192",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
      "abstract": "360Anything lifts arbitrary perspective images and videos to seamless, gravity-aligned 360¬∞ panoramas, without using any camera or 3D information. Project page: https://360anything.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16192",
      "pdf_url": "https://arxiv.org/pdf/2601.16192",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16192",
      "scraped_at": "2026-01-25T02:02:27.152555"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Agentic Uncertainty Quantification",
    "paper_url": "https://huggingface.co/papers/2601.15703",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Uncertainty Quantification",
      "abstract": "üõë Stop the \"Spiral of Hallucination\" in Autonomous Agents! Long-horizon agents often fail because minor early errors snowball into irreversible failures. We introduce Agentic Uncertainty Quantification (AUQ) , a training-free Dual-Process framework inspired by System 1/System 2 thinking: üß† System 1 (Fast): Uncertainty-Aware Memory propagates doubt to prevent blind commitment. ü§î System 2 (Slow): Triggers active reflection only when confidence drops below a specific threshold. Key Wins: ‚úÖ SOTA Performance: Outperforms ReAct & Reflexion on ALFWorld, WebShop, and the new DeepResearch Bench . ‚úÖ Efficiency: Prevents long, futile failure loops, making it more token-efficient than standard methods. ‚úÖ Plug-and-Play: No fine-tuning required. From \"Passive Diagnosis\" to \"Active Control\" ‚Äî make your agents reliable! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15703",
      "pdf_url": "https://arxiv.org/pdf/2601.15703",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15703",
      "scraped_at": "2026-01-25T02:02:28.938062"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Agentic Confidence Calibration",
    "paper_url": "https://huggingface.co/papers/2601.15778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Confidence Calibration",
      "abstract": "üéØ Don't let your Agents be \"Confidently Wrong\"! Traditional calibration works for static text, but Autonomous Agents fail differently‚Äîerrors compound over long trajectories. We introduce Holistic Trajectory Calibration (HTC) , a new paradigm to diagnose the entire execution process. Why it matters: üîç Process-Centric: Extracts rich features (Dynamics, Stability) from the agent's thinking process, not just the final output. üìà SOTA Calibration: Consistently outperforms baselines across 8 benchmarks (SimpleQA, Math500, etc.). üåç Generalization: We release the General Agent Calibrator (GAC) , which achieves the best zero-shot calibration on the challenging GAIA benchmark. Achieve Interpretability, Transferability, and Trust in your AI Agents. üõ°Ô∏è",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15778",
      "pdf_url": "https://arxiv.org/pdf/2601.15778",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15778",
      "scraped_at": "2026-01-25T02:02:30.735636"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15690",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "üó∫Ô∏è The 2026 Roadmap for Reliable AI: Making Uncertainty Actionable We are witnessing a paradigm shift in LLMs: Uncertainty is no longer just a passive score for diagnosis‚Äîit is evolving into an Active Control Signal for real-time decision-making. Our comprehensive survey covers this transformation across three frontiers: üß† Reasoning: Triggering self-correction & optimizing \"thinking budget\" (System 2). ü§ñ Agents: Determining when to use tools, ask for help, or stop generation. üéØ Alignment: Using uncertainty as an intrinsic reward to mitigate reward hacking in RLHF. If you are building Agents or Reasoning Models, this is the functional evolution you need to know. üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15690",
      "pdf_url": "https://arxiv.org/pdf/2601.15690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15690",
      "scraped_at": "2026-01-25T02:02:32.541555"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "paper_url": "https://huggingface.co/papers/2601.15549",
    "authors": [
      "Ryo Hachiuma",
      "Hideo Saito",
      "Ryo Fujii"
    ],
    "stars": "0",
    "details": {
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "abstract": "Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15549",
      "pdf_url": "https://arxiv.org/pdf/2601.15549",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15549",
      "scraped_at": "2026-01-25T02:02:34.330106"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "paper_url": "https://huggingface.co/papers/2601.16134",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LLM Prompt Evaluation for Educational Applications",
      "abstract": "Some of the observations founded are :- -- Prompt design matters as much as the model : The study shows that different prompt templates using the same LLM produce significantly different educational outcomes, proving prompt engineering is a critical lever in AI supported learning. -- Persona + Context Manager is the strongest combination : The Strategic Reading Coach prompt combining Persona and Context Manager patterns outperformed all others with 81‚Äì100% win probability, making it the most effective for follow up educational questions. -- Systematic prompt evaluation beats ad-hoc refinement : The tournament style evaluation using comparative judgment + Glicko2 ranking provides a reproducible, evidence based alternative to informal trial and error prompt tuning. -- Learning theory grounded prompts perform better : Prompts explicitly grounded in adult learning theory, self directed learning, and metacognition consistently generated higher quality educational dialogue than theory light designs -- Theoretical alignment alone is not enough : Some prompts rooted in strong learning theories (e.g. constructivism) still underperformed, highlighting that empirical evaluation is essential good theory must be paired with effective prompt patterns.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16134",
      "pdf_url": "https://arxiv.org/pdf/2601.16134",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16134",
      "scraped_at": "2026-01-25T02:02:36.149629"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "paper_url": "https://huggingface.co/papers/2601.15440",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "abstract": "In this work, we address the performance limitations often encountered in Python-based DLA simulations. By utilizing Numba for just-in-time compilation, we developed an implementation that achieves computational speeds comparable to legacy Fortran codes, offering a speedup over pure Python. We also validated the solver by analyzing the fractal dimension of the generated clusters (D‚âà1.71). We have released the code as a PyPI package named dla-ideal-solver to facilitate easier use and reproducibility. We hope this tool proves useful to those working in computational physics and complex systems, and we welcome any feedback from the community.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15440",
      "pdf_url": "https://arxiv.org/pdf/2601.15440",
      "github_links": [
        "https://github.com/sandyherho/dla-ideal-solver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15440",
      "scraped_at": "2026-01-25T02:02:37.957383"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "paper_url": "https://huggingface.co/papers/2601.08118",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
      "abstract": "The framework is open-sourced at https://github.com/SAP/mirrorbench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08118",
      "pdf_url": "https://arxiv.org/pdf/2601.08118",
      "github_links": [
        "https://github.com/SAP/mirrorbench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08118",
      "scraped_at": "2026-01-25T02:02:39.821072"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "paper_url": "https://huggingface.co/papers/2601.16004",
    "authors": [
      "Cohaerence"
    ],
    "stars": "5",
    "details": {
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts. Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16004",
      "pdf_url": "https://arxiv.org/pdf/2601.16004",
      "github_links": [
        "https://github.com/christopher-altman/ibm-qml-kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16004",
      "scraped_at": "2026-01-25T02:02:41.624215"
    },
    "scraped_date": "2026-01-25"
  },
  {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "paper_url": "https://huggingface.co/papers/2601.15876",
    "authors": [],
    "stars": "154",
    "details": {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "EvoCUA: Evolving Computer Use Agent ü•á #1 Open-Source Model on OSWorld | A General-Purpose Multimodal Model Excelling at Computer Use üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/meituan/EvoCUA üåü Highlights ü•á #1 Open-Source Model on OSWorld : Achieves 56.7% task completion rate, #1 among all open-source models üìà Significant Improvements : +11.7% over OpenCUA-72B (45.0%‚Üí56.7%), +15.1% over Qwen3-VL thinking (41.6%‚Üí56.7%), with fewer parameters and half the steps üñ•Ô∏è End-to-End Multi-Turn Automation : Operates Chrome, Excel, PowerPoint, VSCode and more through screenshots and natural language instructions üß† Novel Training Method : Our data synthesis and training approach consistently improves Computer Use capability across multiple open-source VLMs without degrading general performance üìä Performance Comparison Rank Model Open/Closed Type Max Steps Score 1 Claude-sonnet-4-5 üîí Closed General 100 62.9% 2 Seed-1.8 üîí Closed General 100 61.9% 3 Claude-sonnet-4-5 üîí Closed General 50 58.1% 4 EvoCUA-20260105 (Ours) üü¢ Open General 50 56.7% ü•á 5 DeepMiner-Mano-72B üîí Closed Specialized 100 53.9% 6 UI-TARS-2-2509 üîí Closed General 100 53.1% 7 EvoCUA (Previous Version) üîí Closed General 50 50.3% 8 EvoCUA-8B-20260105 (Ours) üü¢ Open General 50 46.1% 9 OpenCUA-72B üü¢ Open Specialized 100 45.0% ... ... ... ... ... ... 13 Qwen3-VL-Flash üîí Closed General 100 41.6% EvoCUA is #1 among all open-source models , achieving competitive results with only 50 steps . Human-level performance remains significantly higher, indicating substantial room for improvement. üìù Citation If you find EvoCUA useful in your research, please consider citing: @ misc {evocua2026,\n  title={EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience},\n  author={Chong Peng* and Taofeng Xue*},\n  year={2026},\n  url={https://github.com/meituan/EvoCUA},\n  note={* Equal contribution}\n} üìú License This project is licensed under the Apache 2.0 License - see the LICENSE file for details. Built with ‚ù§Ô∏è by Meituan LongCat Team",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15876",
      "pdf_url": "https://arxiv.org/pdf/2601.15876",
      "github_links": [
        "https://github.com/meituan/EvoCUA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15876",
      "scraped_at": "2026-01-26T02:01:16.604428"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.14724",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "abstract": "üöÄ Introducing HERMES: The Future of Real-Time Streaming Video Understanding! While today's Multimodal Large Language Models (MLLMs) perform impressively at offline video comprehension, they often face a \"painful trade-off\" when it comes to real-time streaming video - balancing real-time responses, low memory usage, and high accuracy. To solve this, we introduce the following innovations: üí° The HERMES Breakthrough: 1Ô∏è‚É£ Novel memory architecture: By deeply analyzing attention mechanisms, we' ve introduced a \"Hierarchical Memory\" approach. The KV Cache is now reimagined as a multi-level memory framework: Shallow layers act as Sensory Memory (events that just happened). Deep layers focus on Long-term Memory (frame-level semantic anchors). Middle layers bridge the gap with Working Memory. 2Ô∏è‚É£ Plug-and-play architecture: HERMES achieves highly efficient KV Cache reuse and optimization strategies including cross-layer memory smoothing and position re-indexing , delivering instant responses without the need for additional training, or auxiliary computations when user queries arrive. 3Ô∏è‚É£ Incredible efficiency and performance: ‚ö° Blazing speed: HERMES is 10x faster than previous SOTA in terms of response latency (TTFT)! üöÄ Compact efficiency: Even with 68% fewer video tokens, the model remains rock-solid, achieving up to 11.4% improvement in streaming comprehension tasks! üíæ Memory-friendly: No matter the video length, memory usage stays constant, leaving OOM errors in the past. üî• Join us in exploring this breakthrough: If you're passionate about streaming video understanding and efficient inference, we'd love to discuss and collaborate! üîç Explore the Details : üîó Paper: https://arxiv.org/abs/2601.14724 üíª Code: https://github.com/haowei-freesky/HERMES üåê Project: https://hermes-streaming.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14724",
      "pdf_url": "https://arxiv.org/pdf/2601.14724",
      "github_links": [
        "https://github.com/haowei-freesky/HERMES"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14724",
      "scraped_at": "2026-01-26T02:01:18.574988"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "paper_url": "https://huggingface.co/papers/2601.16206",
    "authors": [],
    "stars": "81",
    "details": {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "Introducing LLM-in-Sandbox ‚Äî put your LLM in a virtual computer to unlock general agentic intelligence for non-code tasks! Significant gains for chemistry, long-context QA, instruction following, and more. No extra training needed. üåê Demo: https://llm-in-sandbox.github.io üíª Code: https://github.com/llm-in-sandbox/llm-in-sandbox pip install llm-in-sandbox Feel free to open issues or discussions  ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16206",
      "pdf_url": "https://arxiv.org/pdf/2601.16206",
      "github_links": [
        "https://github.com/llm-in-sandbox/llm-in-sandbox"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16206",
      "scraped_at": "2026-01-26T02:01:20.577739"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15165",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "abstract": "Links üìÑ paper: https://arxiv.org/abs/2601.15165 üè† project page: https://nzl-thu.github.io/the-flexibility-trap üíª code: https://github.com/LeapLabTHU/JustGRPO ü§ó model: https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15165",
      "pdf_url": "https://arxiv.org/pdf/2601.15165",
      "github_links": [
        "https://github.com/LeapLabTHU/JustGRPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15165",
      "scraped_at": "2026-01-26T02:01:22.559804"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "paper_url": "https://huggingface.co/papers/2601.15197",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "abstract": "üèóÔ∏è Architecture BayesianVLA is a novel framework designed to solve the Vision Shortcut problem in Vision-Language-Action (VLA) models. In current VLA training, goal-driven datasets often make language instructions highly predictable from visual observations alone. This leads to Information Collapse, where the model ignores language and degenerates into a vision-only policy, failing miserably in out-of-distribution (OOD) scenarios. BayesianVLA addresses this by: Bayesian Decomposition : Explicitly modeling a vision-only prior $p(a|v)$ and a language-conditioned posterior $\\pi(a|v, \\ell)$. LLR Optimization : Maximizing the Log-Likelihood Ratio (LLR) to penalize actions that rely solely on visual cues and reward actions that are truly grounded in language instructions. ‚ú® Key Features Dual-Branch Architecture : Uses learnable Latent Action Queries to decouple vision-only and language-conditioned action distributions. Zero Extra Data : Achieves significant performance gains (e.g., +11.3% on SimplerEnv) using the exact same datasets as baselines. Preserves VLM Intelligence : Effectively regularizes the model to prevent the \"catastrophic forgetting\" of general multimodal reasoning capabilities common in standard VLA fine-tuning.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15197",
      "pdf_url": "https://arxiv.org/pdf/2601.15197",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/BayesianVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15197",
      "scraped_at": "2026-01-26T02:01:24.489273"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "paper_url": "https://huggingface.co/papers/2601.16208",
    "authors": [],
    "stars": "129",
    "details": {
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "abstract": "We scale RAE to text-to-image, and its advantage over VAEs still holds!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16208",
      "pdf_url": "https://arxiv.org/pdf/2601.16208",
      "github_links": [
        "https://github.com/ZitengWangNYU/Scale-RAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16208",
      "scraped_at": "2026-01-26T02:01:26.455999"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "paper_url": "https://huggingface.co/papers/2601.15892",
    "authors": [],
    "stars": "28",
    "details": {
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs (2025) LLaDA2.0: Scaling Up Diffusion Language Models to 100B (2025) WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference (2025) CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models (2026) SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding (2025) Fast and Accurate Causal Parallel Decoding using Jacobi Forcing (2025) Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15892",
      "pdf_url": "https://arxiv.org/pdf/2601.15892",
      "github_links": [
        "https://github.com/ByteDance-Seed/Stable-DiffCoder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15892",
      "scraped_at": "2026-01-26T02:01:28.374246"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "paper_url": "https://huggingface.co/papers/2601.16093",
    "authors": [],
    "stars": "1.51k",
    "details": {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Project page: https://zhouyiks.github.io/projects/SAMTok/ Training Code: https://github.com/bytedance/Sa2VA/tree/main/projects/samtok Short Bio:   We present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16093",
      "pdf_url": "https://arxiv.org/pdf/2601.16093",
      "github_links": [
        "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16093",
      "scraped_at": "2026-01-26T02:01:30.339469"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Learning to Discover at Test Time",
    "paper_url": "https://huggingface.co/papers/2601.16175",
    "authors": [],
    "stars": "163",
    "details": {
      "title": "Learning to Discover at Test Time",
      "abstract": "New paper on scientific discovery with test time training. New discoveries on several open scientific problems.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16175",
      "pdf_url": "https://arxiv.org/pdf/2601.16175",
      "github_links": [
        "https://github.com/test-time-training/discover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16175",
      "scraped_at": "2026-01-26T02:01:32.194965"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Qwen3-TTS Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.15621",
    "authors": [],
    "stars": "4.27k",
    "details": {
      "title": "Qwen3-TTS Technical Report",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IndexTTS 2.5 Technical Report (2026) FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2026) DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec (2025) VoiceSculptor: Your Voice, Designed By You (2026) GLM-TTS Technical Report (2025) MiMo-Audio: Audio Language Models are Few-Shot Learners (2025) JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15621",
      "pdf_url": "https://arxiv.org/pdf/2601.15621",
      "github_links": [
        "https://github.com/QwenLM/Qwen3-TTS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15621",
      "scraped_at": "2026-01-26T02:01:34.141141"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "paper_url": "https://huggingface.co/papers/2601.11868",
    "authors": [
      "Boxuan Li",
      "Nicholas Carlini",
      "Alexander G. Shaw",
      "Mike A. Merrill",
      "menorf"
    ],
    "stars": "1.41k",
    "details": {
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts (2026) Real-Time Procedural Learning From Experience for AI Agents (2025) Benchmarking LLM Agents for Wealth-Management Workflows (2025) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (2025) SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (2025) The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11868",
      "pdf_url": "https://arxiv.org/pdf/2601.11868",
      "github_links": [
        "https://github.com/laude-institute/terminal-bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11868",
      "scraped_at": "2026-01-26T02:01:36.017971"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "paper_url": "https://huggingface.co/papers/2601.15727",
    "authors": [
      "Yixin Shen",
      "Haiming Wu",
      "Chi Hsu Tsai",
      "Peiyu Zang",
      "Yang Yu"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "Summary of Key Points Kernel quality is a fundamental bottleneck for modern AI system performance, yet high-quality kernel engineering is expert-intensive, time-consuming, and difficult to scale. Recent advances in large language models (LLMs) and LLM-based agents enable automated kernel generation and optimization by capturing expert knowledge and supporting iterative, feedback-driven optimization loops. Despite rapid progress, existing work is fragmented and lacks a unified, systematic perspective. This survey provides a structured overview of LLM-based kernel generation methods and agentic optimization workflows, and compiles the key datasets and benchmarks used for training and evaluation. The paper further identifies open challenges and outlines future research directions, aiming to serve as a comprehensive reference for next-generation automated kernel optimization. Resources Open-source repository tracking this field: https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15727",
      "pdf_url": "https://arxiv.org/pdf/2601.15727",
      "github_links": [
        "https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15727",
      "scraped_at": "2026-01-26T02:01:37.896798"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2601.15369",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "abstract": "Project Page: https://ucsc-vlaa.github.io/OpenVision3/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15369",
      "pdf_url": "https://arxiv.org/pdf/2601.15369",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15369",
      "scraped_at": "2026-01-26T02:01:39.825845"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "paper_url": "https://huggingface.co/papers/2601.16125",
    "authors": [
      "Dingkun Long",
      "Zhuoning Guo",
      "Mingxin Li",
      "Yanzhao Zhang",
      "songtingyu"
    ],
    "stars": "1",
    "details": {
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "abstract": "A new benchmark for Composed Image Retrieval.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16125",
      "pdf_url": "https://arxiv.org/pdf/2601.16125",
      "github_links": [
        "https://github.com/SighingSnow/edir"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16125",
      "scraped_at": "2026-01-26T02:01:41.667094"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "paper_url": "https://huggingface.co/papers/2601.16163",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "abstract": "Cosmos Policy fine-tunes a pretrained video model in one stage for visuomotor control, enabling action latent frames, future state prediction, and planning, achieving state-of-the-art robotic benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16163",
      "pdf_url": "https://arxiv.org/pdf/2601.16163",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16163",
      "scraped_at": "2026-01-26T02:01:43.638260"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.16148",
    "authors": [],
    "stars": "91",
    "details": {
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "abstract": "ü§óTry it out: https://huggingface.co/spaces/facebook/ActionMesh üåêProject Page: https://remysabathier.github.io/actionmesh/ üìÑPaper: https://remysabathier.github.io/actionmesh/actionmesh_2026.pdf üíªCode: https://github.com/facebookresearch/actionmesh",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16148",
      "pdf_url": "https://arxiv.org/pdf/2601.16148",
      "github_links": [
        "https://github.com/facebookresearch/actionmesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16148",
      "scraped_at": "2026-01-26T02:01:45.631171"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "paper_url": "https://huggingface.co/papers/2601.14255",
    "authors": [],
    "stars": "108",
    "details": {
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "abstract": "Demo: https://huggingface.co/spaces/SammyLim/VideoMaMa Git: https://github.com/cvlab-kaist/VideoMaMa Project Page: https://cvlab-kaist.github.io/VideoMaMa/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14255",
      "pdf_url": "https://arxiv.org/pdf/2601.14255",
      "github_links": [
        "https://github.com/cvlab-kaist/VideoMaMa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14255",
      "scraped_at": "2026-01-26T02:01:47.579204"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15224",
    "authors": [
      "Dingcheng Wang",
      "Haoran Lu",
      "Haosen Sun",
      "Jianshu Zhang",
      "Raymond-Qiancx"
    ],
    "stars": "76",
    "details": {
      "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
      "abstract": "Towards General Progress Understanding for Embodied Agents",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15224",
      "pdf_url": "https://arxiv.org/pdf/2601.15224",
      "github_links": [
        "https://github.com/ProgressLM/ProgressLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15224",
      "scraped_at": "2026-01-26T02:01:49.425233"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Agentic Uncertainty Quantification",
    "paper_url": "https://huggingface.co/papers/2601.15703",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Uncertainty Quantification",
      "abstract": "üõë Stop the \"Spiral of Hallucination\" in Autonomous Agents! Long-horizon agents often fail because minor early errors snowball into irreversible failures. We introduce Agentic Uncertainty Quantification (AUQ) , a training-free Dual-Process framework inspired by System 1/System 2 thinking: üß† System 1 (Fast): Uncertainty-Aware Memory propagates doubt to prevent blind commitment. ü§î System 2 (Slow): Triggers active reflection only when confidence drops below a specific threshold. Key Wins: ‚úÖ SOTA Performance: Outperforms ReAct & Reflexion on ALFWorld, WebShop, and the new DeepResearch Bench . ‚úÖ Efficiency: Prevents long, futile failure loops, making it more token-efficient than standard methods. ‚úÖ Plug-and-Play: No fine-tuning required. From \"Passive Diagnosis\" to \"Active Control\" ‚Äî make your agents reliable! üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15703",
      "pdf_url": "https://arxiv.org/pdf/2601.15703",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15703",
      "scraped_at": "2026-01-26T02:01:51.336711"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
    "paper_url": "https://huggingface.co/papers/2601.16192",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360¬∞",
      "abstract": "360Anything lifts arbitrary perspective images and videos to seamless, gravity-aligned 360¬∞ panoramas, without using any camera or 3D information. Project page: https://360anything.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16192",
      "pdf_url": "https://arxiv.org/pdf/2601.16192",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16192",
      "scraped_at": "2026-01-26T02:01:53.349634"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Agentic Confidence Calibration",
    "paper_url": "https://huggingface.co/papers/2601.15778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Agentic Confidence Calibration",
      "abstract": "üéØ Don't let your Agents be \"Confidently Wrong\"! Traditional calibration works for static text, but Autonomous Agents fail differently‚Äîerrors compound over long trajectories. We introduce Holistic Trajectory Calibration (HTC) , a new paradigm to diagnose the entire execution process. Why it matters: üîç Process-Centric: Extracts rich features (Dynamics, Stability) from the agent's thinking process, not just the final output. üìà SOTA Calibration: Consistently outperforms baselines across 8 benchmarks (SimpleQA, Math500, etc.). üåç Generalization: We release the General Agent Calibrator (GAC) , which achieves the best zero-shot calibration on the challenging GAIA benchmark. Achieve Interpretability, Transferability, and Trust in your AI Agents. üõ°Ô∏è",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15778",
      "pdf_url": "https://arxiv.org/pdf/2601.15778",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15778",
      "scraped_at": "2026-01-26T02:01:55.177454"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15690",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "üó∫Ô∏è The 2026 Roadmap for Reliable AI: Making Uncertainty Actionable We are witnessing a paradigm shift in LLMs: Uncertainty is no longer just a passive score for diagnosis‚Äîit is evolving into an Active Control Signal for real-time decision-making. Our comprehensive survey covers this transformation across three frontiers: üß† Reasoning: Triggering self-correction & optimizing \"thinking budget\" (System 2). ü§ñ Agents: Determining when to use tools, ask for help, or stop generation. üéØ Alignment: Using uncertainty as an intrinsic reward to mitigate reward hacking in RLHF. If you are building Agents or Reasoning Models, this is the functional evolution you need to know. üöÄ",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15690",
      "pdf_url": "https://arxiv.org/pdf/2601.15690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15690",
      "scraped_at": "2026-01-26T02:01:56.990537"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "paper_url": "https://huggingface.co/papers/2601.15549",
    "authors": [
      "Ryo Hachiuma",
      "Hideo Saito",
      "Ryo Fujii"
    ],
    "stars": "0",
    "details": {
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "abstract": "Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15549",
      "pdf_url": "https://arxiv.org/pdf/2601.15549",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15549",
      "scraped_at": "2026-01-26T02:01:58.859945"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "paper_url": "https://huggingface.co/papers/2601.16134",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LLM Prompt Evaluation for Educational Applications",
      "abstract": "Some of the observations founded are :- -- Prompt design matters as much as the model : The study shows that different prompt templates using the same LLM produce significantly different educational outcomes, proving prompt engineering is a critical lever in AI supported learning. -- Persona + Context Manager is the strongest combination : The Strategic Reading Coach prompt combining Persona and Context Manager patterns outperformed all others with 81‚Äì100% win probability, making it the most effective for follow up educational questions. -- Systematic prompt evaluation beats ad-hoc refinement : The tournament style evaluation using comparative judgment + Glicko2 ranking provides a reproducible, evidence based alternative to informal trial and error prompt tuning. -- Learning theory grounded prompts perform better : Prompts explicitly grounded in adult learning theory, self directed learning, and metacognition consistently generated higher quality educational dialogue than theory light designs -- Theoretical alignment alone is not enough : Some prompts rooted in strong learning theories (e.g. constructivism) still underperformed, highlighting that empirical evaluation is essential good theory must be paired with effective prompt patterns.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16134",
      "pdf_url": "https://arxiv.org/pdf/2601.16134",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16134",
      "scraped_at": "2026-01-26T02:02:00.656760"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "paper_url": "https://huggingface.co/papers/2601.16004",
    "authors": [
      "Cohaerence"
    ],
    "stars": "5",
    "details": {
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts. Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16004",
      "pdf_url": "https://arxiv.org/pdf/2601.16004",
      "github_links": [
        "https://github.com/christopher-altman/ibm-qml-kernel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16004",
      "scraped_at": "2026-01-26T02:02:02.464187"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "paper_url": "https://huggingface.co/papers/2601.15440",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "abstract": "In this work, we address the performance limitations often encountered in Python-based DLA simulations. By utilizing Numba for just-in-time compilation, we developed an implementation that achieves computational speeds comparable to legacy Fortran codes, offering a speedup over pure Python. We also validated the solver by analyzing the fractal dimension of the generated clusters (D‚âà1.71). We have released the code as a PyPI package named dla-ideal-solver to facilitate easier use and reproducibility. We hope this tool proves useful to those working in computational physics and complex systems, and we welcome any feedback from the community.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15440",
      "pdf_url": "https://arxiv.org/pdf/2601.15440",
      "github_links": [
        "https://github.com/sandyherho/dla-ideal-solver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15440",
      "scraped_at": "2026-01-26T02:02:05.381786"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "paper_url": "https://huggingface.co/papers/2601.08118",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
      "abstract": "The framework is open-sourced at https://github.com/SAP/mirrorbench",
      "arxiv_page_url": "https://arxiv.org/abs/2601.08118",
      "pdf_url": "https://arxiv.org/pdf/2601.08118",
      "github_links": [
        "https://github.com/SAP/mirrorbench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.08118",
      "scraped_at": "2026-01-26T02:02:07.168371"
    },
    "scraped_date": "2026-01-26"
  },
  {
    "title": "LongCat-Flash-Thinking-2601 Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.16725",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LongCat-Flash-Thinking-2601 Technical Report",
      "abstract": "this is informative.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16725",
      "pdf_url": "https://arxiv.org/pdf/2601.16725",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16725",
      "scraped_at": "2026-01-27T01:57:53.550979"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
    "paper_url": "https://huggingface.co/papers/2601.16746",
    "authors": [],
    "stars": "35",
    "details": {
      "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
      "abstract": "wcÔºånb",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16746",
      "pdf_url": "https://arxiv.org/pdf/2601.16746",
      "github_links": [
        "https://github.com/Ayanami1314/swe-pruner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16746",
      "scraped_at": "2026-01-27T01:57:55.428156"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "paper_url": "https://huggingface.co/papers/2601.14133",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
      "abstract": "TwinBrainVLA , a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14133",
      "pdf_url": "https://arxiv.org/pdf/2601.14133",
      "github_links": [
        "https://github.com/ZGC-EmbodyAI/TwinBrainVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14133",
      "scraped_at": "2026-01-27T01:57:57.312958"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
    "paper_url": "https://huggingface.co/papers/2601.16973",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
      "abstract": "We released VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents. We systematically study the brittleness of vision-language models in multi-step visual interaction, analyze how training choices shape behavior, and open-source the full benchmark, models, and trajectories. X: https://x.com/zwcolin/status/2015812327338287227 Project: https://visgym.github.io/ Paper: https://arxiv.org/abs/2601.16973 Code: https://github.com/visgym/VisGym Data & models: https://huggingface.co/VisGym",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16973",
      "pdf_url": "https://arxiv.org/pdf/2601.16973",
      "github_links": [
        "https://github.com/visgym/VIsGym",
        "https://github.com/visgym/VisGym"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16973",
      "scraped_at": "2026-01-27T01:57:59.237093"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "paper_url": "https://huggingface.co/papers/2601.16296",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Plenoptic Video Generation (2026) Spatia: Video Generation with Updatable Spatial Memory (2025) StoryMem: Multi-shot Long Video Storytelling with Memory (2025) Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation (2025) V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping (2025) BulletTime: Decoupled Control of Time and Camera Pose for Video Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16296",
      "pdf_url": "https://arxiv.org/pdf/2601.16296",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16296",
      "scraped_at": "2026-01-27T01:58:01.102913"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "paper_url": "https://huggingface.co/papers/2601.15808",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "abstract": "The scaling law of verification in deep research agent",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15808",
      "pdf_url": "https://arxiv.org/pdf/2601.15808",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15808",
      "scraped_at": "2026-01-27T01:58:02.949107"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "paper_url": "https://huggingface.co/papers/2601.14243",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
      "abstract": "This paper analyzes why existing FP8 reinforcement learning methods fail. It proposes Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization by eliminating training-inference mismatch.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14243",
      "pdf_url": "https://arxiv.org/pdf/2601.14243",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14243",
      "scraped_at": "2026-01-27T01:58:04.828615"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2601.16515",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
      "abstract": "In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72√ó inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16515",
      "pdf_url": "https://arxiv.org/pdf/2601.16515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16515",
      "scraped_at": "2026-01-27T01:58:06.714348"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "GameTalk: Training LLMs for Strategic Conversation",
    "paper_url": "https://huggingface.co/papers/2601.16276",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GameTalk: Training LLMs for Strategic Conversation",
      "abstract": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16276",
      "pdf_url": "https://arxiv.org/pdf/2601.16276",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16276",
      "scraped_at": "2026-01-27T01:58:08.550432"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
    "paper_url": "https://huggingface.co/papers/2601.07251",
    "authors": [
      "Jianwen Sun",
      "Yukang Feng",
      "Yibin Wang",
      "Chuanhao Li",
      "Zizhen Li"
    ],
    "stars": "18",
    "details": {
      "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
      "abstract": "https://github.com/leroy9472/MeepleLM",
      "arxiv_page_url": "https://arxiv.org/abs/2601.07251",
      "pdf_url": "https://arxiv.org/pdf/2601.07251",
      "github_links": [
        "https://github.com/leroy9472/MeepleLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.07251",
      "scraped_at": "2026-01-27T01:58:10.584314"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
    "paper_url": "https://huggingface.co/papers/2601.16344",
    "authors": [
      "Yongchan Kwon",
      "Federico Bianchi",
      "Harper Hua",
      "Junlin Wang",
      "Fan Nie"
    ],
    "stars": "0",
    "details": {
      "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems (2026) SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence (2025) AInsteinBench: Benchmarking Coding Agents on Scientific Repositories (2025) DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows (2025) HeurekaBench: A Benchmarking Framework for AI Co-scientist (2026) LongDA: Benchmarking LLM Agents for Long-Document Data Analysis (2026) ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16344",
      "pdf_url": "https://arxiv.org/pdf/2601.16344",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16344",
      "scraped_at": "2026-01-27T01:58:12.422648"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
    "paper_url": "https://huggingface.co/papers/2601.16018",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "abstract": "Mecellem Models propose Turkish legal-domain encoders and decoders trained from scratch and via continual pre-training. ModernBERT-based encoders (112.7B tokens) achieve top-3 Turkish retrieval results with high production efficiency, while Qwen3-based decoders show 36.2% perplexity reduction on legal text. Models and datasets are released via Hugging Face to support reproducible and cost-effective legal NLP for Turkish and other low-resource languages.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16018",
      "pdf_url": "https://arxiv.org/pdf/2601.16018",
      "github_links": [
        "https://github.com/newmindai/mecellem-models"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16018",
      "scraped_at": "2026-01-27T01:58:14.317360"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
    "paper_url": "https://huggingface.co/papers/2601.16443",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API From Failure to Mastery: Generating Hard Samples for Tool-use Agents (2026) Training Versatile Coding Agents in Synthetic Environments (2025) One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents (2025) Dr. Zero: Self-Evolving Search Agents without Training Data (2026) NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents (2025) SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning (2025) AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16443",
      "pdf_url": "https://arxiv.org/pdf/2601.16443",
      "github_links": [
        "https://github.com/kanishkg/endless-terminals"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16443",
      "scraped_at": "2026-01-27T01:58:16.138892"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
    "paper_url": "https://huggingface.co/papers/2601.13606",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch",
      "abstract": "High-quality synthetic Chart data and strong Chart reasoning model.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13606",
      "pdf_url": "https://arxiv.org/pdf/2601.13606",
      "github_links": [
        "https://github.com/starriver030515/ChartVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13606",
      "scraped_at": "2026-01-27T01:58:18.182354"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
    "paper_url": "https://huggingface.co/papers/2601.11258",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation",
      "abstract": "Let Your LLMs Use New Knowledge with ‚ÄúPaST‚Äù Skills Paper: https://arxiv.org/abs/2601.11258 Blog: https://past-blog.notion.site",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11258",
      "pdf_url": "https://arxiv.org/pdf/2601.11258",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11258",
      "scraped_at": "2026-01-27T01:58:20.042830"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
    "paper_url": "https://huggingface.co/papers/2601.15715",
    "authors": [
      "Yi R Fung",
      "Zongwei Lyu",
      "Zhitao He"
    ],
    "stars": "0",
    "details": {
      "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance (2026) Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks (2026) CogDoc: Towards Unified thinking in Documents (2025) Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models (2025) Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR (2026) REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation (2025) R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15715",
      "pdf_url": "https://arxiv.org/pdf/2601.15715",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15715",
      "scraped_at": "2026-01-27T01:58:21.862502"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
    "paper_url": "https://huggingface.co/papers/2601.13118",
    "authors": [
      "Gabriele Bavota",
      "Rosalia Tufano",
      "Fiorella Zampetti",
      "Alessandro Midolo",
      "Devy1"
    ],
    "stars": "0",
    "details": {
      "title": "Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization",
      "abstract": ".",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13118",
      "pdf_url": "https://arxiv.org/pdf/2601.13118",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13118",
      "scraped_at": "2026-01-27T01:58:23.760413"
    },
    "scraped_date": "2026-01-27"
  },
  {
    "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
    "paper_url": "https://huggingface.co/papers/2601.16451",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
      "abstract": "üöÄ VISTA-PATH is introduced as the first interactive segmentation foundation model for pathology. It advances computational pathology workflows by enabling more accurate, interpretable, and human-guided quantitative measurements. Key highlights include: 1Ô∏è‚É£ Large-scale training: Trained on over 1.6M image-mask-text pairs 2Ô∏è‚É£ State-of-the-art performance: Accurate segmentation across both in-distribution and out-of-distribution tissues 3Ô∏è‚É£ Interactive refinement: Outputs can be efficiently refined using bounding-box prompts with only a few active-learning steps 4Ô∏è‚É£ Deployment-ready: Fully integrated into https://tissuelab.org and available for immediate use üìÑ Read the arXiv preprint: https://arxiv.org/abs/2601.16451",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16451",
      "pdf_url": "https://arxiv.org/pdf/2601.16451",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16451",
      "scraped_at": "2026-01-27T01:58:25.717710"
    },
    "scraped_date": "2026-01-27"
  }
]