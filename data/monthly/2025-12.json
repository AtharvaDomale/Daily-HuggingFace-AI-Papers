[
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-04T20:19:20.961877"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-04T20:19:23.118436"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-04T20:19:25.281652"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-04T20:19:27.476746"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-04T20:19:29.578861"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-04T20:19:31.728245"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-04T20:19:33.755115"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-04T20:19:36.010946"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-04T20:19:38.067484"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-04T20:19:40.618700"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-04T20:19:42.781709"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-04T20:19:44.858222"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-04T20:19:46.947599"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-04T20:19:49.014636"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-04T20:19:51.293128"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-04T20:19:53.370089"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-04T20:19:55.477733"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-04T20:19:57.520691"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-04T20:19:59.592819"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-04T20:20:01.699407"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-04T20:20:03.794116"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-04T20:20:05.813512"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-04T20:20:07.825029"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-04T20:20:09.829762"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-04T21:17:47.069463"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-04T21:17:49.192535"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-04T21:17:51.363234"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-04T21:17:53.492069"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-04T21:17:55.616626"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-04T21:17:57.841621"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-04T21:17:59.919631"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-04T21:18:02.035416"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-04T21:18:04.082998"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-04T21:18:06.180570"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-04T21:18:08.400168"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-04T21:18:10.545247"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-04T21:18:12.576045"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-04T21:18:14.792437"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-04T21:18:16.953344"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-04T21:18:18.990243"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-04T21:18:21.101903"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-04T21:18:23.201891"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-04T21:18:25.299264"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-04T21:18:27.368119"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-04T21:18:29.397458"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-04T21:18:31.533447"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-04T21:18:33.581102"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-04T21:18:35.717215"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-05T01:45:08.800431"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-05T01:45:10.948720"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-05T01:45:13.120150"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-05T01:45:15.330754"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-05T01:45:17.565020"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-05T01:45:19.723255"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-05T01:45:21.795623"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-05T01:45:23.959532"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization (2025) Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning (2025) DeepEyesV2: Toward Agentic Multimodal Model (2025) Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch (2025) From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning (2025) MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning (2025) Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-05T01:45:26.057231"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-05T01:45:28.203868"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation (2025) DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models (2025) CharCom: Composable Identity Control for Multi-Character Story Illustration (2025) The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment (2025) Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-05T01:45:30.339006"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-05T01:45:32.465445"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-05T01:45:34.611634"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-05T01:45:36.734679"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-05T01:45:38.714190"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-05T01:45:41.479727"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-05T01:45:43.560551"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-05T01:45:45.641544"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-05T01:45:47.648471"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-05T01:45:49.740831"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2512.04025",
    "authors": [
      "Bohan Zhuang",
      "Weijie Wang",
      "Xi Lin",
      "Youping Gu",
      "Xiaolong Li"
    ],
    "stars": "4",
    "details": {
      "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
      "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our project page is at: https://ziplab.co/PSA/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04025",
      "pdf_url": "https://arxiv.org/pdf/2512.04025",
      "github_links": [
        "https://github.com/ziplab/Pyramid-Sparse-Attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04025",
      "scraped_at": "2025-12-05T01:45:51.734925"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-05T01:45:53.791038"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-05T01:45:55.865460"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-05T01:45:58.004342"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-05T01:46:00.069008"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-05T21:45:27.136896"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-05T21:45:29.003036"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-05T21:45:30.836133"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "32",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-05T21:45:32.680328"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "87",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-05T21:45:34.526238"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-05T21:45:36.527491"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "230",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-05T21:45:38.358063"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-05T21:45:40.291130"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-05T21:45:42.094547"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-05T21:45:43.969839"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-05T21:45:45.799525"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-05T21:45:47.632081"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "10",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-05T21:45:49.509345"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-05T21:45:51.439704"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-05T21:45:53.263163"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-05T21:45:55.240172"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "742",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-05T21:45:57.093539"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-05T21:45:58.872854"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-05T21:46:00.832911"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-05T21:46:02.669896"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-05T21:46:04.538104"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-05T21:46:06.330856"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "5",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-05T21:46:08.128956"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-05T21:46:09.903559"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "146",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-05T21:46:11.776860"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-05T21:46:13.573649"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-05T21:46:15.445157"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-05T21:46:17.298562"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-05T21:46:19.102566"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-05T21:46:20.916543"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-05T21:46:22.711806"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-05T21:46:24.551712"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-05T21:46:26.346693"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "1",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-05T21:46:28.138846"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-05T21:46:29.948938"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-05T21:46:31.735937"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-05T21:46:33.515128"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-05T21:46:35.309571"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-06T01:39:15.192900"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-06T01:39:17.129412"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-06T01:39:19.022592"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "33",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-06T01:39:20.928064"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "87",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-06T01:39:22.780299"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-06T01:39:24.694509"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "230",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-06T01:39:26.554770"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-06T01:39:28.390259"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-06T01:39:30.642592"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-06T01:39:32.487748"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-06T01:39:34.345229"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-06T01:39:36.204083"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-06T01:39:38.084093"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "10",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-06T01:39:39.989738"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-06T01:39:41.819845"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "742",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-06T01:39:43.747719"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-06T01:39:45.583608"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-06T01:39:47.490249"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-06T01:39:49.355078"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-06T01:39:51.188158"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-06T01:39:54.131792"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-06T01:39:56.410809"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-06T01:39:58.256378"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "5",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-06T01:40:00.105252"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-06T01:40:01.956187"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-06T01:40:03.845611"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "148",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-06T01:40:05.744152"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-06T01:40:07.557068"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-06T01:40:09.385040"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-06T01:40:11.210013"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-06T01:40:13.057458"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-06T01:40:14.900057"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-06T01:40:16.708726"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "1",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-06T01:40:18.512814"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-06T01:40:20.394175"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-06T01:40:22.227829"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-06T01:40:24.083221"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-06T01:40:25.891953"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-07T01:52:46.094380"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-07T01:52:48.210822"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "69",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-07T01:52:50.202899"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-07T01:52:52.202235"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "101",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-07T01:52:54.160261"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "116",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-07T01:52:56.246187"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "350",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-07T01:52:58.305330"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "33",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-07T01:53:00.340224"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-07T01:53:02.544481"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-07T01:53:04.591650"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-07T01:53:06.578999"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-07T01:53:08.537607"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-07T01:53:10.438186"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-07T01:53:12.449058"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-07T01:53:14.374069"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-07T01:53:16.262997"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-07T01:53:18.204429"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "12",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-07T01:53:20.103764"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "747",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-07T01:53:22.017875"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-07T01:53:23.974486"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-07T01:53:25.852135"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "153",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-07T01:53:27.762613"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-07T01:53:29.694599"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-07T01:53:31.626340"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-07T01:53:33.570046"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-07T01:53:35.457087"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-07T01:53:37.370394"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "17",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-07T01:53:39.404271"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-07T01:53:41.262254"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-07T01:53:43.056982"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-07T01:53:44.927975"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-07T01:53:46.922593"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-07T01:53:48.808339"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-07T01:53:50.767147"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "2",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-07T01:53:52.633019"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-07T01:53:54.486599"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-07T01:53:56.300727"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-07T01:53:58.243503"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "348",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-08T01:45:50.074934"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-08T01:45:52.315362"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-08T01:45:54.258639"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "50",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/open-compass/VLMEvalKit/pull/1334",
        "https://github.com/InternLM/ARM-Thinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-08T01:45:56.554505"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "670",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-08T01:45:58.852851"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "111",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-08T01:46:01.038360"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "180",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-08T01:46:03.070805"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-08T01:46:05.376820"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-08T01:46:07.678018"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-08T01:46:09.658067"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-08T01:46:11.943491"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-08T01:46:15.287360"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-08T01:46:17.530303"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-08T01:46:19.898642"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-08T01:46:22.128948"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-08T01:46:24.087696"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-08T01:46:26.031383"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "749",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-08T01:46:28.371364"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "13",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-08T01:46:30.273623"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-08T01:46:32.517669"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-08T01:46:34.496564"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-08T01:46:36.498721"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "157",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-08T01:46:38.851452"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-08T01:46:41.174546"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-08T01:46:43.171361"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-08T01:46:45.529468"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "22",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-08T01:46:47.448319"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-08T01:46:49.455630"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-08T01:46:52.138190"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-08T01:46:54.052433"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-08T01:46:56.366922"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-08T01:46:58.622222"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-08T01:47:00.882199"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-08T01:47:03.075799"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "2",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-08T01:47:05.025805"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-08T01:47:07.334262"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-08T01:47:09.230588"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-08T01:47:11.482221"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "paper_url": "https://huggingface.co/papers/2512.05150",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
      "abstract": "Taming 20B full-parameter few-step training with self-adversarial flows! üëèüèª One-model Simplicity: We eliminate the need for auxiliary networks (discriminators, teachers, fake score estimators...), everything in one model! Scalability on Large Models: We transform Qwen-Image-20B into high-quality few-step generators by full-parameter training (Optimized for human figure generation!). Checkout our 2-NFE images generated by our TwinFlow-Qwen-Image! üëá We are also working on Z-Image-Turbo , stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05150",
      "pdf_url": "https://arxiv.org/pdf/2512.05150",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05150",
      "scraped_at": "2025-12-09T01:44:30.991822"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
    "paper_url": "https://huggingface.co/papers/2512.05965",
    "authors": [
      "Ziyu Guo",
      "Manyuan Zhang",
      "Longin-Yu",
      "zhengli1013",
      "appletea2333"
    ],
    "stars": "25",
    "details": {
      "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
      "abstract": "Instruction-based image editing has emerged as a prominent research area. Benefiting from image generation foundation models, it has achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to \"think\" while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions, followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produces the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05965",
      "pdf_url": "https://arxiv.org/pdf/2512.05965",
      "github_links": [
        "https://github.com/appletea233/EditThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05965",
      "scraped_at": "2025-12-09T01:44:32.917190"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "paper_url": "https://huggingface.co/papers/2512.02580",
    "authors": [
      "Yang Li",
      "Shuai Zhang",
      "Yuchen Liu",
      "Jinyang Wu",
      "Changpeng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
      "abstract": "üöÄ [New Paper] CAPO: From Imitation to Discrimination ‚Äì Rethinking Advantage in RL Early RL training often suffers from instability due to \"mixed signals\" (simultaneous positive & negative feedback). Inspired by child cognitive development, we propose CAPO (Curriculum Advantage Policy Optimization) . ‚ú® The Core Intuition: Instead of a static curriculum, we leverage Advantage values to create a dynamic, two-phase process: 1Ô∏è‚É£ Imitation Phase: Train on Positive Advantage only. This reduces variance and establishes a stable behavioral foundation (Imitate to learn). 2Ô∏è‚É£ Discrimination Phase: Introduce Negative Signals later. [cite_start]This restores unbiased estimation and refines decision boundaries (Discriminate to generalize). üìà Highlights: Plug-and-Play: Delivers consistent gains when compatible with GRPO, PPO, RLOO, & Reinforce++ . Cross-Domain: Not just for Math! CAPO demonstrates impressive generalization on Multimodal GUI Agent tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02580",
      "pdf_url": "https://arxiv.org/pdf/2512.02580",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02580",
      "scraped_at": "2025-12-09T01:44:34.806051"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
    "paper_url": "https://huggingface.co/papers/2512.04810",
    "authors": [
      "Qi Tian",
      "Lingxi Xie",
      "Jianbo Ouyang",
      "Longhui Wei",
      "Xin He"
    ],
    "stars": "13",
    "details": {
      "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
      "abstract": "The project page https://emma-umm.github.io/emma/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04810",
      "pdf_url": "https://arxiv.org/pdf/2512.04810",
      "github_links": [
        "https://github.com/umm-emma/emma"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04810",
      "scraped_at": "2025-12-09T01:44:36.824643"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
    "paper_url": "https://huggingface.co/papers/2512.04784",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
      "abstract": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04784",
      "pdf_url": "https://arxiv.org/pdf/2512.04784",
      "github_links": [
        "https://github.com/X-GenGroup/PaCo-RL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04784",
      "scraped_at": "2025-12-09T01:44:38.814591"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
    "paper_url": "https://huggingface.co/papers/2512.05905",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
      "abstract": "SCAIL is a new framework for studio-grade character animation that uses a novel 3D pose representation and full-sequence context injection to deliver more stable, realistic motion transfer under complex and cross-identity scenarios.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05905",
      "pdf_url": "https://arxiv.org/pdf/2512.05905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05905",
      "scraped_at": "2025-12-09T01:44:40.683227"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.05591",
    "authors": [
      "Zijia Lin",
      "Tiehua Mei",
      "Minxuan Lv",
      "Leiyu Pan",
      "Suu"
    ],
    "stars": "0",
    "details": {
      "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
      "abstract": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05591",
      "pdf_url": "https://arxiv.org/pdf/2512.05591",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05591",
      "scraped_at": "2025-12-09T01:44:42.544097"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.05044",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
      "abstract": "Project Page: https://ivg-yanranzhang.github.io/MoRe4D/ Github Repo: https://github.com/Zhangyr2022/MoRe4D The dataset is coming soon. Stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05044",
      "pdf_url": "https://arxiv.org/pdf/2512.05044",
      "github_links": [
        "https://github.com/Zhangyr2022/MoRe4D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05044",
      "scraped_at": "2025-12-09T01:44:44.580407"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04563",
    "authors": [
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Hengzhu Tang",
      "CUDAOUTOFMEMORY",
      "Starrrrrry"
    ],
    "stars": "5",
    "details": {
      "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
      "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04563",
      "pdf_url": "https://arxiv.org/pdf/2512.04563",
      "github_links": [
        "https://github.com/zhangzef/COOPER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04563",
      "scraped_at": "2025-12-09T01:44:46.516932"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
    "paper_url": "https://huggingface.co/papers/2512.00473",
    "authors": [
      "Zilong Huang",
      "Dongzhi Jiang",
      "Yuncheng Guo",
      "Leiqi Zhu",
      "Junyan Ye"
    ],
    "stars": "65",
    "details": {
      "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
      "abstract": "arXiv: https://arxiv.org/abs/2512.00473 code: https://github.com/yejy53/RealGen project page: https://yejy53.github.io/RealGen/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.00473",
      "pdf_url": "https://arxiv.org/pdf/2512.00473",
      "github_links": [
        "https://github.com/yejy53/RealGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.00473",
      "scraped_at": "2025-12-09T01:44:48.413384"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Self-Improving VLM Judges Without Human Annotations",
    "paper_url": "https://huggingface.co/papers/2512.05145",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving VLM Judges Without Human Annotations",
      "abstract": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05145",
      "pdf_url": "https://arxiv.org/pdf/2512.05145",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05145",
      "scraped_at": "2025-12-09T01:44:50.242883"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "paper_url": "https://huggingface.co/papers/2512.05927",
    "authors": [
      "Anirudha Majumdar",
      "Ola Shorinwa",
      "Micah Baker",
      "Tenny Yin",
      "Zhiting Mei"
    ],
    "stars": "0",
    "details": {
      "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
      "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05927",
      "pdf_url": "https://arxiv.org/pdf/2512.05927",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05927",
      "scraped_at": "2025-12-09T01:44:52.149809"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "paper_url": "https://huggingface.co/papers/2512.05343",
    "authors": [
      "Marc Pollefeys",
      "Or Litany",
      "Ian Huang",
      "Francis Engelmann",
      "efedele"
    ],
    "stars": "0",
    "details": {
      "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
      "abstract": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at this https URL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05343",
      "pdf_url": "https://arxiv.org/pdf/2512.05343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05343",
      "scraped_at": "2025-12-09T01:44:54.015630"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.02835",
    "authors": [
      "Shengju Qian",
      "Weikai Chen",
      "Lingting Zhu",
      "Yingda Yin",
      "Tangerine24"
    ],
    "stars": "5",
    "details": {
      "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
      "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02835",
      "pdf_url": "https://arxiv.org/pdf/2512.02835",
      "github_links": [
        "https://github.com/Clementine24/ReVSeg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02835",
      "scraped_at": "2025-12-09T01:44:55.832519"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
    "paper_url": "https://huggingface.co/papers/2512.05356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
      "abstract": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05356",
      "pdf_url": "https://arxiv.org/pdf/2512.05356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05356",
      "scraped_at": "2025-12-09T01:44:57.637941"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
    "paper_url": "https://huggingface.co/papers/2512.03514",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
      "abstract": "Can we build universal document retrievers that maintain strong results across typologically diverse languages without losing English performance. This question led us to design synthetic training data and multilingual benchmarks to teach a model to match documents across scripts and formats. We are excited to launch NetraEmbed our SoTA model for multimodal multilingual document retrieval along with the M3DR: Towards Universal Multilingual Multimodal Document Retrieval paper. The release includes the NetraEmbed model which produces a single dense embedding with matryoshka support at 768,1536 and 2560 dimensions and the ColNetraEmbed model which produces patch level multivector embeddings. Both models are finetuned on Gemma3-4B-it and gained ~150% improvement over baselines. To measure progress we also built the NayanaIR Benchmark with 22 multilingual and 1 cross lingual dataset and documented the full framework in the M3DR paper . Links Blog: https://www.cognitivelab.in/blog/introducing-netraembed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03514",
      "pdf_url": "https://arxiv.org/pdf/2512.03514",
      "github_links": [
        "https://github.com/adithya-s-k/colpali"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03514",
      "scraped_at": "2025-12-09T01:44:59.738369"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
    "paper_url": "https://huggingface.co/papers/2512.05277",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
      "abstract": "This work introduces TAD, the first benchmark targeting temporal understanding in ego-centric autonomous-driving videos, evaluates SoTA VLMs, and boosts their performance with two training-free motion-reasoning methods (Scene-CoT and TCogMap).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05277",
      "pdf_url": "https://arxiv.org/pdf/2512.05277",
      "github_links": [
        "https://github.com/vbdi/tad_bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05277",
      "scraped_at": "2025-12-09T01:45:01.601687"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "paper_url": "https://huggingface.co/papers/2512.05564",
    "authors": [
      "Yuhao Cheng",
      "Terry Jingchen Zhang",
      "Jing Wang",
      "Panwen Hu",
      "Zijun Wang"
    ],
    "stars": "0",
    "details": {
      "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
      "abstract": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05564",
      "pdf_url": "https://arxiv.org/pdf/2512.05564",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05564",
      "scraped_at": "2025-12-09T01:45:04.335515"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.05409",
    "authors": [
      "Minghui Yu",
      "Jinyuan Shi",
      "Hantao Huang",
      "Hao Zeng",
      "Ruixuan Huang"
    ],
    "stars": "0",
    "details": {
      "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
      "abstract": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05409",
      "pdf_url": "https://arxiv.org/pdf/2512.05409",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05409",
      "scraped_at": "2025-12-09T01:45:06.133971"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
    "paper_url": "https://huggingface.co/papers/2512.04694",
    "authors": [
      "Salih Tileylioglu",
      "Erdem Akag√ºnd√ºz",
      "Bevan Deniz Cilgin",
      "Barisylmz"
    ],
    "stars": "0",
    "details": {
      "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
      "abstract": "This work presents a transformer-based generative model for complex time-series signals, with experiments on seismic accelerometer data. Key idea: treat seismic waveforms as structured high-dimensional sequences and learn a latent trajectory that captures both physical dynamics and long-range temporal dependencies.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04694",
      "pdf_url": "https://arxiv.org/pdf/2512.04694",
      "github_links": [
        "https://github.com/brsylmz23/TimesNet-Gen/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04694",
      "scraped_at": "2025-12-09T01:45:07.983185"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.03667",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
      "abstract": "Colonoscopy saves lives ‚Äî but AI for colonoscopy is still far from intelligent. We are excited to launch the Colon-X project, an open initiative aimed at advancing multimodal intelligence in colonoscopy and beyond. Beyond serving as a community-wide data foundation, we're focused on a critical yet under-explored transition ‚Äì evolving from multimodal understanding to clinical reasoning. Keywords: Multimodal Colonoscopy Analysis, Multimodal Understanding, Clinical Reasoning, Reinforcement Learning, Multimodal Benchmark, AI Healthcare, and Abdomen",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03667",
      "pdf_url": "https://arxiv.org/pdf/2512.03667",
      "github_links": [
        "https://github.com/ai4colonoscopy/Colon-X"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03667",
      "scraped_at": "2025-12-09T01:45:09.862566"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.05774",
    "authors": [
      "Caiming Xiong",
      "Junnan Li",
      "Shijie Wang",
      "Honglu Zhou",
      "Ziyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05774",
      "pdf_url": "https://arxiv.org/pdf/2512.05774",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05774",
      "scraped_at": "2025-12-09T01:45:11.724493"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04142",
    "authors": [
      "Aimee van Wynsberghe",
      "Lisa Biber-Freudenberger",
      "Sasha Luccioni",
      "nicholasKluge",
      "sophia-falk"
    ],
    "stars": "0",
    "details": {
      "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
      "abstract": "The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4 üå±üçÉ",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04142",
      "pdf_url": "https://arxiv.org/pdf/2512.04142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04142",
      "scraped_at": "2025-12-09T01:45:13.587428"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.05339",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
      "abstract": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05339",
      "pdf_url": "https://arxiv.org/pdf/2512.05339",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05339",
      "scraped_at": "2025-12-09T01:45:15.478241"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.07461",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
      "abstract": "Paper: https://arxiv.org/abs/2512.07461 Code: https://github.com/bigai-nlco/Native-Parallel-Reasoner Model & Data: https://huggingface.co/bigai-NPR Website: https://bigai-nlco.github.io/Native-Parallel-Reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07461",
      "pdf_url": "https://arxiv.org/pdf/2512.07461",
      "github_links": [
        "https://github.com/bigai-nlco/Native-Parallel-Reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07461",
      "scraped_at": "2025-12-10T01:46:58.731334"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
    "paper_url": "https://huggingface.co/papers/2512.07525",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
      "abstract": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07525",
      "pdf_url": "https://arxiv.org/pdf/2512.07525",
      "github_links": [
        "https://github.com/OpenMOSS/rope_pp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07525",
      "scraped_at": "2025-12-10T01:47:00.619948"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Unified Video Editing with Temporal Reasoner",
    "paper_url": "https://huggingface.co/papers/2512.07469",
    "authors": [],
    "stars": "25",
    "details": {
      "title": "Unified Video Editing with Temporal Reasoner",
      "abstract": "A Chain of Frames video editing method enbale temporal reasoning and 4x video length extrapolation with just 50k training pairs! üè† Page: videocof.github.io/ üìÑ Paper: arxiv.org/abs/2512.07469 üíª Code: github.com/knightyxp/VideoCoF",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07469",
      "pdf_url": "https://arxiv.org/pdf/2512.07469",
      "github_links": [
        "https://github.com/knightyxp/VideoCoF"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07469",
      "scraped_at": "2025-12-10T01:47:02.532632"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "paper_url": "https://huggingface.co/papers/2512.07834",
    "authors": [
      "Yu-Lun Liu",
      "chien90190",
      "JiewenChan",
      "YiChuanH"
    ],
    "stars": "0",
    "details": {
      "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
      "abstract": "Stylized voxel art is widely used in games and digital media, but turning 3D meshes into visually appealing voxel forms remains challenging and often requires manual effort. Existing methods struggle to preserve semantic structure and offer limited control over stylization, particularly in discrete color and abstraction. We present Voxify3D, a differentiable two-stage framework for generating stylized voxel art from 3D meshes. In the first stage, we initialize a coarse voxel grid via neural volume rendering. In the second stage, we refine the grid under six-view orthographic pixel art supervision, guided by a discrete color palette derived from clustering strategies (e.g., K-means, Max-Min, Median Cut). To support differentiable palette-based quantization, we design a rendering mechanism based on Gumbel-Softmax and incorporate a CLIP-based perceptual loss to enforce semantic alignment between voxel renderings and the original mesh.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07834",
      "pdf_url": "https://arxiv.org/pdf/2512.07834",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07834",
      "scraped_at": "2025-12-10T01:47:04.380187"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Scaling Zero-Shot Reference-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.06905",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Scaling Zero-Shot Reference-to-Video Generation",
      "abstract": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06905",
      "pdf_url": "https://arxiv.org/pdf/2512.06905",
      "github_links": [
        "https://github.com/franciszzj/Saber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06905",
      "scraped_at": "2025-12-10T01:47:06.279606"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "paper_url": "https://huggingface.co/papers/2512.06749",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
      "abstract": "Project website with an intro video is available at: https://aka.ms/DoVer .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06749",
      "pdf_url": "https://arxiv.org/pdf/2512.06749",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06749",
      "scraped_at": "2025-12-10T01:47:08.156914"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Distribution Matching Variational AutoEncoder",
    "paper_url": "https://huggingface.co/papers/2512.07778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Distribution Matching Variational AutoEncoder",
      "abstract": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder‚Äôs latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at \\url{ https://github.com/sen-ye/dmvae}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07778",
      "pdf_url": "https://arxiv.org/pdf/2512.07778",
      "github_links": [
        "https://github.com/sen-ye/dmvae%7D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07778",
      "scraped_at": "2025-12-10T01:47:10.004095"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "paper_url": "https://huggingface.co/papers/2512.06065",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
      "abstract": "We propose a framework for real-time egocentric video editing. Our system is composed of: EgoEditData, a manually curated dataset of 100k video editing pairs focusing on the egocentric case and featuring object substitution and removal under challenging hand occlusions, interactions, and large egomotion; EgoEdit the first real-time autoregressive model for egocentric video editing running in real time on a single H100 with 855ms first-frame latency and enabling live augmented reality (AR) interactions; EgoEditBench, a comprehensive benchmark for evaluation of egocentric video editing systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06065",
      "pdf_url": "https://arxiv.org/pdf/2512.06065",
      "github_links": [
        "https://github.com/snap-research/EgoEdit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06065",
      "scraped_at": "2025-12-10T01:47:11.841255"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Relational Visual Similarity",
    "paper_url": "https://huggingface.co/papers/2512.07833",
    "authors": [
      "Jing Shi",
      "Yilin Wang",
      "Krishna Kumar Singh",
      "Sicheng Mo",
      "thaoshibe"
    ],
    "stars": "14",
    "details": {
      "title": "Relational Visual Similarity",
      "abstract": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07833",
      "pdf_url": "https://arxiv.org/pdf/2512.07833",
      "github_links": [
        "https://github.com/thaoshibe/relsim"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07833",
      "scraped_at": "2025-12-10T01:47:13.783773"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "paper_url": "https://huggingface.co/papers/2512.07806",
    "authors": [
      "Jungwoo Kim",
      "Younggeun Lee",
      "Seungtae Nam",
      "Seungkwon Yang",
      "Gynjn"
    ],
    "stars": "56",
    "details": {
      "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
      "abstract": "We are excited to share our recent work \"Multi-view Pyramid Transformer: Look Coarser to See Broader\" Paper: https://arxiv.org/abs/2512.07806 Project page: https://gynjn.github.io/MVP/ Code: https://github.com/Gynjn/MVP",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07806",
      "pdf_url": "https://arxiv.org/pdf/2512.07806",
      "github_links": [
        "https://github.com/Gynjn/MVP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07806",
      "scraped_at": "2025-12-10T01:47:15.632640"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "LongCat-Image Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.07584",
    "authors": [],
    "stars": "307",
    "details": {
      "title": "LongCat-Image Technical Report",
      "abstract": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07584",
      "pdf_url": "https://arxiv.org/pdf/2512.07584",
      "github_links": [
        "https://github.com/meituan-longcat/LongCat-Image"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07584",
      "scraped_at": "2025-12-10T01:47:17.470709"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.07831",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
      "abstract": "Project Website https://jackailab.github.io/Projects/UnityVideo/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07831",
      "pdf_url": "https://arxiv.org/pdf/2512.07831",
      "github_links": [
        "https://github.com/dvlab-research/UnityVideo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07831",
      "scraped_at": "2025-12-10T01:47:19.511510"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "paper_url": "https://huggingface.co/papers/2512.07783",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
      "abstract": "We develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model‚Äôs edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07783",
      "pdf_url": "https://arxiv.org/pdf/2512.07783",
      "github_links": [
        "https://github.com/Interplay-LM-Reasoning/Interplay-LM-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07783",
      "scraped_at": "2025-12-10T01:47:21.314074"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.03244",
    "authors": [
      "Nanyun Peng",
      "Swastik Roy",
      "Arpit Gupta",
      "Sruthi Gorantla",
      "Salman Rahman"
    ],
    "stars": "0",
    "details": {
      "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
      "abstract": "Please find our paper on training process reward models without ground truth by leveraging inference-time scaling methods, enabling reinforcement learning in domains where verifiable answers are unavailable.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03244",
      "pdf_url": "https://arxiv.org/pdf/2512.03244",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03244",
      "scraped_at": "2025-12-10T01:47:23.147318"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.03621",
    "authors": [
      "Taojun Ding",
      "Jiehui Huang",
      "Mantang Guo",
      "wangshx",
      "Iron-lyk"
    ],
    "stars": "23",
    "details": {
      "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
      "abstract": "Project page: https://recamdriving.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03621",
      "pdf_url": "https://arxiv.org/pdf/2512.03621",
      "github_links": [
        "https://github.com/Iron-LYK/ReCamDriving"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03621",
      "scraped_at": "2025-12-10T01:47:24.938487"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.06533",
    "authors": [
      "Jiacheng Chen",
      "Ziniu Li",
      "Sheng Tang",
      "Ming Chen",
      "trxcc2002"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06533",
      "pdf_url": "https://arxiv.org/pdf/2512.06533",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06533",
      "scraped_at": "2025-12-10T01:47:26.723851"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.06373",
    "authors": [
      "Yansong Tang",
      "Haoji Zhang",
      "Jingxuan Niu",
      "Wenlong Liu",
      "VoyageWang"
    ],
    "stars": "11",
    "details": {
      "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
      "abstract": "The project page is https://github.com/VoyageWang/VG-Refiner",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06373",
      "pdf_url": "https://arxiv.org/pdf/2512.06373",
      "github_links": [
        "https://github.com/VoyageWang/VG-Refiner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06373",
      "scraped_at": "2025-12-10T01:47:28.523139"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.06589",
    "authors": [
      "Simeng Qin",
      "Teng Ma",
      "Qi Guo",
      "Jie Liao",
      "jiaxiaojunQAQ"
    ],
    "stars": "0",
    "details": {
      "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
      "abstract": "This work presents OmniSafeBench-MM, a unified, open-source benchmark and toolbox designed for comprehensive evaluation of multimodal jailbreak attack and defense methods. It integrates 13 representative attack techniques, 15 defense strategies, and a diverse dataset spanning 9 risk domains and 50 fine-grained categories, covering real-world‚Äìrelevant query types (consultative, imperative, declarative). We also propose a three-dimensional evaluation protocol measuring harmfulness (from low-impact individual harm to societal-level threats), intent alignment, and response detail ‚Äî allowing nuanced safety-utility tradeoff analysis. Our extensive experiments across 10 open-source and 8 closed-source multimodal LLMs reveal widespread vulnerabilities to multimodal jailbreak attacks. By unifying data, methods, and evaluation, OmniSafeBench-MM offers a standardized, reproducible platform ‚Äî which we hope will become a foundational resource for future research on safe, robust multimodal LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06589",
      "pdf_url": "https://arxiv.org/pdf/2512.06589",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06589",
      "scraped_at": "2025-12-10T01:47:30.348829"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.07829",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
      "abstract": "We proposed FAE which adapts pretrained ViT as the latent space for visual generative models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07829",
      "pdf_url": "https://arxiv.org/pdf/2512.07829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07829",
      "scraped_at": "2025-12-10T01:47:32.150504"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Group Representational Position Encoding",
    "paper_url": "https://huggingface.co/papers/2512.07805",
    "authors": [],
    "stars": "30",
    "details": {
      "title": "Group Representational Position Encoding",
      "abstract": "Introducing GRAPE: Group Representational Position Encoding. Embracing General Relative Law of Position Encoding, unifying and improving Multiplicative and Additive Position Encoding, such as RoPE and Alibi! Better performance with a clear theoretical formulation! Project Page: https://model-architectures.github.io/GRAPE/ Paper: https://model-architectures.github.io/GRAPE/GRAPE.pdf Devoted to the frontier of superintelligence, hope you will enjoy it!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07805",
      "pdf_url": "https://arxiv.org/pdf/2512.07805",
      "github_links": [
        "https://github.com/model-architectures/GRAPE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07805",
      "scraped_at": "2025-12-10T01:47:33.925881"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.06835",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
      "abstract": "Experiment Results üìä We evaluate DoGe on 7 benchmarks covering: General visual reasoning & hallucination (MMMU, MMStar, HallBench) Specialized domain reasoning (MathVision, MathVista, ChemBench, MSEarthMCQ) 3B-level Models Performance Method MMMU MMStar HallBench MathVision MathVista ChemBench MSEarthMCQ Avg. InternVL2.5-2B 43.6 53.7 42.6 13.5 51.3 - - - Visionary-3B 40.7 50.5 59.8 17.1 54.7 40.8 38.2 43.1 Qwen2.5VL-3B* (Base) 41.0 49.3 60.6 18.7 48.8 43.4 40.8 43.2 DoGe-3B (Iter1) 46.6 54.5 61.5 21.7 ü•á57.9 45.8 ü•á48.3 48.0 DoGe-3B (Iter2) 48.9 52.5 ü•á62.5 23.1 54.2 ü•á47.7 46.2 47.9 DoGe-3B (Iter3) ü•á50.2 ü•á54.7 61.8 ü•á24.2 57.0 46.9 47.3 ü•á48.9 ‚¨ÜÔ∏è Max Gain (vs. Base) +9.2 +5.4 +1.9 +5.5 +9.1 +4.3 +7.5 +5.7 7B-level Models Performance Method MMMU MMStar HallBench MathVision MathVista ChemBench MSEarthMCQ Avg. InternVL2.5-8B 48.9 62.8 50.1 22.0 64.4 - - - Vision-R1-7B 46.9 60.8 66.7 ü•á29.0 68.5 46.0 44.1 51.7 Qwen2.5VL-7B* (Base) 49.9 60.7 66.3 23.6 64.1 48.6 43.3 50.9 DoGe-7B (Iter1) 53.1 ü•á63.2 54.4 24.3 62.1 48.7 46.4 50.3 DoGe-7B (Iter2) 50.9 60.0 ü•á68.3 25.3 ü•á68.8 ü•á49.0 ü•á46.5 52.7 DoGe-7B (Iter3) ü•á53.6 63.0 68.0 25.2 68.3 48.5 45.8 ü•á53.2 ‚¨ÜÔ∏è Max Gain (vs. Base) +3.7 +2.5 +2.0 +1.7 +4.7 +0.4 +3.2 +2.3 Key Takeaways ‚ú® Stable Self-Evolution : DoGe achieves consistent performance improvement across 3 iterations for both 3B and 7B models Domain Generalization : 3B models: Average +5.7% performance gain across all benchmarks 7B models: Average +2.3% performance gain (maintains superiority over strong baselines) Hallucination Reduction : +2.0% average improvement on HallBench, mitigating visual hallucination Data Efficiency : Excels in data-scarce domains (Chemistry, Earth Science) with limited manual annotations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06835",
      "pdf_url": "https://arxiv.org/pdf/2512.06835",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06835",
      "scraped_at": "2025-12-10T01:47:35.762943"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "paper_url": "https://huggingface.co/papers/2512.06963",
    "authors": [
      "Yaobo Liang",
      "Zhiying Du",
      "Fangyun Wei",
      "godjiaolongge",
      "ys3197"
    ],
    "stars": "0",
    "details": {
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "abstract": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06963",
      "pdf_url": "https://arxiv.org/pdf/2512.06963",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06963",
      "scraped_at": "2025-12-10T01:47:37.689331"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2512.06421",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
      "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06421",
      "pdf_url": "https://arxiv.org/pdf/2512.06421",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06421",
      "scraped_at": "2025-12-10T01:47:39.463814"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
    "paper_url": "https://huggingface.co/papers/2512.06791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
      "abstract": "Gradient methods in games are usually proven to converge only under strong monotonicity in the Euclidean geometry (Rosen-style assumptions). That fails even for simple coupled quadratic games, yet in practice we still often see convergence. This paper takes a structural ‚Äúdesign the geometry‚Äù viewpoint. Small-Gain Nash (SGN) uses per-player curvature and cross-player coupling bounds to build a block-weighted metric where the pseudo-gradient becomes strongly monotone and the joint gradient flow is contracting on a certified region. Once SGN holds on a region, you get: ‚Äì existence + uniqueness of a Nash equilibrium there, ‚Äì exponential contraction of the continuous flow, ‚Äì explicit safe step-size bounds for projected Euler and RK4 via a game-theoretic CFL number, and ‚Äì a finite ‚Äútimescale band‚Äù that plays a TTUR-like role i.e instead of vanishing two-timescale step sizes, it tells you for which relative player weights a single global step size is provably stable. The paper ends with an offline certification pipeline that estimates curvature/couplings on compact regions, optimizes the metric to enlarge the SGN margin, and certifies convergence in non-monotone quadratic and Markov games (including mirror/Fisher geometries for entropy-regularized policy gradient). code: https://github.com/AashVed/SmallGainNash",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06791",
      "pdf_url": "https://arxiv.org/pdf/2512.06791",
      "github_links": [
        "https://github.com/AashVed/SmallGainNash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06791",
      "scraped_at": "2025-12-10T01:47:41.245034"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "paper_url": "https://huggingface.co/papers/2512.06609",
    "authors": [
      "Wendi Zheng",
      "jerytang",
      "Ya-Qin",
      "jmhernandezlobato",
      "xutongda"
    ],
    "stars": "10",
    "details": {
      "title": "Vector Quantization using Gaussian Variational Autoencoder",
      "abstract": "State-of-the-Art VQ-VAE from Gaussian VAE without Training! We train a Gaussian VAE, convert it into VQ-VAE with almost 100% codebook usage, and keeps reconstruction performance! As flexible to setup as VQ-VAE, supporting: codebook size, codebook dimension, codebook number. Pre-trained models can be found in [Huggingface] Paper can be found in [Arxiv] Code can be found in [Github] Quick Start Install dependency dependency in environment.yaml conda env create --file=environment.yaml\nconda activate tokenizer Install this package from source pip install -e . [optional] CUDA kernel for fast run time cd gq_cuda_extension\npip install --no-build-isolation -e . Download pre-trained model Download model \"sd3unet_gq_0.25.ckpt\" from [Huggingface] : mkdir model_256 mv \"sd3unet_gq_0.25.ckpt\" ./model_256 This is a VQ-VAE with codebook_size=2**16=65536 and codebook_dim=16 Infer the model as VQ-VAE Then use the model as follows from PIL import Image from torchvision import transforms from omegaconf import OmegaConf from pit.util import instantiate_from_config import torch\n\ntransform = transforms.Compose([\n    transforms.Resize(( 256 , 256 )),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[ 0.5 , 0.5 , 0.5 ],\n                        std=[ 0.5 , 0.5 , 0.5 ])\n])\n\nimg = transform(Image. open ( \"demo.png\" )).unsqueeze( 0 ).cuda()\nconfig = OmegaConf.load( \"./configs/sd3unet_gq_0.25.yaml\" )\nvae = instantiate_from_config(config.model)\nvae.load_state_dict(\n    torch.load( \"models_256/sd3unet_gq_0.25.ckpt\" ,\n        map_location=torch.device( 'cpu' ))[ \"state_dict\" ],strict= False )\nvae = vae. eval ().cuda()\n\nvae. eval ()\nz, log = vae.encode(img, return_reg_log= True ) \nimg_hat = vae.dequant(log[ \"indices\" ]) # discrete indices img_hat = vae.decode(z) # quantized latent Infer the model as Gaussian VAE Alternatively, the model can be used as a Vanilla Gaussian VAE: from PIL import Image from torchvision import transforms from omegaconf import OmegaConf from pit.util import instantiate_from_config import torch\n\ntransform = transforms.Compose([\n    transforms.Resize(( 256 , 256 )),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[ 0.5 , 0.5 , 0.5 ],\n                        std=[ 0.5 , 0.5 , 0.5 ])\n])\n\nimg = transform(Image. open ( \"demo.png\" )).unsqueeze( 0 ).cuda()\nconfig = OmegaConf.load( \"./configs/sd3unet_gq_0.25.yaml\" )\nvae = instantiate_from_config(config.model)\nvae.load_state_dict(\n    torch.load( \"models_256/sd3unet_gq_0.25.ckpt\" ,\n        map_location=torch.device( 'cpu' ))[ \"state_dict\" ],strict= False )\nvae = vae. eval ().cuda()\n\nvae. eval ()\n\nz = vae.encode(img, return_reg_log= True )[ 1 ][ \"zhat_noquant\" ] # Gaussian VAE latents img_hat = vae.decode(z) Train your own VQ-VAE Determine the VQ-VAE parameters: codebook_size: the codebook size, must be 2**N codebook_dimension: the dimension for each codebook codebook_number: number of sub codebook per spatial dimension Setup \"sd3unet_gq_0.25.yaml\" according to VQ-VAE parameters: n_samples: = codebook_size size, must be 2**N group: = codebook_dimension, dim of each codebook z_channels: = codebook_dimension * codebook_number, total dim of codebook Setup \"sd3unet_gq_0.25.yaml\" according to dataset path root: dataset root image_size: target image size batch_size: batch size Run the training! The default \"sd3unet_gq_0.25.yaml\" is setup for codebook_dimension=16, codebook_number=1, codebook_size=2**16=65536 export WANDB_API_KEY= $YOUR_WANDB_API_KEY python main.py --base configs/sd3unet_gq_0.25.yaml --wandb Run the evaluation! After the training, obtain the ckpt in $CKPT_PATH. Then, evaluate the model as python -m torch.distributed.launch --standalone --use-env \\\n    --nproc-per-node=8 eval.py \\\n    --bs=16 \\\n    --img_size 256 \\\n    --base=/workspace/cogview_dev/xutd/xu/pytorch-image-tokenizer/configs/sd3unet_gq_0.25.yaml \\\n    --ckpt= $CKPT_PATH \\\n    --dataset= $IMAGE_FOLDER_PATH Train with VAVAE Like Alignment See \"configs/sd3unet_gq_0.25_vf.yaml\". Why it Works? The only difference between our Gaussian VAE and vanilla Gaussian VAE is the KL divergence penralization. The key difference is class \"GaussianQuantRegularizer\" in \"./pit/quantization/gaussian.py\". During training, GaussianQuantRegularizer forces each dimension of KL be the same and achieve log(codebook_size). kl2 = 1.4426 * 0.5 * (torch. pow (mu, 2 ) + var - 1.0 - logvar)\nkl2 = kl2.reshape(b,l,self.group,c//self.group)\nkl2 = torch. sum (kl2,dim= 2 ) # sum over group dimension kl2_mean, kl2_min, kl2_max = torch.mean(kl2), torch. min (kl2), torch. max (kl2)\n\nge = (kl2 > self.log_n_samples + self.tolerance). type (kl2.dtype) * self.lam_max\neq = (kl2 <= self.log_n_samples + self.tolerance). type (kl2.dtype) * (\n    kl2 >= self.log_n_samples - self.tolerance\n). type (kl2.dtype)\nle = (kl2 < self.log_n_samples - self.tolerance). type (kl2.dtype) * self.lam_min\nkl_loss = torch. sum ((ge * kl2 + eq * kl2 + le * kl2), dim=[ 1 , 2 ])\nkl_loss = torch. sum (kl_loss) / kl_loss.shape[ 0 ] During inference, GaussianQuantRegularizer create a codebook of iid Gaussian, and find the cloest sample to posterior mean. q_normal_dist = Normal(mu_q[:, None , :], std_q[:, None , :])\nlog_ratios = (\n    q_normal_dist.log_prob(self.prior_samples[ None ])\n    - self.normal_log_prob[ None ] * self.beta\n)\nperturbed = torch. sum (log_ratios, dim= 2 )\nargmax_indices = torch.argmax(perturbed, dim= 1 )\nzhat[i : i + bs] = torch.index_select(self.prior_samples, 0 , argmax_indices)\nindices[i : i + bs] = argmax_indices Basically we limit the KL divergence of Gaussian VAE close to log2 codebook size. Once this constraint is met, the Gaussian VAE can be converted to VQ-VAE without much loss. For more information, see our paper! Contact & Ack Largely from https://github.com/Stability-AI/generative-models Any questions or comments goes to: x.tongda@nyu.edu Or if you have wechat: 18510201763 Reference @ misc {xu2025vectorquantizationusinggaussian,\n      title={Vector Quantization using Gaussian Variational Autoencoder}, \n      author={Tongda Xu and Wendi Zheng and Jiajun He and Jose Miguel Hernandez-Lobato and Yan Wang and Ya-Qin Zhang and Jie Tang},\n      year={2025},\n      eprint={2512.06609},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2512.06609}, \n}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06609",
      "pdf_url": "https://arxiv.org/pdf/2512.06609",
      "github_links": [
        "https://github.com/Stability-AI/generative-models",
        "https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06609",
      "scraped_at": "2025-12-10T01:47:43.136663"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
    "paper_url": "https://huggingface.co/papers/2512.03704",
    "authors": [
      "YijunLiao"
    ],
    "stars": "2",
    "details": {
      "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
      "abstract": "üî• Solving \"State Inertia\" in Long-Context LLMs! We introduce DZ-TDPO, a non-destructive alignment framework. Problem: Standard DPO causes \"Alignment Tax\" (PPL explosion >100) when updating user states in long context. Solution: Dynamic KL Constraints + Dual-Zone Temporal Attention. Result: SOTA 55.4% Win Rate on MSC dataset with Zero PPL degradation (PPL ~26.0). üöÄ Code & SOTA Model (Phi-3.5) are released! Check the Linked Models section.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03704",
      "pdf_url": "https://arxiv.org/pdf/2512.03704",
      "github_links": [
        "https://github.com/lyj20071013/DZ-TDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03704",
      "scraped_at": "2025-12-10T01:47:44.960333"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
    "paper_url": "https://huggingface.co/papers/2512.07168",
    "authors": [
      "Linsey Pang",
      "Aaron Elkins",
      "Aman Chadha",
      "Christos Constantinou",
      "Georgios Ioannides"
    ],
    "stars": "0",
    "details": {
      "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
      "abstract": "This paper introduces JEPA+DAAM , a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Gaussian mixture‚Äìbased Density Adaptive Attention Mechanism (DAAM) to learn semantically rich and highly compressible speech representations, achieving reversible neural tokenization at 47.5 tokens/sec with strong reconstruction quality. ‚û°Ô∏è ùêäùêûùê≤ ùêáùê¢ùê†ùê°ùê•ùê¢ùê†ùê°ùê≠ùê¨ ùê®ùêü ùêâùêÑùêèùêÄ+ùêÉùêÄùêÄùêå: üß† ùë±ùë¨ùë∑ùë® ùë≠ùíêùíì ùë∫ùíÜùíçùíá-ùë∫ùíñùíëùíÜùíìùíóùíäùíîùíÜùíÖ ùë∫ùíëùíÜùíÜùíÑùíâ ùë¨ùíèùíÑùíêùíÖùíäùíèùíà: Decouples representation learning from waveform reconstruction using a masked-prediction JEPA objective in latent space. The encoder learns semantically meaningful embeddings at 2.5 Hz without requiring low-level waveform loss, enabling robust self-supervised pretraining and cross-task adaptability (ASR, TTS, voice conversion). üéØ ùë´ùíÜùíèùíîùíäùíïùíö ùë®ùíÖùíÇùíëùíïùíäùíóùíÜ ùë®ùíïùíïùíÜùíèùíïùíäùíêùíè (ùë´ùë®ùë®ùë¥): Introduces a Gaussian mixture‚Äìbased gating attention mechanism that modulates temporal features based on local statistical salience rather than pairwise dot-products. This allows adaptive feature selection and hierarchical speech-structure discovery with linear complexity, improving JEPA convergence (loss 0.09 vs 0.17 without DAAM). üß© ùë≠ùë∫ùë∏ + ùë¥ùíäùíôùíÜùíÖ-ùëπùíÇùíÖùíäùíô ùëªùíêùíåùíÜùíèùíäùíõùíÇùíïùíäùíêùíè: Implements Finite Scalar Quantization (FSQ) with mixed-radix integer packing for reversible, codebook-free tokenization (47.5 tokens/sec, 16 384-way vocabulary). Combined with a HiFi-GAN decoder, it achieves high-fidelity waveform reconstruction, outperforming or matching neural audio codecs like SoundStream and EnCodec at dramatically lower frame rates.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07168",
      "pdf_url": "https://arxiv.org/pdf/2512.07168",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07168",
      "scraped_at": "2025-12-10T01:47:46.822030"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "paper_url": "https://huggingface.co/papers/2512.06558",
    "authors": [
      "Ganesh Nanduru",
      "amanchadha",
      "Anubis91",
      "alexiglad",
      "mmiakashs"
    ],
    "stars": "0",
    "details": {
      "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
      "abstract": "The paper introduces Refer360 , a comprehensive multimodal dataset for embodied referring expression comprehension in human-robot interaction (HRI), and proposes MuRes , a lightweight guided residual module that selectively reinforces modality-specific features to improve multimodal grounding performance in real-world scenarios. ‚û°Ô∏è ùêäùêûùê≤ ùêáùê¢ùê†ùê°ùê•ùê¢ùê†ùê°ùê≠ùê¨ ùê®ùêü ùê≠ùê°ùêû ùêëùêûùêüùêûùê´ùüëùüîùüé ùêÅùêûùêßùêúùê°ùê¶ùêöùê´ùê§ + ùêåùêÆùêëùêûùê¨ ùêåùê®ùêùùêÆùê•ùêû : üß† ùëπùíÜùíáùíÜùíìùüëùüîùüé: ùë≠ùíäùíìùíîùíï ùë¨ùíéùíÉùíêùíÖùíäùíÜùíÖ ùëπùë¨ ùë´ùíÇùíïùíÇùíîùíÜùíï ùíòùíäùíïùíâ ùë¥ùíñùíçùíïùíä-ùëΩùíäùíÜùíò, ùë¥ùíñùíçùíïùíä-ùë∫ùíÜùíèùíîùíêùíì ùë¥ùíêùíÖùíÇùíçùíäùíïùíäùíÜùíî : Introduces a dataset with synchronized egocentric and exocentric views , RGB, depth, infrared, 3D skeleton , eye gaze , and audio , across indoor and outdoor environments. With 13,990 annotated interactions (3.2M frames), it overcomes biases in existing datasets (e.g., single view, indoor-only, no gesture/gaze integration). üîÅ ùë¥ùíñùëπùíÜùíî: ùëÆùíñùíäùíÖùíÜùíÖ ùëπùíÜùíîùíäùíÖùíñùíÇùíç ùë©ùíêùíïùíïùíçùíÜùíèùíÜùíÑùíå ùíáùíêùíì ùë¥ùíñùíçùíïùíäùíéùíêùíÖùíÇùíç ùë≠ùíñùíîùíäùíêùíè : Proposes a novel residual architecture that uses cross-attention to guide modality-specific signals (visual/language) through an information bottleneck , preventing feature dilution during fusion and outperforming both vanilla residuals and attention-only fusion across 4 datasets. üìà ùë∫ùíäùíàùíèùíäùíáùíäùíÑùíÇùíèùíï ùëÆùíÇùíäùíèùíî ùíÇùíÑùíìùíêùíîùíî ùëØùëπùë∞ ùíÇùíèùíÖ ùëΩùë∏ùë® ùëªùíÇùíîùíåùíî : On Refer360, integrating MuRes into CLIP improved IOU-25 by +3.4% , and on CAESAR-PRO by +4.99% . For broader VQA tasks like ScienceQA and A-OKVQA, MuRes boosted model accuracy by up to +30% , highlighting its generalization ability across task domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06558",
      "pdf_url": "https://arxiv.org/pdf/2512.06558",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06558",
      "scraped_at": "2025-12-10T01:47:48.625230"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.06032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
      "abstract": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3 (also called SAMv2 and SAMv3). We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAMv3. SAMv2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAMv3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAMv2 with multimodal fusion and text-conditioned mask generation of SAMv3; (2) Architectural Divergence, detailing pure vision-temporal design of SAMv2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAMv3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAMv3; (4) Training and Hyperparameter Distinctions, showing why SAMv2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAMv3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06032",
      "pdf_url": "https://arxiv.org/pdf/2512.06032",
      "github_links": [
        "https://github.com/Applied-AI-Research-Lab/The-SAM2-to-SAM3-Gap-in-the-Segment-Anything-Model-Family"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06032",
      "scraped_at": "2025-12-10T01:47:50.436796"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "paper_url": "https://huggingface.co/papers/2512.08765",
    "authors": [],
    "stars": "197",
    "details": {
      "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
      "abstract": "NeurIPS 2025: Wan-Move: Motion-controllable Video Generation viaLatent Trajectory Guidance",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08765",
      "pdf_url": "https://arxiv.org/pdf/2512.08765",
      "github_links": [
        "https://github.com/ali-vilab/Wan-Move"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08765",
      "scraped_at": "2025-12-11T01:47:36.940817"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "paper_url": "https://huggingface.co/papers/2512.08478",
    "authors": [
      "Muyao Niu",
      "Yifan Zhan",
      "Yifei Liu",
      "Yuning Gong",
      "Zuica96"
    ],
    "stars": "162",
    "details": {
      "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
      "abstract": "TL;DR: Visionary is an open, web-native platform built on WebGPU and ONNX Runtime. Enabling real-time rendering of diverse Gaussian Splatting variants (3DGS, MLP-based 3DGS, 4DGS, Neural Avatars and ‚ú®any future algorithms‚ú®), and traditional 3d Mesh, directly in the browser. It also supports post-processing using feed-forward networks. ‚Ä¢ üíª GitHub: https://github.com/Visionary-Laboratory/visionary ‚Ä¢ üåç Project pageÔºö https://visionary-laboratory.github.io/visionary/ ‚Ä¢ üé¨ Video: https://www.youtube.com/watch?v=-K8EjMfk09c ‚Ä¢ üìù Technical report: https://arxiv.org/abs/2512.08478",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08478",
      "pdf_url": "https://arxiv.org/pdf/2512.08478",
      "github_links": [
        "https://github.com/Visionary-Laboratory/visionary"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08478",
      "scraped_at": "2025-12-11T01:47:38.895910"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
    "paper_url": "https://huggingface.co/papers/2512.07951",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
      "abstract": "Project webpage: this https URL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07951",
      "pdf_url": "https://arxiv.org/pdf/2512.07951",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07951",
      "scraped_at": "2025-12-11T01:47:40.786795"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
    "paper_url": "https://huggingface.co/papers/2512.07802",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07802",
      "pdf_url": "https://arxiv.org/pdf/2512.07802",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07802",
      "scraped_at": "2025-12-11T01:47:42.750925"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
    "paper_url": "https://huggingface.co/papers/2512.07843",
    "authors": [
      "Xiuyu Li",
      "Tsu-Jui Fu",
      "Sida Wang",
      "katanaxu",
      "longlian"
    ],
    "stars": "0",
    "details": {
      "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07843",
      "pdf_url": "https://arxiv.org/pdf/2512.07843",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07843",
      "scraped_at": "2025-12-11T01:47:44.703963"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
    "paper_url": "https://huggingface.co/papers/2512.06864",
    "authors": [
      "Dim P. Papadopoulos",
      "Kaixuan Lu",
      "monurcan"
    ],
    "stars": "3",
    "details": {
      "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
      "abstract": "Accepted at WACV'26! Keywords: Video Instance Segmentation; Unsupervised Learning; Segmentation Quality Assessment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06864",
      "pdf_url": "https://arxiv.org/pdf/2512.06864",
      "github_links": [
        "https://github.com/wcbup/AutoQ-VIS/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06864",
      "scraped_at": "2025-12-11T01:47:46.583510"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
    "paper_url": "https://huggingface.co/papers/2512.05033",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
      "abstract": "Modern large language models achieve impressive reasoning capabilities with long chains of thought, but they incur substantial computational cost at inference time. Speculative decoding improves efficiency by using a fast, less accurate draft model to propose tokens that are then verified in parallel by a stronger target model. However, on reasoning tasks, traditional token-level speculative decoding often rejects many semantically valid steps due to superficial token mismatches. Recent step-level semantic verification methods mitigate this by accepting or rejecting entire reasoning steps, but they still waste target compute by regenerating many rejected steps that yield little quality gain. We propose ARBITRAGE , a step-level speculative generation framework that dynamically routes generation based on the relative advantage of the target model over the draft model. Instead of relying on a fixed acceptance threshold, ARBITRAGE uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal ‚Äúarbitrage oracle‚Äù that always selects the higher-quality step, achieving near-optimal efficiency‚Äìaccuracy trade-offs. Across multiple mathematical reasoning benchmarks, ARBITRAGE consistently outperforms prior step-level speculative decoding baselines, reducing inference latency by up to approximately 2√ó at matched accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05033",
      "pdf_url": "https://arxiv.org/pdf/2512.05033",
      "github_links": [
        "https://github.com/SqueezeAILab/Arbitrage"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05033",
      "scraped_at": "2025-12-11T01:47:48.469544"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "paper_url": "https://huggingface.co/papers/2512.06628",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
      "abstract": "We propose MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06628",
      "pdf_url": "https://arxiv.org/pdf/2512.06628",
      "github_links": [
        "https://github.com/Richard-Zhang-AI/MIND-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06628",
      "scraped_at": "2025-12-11T01:47:50.392850"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.02231",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
      "abstract": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02231",
      "pdf_url": "https://arxiv.org/pdf/2512.02231",
      "github_links": [
        "https://github.com/plnguyen2908/AV-SpeakerBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02231",
      "scraped_at": "2025-12-11T01:47:52.347320"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "DeepCode: Open Agentic Coding",
    "paper_url": "https://huggingface.co/papers/2512.07921",
    "authors": [
      "Chao Huang",
      "Xubin Ren",
      "Zirui Guo",
      "Zhonghang Li",
      "Zongwei Li"
    ],
    "stars": "11.8k",
    "details": {
      "title": "DeepCode: Open Agentic Coding",
      "abstract": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07921",
      "pdf_url": "https://arxiv.org/pdf/2512.07921",
      "github_links": [
        "https://github.com/HKUDS/DeepCode"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07921",
      "scraped_at": "2025-12-11T01:47:54.229208"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.08153",
    "authors": [
      "Weirui Ye",
      "Zheng Ding"
    ],
    "stars": "0",
    "details": {
      "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
      "abstract": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4√ó faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08153",
      "pdf_url": "https://arxiv.org/pdf/2512.08153",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08153",
      "scraped_at": "2025-12-11T01:47:56.074458"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
    "paper_url": "https://huggingface.co/papers/2512.06776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
      "abstract": "NBDiff: A principled path from AR to Diffusion LLMs",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06776",
      "pdf_url": "https://arxiv.org/pdf/2512.06776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06776",
      "scraped_at": "2025-12-11T01:47:57.970248"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
    "paper_url": "https://huggingface.co/papers/2512.08924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
      "abstract": "üìç A simple, unified interface for 3D tracking, depth, and pose üåü SOTA results on 4D reconstruction & tracking üöÄ Up to 100x faster pose estimation than prior works",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08924",
      "pdf_url": "https://arxiv.org/pdf/2512.08924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08924",
      "scraped_at": "2025-12-11T01:47:59.945687"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Modular Neural Image Signal Processing",
    "paper_url": "https://huggingface.co/papers/2512.08564",
    "authors": [
      "Michael S. Brown",
      "Ran Zhang",
      "Zhongling Wang",
      "mafifi"
    ],
    "stars": "1",
    "details": {
      "title": "Modular Neural Image Signal Processing",
      "abstract": "Modular Neural Image Signal Processing üé¨ Click to watch the video We present a modular neural image signal processing (ISP) framework that produces high-quality display-referred images while providing a high degree of modularity with explicit control over multiple intermediate stages of the rendering pipeline. Our ISP is fully differentiable and requires no manual tuning, and its modular structure not only improves rendering accuracy but also enhances scalability, debuggability, generalization to unseen cameras, and flexibility to support different user-preference picture styles within a lightweight and efficient design. On top of this modular neural ISP, we developed a user-interactive photo-editing tool that supports diverse editing operations, different picture styles, and enables unlimited post-editable re-rendering and re-styling. The tool accepts DNG raw images from any camera as well as sRGB images from third-party sources. Across multiple test sets, our method consistently delivers competitive qualitative and quantitative performance. Links: paper - code",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08564",
      "pdf_url": "https://arxiv.org/pdf/2512.08564",
      "github_links": [
        "https://github.com/mahmoudnafifi/modular_neural_isp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08564",
      "scraped_at": "2025-12-11T01:48:01.936587"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
    "paper_url": "https://huggingface.co/papers/2512.08186",
    "authors": [],
    "stars": "455",
    "details": {
      "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
      "abstract": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-Language Navigation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08186",
      "pdf_url": "https://arxiv.org/pdf/2512.08186",
      "github_links": [
        "https://github.com/InternRobotics/InternNav"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08186",
      "scraped_at": "2025-12-11T01:48:03.889865"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
    "paper_url": "https://huggingface.co/papers/2512.08868",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
      "abstract": "EcomBench introduces a holistic e-commerce benchmark to evaluate foundation agents on real-world tasks, emphasizing deep retrieval, multi-step reasoning, and cross-source knowledge integration.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08868",
      "pdf_url": "https://arxiv.org/pdf/2512.08868",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08868",
      "scraped_at": "2025-12-11T01:48:05.859496"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
    "paper_url": "https://huggingface.co/papers/2512.08358",
    "authors": [
      "Tianyu Huang",
      "Peng Li",
      "Jiacheng Deng",
      "Jiahao Lu",
      "xwt123"
    ],
    "stars": "0",
    "details": {
      "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
      "abstract": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08358",
      "pdf_url": "https://arxiv.org/pdf/2512.08358",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08358",
      "scraped_at": "2025-12-11T01:48:07.804365"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
    "paper_url": "https://huggingface.co/papers/2512.07197",
    "authors": [
      "Soohyun Lee",
      "Seokhyun Youn",
      "ozbro",
      "shbae84",
      "klavna"
    ],
    "stars": "6",
    "details": {
      "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
      "abstract": "project page: https://cmlab-korea.github.io/Awesome-Efficient-GS/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07197",
      "pdf_url": "https://arxiv.org/pdf/2512.07197",
      "github_links": [
        "https://github.com/CMLab-Korea/Awesome-Efficient-GS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07197",
      "scraped_at": "2025-12-11T01:48:09.673066"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
    "paper_url": "https://huggingface.co/papers/2512.06531",
    "authors": [
      "arghadip2002",
      "Necromancer0912"
    ],
    "stars": "0",
    "details": {
      "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
      "abstract": "We are excited to share our new work tackling the critical challenge of brain tumor detection from MRI scans. Due to high data volume and generalization issues in existing systems, we developed two novel deep learning architectures: SAETCN (Self-Attention Enhancement Tumor Classification Network): A classification model achieving a state-of-the-art 99.38% validation accuracy in classifying four classes (glioma, meningioma, pituitary, and non-tumor). Its self-attention mechanism significantly improves generalization and robustness, overcoming common pitfalls in CAD systems. SAS-Net (Self-Attentive Segmentation Network): For precise tumor localization, achieving 99.23% overall pixel accuracy in segmentation. This paper proposes one of the most accurate and generalized DL architectures for early, automatic brain tumor detection. We hope this work can serve as a strong baseline for future Computer-Aided Diagnosis systems. Check out the paper, and we welcome your feedback and discussions! #MedicalImaging #DeepLearning #SelfAttention #CAD #BrainTumor",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06531",
      "pdf_url": "https://arxiv.org/pdf/2512.06531",
      "github_links": [
        "https://github.com/arghadip2002/SAETCN-and-SASNET-Architectures"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06531",
      "scraped_at": "2025-12-11T01:48:11.542854"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05325",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference (2025) Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning (2025) Temporal Predictors of Outcome in Reasoning Language Models (2025) Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models (2025) C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning (2025) Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads (2025) Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05325",
      "pdf_url": "https://arxiv.org/pdf/2512.05325",
      "github_links": [
        "https://github.com/farukakgul/LYNX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05325",
      "scraped_at": "2025-12-11T01:48:13.451502"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
    "paper_url": "https://huggingface.co/papers/2512.08406",
    "authors": [
      "Jungong Han",
      "Yunqi Miao",
      "gaomingqi"
    ],
    "stars": "15",
    "details": {
      "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
      "abstract": "Code & Gradio Demo : https://github.com/gaomingqi/sam-body4d See our FULL demo and Gradio Demo video below:",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08406",
      "pdf_url": "https://arxiv.org/pdf/2512.08406",
      "github_links": [
        "https://github.com/gaomingqi/sam-body4d"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08406",
      "scraped_at": "2025-12-11T01:48:15.477654"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
    "paper_url": "https://huggingface.co/papers/2512.04763",
    "authors": [
      "Mete Ozay",
      "Zeynep Akata",
      "Umberto Michieli",
      "Ondrej Bohdal",
      "mwbini"
    ],
    "stars": "0",
    "details": {
      "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API LightMem: Lightweight and Efficient Memory-Augmented Generation (2025) MemVerse: Multimodal Memory for Lifelong Learning Agents (2025) Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks (2025) BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models (2025) HaluMem: Evaluating Hallucinations in Memory Systems of Agents (2025) ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents (2025) LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04763",
      "pdf_url": "https://arxiv.org/pdf/2512.04763",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04763",
      "scraped_at": "2025-12-11T01:48:17.833409"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
    "paper_url": "https://huggingface.co/papers/2512.04434",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
      "abstract": "This paper introduces a new deep learning algorithem to model transient flow around varied complex geometries using the deep operator network (DeepONet)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04434",
      "pdf_url": "https://arxiv.org/pdf/2512.04434",
      "github_links": [
        "https://github.com/baskargroup/TimeDependent-DeepONet"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04434",
      "scraped_at": "2025-12-11T01:48:19.704175"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
    "paper_url": "https://huggingface.co/papers/2512.08923",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
      "abstract": "Paper that evaluates and analyses consistency of MLLMs when providing questions in text vs as rendered-text.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08923",
      "pdf_url": "https://arxiv.org/pdf/2512.08923",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08923",
      "scraped_at": "2025-12-11T01:48:21.599664"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
    "paper_url": "https://huggingface.co/papers/2512.08309",
    "authors": [
      "xandergos"
    ],
    "stars": "1",
    "details": {
      "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
      "abstract": "Terrain Diffusion introduces a procedural generation primitive built around InfiniteDiffusion, a sampling method that delivers seamless, seed-consistent, infinite-domain generation with constant-time random access. A multi-scale diffusion hierarchy models planetary structure through a stack of diffusion models that couples planetary context with local detail,. The framework can stream entire worlds and is demonstrated in real time through a full Minecraft integration.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08309",
      "pdf_url": "https://arxiv.org/pdf/2512.08309",
      "github_links": [
        "https://github.com/xandergos/terrain-diffusion"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08309",
      "scraped_at": "2025-12-11T01:48:23.507617"
    },
    "scraped_date": "2025-12-11"
  },
  {
    "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.09363",
    "authors": [
      "Guixun Luo",
      "Hanwen Liang",
      "Longfei Li",
      "yuyangyin",
      "KXingLab"
    ],
    "stars": "0",
    "details": {
      "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
      "abstract": "StereoWorld presents geometry-aware monocular-to-stereo video generation using a pretrained video generator with geometry regularization and tiling for high-resolution, consistent stereo videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09363",
      "pdf_url": "https://arxiv.org/pdf/2512.09363",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09363",
      "scraped_at": "2025-12-12T01:47:07.328052"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
    "paper_url": "https://huggingface.co/papers/2512.08560",
    "authors": [
      "tamarott",
      "Antoniotorralbaborruel",
      "yuvalgolbari",
      "mcosarinsky",
      "navvew"
    ],
    "stars": "0",
    "details": {
      "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
      "abstract": "We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08560",
      "pdf_url": "https://arxiv.org/pdf/2512.08560",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08560",
      "scraped_at": "2025-12-12T01:47:09.440111"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
    "paper_url": "https://huggingface.co/papers/2512.09247",
    "authors": [
      "Cheng Liu",
      "AnalMom",
      "wanghaofan",
      "yiren98"
    ],
    "stars": "0",
    "details": {
      "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
      "abstract": "OmniPSD presents a diffusion-transformer framework for text-to-PSD generation and image-to-PSD decomposition, enabling layered, transparent PSDs with hierarchical, editable channels via in-context learning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09247",
      "pdf_url": "https://arxiv.org/pdf/2512.09247",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09247",
      "scraped_at": "2025-12-12T01:47:11.525771"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
    "paper_url": "https://huggingface.co/papers/2512.09824",
    "authors": [],
    "stars": "45",
    "details": {
      "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
      "abstract": "We introduce Bind & Compose (BiCo), a one-shot method that enables flexible visual concept composition by binding visual concepts with the corresponding prompt tokens and composing the target prompt with bound tokens from various sources. üåç Project page: https://refkxh.github.io/BiCo_Webpage/ üíª GitHub: https://github.com/refkxh/bico",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09824",
      "pdf_url": "https://arxiv.org/pdf/2512.09824",
      "github_links": [
        "https://github.com/refkxh/bico"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09824",
      "scraped_at": "2025-12-12T01:47:13.560373"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.08829",
    "authors": [],
    "stars": "30",
    "details": {
      "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
      "abstract": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08829",
      "pdf_url": "https://arxiv.org/pdf/2512.08829",
      "github_links": [
        "https://github.com/hustvl/InfiniteVL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08829",
      "scraped_at": "2025-12-12T01:47:15.739563"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2512.09928",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
      "abstract": "Code and checkpoints are available! Github: https://github.com/OpenHelix-Team/HiF-VLA Project page: https://hifvla.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09928",
      "pdf_url": "https://arxiv.org/pdf/2512.09928",
      "github_links": [
        "https://github.com/OpenHelix-Team/HiF-VLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09928",
      "scraped_at": "2025-12-12T01:47:17.792831"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
    "paper_url": "https://huggingface.co/papers/2512.02892",
    "authors": [
      "Yang Zhang",
      "guokan-shang",
      "mvazirg",
      "amr-mohamed"
    ],
    "stars": "0",
    "details": {
      "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
      "abstract": "SchED introduces a training-free, early-exit decoding criterion for diffusion LLMs , halting sampling once a smooth, progress-adaptive confidence threshold is satisfied. SchED achieves up to ~4√ó decoding speedups on average with ‚â•99‚Äì100% performance retention across benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02892",
      "pdf_url": "https://arxiv.org/pdf/2512.02892",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02892",
      "scraped_at": "2025-12-12T01:47:19.828816"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Rethinking Chain-of-Thought Reasoning for Videos",
    "paper_url": "https://huggingface.co/papers/2512.09616",
    "authors": [
      "Liwei Wang",
      "Yin Li",
      "Zi-Yuan Hu",
      "Yiwu Zhong"
    ],
    "stars": "4",
    "details": {
      "title": "Rethinking Chain-of-Thought Reasoning for Videos",
      "abstract": "Rethinking Chain-of-Thought Reasoning for Videos",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09616",
      "pdf_url": "https://arxiv.org/pdf/2512.09616",
      "github_links": [
        "https://github.com/LaVi-Lab/Rethink_CoT_Video"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09616",
      "scraped_at": "2025-12-12T01:47:21.778327"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
    "paper_url": "https://huggingface.co/papers/2512.04753",
    "authors": [
      "Chenglin Li",
      "Wenhong Zhu",
      "Ruilin Li",
      "Rethinker",
      "CodeGoat24"
    ],
    "stars": "5",
    "details": {
      "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
      "abstract": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04753",
      "pdf_url": "https://arxiv.org/pdf/2512.04753",
      "github_links": [
        "https://github.com/RlinL/EtCon"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04753",
      "scraped_at": "2025-12-12T01:47:23.795160"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "paper_url": "https://huggingface.co/papers/2512.09864",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
      "abstract": "Proposes UniUGP, a unified framework integrating scene understanding, video generation, and trajectory planning for autonomous driving with visual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09864",
      "pdf_url": "https://arxiv.org/pdf/2512.09864",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09864",
      "scraped_at": "2025-12-12T01:47:25.792923"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "WonderZoom: Multi-Scale 3D World Generation",
    "paper_url": "https://huggingface.co/papers/2512.09164",
    "authors": [
      "Jiajun Wu",
      "Hong-Xing Yu",
      "Jin Cao"
    ],
    "stars": "0",
    "details": {
      "title": "WonderZoom: Multi-Scale 3D World Generation",
      "abstract": "WonderZoom enables multi-scale 3D world generation from a single image via scale-adaptive Gaussian surfels and progressive detail synthesis for zoomed-in realism.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09164",
      "pdf_url": "https://arxiv.org/pdf/2512.09164",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09164",
      "scraped_at": "2025-12-12T01:47:27.809897"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Learning Unmasking Policies for Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.09106",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Learning Unmasking Policies for Diffusion Language Models",
      "abstract": "Trains a lightweight RL-based policy to unmask tokens in masked diffusion LMs, achieving competitive performance with heuristics and generalizing to new models and longer sequences.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09106",
      "pdf_url": "https://arxiv.org/pdf/2512.09106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09106",
      "scraped_at": "2025-12-12T01:47:29.698219"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Towards a Science of Scaling Agent Systems",
    "paper_url": "https://huggingface.co/papers/2512.08296",
    "authors": [
      "Samuel Schmidgall",
      "Chunjong Park",
      "Chanwoo Park",
      "Ken Gu",
      "Yubin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Towards a Science of Scaling Agent Systems",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08296",
      "pdf_url": "https://arxiv.org/pdf/2512.08296",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08296",
      "scraped_at": "2025-12-12T01:47:31.808737"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
    "paper_url": "https://huggingface.co/papers/2512.09663",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
      "abstract": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09663",
      "pdf_url": "https://arxiv.org/pdf/2512.09663",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09663",
      "scraped_at": "2025-12-12T01:47:33.812624"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
    "paper_url": "https://huggingface.co/papers/2512.05446",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
      "abstract": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05446",
      "pdf_url": "https://arxiv.org/pdf/2512.05446",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05446",
      "scraped_at": "2025-12-12T01:47:35.646412"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
    "paper_url": "https://huggingface.co/papers/2512.08006",
    "authors": [
      "Morteza Abolghasemi",
      "hrrabiee",
      "ZahraDehghanian97",
      "dninvb",
      "MahtaFetrat"
    ],
    "stars": "7",
    "details": {
      "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
      "abstract": "Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance. This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08006",
      "pdf_url": "https://arxiv.org/pdf/2512.08006",
      "github_links": [
        "https://github.com/MahtaFetrat/Piper-with-LCA-Phonemizer"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08006",
      "scraped_at": "2025-12-12T01:47:37.543590"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
    "paper_url": "https://huggingface.co/papers/2512.04519",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
      "abstract": "We introduce VideoSSM, an AR video diffusion model equipped with a novel hybrid memory architecture that combines a causal sliding-window local lossless cache with an SSM-based global compressed memory for long video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04519",
      "pdf_url": "https://arxiv.org/pdf/2512.04519",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04519",
      "scraped_at": "2025-12-12T01:47:39.448056"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.09112",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09112",
      "pdf_url": "https://arxiv.org/pdf/2512.09112",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09112",
      "scraped_at": "2025-12-12T01:47:41.437688"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.07222",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
      "abstract": "We had an interesting yet explorable observation that lowering the attention on function words of VLMs increaes robustness and zero-shot performance on several datasets/models/tasks, casuing little or no performance drops , surpasing SOTA adversarial training methods including TeCoA and FARE.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07222",
      "pdf_url": "https://arxiv.org/pdf/2512.07222",
      "github_links": [
        "https://github.com/michaeltian108/FDA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07222",
      "scraped_at": "2025-12-12T01:47:43.496736"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
    "paper_url": "https://huggingface.co/papers/2512.05402",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series (2025) Partial multivariate transformer as a tool for cryptocurrencies time series prediction (2025) Technical Analysis Meets Machine Learning: Bitcoin Evidence (2025) Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05402",
      "pdf_url": "https://arxiv.org/pdf/2512.05402",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05402",
      "scraped_at": "2025-12-12T01:47:45.402296"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication",
    "paper_url": "https://huggingface.co/papers/2512.01453",
    "authors": [
      "Hengshu Zhu",
      "Hongke Zhao",
      "ChuangZhao",
      "likang03",
      "zxq1942461723"
    ],
    "stars": "0",
    "details": {
      "title": "Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication",
      "abstract": "Fresh medical LLM survey",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01453",
      "pdf_url": "https://arxiv.org/pdf/2512.01453",
      "github_links": [
        "https://github.com/xqz614/Awesome-Agentic-Clinical-Dialogue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01453",
      "scraped_at": "2025-12-12T01:47:47.356492"
    },
    "scraped_date": "2025-12-12"
  },
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "paper_url": "https://huggingface.co/papers/2512.10430",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
      "abstract": "T-pro 2.0 is an open-weight Russian LLM with hybrid reasoning and fast inference, released with datasets, benchmarks, and an optimized decoding pipeline to support reproducible research and practical applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10430",
      "pdf_url": "https://arxiv.org/pdf/2512.10430",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10430",
      "scraped_at": "2025-12-13T01:41:43.039643"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2512.10739",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
      "abstract": "Due to a user error, the abstract displayed in this paper contains some errors üò≠ (the abstract in the PDF is correct). The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO.  Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10739",
      "pdf_url": "https://arxiv.org/pdf/2512.10739",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10739",
      "scraped_at": "2025-12-13T01:41:45.098298"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "paper_url": "https://huggingface.co/papers/2512.10949",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "abstract": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1 . Model is released at https://huggingface.co/IvanTang/3DGen-R1 .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10949",
      "pdf_url": "https://arxiv.org/pdf/2512.10949",
      "github_links": [
        "https://github.com/Ivan-Tang-3D/3DGen-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10949",
      "scraped_at": "2025-12-13T01:41:47.228267"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "paper_url": "https://huggingface.co/papers/2512.10756",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
      "abstract": "We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10756",
      "pdf_url": "https://arxiv.org/pdf/2512.10756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10756",
      "scraped_at": "2025-12-13T01:41:49.258354"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.10534",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "abstract": "InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10534",
      "pdf_url": "https://arxiv.org/pdf/2512.10534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10534",
      "scraped_at": "2025-12-13T01:41:51.214627"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2512.10881",
    "authors": [
      "Mingxi Xu",
      "DonaldLian",
      "weixia111111",
      "wzy27",
      "kehong"
    ],
    "stars": "0",
    "details": {
      "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
      "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10881",
      "pdf_url": "https://arxiv.org/pdf/2512.10881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10881",
      "scraped_at": "2025-12-13T01:41:53.206958"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "paper_url": "https://huggingface.co/papers/2512.05439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BEAVER: An Efficient Deterministic LLM Verifier",
      "abstract": "BEAVER is the first practical framework to formally verify an LLM‚Äôs output distribution. It enables rigorous assessment and comparison beyond traditional sampling-based evaluation. BEAVER computes deterministic, sound bounds on the total probability that a model‚Äôs responses satisfy given specifications. Across popular benchmarks, it yields tight certificates for correctness, security, and privacy and scales efficiently to large open-weight LLMs (e.g., 70B).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05439",
      "pdf_url": "https://arxiv.org/pdf/2512.05439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05439",
      "scraped_at": "2025-12-13T01:41:55.187789"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10867",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
      "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark frameworkMiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10867",
      "pdf_url": "https://arxiv.org/pdf/2512.10867",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10867",
      "scraped_at": "2025-12-13T01:41:57.166806"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "paper_url": "https://huggingface.co/papers/2511.23386",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
      "abstract": "arXiv: https://arxiv.org/pdf/2511.23386 Overall Architecture",
      "arxiv_page_url": "https://arxiv.org/abs/2511.23386",
      "pdf_url": "https://arxiv.org/pdf/2511.23386",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.23386",
      "scraped_at": "2025-12-13T01:41:59.104193"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "paper_url": "https://huggingface.co/papers/2512.10675",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
      "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10675",
      "pdf_url": "https://arxiv.org/pdf/2512.10675",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10675",
      "scraped_at": "2025-12-13T01:42:01.098563"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "paper_url": "https://huggingface.co/papers/2512.08511",
    "authors": [
      "Qixiang Ye",
      "Fang Wan",
      "callsys",
      "ywenxi"
    ],
    "stars": "11",
    "details": {
      "title": "Thinking with Images via Self-Calling Agent",
      "abstract": "üß†üñºÔ∏è Vision-language models are getting smarter‚Äîbut also harder to train. Many recent systems ‚Äúthink with images,‚Äù weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually requires LLMs to reason across modalites. ‚ú® This paper introduces thinking-with-images-through-self-calling (sCoT) -- a simpler idea: let the model think in language, break problems into atomic steps, and call itself to solve them . Instead of mixing text and images throughout its reasoning, a main agent splits a visual problem into small pieces‚Äîlike reading text or spotting an object‚Äîand delegates them to lightweight subagents ü§ñ. These subagents are virtual copies of the same model that answer one focused visual question and return a short text response. The main agent then combines everything through pure language reasoning. üöÄ The result? Easier training and stronger performance. The sCoT-based model trained with end-to-end RL, named as SubagentVL , outperforms previous state-of-the-art methods on challenging high-resolution benchmarks (V* and HR-Bench) with less GPU hours. üëâ Bottom line: smarter visual reasoning doesn‚Äôt require more complex multimodal thinking‚Äî letting models reason in language and ask for help from its virtual replicas . Code is available at github repo . Paper is available at arxiv",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08511",
      "pdf_url": "https://arxiv.org/pdf/2512.08511",
      "github_links": [
        "https://github.com/YWenxi/think-with-images-through-self-calling"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08511",
      "scraped_at": "2025-12-13T01:42:02.982574"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "paper_url": "https://huggingface.co/papers/2512.10959",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
      "abstract": "Project page: https://huggingface.co/spaces/prs-eth/stereospace_web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10959",
      "pdf_url": "https://arxiv.org/pdf/2512.10959",
      "github_links": [
        "https://github.com/prs-eth/stereospace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10959",
      "scraped_at": "2025-12-13T01:42:04.966010"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "paper_url": "https://huggingface.co/papers/2512.10938",
    "authors": [
      "Zhuang Liu",
      "Mingjie Sun",
      "Jiachen Zhu",
      "TaiMingLu",
      "Fishloong"
    ],
    "stars": "0",
    "details": {
      "title": "Stronger Normalization-Free Transformers",
      "abstract": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x)=erf(Œ±x+s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10938",
      "pdf_url": "https://arxiv.org/pdf/2512.10938",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10938",
      "scraped_at": "2025-12-13T01:42:08.136184"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "paper_url": "https://huggingface.co/papers/2512.10791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10791",
      "pdf_url": "https://arxiv.org/pdf/2512.10791",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10791",
      "scraped_at": "2025-12-13T01:42:10.130589"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "paper_url": "https://huggingface.co/papers/2512.10359",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
      "abstract": "Tool-augmented VideoQA system, accepted by NeurIPS'25 main track.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10359",
      "pdf_url": "https://arxiv.org/pdf/2512.10359",
      "github_links": [
        "https://github.com/fansunqi/VideoTool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10359",
      "scraped_at": "2025-12-13T01:42:12.057289"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "paper_url": "https://huggingface.co/papers/2512.09406",
    "authors": [
      "Pei Yang",
      "Xiaokang Liu",
      "AnalMom",
      "yiren98",
      "HaiCi"
    ],
    "stars": "13",
    "details": {
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "abstract": "A framework to translate human object interaction (HOI) videos into grounded robot object interaction (ROI) videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09406",
      "pdf_url": "https://arxiv.org/pdf/2512.09406",
      "github_links": [
        "https://github.com/showlab/H2R-Grounder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09406",
      "scraped_at": "2025-12-13T01:42:14.077733"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "paper_url": "https://huggingface.co/papers/2512.09270",
    "authors": [
      "Won-Sik Cheong",
      "Geonho Kim",
      "shurek20",
      "klavna",
      "sangwoonkwak"
    ],
    "stars": "5",
    "details": {
      "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression (2025) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting (2025) MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting (2025) UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction (2025) StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video (2025) Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09270",
      "pdf_url": "https://arxiv.org/pdf/2512.09270",
      "github_links": [
        "https://github.com/CMLab-Korea/MoRel-arXiv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09270",
      "scraped_at": "2025-12-13T01:42:15.992091"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "paper_url": "https://huggingface.co/papers/2512.10955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "abstract": "This work can isolate a specific attribute from any image and merge those selected attributes from multiple images into a coherent generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10955",
      "pdf_url": "https://arxiv.org/pdf/2512.10955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10955",
      "scraped_at": "2025-12-13T01:42:17.913989"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "paper_url": "https://huggingface.co/papers/2512.10398",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
      "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10398",
      "pdf_url": "https://arxiv.org/pdf/2512.10398",
      "github_links": [
        "https://github.com/facebook/confucius"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10398",
      "scraped_at": "2025-12-13T01:42:19.859409"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "paper_url": "https://huggingface.co/papers/2512.09924",
    "authors": [
      "SuaLily",
      "whluo",
      "Yanbiao",
      "LewisPan",
      "JacobYuan"
    ],
    "stars": "3",
    "details": {
      "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
      "abstract": "Code: https://github.com/Liuxinyv/ReViSE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09924",
      "pdf_url": "https://arxiv.org/pdf/2512.09924",
      "github_links": [
        "https://github.com/Liuxinyv/ReViSE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09924",
      "scraped_at": "2025-12-13T01:42:21.819725"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "paper_url": "https://huggingface.co/papers/2512.08870",
    "authors": [
      "Xiaodong Gu",
      "Yuchao Qiu",
      "Xiang Chen",
      "lanqz7766",
      "YerbaPage"
    ],
    "stars": "0",
    "details": {
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "abstract": "Check this out!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08870",
      "pdf_url": "https://arxiv.org/pdf/2512.08870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08870",
      "scraped_at": "2025-12-13T01:42:23.701246"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "paper_url": "https://huggingface.co/papers/2512.09756",
    "authors": [
      "Fei Huang",
      "Ke Wang",
      "Yongbin-Li",
      "yuchuan123",
      "ChonghuaLiao"
    ],
    "stars": "0",
    "details": {
      "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
      "abstract": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09756",
      "pdf_url": "https://arxiv.org/pdf/2512.09756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09756",
      "scraped_at": "2025-12-13T01:42:25.570321"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "paper_url": "https://huggingface.co/papers/2512.04537",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
      "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04537",
      "pdf_url": "https://arxiv.org/pdf/2512.04537",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04537",
      "scraped_at": "2025-12-13T01:42:27.487497"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "paper_url": "https://huggingface.co/papers/2512.10894",
    "authors": [
      "Jing Liao",
      "Yiran Xu",
      "Matthew Fisher",
      "Nanxuan Zhao",
      "Peiying Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
      "abstract": "We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10894",
      "pdf_url": "https://arxiv.org/pdf/2512.10894",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10894",
      "scraped_at": "2025-12-13T01:42:29.345987"
    },
    "scraped_date": "2025-12-13"
  },
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "paper_url": "https://huggingface.co/papers/2512.10430",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
      "abstract": "T-pro 2.0 is an open-weight Russian LLM with hybrid reasoning and fast inference, released with datasets, benchmarks, and an optimized decoding pipeline to support reproducible research and practical applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10430",
      "pdf_url": "https://arxiv.org/pdf/2512.10430",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10430",
      "scraped_at": "2025-12-14T01:53:13.679060"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2512.10739",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
      "abstract": "Due to a user error, the abstract displayed in this paper contains some errors üò≠ (the abstract in the PDF is correct). The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO.  Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10739",
      "pdf_url": "https://arxiv.org/pdf/2512.10739",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10739",
      "scraped_at": "2025-12-14T01:53:15.693838"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "paper_url": "https://huggingface.co/papers/2512.10949",
    "authors": [],
    "stars": "45",
    "details": {
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "abstract": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1 . Model is released at https://huggingface.co/IvanTang/3DGen-R1 .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10949",
      "pdf_url": "https://arxiv.org/pdf/2512.10949",
      "github_links": [
        "https://github.com/Ivan-Tang-3D/3DGen-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10949",
      "scraped_at": "2025-12-14T01:53:17.733362"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "paper_url": "https://huggingface.co/papers/2512.10756",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
      "abstract": "We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10756",
      "pdf_url": "https://arxiv.org/pdf/2512.10756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10756",
      "scraped_at": "2025-12-14T01:53:19.654624"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.10534",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "abstract": "InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10534",
      "pdf_url": "https://arxiv.org/pdf/2512.10534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10534",
      "scraped_at": "2025-12-14T01:53:21.621411"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2512.10881",
    "authors": [
      "Mingxi Xu",
      "DonaldLian",
      "weixia111111",
      "wzy27",
      "kehong"
    ],
    "stars": "0",
    "details": {
      "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
      "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10881",
      "pdf_url": "https://arxiv.org/pdf/2512.10881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10881",
      "scraped_at": "2025-12-14T01:53:23.610152"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "paper_url": "https://huggingface.co/papers/2512.05439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BEAVER: An Efficient Deterministic LLM Verifier",
      "abstract": "BEAVER is the first practical framework to formally verify an LLM‚Äôs output distribution. It enables rigorous assessment and comparison beyond traditional sampling-based evaluation. BEAVER computes deterministic, sound bounds on the total probability that a model‚Äôs responses satisfy given specifications. Across popular benchmarks, it yields tight certificates for correctness, security, and privacy and scales efficiently to large open-weight LLMs (e.g., 70B).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05439",
      "pdf_url": "https://arxiv.org/pdf/2512.05439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05439",
      "scraped_at": "2025-12-14T01:53:25.561512"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "paper_url": "https://huggingface.co/papers/2512.08511",
    "authors": [
      "Qixiang Ye",
      "Fang Wan",
      "callsys",
      "ywenxi"
    ],
    "stars": "11",
    "details": {
      "title": "Thinking with Images via Self-Calling Agent",
      "abstract": "üß†üñºÔ∏è Vision-language models are getting smarter‚Äîbut also harder to train. Many recent systems ‚Äúthink with images,‚Äù weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually requires LLMs to reason across modalites. ‚ú® This paper introduces thinking-with-images-through-self-calling (sCoT) -- a simpler idea: let the model think in language, break problems into atomic steps, and call itself to solve them . Instead of mixing text and images throughout its reasoning, a main agent splits a visual problem into small pieces‚Äîlike reading text or spotting an object‚Äîand delegates them to lightweight subagents ü§ñ. These subagents are virtual copies of the same model that answer one focused visual question and return a short text response. The main agent then combines everything through pure language reasoning. üöÄ The result? Easier training and stronger performance. The sCoT-based model trained with end-to-end RL, named as SubagentVL , outperforms previous state-of-the-art methods on challenging high-resolution benchmarks (V* and HR-Bench) with less GPU hours. üëâ Bottom line: smarter visual reasoning doesn‚Äôt require more complex multimodal thinking‚Äî letting models reason in language and ask for help from its virtual replicas . Code is available at github repo . Paper is available at arxiv",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08511",
      "pdf_url": "https://arxiv.org/pdf/2512.08511",
      "github_links": [
        "https://github.com/YWenxi/think-with-images-through-self-calling"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08511",
      "scraped_at": "2025-12-14T01:53:27.525310"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10867",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
      "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark frameworkMiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10867",
      "pdf_url": "https://arxiv.org/pdf/2512.10867",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10867",
      "scraped_at": "2025-12-14T01:53:29.479926"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "paper_url": "https://huggingface.co/papers/2512.10938",
    "authors": [
      "Zhuang Liu",
      "Mingjie Sun",
      "Jiachen Zhu",
      "TaiMingLu",
      "Fishloong"
    ],
    "stars": "31",
    "details": {
      "title": "Stronger Normalization-Free Transformers",
      "abstract": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x)=erf(Œ±x+s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10938",
      "pdf_url": "https://arxiv.org/pdf/2512.10938",
      "github_links": [
        "https://github.com/zlab-princeton/Derf"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10938",
      "scraped_at": "2025-12-14T01:53:31.531198"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "paper_url": "https://huggingface.co/papers/2511.23386",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
      "abstract": "arXiv: https://arxiv.org/pdf/2511.23386 Overall Architecture",
      "arxiv_page_url": "https://arxiv.org/abs/2511.23386",
      "pdf_url": "https://arxiv.org/pdf/2511.23386",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.23386",
      "scraped_at": "2025-12-14T01:53:33.460748"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "paper_url": "https://huggingface.co/papers/2512.10959",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
      "abstract": "Project page: https://huggingface.co/spaces/prs-eth/stereospace_web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10959",
      "pdf_url": "https://arxiv.org/pdf/2512.10959",
      "github_links": [
        "https://github.com/prs-eth/stereospace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10959",
      "scraped_at": "2025-12-14T01:53:35.489425"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "paper_url": "https://huggingface.co/papers/2512.10675",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
      "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10675",
      "pdf_url": "https://arxiv.org/pdf/2512.10675",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10675",
      "scraped_at": "2025-12-14T01:53:37.369827"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "paper_url": "https://huggingface.co/papers/2512.09270",
    "authors": [
      "Won-Sik Cheong",
      "Geonho Kim",
      "shurek20",
      "klavna",
      "sangwoonkwak"
    ],
    "stars": "6",
    "details": {
      "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression (2025) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting (2025) MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting (2025) UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction (2025) StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video (2025) Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09270",
      "pdf_url": "https://arxiv.org/pdf/2512.09270",
      "github_links": [
        "https://github.com/CMLab-Korea/MoRel-arXiv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09270",
      "scraped_at": "2025-12-14T01:53:39.264748"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "paper_url": "https://huggingface.co/papers/2512.10791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10791",
      "pdf_url": "https://arxiv.org/pdf/2512.10791",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10791",
      "scraped_at": "2025-12-14T01:53:41.169449"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "paper_url": "https://huggingface.co/papers/2512.10359",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
      "abstract": "Tool-augmented VideoQA system, accepted by NeurIPS'25 main track.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10359",
      "pdf_url": "https://arxiv.org/pdf/2512.10359",
      "github_links": [
        "https://github.com/fansunqi/VideoTool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10359",
      "scraped_at": "2025-12-14T01:53:43.080671"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "paper_url": "https://huggingface.co/papers/2512.09924",
    "authors": [
      "SuaLily",
      "whluo",
      "Yanbiao",
      "LewisPan",
      "JacobYuan"
    ],
    "stars": "4",
    "details": {
      "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
      "abstract": "Code: https://github.com/Liuxinyv/ReViSE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09924",
      "pdf_url": "https://arxiv.org/pdf/2512.09924",
      "github_links": [
        "https://github.com/Liuxinyv/ReViSE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09924",
      "scraped_at": "2025-12-14T01:53:44.960597"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "paper_url": "https://huggingface.co/papers/2512.09406",
    "authors": [
      "Pei Yang",
      "Xiaokang Liu",
      "AnalMom",
      "yiren98",
      "HaiCi"
    ],
    "stars": "13",
    "details": {
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "abstract": "A framework to translate human object interaction (HOI) videos into grounded robot object interaction (ROI) videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09406",
      "pdf_url": "https://arxiv.org/pdf/2512.09406",
      "github_links": [
        "https://github.com/showlab/H2R-Grounder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09406",
      "scraped_at": "2025-12-14T01:53:47.039412"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "paper_url": "https://huggingface.co/papers/2512.08870",
    "authors": [
      "Xiaodong Gu",
      "Yuchao Qiu",
      "Xiang Chen",
      "lanqz7766",
      "YerbaPage"
    ],
    "stars": "0",
    "details": {
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "abstract": "Check this out!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08870",
      "pdf_url": "https://arxiv.org/pdf/2512.08870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08870",
      "scraped_at": "2025-12-14T01:53:48.951709"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "paper_url": "https://huggingface.co/papers/2512.10955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "abstract": "This work can isolate a specific attribute from any image and merge those selected attributes from multiple images into a coherent generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10955",
      "pdf_url": "https://arxiv.org/pdf/2512.10955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10955",
      "scraped_at": "2025-12-14T01:53:50.790956"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "paper_url": "https://huggingface.co/papers/2512.10894",
    "authors": [
      "Jing Liao",
      "Yiran Xu",
      "Matthew Fisher",
      "Nanxuan Zhao",
      "Peiying Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
      "abstract": "We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10894",
      "pdf_url": "https://arxiv.org/pdf/2512.10894",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10894",
      "scraped_at": "2025-12-14T01:53:52.732535"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "paper_url": "https://huggingface.co/papers/2512.10398",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
      "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10398",
      "pdf_url": "https://arxiv.org/pdf/2512.10398",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10398",
      "scraped_at": "2025-12-14T01:53:54.647927"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "paper_url": "https://huggingface.co/papers/2512.04537",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
      "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04537",
      "pdf_url": "https://arxiv.org/pdf/2512.04537",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04537",
      "scraped_at": "2025-12-14T01:53:56.750920"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "paper_url": "https://huggingface.co/papers/2512.09756",
    "authors": [
      "Fei Huang",
      "Ke Wang",
      "Yongbin-Li",
      "yuchuan123",
      "ChonghuaLiao"
    ],
    "stars": "0",
    "details": {
      "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
      "abstract": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09756",
      "pdf_url": "https://arxiv.org/pdf/2512.09756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09756",
      "scraped_at": "2025-12-14T01:53:58.658360"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "DragMesh: Interactive 3D Generation Made Easy",
    "paper_url": "https://huggingface.co/papers/2512.06424",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "DragMesh: Interactive 3D Generation Made Easy",
      "abstract": "DragMesh enables real time, physically valid 3D object articulation by decoupling kinematic reasoning from motion generation and producing plausible motions via a dual quaternion based generative model.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06424",
      "pdf_url": "https://arxiv.org/pdf/2512.06424",
      "github_links": [
        "https://github.com/AIGeeksGroup/DragMesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06424",
      "scraped_at": "2025-12-14T01:54:00.686897"
    },
    "scraped_date": "2025-12-14"
  },
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "paper_url": "https://huggingface.co/papers/2512.10430",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
      "abstract": "T-pro 2.0 is an open-weight Russian LLM with hybrid reasoning and fast inference, released with datasets, benchmarks, and an optimized decoding pipeline to support reproducible research and practical applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10430",
      "pdf_url": "https://arxiv.org/pdf/2512.10430",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10430",
      "scraped_at": "2025-12-15T01:51:12.704072"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "paper_url": "https://huggingface.co/papers/2512.10739",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
      "abstract": "Due to a user error, the abstract displayed in this paper contains some errors üò≠ (the abstract in the PDF is correct). The correct and complete abstract is as follows: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO.  Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10739",
      "pdf_url": "https://arxiv.org/pdf/2512.10739",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10739",
      "scraped_at": "2025-12-15T01:51:14.519089"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "paper_url": "https://huggingface.co/papers/2512.10949",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "abstract": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1 . Model is released at https://huggingface.co/IvanTang/3DGen-R1 .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10949",
      "pdf_url": "https://arxiv.org/pdf/2512.10949",
      "github_links": [
        "https://github.com/Ivan-Tang-3D/3DGen-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10949",
      "scraped_at": "2025-12-15T01:51:16.372862"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "paper_url": "https://huggingface.co/papers/2512.10756",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
      "abstract": "We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10756",
      "pdf_url": "https://arxiv.org/pdf/2512.10756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10756",
      "scraped_at": "2025-12-15T01:51:18.171391"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.10534",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "abstract": "InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10534",
      "pdf_url": "https://arxiv.org/pdf/2512.10534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10534",
      "scraped_at": "2025-12-15T01:51:20.001218"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "paper_url": "https://huggingface.co/papers/2512.05439",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BEAVER: An Efficient Deterministic LLM Verifier",
      "abstract": "BEAVER is the first practical framework to formally verify an LLM‚Äôs output distribution. It enables rigorous assessment and comparison beyond traditional sampling-based evaluation. BEAVER computes deterministic, sound bounds on the total probability that a model‚Äôs responses satisfy given specifications. Across popular benchmarks, it yields tight certificates for correctness, security, and privacy and scales efficiently to large open-weight LLMs (e.g., 70B).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05439",
      "pdf_url": "https://arxiv.org/pdf/2512.05439",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05439",
      "scraped_at": "2025-12-15T01:51:21.929291"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "paper_url": "https://huggingface.co/papers/2512.10881",
    "authors": [
      "Mingxi Xu",
      "DonaldLian",
      "weixia111111",
      "wzy27",
      "kehong"
    ],
    "stars": "0",
    "details": {
      "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
      "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10881",
      "pdf_url": "https://arxiv.org/pdf/2512.10881",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10881",
      "scraped_at": "2025-12-15T01:51:23.926766"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "paper_url": "https://huggingface.co/papers/2512.08511",
    "authors": [
      "Qixiang Ye",
      "Fang Wan",
      "callsys",
      "ywenxi"
    ],
    "stars": "12",
    "details": {
      "title": "Thinking with Images via Self-Calling Agent",
      "abstract": "üß†üñºÔ∏è Vision-language models are getting smarter‚Äîbut also harder to train. Many recent systems ‚Äúthink with images,‚Äù weaving visual information directly into their reasoning. While powerful, this approach can be hard to incentivize, as it usually requires LLMs to reason across modalites. ‚ú® This paper introduces thinking-with-images-through-self-calling (sCoT) -- a simpler idea: let the model think in language, break problems into atomic steps, and call itself to solve them . Instead of mixing text and images throughout its reasoning, a main agent splits a visual problem into small pieces‚Äîlike reading text or spotting an object‚Äîand delegates them to lightweight subagents ü§ñ. These subagents are virtual copies of the same model that answer one focused visual question and return a short text response. The main agent then combines everything through pure language reasoning. üöÄ The result? Easier training and stronger performance. The sCoT-based model trained with end-to-end RL, named as SubagentVL , outperforms previous state-of-the-art methods on challenging high-resolution benchmarks (V* and HR-Bench) with less GPU hours. üëâ Bottom line: smarter visual reasoning doesn‚Äôt require more complex multimodal thinking‚Äî letting models reason in language and ask for help from its virtual replicas . Code is available at github repo . Paper is available at arxiv",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08511",
      "pdf_url": "https://arxiv.org/pdf/2512.08511",
      "github_links": [
        "https://github.com/YWenxi/think-with-images-through-self-calling"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08511",
      "scraped_at": "2025-12-15T01:51:25.763062"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "paper_url": "https://huggingface.co/papers/2512.10938",
    "authors": [
      "Zhuang Liu",
      "Mingjie Sun",
      "Jiachen Zhu",
      "TaiMingLu",
      "Fishloong"
    ],
    "stars": "47",
    "details": {
      "title": "Stronger Normalization-Free Transformers",
      "abstract": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x)=erf(Œ±x+s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10938",
      "pdf_url": "https://arxiv.org/pdf/2512.10938",
      "github_links": [
        "https://github.com/zlab-princeton/Derf"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10938",
      "scraped_at": "2025-12-15T01:51:27.569161"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10867",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
      "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark frameworkMiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10867",
      "pdf_url": "https://arxiv.org/pdf/2512.10867",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10867",
      "scraped_at": "2025-12-15T01:51:29.477912"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "paper_url": "https://huggingface.co/papers/2511.23386",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
      "abstract": "arXiv: https://arxiv.org/pdf/2511.23386 Overall Architecture",
      "arxiv_page_url": "https://arxiv.org/abs/2511.23386",
      "pdf_url": "https://arxiv.org/pdf/2511.23386",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.23386",
      "scraped_at": "2025-12-15T01:51:31.292465"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "paper_url": "https://huggingface.co/papers/2512.10959",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
      "abstract": "Project page: https://huggingface.co/spaces/prs-eth/stereospace_web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10959",
      "pdf_url": "https://arxiv.org/pdf/2512.10959",
      "github_links": [
        "https://github.com/prs-eth/stereospace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10959",
      "scraped_at": "2025-12-15T01:51:33.104448"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "paper_url": "https://huggingface.co/papers/2512.10675",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
      "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10675",
      "pdf_url": "https://arxiv.org/pdf/2512.10675",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10675",
      "scraped_at": "2025-12-15T01:51:34.910861"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "paper_url": "https://huggingface.co/papers/2512.10955",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "abstract": "This work can isolate a specific attribute from any image and merge those selected attributes from multiple images into a coherent generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10955",
      "pdf_url": "https://arxiv.org/pdf/2512.10955",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10955",
      "scraped_at": "2025-12-15T01:51:36.679902"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "paper_url": "https://huggingface.co/papers/2512.04537",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
      "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04537",
      "pdf_url": "https://arxiv.org/pdf/2512.04537",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04537",
      "scraped_at": "2025-12-15T01:51:38.611203"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "paper_url": "https://huggingface.co/papers/2512.10791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10791",
      "pdf_url": "https://arxiv.org/pdf/2512.10791",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10791",
      "scraped_at": "2025-12-15T01:51:40.432391"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "paper_url": "https://huggingface.co/papers/2512.09270",
    "authors": [
      "Won-Sik Cheong",
      "Geonho Kim",
      "shurek20",
      "klavna",
      "sangwoonkwak"
    ],
    "stars": "6",
    "details": {
      "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer (2025) TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression (2025) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting (2025) MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting (2025) UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction (2025) StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video (2025) Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09270",
      "pdf_url": "https://arxiv.org/pdf/2512.09270",
      "github_links": [
        "https://github.com/CMLab-Korea/MoRel-arXiv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09270",
      "scraped_at": "2025-12-15T01:51:42.273910"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "paper_url": "https://huggingface.co/papers/2512.10359",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
      "abstract": "Tool-augmented VideoQA system, accepted by NeurIPS'25 main track.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10359",
      "pdf_url": "https://arxiv.org/pdf/2512.10359",
      "github_links": [
        "https://github.com/fansunqi/VideoTool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10359",
      "scraped_at": "2025-12-15T01:51:44.717904"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "paper_url": "https://huggingface.co/papers/2512.09924",
    "authors": [
      "SuaLily",
      "whluo",
      "Yanbiao",
      "LewisPan",
      "JacobYuan"
    ],
    "stars": "5",
    "details": {
      "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
      "abstract": "Code: https://github.com/Liuxinyv/ReViSE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09924",
      "pdf_url": "https://arxiv.org/pdf/2512.09924",
      "github_links": [
        "https://github.com/Liuxinyv/ReViSE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09924",
      "scraped_at": "2025-12-15T01:51:46.477889"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "paper_url": "https://huggingface.co/papers/2512.09406",
    "authors": [
      "Pei Yang",
      "Xiaokang Liu",
      "AnalMom",
      "yiren98",
      "HaiCi"
    ],
    "stars": "14",
    "details": {
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "abstract": "A framework to translate human object interaction (HOI) videos into grounded robot object interaction (ROI) videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09406",
      "pdf_url": "https://arxiv.org/pdf/2512.09406",
      "github_links": [
        "https://github.com/showlab/H2R-Grounder"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09406",
      "scraped_at": "2025-12-15T01:51:48.269947"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "paper_url": "https://huggingface.co/papers/2512.08870",
    "authors": [
      "Xiaodong Gu",
      "Yuchao Qiu",
      "Xiang Chen",
      "lanqz7766",
      "YerbaPage"
    ],
    "stars": "0",
    "details": {
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "abstract": "Check this out!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08870",
      "pdf_url": "https://arxiv.org/pdf/2512.08870",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08870",
      "scraped_at": "2025-12-15T01:51:50.042296"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "paper_url": "https://huggingface.co/papers/2512.10894",
    "authors": [
      "Jing Liao",
      "Yiran Xu",
      "Matthew Fisher",
      "Nanxuan Zhao",
      "Peiying Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
      "abstract": "We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10894",
      "pdf_url": "https://arxiv.org/pdf/2512.10894",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10894",
      "scraped_at": "2025-12-15T01:51:51.913914"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "paper_url": "https://huggingface.co/papers/2512.10398",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
      "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10398",
      "pdf_url": "https://arxiv.org/pdf/2512.10398",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10398",
      "scraped_at": "2025-12-15T01:51:53.696504"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "paper_url": "https://huggingface.co/papers/2512.09756",
    "authors": [
      "Fei Huang",
      "Ke Wang",
      "Yongbin-Li",
      "yuchuan123",
      "ChonghuaLiao"
    ],
    "stars": "0",
    "details": {
      "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
      "abstract": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09756",
      "pdf_url": "https://arxiv.org/pdf/2512.09756",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09756",
      "scraped_at": "2025-12-15T01:51:55.466731"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "DragMesh: Interactive 3D Generation Made Easy",
    "paper_url": "https://huggingface.co/papers/2512.06424",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DragMesh: Interactive 3D Generation Made Easy",
      "abstract": "DragMesh enables real time, physically valid 3D object articulation by decoupling kinematic reasoning from motion generation and producing plausible motions via a dual quaternion based generative model.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06424",
      "pdf_url": "https://arxiv.org/pdf/2512.06424",
      "github_links": [
        "https://github.com/AIGeeksGroup/DragMesh"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06424",
      "scraped_at": "2025-12-15T01:51:57.292467"
    },
    "scraped_date": "2025-12-15"
  },
  {
    "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "paper_url": "https://huggingface.co/papers/2512.08269",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08269",
      "pdf_url": "https://arxiv.org/pdf/2512.08269",
      "github_links": [
        "https://github.com/KEH0T0/EgoX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08269",
      "scraped_at": "2025-12-16T01:48:10.403542"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "paper_url": "https://huggingface.co/papers/2512.11558",
    "authors": [
      "Yanchao Li",
      "Junjie Zhao",
      "Jiaming Zhang",
      "Zhenyang Cai",
      "CocoNutZENG"
    ],
    "stars": "0",
    "details": {
      "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
      "abstract": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT , a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11558",
      "pdf_url": "https://arxiv.org/pdf/2512.11558",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11558",
      "scraped_at": "2025-12-16T01:48:12.307882"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "paper_url": "https://huggingface.co/papers/2512.11749",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11749",
      "pdf_url": "https://arxiv.org/pdf/2512.11749",
      "github_links": [
        "https://github.com/KlingTeam/SVG-T2I"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11749",
      "scraped_at": "2025-12-16T01:48:14.370200"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "paper_url": "https://huggingface.co/papers/2512.11799",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
      "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11799",
      "pdf_url": "https://arxiv.org/pdf/2512.11799",
      "github_links": [
        "https://github.com/Aleafy/V-RGBX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11799",
      "scraped_at": "2025-12-16T01:48:16.319310"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Sliding Window Attention Adaptation",
    "paper_url": "https://huggingface.co/papers/2512.10411",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Sliding Window Attention Adaptation",
      "abstract": "We propose a set of practical recipes that can let a full-attention LLM use sliding window attention to improve efficiency. For example, some can achieve nearly 100% acceleration of LLM long-context inference speed with 90% accuracy retainment; some can only achieve about 30% acceleration but with nearly 100% accuracy retainment. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10411",
      "pdf_url": "https://arxiv.org/pdf/2512.10411",
      "github_links": [
        "https://github.com/yuyijiong/sliding-window-attention-adaptation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10411",
      "scraped_at": "2025-12-16T01:48:18.230209"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
    "paper_url": "https://huggingface.co/papers/2512.11253",
    "authors": [],
    "stars": "206",
    "details": {
      "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
      "abstract": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11253",
      "pdf_url": "https://arxiv.org/pdf/2512.11253",
      "github_links": [
        "https://github.com/GVCLab/PersonaLive"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11253",
      "scraped_at": "2025-12-16T01:48:20.225617"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "paper_url": "https://huggingface.co/papers/2512.11464",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
      "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11464",
      "pdf_url": "https://arxiv.org/pdf/2512.11464",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11464",
      "scraped_at": "2025-12-16T01:48:22.165015"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.11792",
    "authors": [
      "Qifeng Chen",
      "Jingyuan Liu",
      "George Stoica",
      "Tim666",
      "sunfly"
    ],
    "stars": "0",
    "details": {
      "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
      "abstract": "We introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11792",
      "pdf_url": "https://arxiv.org/pdf/2512.11792",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11792",
      "scraped_at": "2025-12-16T01:48:24.130045"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "paper_url": "https://huggingface.co/papers/2512.06818",
    "authors": [
      "Matheus Gadelha",
      "Daniel Rebain",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Jan Held"
    ],
    "stars": "273",
    "details": {
      "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
      "abstract": "MeshSplatting introduces a differentiable rendering approach that reconstructs connected, fully opaque triangle meshes for fast, memory efficient, high quality novel view synthesis.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06818",
      "pdf_url": "https://arxiv.org/pdf/2512.06818",
      "github_links": [
        "https://github.com/meshsplatting/mesh-splatting"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06818",
      "scraped_at": "2025-12-16T01:48:26.152847"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
    "paper_url": "https://huggingface.co/papers/2512.10605",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
      "abstract": "A general-purpose robotic agent framework based on LLMs. The LLM can independently reason, plan, and execute actions to operate diverse robot types across various scenarios to complete unpredictable, complex tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10605",
      "pdf_url": "https://arxiv.org/pdf/2512.10605",
      "github_links": [
        "https://github.com/LegendLeoChen/LEO-RobotAgent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10605",
      "scraped_at": "2025-12-16T01:48:28.199459"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
    "paper_url": "https://huggingface.co/papers/2512.11150",
    "authors": [
      "elandy"
    ],
    "stars": "7",
    "details": {
      "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
      "abstract": "LLM-as-judge evals are convenient, but meaningful (fixable) failure modes lurk beneath the surface. CJE treats LLM-judge evaluation as a statistics problem: ‚Ä¢ calibrate a cheap judge to a small oracle slice of high-quality labels ‚Ä¢ quantify uncertainty ‚Ä¢ flag when the method is breaking On Chatbot Arena prompts, we match oracle-quality pairwise policy ranking (99%) while cutting oracle labeling cost by ~14√ó. If you run an eval pipeline: what are the most important failure modes you‚Äôve seen? I‚Äôd love to hear where this breaks first.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11150",
      "pdf_url": "https://arxiv.org/pdf/2512.11150",
      "github_links": [
        "https://github.com/cimo-labs/cje"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11150",
      "scraped_at": "2025-12-16T01:48:30.171508"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
    "paper_url": "https://huggingface.co/papers/2512.02901",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
      "abstract": "Is it possible to run LLMs at 2-bit with virtually NO loss in accuracy? ü§î No with Real numbers, but Yes with Complex ones! üöÄ Meet Fairy2i-W2(2bit): QAT from LLaMA-2 7B with Complex Phase quant PPL: 7.85 (vs FP16's 6.63) Accuracy: 62.00% (vs FP16's 64.72%) But isn't LLaMA real-valued? Yes, but we built a bridge. üåâ We prove a mathematical equivalence: Any real linear layer can be losslessly re-parameterized into a \"Widely-Linear Complex Form\". Which means no retraining needed! Another secret sauce: Recursive Residual Quantization. üéØ Instead of just quantize once.We also quantize the remaining error to wipe out noise. Best part? These stages are Data-Independent, so they run in PARALLEL. You get high accuracy with virtually NO latency penalty. But isn't Complex arithmetic slow?\" ü§îNot with Fairy2i.Since weights are quantized to the unit circle ${ \\pm 1, \\pm i }$, we achieve Multiplication-Free Inference.Heavy Matrix Muls turn into simple Adds, Subs, and Swaps. This efficiency is key for running LLMs on edge devices We've only scratched the surface (QAT on just 30B tokens) We believe with more training data, surpassing the full-precision model is just around the corner Resources: arXiv: https://arxiv.org/pdf/2512.02901 huggingface: https://huggingface.co/PKU-DS-LAB/Fairy2i-W2 github: https://github.com/PKULab1806/Fairy2i-W2",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02901",
      "pdf_url": "https://arxiv.org/pdf/2512.02901",
      "github_links": [
        "https://github.com/PKULab1806/Fairy2i-W2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02901",
      "scraped_at": "2025-12-16T01:48:32.083569"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
    "paper_url": "https://huggingface.co/papers/2512.11437",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
      "abstract": "First and largest multilingual trustworthiness benchmark for healthcare",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11437",
      "pdf_url": "https://arxiv.org/pdf/2512.11437",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11437",
      "scraped_at": "2025-12-16T01:48:33.973540"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
    "paper_url": "https://huggingface.co/papers/2512.06951",
    "authors": [
      "Akash Karnatak",
      "Gleb Zarin",
      "IliaLarchenko"
    ],
    "stars": "111",
    "details": {
      "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
      "abstract": "We present our 1st place solution to the 2025 NeurIPS BEHAVIOR Challenge, where a single Vision-Language-Action robotics policy is trained to perform 50 household manipulation tasks in a photorealistic simulator. The approach builds on Pi0.5 with several practical architecture, training, and inference modifications. We open-source the full solution, including code, model weights, and a detailed technical report, for anyone exploring or adapting VLAs to real tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06951",
      "pdf_url": "https://arxiv.org/pdf/2512.06951",
      "github_links": [
        "https://github.com/IliaLarchenko/behavior-1k-solution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06951",
      "scraped_at": "2025-12-16T01:48:36.073880"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
    "paper_url": "https://huggingface.co/papers/2512.11130",
    "authors": [],
    "stars": "45",
    "details": {
      "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
      "abstract": "A real-time foundation model for stereo depth estimation, which is crucial for robotics/humanoid 3D spatial perception.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11130",
      "pdf_url": "https://arxiv.org/pdf/2512.11130",
      "github_links": [
        "https://github.com/NVlabs/Fast-FoundationStereo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11130",
      "scraped_at": "2025-12-16T01:48:37.954375"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Scaling Behavior of Discrete Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.10858",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Scaling Behavior of Discrete Diffusion Language Models",
      "abstract": "We scale diffusion language models up to 3B (masked and uniform diffusion) and 10B (uniform diffusion) parameters,  pre-trained on a pure diffusion objective (mixture of unconditional and conditional) via Nemotron-CC. ü§ñ GitHub: https://github.com/dvruette/gidd-easydel ü§ó Huggingface: https://huggingface.co/collections/dvruette/scaling-behavior-of-discrete-diffusion-language-models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10858",
      "pdf_url": "https://arxiv.org/pdf/2512.10858",
      "github_links": [
        "https://github.com/dvruette/gidd-easydel"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10858",
      "scraped_at": "2025-12-16T01:48:39.888994"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
    "paper_url": "https://huggingface.co/papers/2512.10715",
    "authors": [
      "Enzo Ferrante",
      "Rodrigo Echeveste",
      "Nicolas Gaggion",
      "Matias Cosarinsky"
    ],
    "stars": "1",
    "details": {
      "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
      "abstract": "We present CheXmask-U , a framework for quantifying uncertainty in landmark-based anatomical segmentation models on chest X-rays and release the CheXmask-U dataset providing per-node uncertainty estimates to support research in robust and safe medical imaging.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10715",
      "pdf_url": "https://arxiv.org/pdf/2512.10715",
      "github_links": [
        "https://github.com/mcosarinsky/CheXmask-U"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10715",
      "scraped_at": "2025-12-16T01:48:41.791510"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
    "paper_url": "https://huggingface.co/papers/2512.11393",
    "authors": [
      "Dima Damen",
      "Yoichi Sato",
      "Yifei Huang",
      "Zhifan Zhu"
    ],
    "stars": "0",
    "details": {
      "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
      "abstract": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11393",
      "pdf_url": "https://arxiv.org/pdf/2512.11393",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11393",
      "scraped_at": "2025-12-16T01:48:43.710961"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "paper_url": "https://huggingface.co/papers/2512.10685",
    "authors": [],
    "stars": "139",
    "details": {
      "title": "Sharp Monocular View Synthesis in Less Than a Second",
      "abstract": "Sharp Monocular View Synthesis in Less Than a Second https://huggingface.co/papers/2512.10685 Real-time photorealistic view synthesis from a single image. Given a single photograph, regresses the parameters of a 3D Gaussian representation of the depicted scene. Synthesis in less than a second on a standard GPU via a single feedforward pass through a neural network. The synthesized representation is then rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Robust zero-shot generalization. SOTA on multiple datasets while lowering the synthesis time by three orders of magnitude. Learn mode at  and https://huggingface.co/apple/Sharp",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10685",
      "pdf_url": "https://arxiv.org/pdf/2512.10685",
      "github_links": [
        "https://github.com/apple/ml-sharp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10685",
      "scraped_at": "2025-12-16T01:48:45.671499"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
    "paper_url": "https://huggingface.co/papers/2512.10092",
    "authors": [
      "Neel Nanda",
      "Lewis Smith",
      "Lisa Dunlap",
      "Xiaoqing Sun",
      "Nick Jiang"
    ],
    "stars": "9",
    "details": {
      "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
      "abstract": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data. Project page: https://www.interp-embed.com Code: https://github.com/nickjiang2378/interp_embed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10092",
      "pdf_url": "https://arxiv.org/pdf/2512.10092",
      "github_links": [
        "https://github.com/nickjiang2378/interp_embed"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10092",
      "scraped_at": "2025-12-16T01:48:47.552453"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "paper_url": "https://huggingface.co/papers/2512.11798",
    "authors": [
      "Joan Lasenby",
      "Christian Rupprecht",
      "Chuanxia Zheng",
      "Yuxin Yao",
      "Ruining Li"
    ],
    "stars": "0",
    "details": {
      "title": "Particulate: Feed-Forward 3D Object Articulation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11798",
      "pdf_url": "https://arxiv.org/pdf/2512.11798",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11798",
      "scraped_at": "2025-12-16T01:48:49.433966"
    },
    "scraped_date": "2025-12-16"
  },
  {
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "paper_url": "https://huggingface.co/papers/2512.13586",
    "authors": [
      "Chongxuan Li",
      "Wei Wu",
      "Jian Guan",
      "JinaLeejnl"
    ],
    "stars": "18",
    "details": {
      "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
      "abstract": "ReFusion is a masked diffusion model that achieves superior performance and efficiency, featuring full KV cache reuse while simultaneously supporting any-order generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13586",
      "pdf_url": "https://arxiv.org/pdf/2512.13586",
      "github_links": [
        "https://github.com/ML-GSAI/ReFusion"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13586",
      "scraped_at": "2025-12-17T01:43:44.759416"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
    "paper_url": "https://huggingface.co/papers/2512.13687",
    "authors": [],
    "stars": "92",
    "details": {
      "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
      "abstract": "GitHub codes: https://github.com/MiniMax-AI/VTP Huggingface weights: https://huggingface.co/collections/MiniMaxAI/vtp collaborated with HUST Vision Lab: https://github.com/hustvl",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13687",
      "pdf_url": "https://arxiv.org/pdf/2512.13687",
      "github_links": [
        "https://github.com/hustvl",
        "https://github.com/MiniMax-AI/VTP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13687",
      "scraped_at": "2025-12-17T01:43:46.637297"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Memory in the Age of AI Agents",
    "paper_url": "https://huggingface.co/papers/2512.13564",
    "authors": [
      "Jeryi",
      "zstanjj",
      "KYLN24",
      "Liusc2020",
      "namespace-ERI"
    ],
    "stars": "115",
    "details": {
      "title": "Memory in the Age of AI Agents",
      "abstract": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13564",
      "pdf_url": "https://arxiv.org/pdf/2512.13564",
      "github_links": [
        "https://github.com/Shichun-Liu/Agent-Memory-Paper-List"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13564",
      "scraped_at": "2025-12-17T01:43:48.475578"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "paper_url": "https://huggingface.co/papers/2512.12967",
    "authors": [],
    "stars": "312",
    "details": {
      "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
      "abstract": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12967",
      "pdf_url": "https://arxiv.org/pdf/2512.12967",
      "github_links": [
        "https://github.com/Tongyi-Zhiwen/Qwen-Doc",
        "https://github.com/Tongyi-Zhiwen/Qwen-Doc/tree/main/QwenLong-L1.5"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12967",
      "scraped_at": "2025-12-17T01:43:50.394126"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "paper_url": "https://huggingface.co/papers/2512.13604",
    "authors": [
      "Xian Liu",
      "Zhaoxi Chen",
      "Jianxiong Gao",
      "ChenyangSi",
      "JunhaoZhuang"
    ],
    "stars": "0",
    "details": {
      "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
      "abstract": "Page: https://vchitect.github.io/LongVie2-project/ Github: https://github.com/Vchitect/LongVie Huggingface: https://huggingface.co/Vchitect/LongVie2",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13604",
      "pdf_url": "https://arxiv.org/pdf/2512.13604",
      "github_links": [
        "https://github.com/Vchitect/LongVie"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13604",
      "scraped_at": "2025-12-17T01:43:52.206654"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "paper_url": "https://huggingface.co/papers/2512.13168",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
      "abstract": "Real-world F&A work is messy, spanning heterogeneous and large-scale artifacts such as spreadsheets and PDFs. It's also long-horizon and knowledge-intensive: workflows interleave multiple tasks and span diverse domains such as budgeting, trading, asset management, and operations. The workflows are derived from real-world enterprise workspaces (primarily Enron, as well as corporations in the EUSES Corpus, investment and securities companies, World Bank, Canadian/British government agencies, and more).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13168",
      "pdf_url": "https://arxiv.org/pdf/2512.13168",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13168",
      "scraped_at": "2025-12-17T01:43:54.033557"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "paper_url": "https://huggingface.co/papers/2512.12730",
    "authors": [
      "yo37",
      "kkish",
      "YueHou",
      "coffiney",
      "JingzheDing"
    ],
    "stars": "25",
    "details": {
      "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
      "abstract": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents. https://github.com/multimodal-art-projection/NL2RepoBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12730",
      "pdf_url": "https://arxiv.org/pdf/2512.12730",
      "github_links": [
        "https://github.com/multimodal-art-projection/NL2RepoBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12730",
      "scraped_at": "2025-12-17T01:43:55.846055"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
    "paper_url": "https://huggingface.co/papers/2512.12602",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
      "abstract": "Error-Free Linear Attention is a Free Lunch!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12602",
      "pdf_url": "https://arxiv.org/pdf/2512.12602",
      "github_links": [
        "https://github.com/declare-lab/EFLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12602",
      "scraped_at": "2025-12-17T01:43:57.608577"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "KlingAvatar 2.0 Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.13313",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "KlingAvatar 2.0 Technical Report",
      "abstract": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13313",
      "pdf_url": "https://arxiv.org/pdf/2512.13313",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13313",
      "scraped_at": "2025-12-17T01:43:59.445920"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
    "paper_url": "https://huggingface.co/papers/2512.09636",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09636",
      "pdf_url": "https://arxiv.org/pdf/2512.09636",
      "github_links": [
        "https://github.com/elsa66666/MentraSuite"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09636",
      "scraped_at": "2025-12-17T01:44:01.296073"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
    "paper_url": "https://huggingface.co/papers/2512.10071",
    "authors": [
      "Jinwei Gu",
      "Qizhi Chen",
      "Yu-Wei Chao",
      "Junjie Bai",
      "delinqu"
    ],
    "stars": "148",
    "details": {
      "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
      "abstract": "OpenPi Comet is the submission of Team Comet for the 2025 BEHAVIOR Challenge . We provides a unified framework for pre-training, post-training, data generation and evaluation of œÄ0.5 (Pi05) models on BEHAVIOR-1K. üìÑ Arxiv: https://arxiv.org/pdf/2512.10071 ü§ó Code: https://github.com/mli0603/openpi-comet üìù Blog: https://lnkd.in/gSv2K5ua Below are 8 representative tasks that showcase some of the most interesting results from our system.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10071",
      "pdf_url": "https://arxiv.org/pdf/2512.10071",
      "github_links": [
        "https://github.com/mli0603/openpi-comet"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10071",
      "scraped_at": "2025-12-17T01:44:03.209280"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "paper_url": "https://huggingface.co/papers/2512.13080",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
      "abstract": "We propose VIPA-VLA , which learns 2D‚Äìto‚Äì3D visual‚Äìphysical grounding from human videos with Spatial-Aware VLA Pretraining, enabling robot policies with stronger spatial understanding and generalization. Website: https://beingbeyond.github.io/VIPA-VLA arXiv: https://arxiv.org/abs/2512.13080",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13080",
      "pdf_url": "https://arxiv.org/pdf/2512.13080",
      "github_links": [
        "https://github.com/BeingBeyond/VIPA-VLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13080",
      "scraped_at": "2025-12-17T01:44:05.071292"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
    "paper_url": "https://huggingface.co/papers/2512.12692",
    "authors": [
      "Md Rizwan Parvez",
      "Mohammed Eunus Ali",
      "Tanzima Hashem",
      "mahirlabibdihan"
    ],
    "stars": "9",
    "details": {
      "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
      "abstract": "We are excited to share our recent work titled \"WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment\". üìÉ Paper: https://arxiv.org/abs/2512.12692 üíª Code: https://github.com/kagnlp/WebOperator üè† Homepage: https://kagnlp.github.io/WebOperator",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12692",
      "pdf_url": "https://arxiv.org/pdf/2512.12692",
      "github_links": [
        "https://github.com/kagnlp/WebOperator"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12692",
      "scraped_at": "2025-12-17T01:44:06.846918"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "paper_url": "https://huggingface.co/papers/2512.12799",
    "authors": [
      "Zining Wang",
      "Siming Yan",
      "Rui Yang",
      "Runhui Huang",
      "happinessqq"
    ],
    "stars": "22",
    "details": {
      "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
      "abstract": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12799",
      "pdf_url": "https://arxiv.org/pdf/2512.12799",
      "github_links": [
        "https://github.com/happinesslz/DrivePI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12799",
      "scraped_at": "2025-12-17T01:44:08.645067"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "paper_url": "https://huggingface.co/papers/2512.11995",
    "authors": [
      "Kwesi Cobbina",
      "Shweta Bhardwaj",
      "Yijun Liang",
      "zhoutianyi",
      "Fcr09"
    ],
    "stars": "2",
    "details": {
      "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
      "abstract": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11995",
      "pdf_url": "https://arxiv.org/pdf/2512.11995",
      "github_links": [
        "https://github.com/tianyi-lab/VREX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11995",
      "scraped_at": "2025-12-17T01:44:10.467977"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
    "paper_url": "https://huggingface.co/papers/2512.13250",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
      "abstract": "Project page: https://active-view-selection.github.io Arxiv: https://arxiv.org/abs/2512.13250 Code: https://github.com/KAIST-Visual-AI-Group/VG-AVS",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13250",
      "pdf_url": "https://arxiv.org/pdf/2512.13250",
      "github_links": [
        "https://github.com/KAIST-Visual-AI-Group/VG-AVS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13250",
      "scraped_at": "2025-12-17T01:44:12.235666"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Image Diffusion Preview with Consistency Solver",
    "paper_url": "https://huggingface.co/papers/2512.13592",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "Image Diffusion Preview with Consistency Solver",
      "abstract": "The slow inference process of image diffusion models significantly degrades interactive user experiences. We introduce Diffusion Preview , a novel preview-and-refine paradigm that generates rapid, low-step preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. This workflow enables users to quickly iterate through different prompts or random seeds with minimal computational cost, only triggering expensive full-step sampling when a preview meets their expectations. Diffusion Preview framework: Fast preview generation followed by full-step refinement. To achieve high-quality and consistent previews, we propose ConsistencySolver , a learnable high-order ODE solver derived from Linear Multistep Methods and optimized via Reinforcement Learning. Unlike existing training-free solvers that rely on rigid numerical schemes or distillation methods that sacrifice consistency, ConsistencySolver dynamically adapts its integration strategy to maximize alignment between low-step previews and high-step reference generations, ensuring previews serve as reliable proxies for final outputs. Overview of our RL framework for optimizing a learnable ODE solver in diffusion sampling. Empirical validation demonstrates that ConsistencySolver significantly outperforms training-free ODE solvers (e.g., DDIM, UniPC, Multistep DPM), distillation-based methods (e.g., LCM, PCM, DMD2), and distillation-based solvers (e.g., AMED) across both consistency metrics and FID scores. Quantitative Results on Stable Diffusion v1-5 for Text-to-Image Generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13592",
      "pdf_url": "https://arxiv.org/pdf/2512.13592",
      "github_links": [
        "https://github.com/G-U-N/consolver"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13592",
      "scraped_at": "2025-12-17T01:44:14.044524"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
    "paper_url": "https://huggingface.co/papers/2512.11891",
    "authors": [
      "Zihan Meng",
      "Jun Cen",
      "Shuang Liu",
      "Zeyi Liu",
      "Songqiao Hu"
    ],
    "stars": "0",
    "details": {
      "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
      "abstract": "Project Page: https://vlsa-aegis.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11891",
      "pdf_url": "https://arxiv.org/pdf/2512.11891",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11891",
      "scraped_at": "2025-12-17T01:44:15.788582"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.12751",
    "authors": [
      "Chenxuan Miao",
      "Liping Hou",
      "Yuxiang Lu",
      "Zhe Liu",
      "ANIYA673"
    ],
    "stars": "0",
    "details": {
      "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12751",
      "pdf_url": "https://arxiv.org/pdf/2512.12751",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12751",
      "scraped_at": "2025-12-17T01:44:17.535516"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
    "paper_url": "https://huggingface.co/papers/2512.11883",
    "authors": [
      "Shan Du",
      "Khalad Hasan",
      "Qingyun Qian",
      "weathon"
    ],
    "stars": "0",
    "details": {
      "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
      "abstract": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11883",
      "pdf_url": "https://arxiv.org/pdf/2512.11883",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11883",
      "scraped_at": "2025-12-17T01:44:19.312596"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Towards Interactive Intelligence for Digital Humans",
    "paper_url": "https://huggingface.co/papers/2512.13674",
    "authors": [
      "Yifei Huang",
      "Sitong Gong",
      "Xiwei Gao",
      "Xuangeng Chu",
      "Yiyi Cai"
    ],
    "stars": "0",
    "details": {
      "title": "Towards Interactive Intelligence for Digital Humans",
      "abstract": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13674",
      "pdf_url": "https://arxiv.org/pdf/2512.13674",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13674",
      "scraped_at": "2025-12-17T01:44:21.116402"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "RecTok: Reconstruction Distillation along Rectified Flow",
    "paper_url": "https://huggingface.co/papers/2512.13421",
    "authors": [
      "Yujing Wang",
      "Kaidong Yu",
      "Size Wu",
      "BryanW",
      "QingyuShi"
    ],
    "stars": "6",
    "details": {
      "title": "RecTok: Reconstruction Distillation along Rectified Flow",
      "abstract": "arXiv: https://arxiv.org/abs/2512.13421 Project: https://shi-qingyu.github.io/rectok.github.io/ Code: https://github.com/Shi-qingyu/RecTok",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13421",
      "pdf_url": "https://arxiv.org/pdf/2512.13421",
      "github_links": [
        "https://github.com/Shi-qingyu/RecTok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13421",
      "scraped_at": "2025-12-17T01:44:22.970902"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
    "paper_url": "https://huggingface.co/papers/2512.13006",
    "authors": [],
    "stars": "91",
    "details": {
      "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
      "abstract": "A Systematic Study of Diffusion Distillation for Text-to-Image Synthesis towards truly applicable few steps distillation, casting existing distillation methods (sCM, MeanFlow and IMM) into a unified framework for fair comparison. Code is available at https://github.com/alibaba-damo-academy/T2I-Distill.git",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13006",
      "pdf_url": "https://arxiv.org/pdf/2512.13006",
      "github_links": [
        "https://github.com/alibaba-damo-academy/T2I-Distill.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13006",
      "scraped_at": "2025-12-17T01:44:24.814497"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.11438",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction (2025) VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory (2025) Uniform Discrete Diffusion with Metric Path for Video Generation (2025) Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context (2025) FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion (2025) Generative Neural Video Compression via Video Diffusion Prior (2025) JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11438",
      "pdf_url": "https://arxiv.org/pdf/2512.11438",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11438",
      "scraped_at": "2025-12-17T01:44:27.047805"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "paper_url": "https://huggingface.co/papers/2512.10794",
    "authors": [
      "Richard Zhang",
      "Liang Zheng",
      "Zongze Wu",
      "Xingjian Leng",
      "Jaskirat Singh"
    ],
    "stars": "80",
    "details": {
      "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10794",
      "pdf_url": "https://arxiv.org/pdf/2512.10794",
      "github_links": [
        "https://github.com/end2end-diffusion/irepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10794",
      "scraped_at": "2025-12-17T01:44:29.079121"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.10655",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
      "abstract": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10655",
      "pdf_url": "https://arxiv.org/pdf/2512.10655",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10655",
      "scraped_at": "2025-12-17T01:44:30.921512"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "paper_url": "https://huggingface.co/papers/2512.13690",
    "authors": [
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "Chongjian Ge",
      "Susung Hong"
    ],
    "stars": "0",
    "details": {
      "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
      "abstract": "Project page: https://susunghong.github.io/DiffusionBrowser",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13690",
      "pdf_url": "https://arxiv.org/pdf/2512.13690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13690",
      "scraped_at": "2025-12-17T01:44:32.692263"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "paper_url": "https://huggingface.co/papers/2512.13689",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "LitePT: Lighter Yet Stronger Point Transformer",
      "abstract": "LitePT: Lighter Yet Stronger Point Transformer LitePT is a lightweight, high-performance 3D point cloud architecture for various point cloud processing tasks. It embodies the simple principle \"convolutions for low-level geometry, attention for high-level relations\" and strategically places only the required operations at each hierarchy level, avoiding wasted computations. We equip LitePT with parameter-free PointROPE positional encoding to compensate for the loss of spatial layout information that occurs when discarding convolutional layers. Together, these integrated designs give rise to a state-of-the-art backbone for point cloud analysis. Arxiv: https://arxiv.org/abs/2512.13689 Project page: https://litept.github.io/ Code: https://github.com/prs-eth/LitePT",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13689",
      "pdf_url": "https://arxiv.org/pdf/2512.13689",
      "github_links": [
        "https://github.com/prs-eth/LitePT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13689",
      "scraped_at": "2025-12-17T01:44:34.444564"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
    "paper_url": "https://huggingface.co/papers/2512.13683",
    "authors": [
      "Aniket Bera",
      "Yichen Sheng",
      "Yunhao Ge",
      "Lu Ling"
    ],
    "stars": "0",
    "details": {
      "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13683",
      "pdf_url": "https://arxiv.org/pdf/2512.13683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13683",
      "scraped_at": "2025-12-17T01:44:36.235573"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.13672",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
      "abstract": "Hi everyone! üëã We investigated why Textual Inversion (TI) often ignores context and traced the issue to embedding norm inflation. We found that standard TI learns tokens with massive magnitudes (often >20) compared to the model's native vocabulary (‚âà0.4), which we prove theoretically breaks the representation update in pre-norm Transformers. Our solution, Directional Textual Inversion (DTI) , fixes the magnitude to an in-distribution scale and optimizes only the direction on the hypersphere using Riemannian SGD. This simple change significantly improves prompt fidelity and enables smooth spherical interpolation (slerp) between concepts. We‚Äôd love for you to try it out! Code is available here: https://github.com/kunheek/dti",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13672",
      "pdf_url": "https://arxiv.org/pdf/2512.13672",
      "github_links": [
        "https://github.com/kunheek/dti"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13672",
      "scraped_at": "2025-12-17T01:44:37.974113"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.12196",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
      "abstract": "arxiv: https://arxiv.org/abs/2512.12196v1 GitHub: https://github.com/multimodal-art-projection/AutoMV Website: https://m-a-p.ai/AutoMV/ Apache-2.0 license",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12196v1",
      "pdf_url": "https://arxiv.org/pdf/2512.12196",
      "github_links": [
        "https://github.com/multimodal-art-projection/AutoMV"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12196",
      "scraped_at": "2025-12-17T01:44:39.759871"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
    "paper_url": "https://huggingface.co/papers/2512.10927",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
      "abstract": "FoundationMotion offers a scalable way to curate detailed motion datasets, enabling effective fine-tuning of diverse models (VLM / VLA / world models) to improve motion and spatial reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10927",
      "pdf_url": "https://arxiv.org/pdf/2512.10927",
      "github_links": [
        "https://github.com/Wolfv0/FoundationMotion/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10927",
      "scraped_at": "2025-12-17T01:44:41.559612"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "START: Spatial and Textual Learning for Chart Understanding",
    "paper_url": "https://huggingface.co/papers/2512.07186",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "START: Spatial and Textual Learning for Chart Understanding",
      "abstract": "Does visual grounding help visual reasoning in Chart Understanding? üìäüß† I am excited to share our latest paper, \"START: Spatial and Textual Learning for Chart Understanding,\" which explores how we can teach Multimodal LLMs (MLLMs) to better understand complex, real-world charts. The Challenge: In real-world scenarios (like scientific papers), charts often have complex layouts with multiple subplots. Current models often fail because they jump to reasoning without first \"grounding\" (locating) the correct visual elements or understanding the underlying data. Our Solution - START: We propose a spatial and textual learning framework that trains MLLMs using two auxiliary tasks alongside Chart QA: Chart Element Grounding (Spatial): Explicitly teaching the model to locate specific components (legends, subplots), which boosts spatial reasoning. Chart-to-Code Generation (Textual): Recovering the Python code used to render the chart to understand data details. Key Contributions: START-Dataset: We developed a novel pipeline that converts real chart images (from ArXiv) into executable Python code and precise element locations, preserving real-world visual complexity. CS-Bench: A new benchmark specifically designed to evaluate chart spatial understanding. SOTA Results: Our model, START-RL-7B, outperforms previous state-of-the-art models (like Chart-R1) by a clear margin on benchmarks like CharXiv, ChartMimic, and ChartQAPro. This work has been accepted to WACV2026.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07186",
      "pdf_url": "https://arxiv.org/pdf/2512.07186",
      "github_links": [
        "https://github.com/dragonlzm/START"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07186",
      "scraped_at": "2025-12-17T01:44:43.352197"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
    "paper_url": "https://huggingface.co/papers/2512.05272",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
      "abstract": "Our method turns videos into compositional 4D scenes with explicit meshes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05272",
      "pdf_url": "https://arxiv.org/pdf/2512.05272",
      "github_links": [
        "https://github.com/insait-institute/COM4D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05272",
      "scraped_at": "2025-12-17T01:44:45.245087"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.13330",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
      "abstract": "Our paper introduces FIN-bench-v2, a unified and robust benchmark suite for evaluating large language models in Finnish, addressing the scarcity of high-quality evaluation resources for low-resource languages. This new suite modernizes the original FIN-bench, migrating it to the LM Evaluation Harness and converting all retained and new datasets into the consistent HuggingFace Datasets format for long-term maintainability. A key feature is the inclusion of both Cloze Formulation (CF) and Multiple-Choice Formulation (MCF) prompts and following the practice established in NorEval ( https://aclanthology.org/2025.findings-acl.181/ ) and HPLT 3.0 ( https://arxiv.org/abs/2511.01066 ) to create five separate variants to account for prompt sensitivity. We utilize the FineTasks selection process ( https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks ) to ensure only robust, high-signal tasks are included. üìù‚Äã Our task configurations can be found at https://github.com/LumiOpen/lm-evaluation-harness/tree/main/lm_eval/tasks/finbench_v2 .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.01066",
      "pdf_url": "https://arxiv.org/pdf/2512.13330",
      "github_links": [
        "https://github.com/LumiOpen/lm-evaluation-harness/tree/main/lm_eval/tasks/finbench_v2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13330",
      "scraped_at": "2025-12-17T01:44:47.023422"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "paper_url": "https://huggingface.co/papers/2512.12777",
    "authors": [
      "Yoav Goldberg",
      "Shauli Ravfogel",
      "Zohar Elyoseph",
      "Mosh Levy"
    ],
    "stars": "0",
    "details": {
      "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
      "abstract": "One of the most captivating features of recent chatbot models is their apparent transparency when they \"think\" out loud, generating step-by-step text before their answer. This might suggest we can trust them because we can verify their logic, but growing evidence shows this is an illusion. The text looks like a human explanation, but it functions as something fundamentally different: a computational mechanism we suggest calling State over Tokens. Mistaking this mechanical state for a transparent account of reasoning is a category error‚Äîone that risks undermining AI safety, regulation, and public trust. This paper characterizes what this \"text\" actually is, and why it doesn't do what you think it does.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12777",
      "pdf_url": "https://arxiv.org/pdf/2512.12777",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12777",
      "scraped_at": "2025-12-17T01:44:48.835998"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.12768",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
      "abstract": "A dual semantic + geometric reasoning framework with octant-based 3D tokens and multi-critic GRPO, achieving SoTA on text-to-3D, image-to-3D, and 3D captioning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12768",
      "pdf_url": "https://arxiv.org/pdf/2512.12768",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12768",
      "scraped_at": "2025-12-17T01:44:50.645470"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "paper_url": "https://huggingface.co/papers/2512.11470",
    "authors": [
      "Qi Zhu",
      "Jiyao Yuan",
      "Jiayang Lv",
      "Yuhan Chen",
      "Bowen Ding"
    ],
    "stars": "5",
    "details": {
      "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
      "abstract": "The systematic study of expert trajectory utilization in LLM post-training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11470",
      "pdf_url": "https://arxiv.org/pdf/2512.11470",
      "github_links": [
        "https://github.com/LINs-lab/RETU"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11470",
      "scraped_at": "2025-12-17T01:44:52.448484"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Learning Robot Manipulation from Audio World Models",
    "paper_url": "https://huggingface.co/papers/2512.08405",
    "authors": [
      "Michael Gienger",
      "Fanzhri"
    ],
    "stars": "0",
    "details": {
      "title": "Learning Robot Manipulation from Audio World Models",
      "abstract": "Paper page: https://arxiv.org/abs/2409.01083",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08405",
      "pdf_url": "https://arxiv.org/pdf/2512.08405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08405",
      "scraped_at": "2025-12-17T01:44:54.296182"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
    "paper_url": "https://huggingface.co/papers/2512.08400",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
      "abstract": "Link to the AutoFish dataset: https://huggingface.co/datasets/vapaau/autofish",
      "arxiv_page_url": "https://arxiv.org/abs/2512.08400",
      "pdf_url": "https://arxiv.org/pdf/2512.08400",
      "github_links": [
        "https://github.com/msamdk/Fish_Re_Identification.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.08400",
      "scraped_at": "2025-12-17T01:44:56.105225"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
    "paper_url": "https://huggingface.co/papers/2512.09069",
    "authors": [
      "Ali Nourbakhsh",
      "Nasrin Sanjari",
      "Erfan-Nourbakhsh"
    ],
    "stars": "0",
    "details": {
      "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
      "abstract": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09069",
      "pdf_url": "https://arxiv.org/pdf/2512.09069",
      "github_links": [
        "https://github.com/erfan-nourbakhsh/KD-OCT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09069",
      "scraped_at": "2025-12-17T01:44:57.863705"
    },
    "scraped_date": "2025-12-17"
  },
  {
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.14691",
    "authors": [
      "Haozhe Zhao",
      "Haoyi Qiu",
      "Zefan Cai",
      "ZGZzz",
      "SueMintony"
    ],
    "stars": "0",
    "details": {
      "title": "MMGR: Multi-Modal Generative Reasoning",
      "abstract": "MMGR proposes a principled, multi-domain benchmark for evaluating generative models' physical, logical, and spatial reasoning in video and image generation, diagnosing global consistency and causal correctness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14691",
      "pdf_url": "https://arxiv.org/pdf/2512.14691",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14691",
      "scraped_at": "2025-12-18T01:44:00.300528"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "paper_url": "https://huggingface.co/papers/2512.13281",
    "authors": [
      "Rui Zhao",
      "Yi Zhan",
      "Weijia Wu",
      "Jiaqi Wang",
      "KevinQHLin"
    ],
    "stars": "13",
    "details": {
      "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
      "abstract": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56% accuracy (random 50%), far below that of human experts (81.25%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13281",
      "pdf_url": "https://arxiv.org/pdf/2512.13281",
      "github_links": [
        "https://github.com/video-reality-test/video-reality-test"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13281",
      "scraped_at": "2025-12-18T01:44:02.511598"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.14614",
    "authors": [
      "Zehan Wang",
      "Junta Wu",
      "Haoyuan Wang",
      "Haiyu Zhang",
      "wenqsun"
    ],
    "stars": "302",
    "details": {
      "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
      "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14614",
      "pdf_url": "https://arxiv.org/pdf/2512.14614",
      "github_links": [
        "https://github.com/Tencent-Hunyuan/HY-WorldPlay"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14614",
      "scraped_at": "2025-12-18T01:44:04.445977"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
    "paper_url": "https://huggingface.co/papers/2512.12675",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
      "abstract": "Code: https://github.com/Ryann-Ran/Scone",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12675",
      "pdf_url": "https://arxiv.org/pdf/2512.12675",
      "github_links": [
        "https://github.com/Ryann-Ran/Scone"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12675",
      "scraped_at": "2025-12-18T01:44:06.318404"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "paper_url": "https://huggingface.co/papers/2512.13660",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
      "abstract": "Project Page: https://zhoues.github.io/RoboTracer/ We present RoboTracer, the first 3D-aware VLM for multi-step metric-grounded spatial tracing with explicit reasoning. Highlights: RoboTracer first acquires both 3D spatial referring and measuring via SFT, and further advances multi-step metric-grounded spatial tracing via RFT. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes, and containing complex reasoning processes (up to 9 steps). SFT-trained RoboTracer achieves SOTA spatial understanding/measuring/referring, and RFT-trained RoboTracer exhibits strong spatial tracing under novel cluttered and dynamic scenes with complex reasoning processes. Motivation: Model Framework: Dataset Construction:",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13660",
      "pdf_url": "https://arxiv.org/pdf/2512.13660",
      "github_links": [
        "https://github.com/Zhoues/RoboTracer"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13660",
      "scraped_at": "2025-12-18T01:44:08.268676"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
    "paper_url": "https://huggingface.co/papers/2512.14051",
    "authors": [
      "Xin Gao",
      "Mengzhang Cai",
      "ChampionZhong",
      "Xiaoyang318",
      "Word2Li"
    ],
    "stars": "80",
    "details": {
      "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
      "abstract": "https://opendataarena.github.io/index.html",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14051",
      "pdf_url": "https://arxiv.org/pdf/2512.14051",
      "github_links": [
        "https://github.com/OpenDataArena/OpenDataArena-Tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14051",
      "scraped_at": "2025-12-18T01:44:10.190645"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
    "paper_url": "https://huggingface.co/papers/2512.12980",
    "authors": [
      "Hua Fan",
      "Haotian Wu",
      "Jiahua Wu",
      "Cong Fu",
      "Tingyang-Chen"
    ],
    "stars": "1",
    "details": {
      "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
      "abstract": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice. We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12980",
      "pdf_url": "https://arxiv.org/pdf/2512.12980",
      "github_links": [
        "https://github.com/ZJU-DAILY/Iceberg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12980",
      "scraped_at": "2025-12-18T01:44:12.160681"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
    "paper_url": "https://huggingface.co/papers/2512.14336",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
      "abstract": "Project page: https://yeolj00.github.io/personal-projects/vector-prism/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14336",
      "pdf_url": "https://arxiv.org/pdf/2512.14336",
      "github_links": [
        "https://github.com/YeolJ00/vector-prism"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14336",
      "scraped_at": "2025-12-18T01:44:14.032624"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
    "paper_url": "https://huggingface.co/papers/2512.14699",
    "authors": [
      "Xin Tao",
      "Shuai Yang",
      "Xi Chen",
      "Sihui Ji",
      "Hengshuang"
    ],
    "stars": "0",
    "details": {
      "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
      "abstract": "MemFlow uses a retrieval-driven adaptive memory and selective attention to maintain narrative coherence in long-streaming video generation with minimal overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14699",
      "pdf_url": "https://arxiv.org/pdf/2512.14699",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14699",
      "scraped_at": "2025-12-18T01:44:15.916926"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "RecGPT-V2 Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.14503",
    "authors": [
      "Dian Chen",
      "Chao Yi",
      "zhjgao",
      "TangJiakai5704",
      "hairlatic"
    ],
    "stars": "0",
    "details": {
      "title": "RecGPT-V2 Technical Report",
      "abstract": "üåü RecGPT-V2: A Major Leap in LLM-Powered Recommendation (RecGPT-V1‚Äôs Power Upgrade!) üåü Thrilled to unveil RecGPT-V2‚Äîthe highly anticipated successor to RecGPT-V1! This agentic framework addresses V1‚Äôs core limitations, fusing cognitive reasoning with industrial scalability for next-gen intent-centric recommendations. üî• Core Innovations: Hierarchical Multi-Agent + Hybrid Representation: 60% less GPU usage, 9.39%‚Üí10.99% exclusive recall, and 32K‚Üí11K token compression (context intact). Meta-Prompting: +7.3% explanation diversity with adaptive, non-generic prompts. Constrained RL: Resolves multi-reward conflicts‚Äî+24.1% better tag prediction and +13.0% higher explanation acceptance vs. V1. Agent-as-a-Judge: Human-like multi-step evaluation, closer alignment with real-world standards. üöÄ Taobao A/B Test Results: +2.98% CTR | +3.71% IPV | +2.19% TV | +11.46% NER (Novelty Exposure Rate) Validated for large-scale deployment‚Äîbridging cognitive AI and practical utility, with room to evolve. üéØ Why It Matters: Fixes V1‚Äôs pain points (computational bloat, rigid explanations, weak generalization, oversimplified evaluation) to deliver a scalable, efficient, human-aligned paradigm. Perfect for researchers and engineers‚Äîthis is just a key milestone in refining intent-driven AI! üëâ Dive into the full technical report to unlock scalable intent-driven recommendations. Let‚Äôs shape personalized AI4Rec‚Äôs future!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14503",
      "pdf_url": "https://arxiv.org/pdf/2512.14503",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14503",
      "scraped_at": "2025-12-18T01:44:18.114477"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
    "paper_url": "https://huggingface.co/papers/2512.13303",
    "authors": [
      "Zhaohe Liao",
      "Junjie Zhou",
      "Pandeng Li",
      "Xiaoyi Bao",
      "lntzm"
    ],
    "stars": "0",
    "details": {
      "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
      "abstract": "Nano Banana Pro excels at this. We hope our methods and bench can draw more community's attention to this type of genenration ability.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13303",
      "pdf_url": "https://arxiv.org/pdf/2512.13303",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13303",
      "scraped_at": "2025-12-18T01:44:20.041763"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
    "paper_url": "https://huggingface.co/papers/2512.13678",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
      "abstract": "Very cool model that lets you edit 3D digital objects into whatever way you like, using natural language instructions! Project Home: https://glab-caltech.github.io/steer3d/ Demo: https://glab-caltech.github.io/steer3d/#demo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13678",
      "pdf_url": "https://arxiv.org/pdf/2512.13678",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13678",
      "scraped_at": "2025-12-18T01:44:21.943698"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "paper_url": "https://huggingface.co/papers/2512.13607",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
      "abstract": "The Nemotron-Cascade models and the full collection of training data are released at: https://huggingface.co/collections/nvidia/nemotron-cascade",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13607",
      "pdf_url": "https://arxiv.org/pdf/2512.13607",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13607",
      "scraped_at": "2025-12-18T01:44:23.850652"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Olmo 3",
    "paper_url": "https://huggingface.co/papers/2512.13961",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Olmo 3",
      "abstract": "We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13961",
      "pdf_url": "https://arxiv.org/pdf/2512.13961",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13961",
      "scraped_at": "2025-12-18T01:44:25.721331"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Differentiable Evolutionary Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.13399",
    "authors": [
      "Difan Zou",
      "Xunjian Yin",
      "Xuhan Huang",
      "Tianle Li",
      "sitao"
    ],
    "stars": "0",
    "details": {
      "title": "Differentiable Evolutionary Reinforcement Learning",
      "abstract": "Code: https://github.com/sitaocheng/DERL Models: https://huggingface.co/DifferentiableEvolutionaryRL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13399",
      "pdf_url": "https://arxiv.org/pdf/2512.13399",
      "github_links": [
        "https://github.com/sitaocheng/DERL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13399",
      "scraped_at": "2025-12-18T01:44:27.512913"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
    "paper_url": "https://huggingface.co/papers/2512.14531",
    "authors": [],
    "stars": "924",
    "details": {
      "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
      "abstract": "The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at this https URL.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14531",
      "pdf_url": "https://arxiv.org/pdf/2512.14531",
      "github_links": [
        "https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14531",
      "scraped_at": "2025-12-18T01:44:29.387073"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.14442",
    "authors": [
      "Hanqing Wang",
      "Kanghao Chen",
      "Chenfei-Liao",
      "Harold328",
      "zhangzixin02"
    ],
    "stars": "17",
    "details": {
      "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
      "abstract": "Project Page: https://zixinzhang02.github.io/A4-Agent-page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14442",
      "pdf_url": "https://arxiv.org/pdf/2512.14442",
      "github_links": [
        "https://github.com/EnVision-Research/A4-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14442",
      "scraped_at": "2025-12-18T01:44:31.201991"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
    "paper_url": "https://huggingface.co/papers/2512.14284",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
      "abstract": "project page: https://lizb6626.github.io/SS4D/ code: https://github.com/Lizb6626/SS4D/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14284",
      "pdf_url": "https://arxiv.org/pdf/2512.14284",
      "github_links": [
        "https://github.com/Lizb6626/SS4D/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14284",
      "scraped_at": "2025-12-18T01:44:33.080941"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2512.14008",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
      "abstract": "Efficient Training and Inference for unified multi-modal diffusion language models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14008",
      "pdf_url": "https://arxiv.org/pdf/2512.14008",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14008",
      "scraped_at": "2025-12-18T01:44:34.899723"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "paper_url": "https://huggingface.co/papers/2512.14697",
    "authors": [
      "Chutong Yang",
      "Zhenlin Xu",
      "Hanwen Jiang",
      "eadeli42",
      "zhaoyue-zephyrus"
    ],
    "stars": "2",
    "details": {
      "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
      "abstract": "Blog: https://ai.stanford.edu/~yzz/blog/articles/npq.html Code for reconstruction and compression: https://github.com/zhaoyue-zephyrus/bsq-vit Code for generation with InfinityCC: https://github.com/zhaoyue-zephyrus/InfinityCC",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14697",
      "pdf_url": "https://arxiv.org/pdf/2512.14697",
      "github_links": [
        "https://github.com/zhaoyue-zephyrus/InfinityCC",
        "https://github.com/zhaoyue-zephyrus/bsq-vit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14697",
      "scraped_at": "2025-12-18T01:44:36.703348"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
    "paper_url": "https://huggingface.co/papers/2512.14696",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
      "abstract": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2% to 6.9% on human-centric video benchmarks (EMDB, PROX), while delivering a 43% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR. Code and interactive demos are available at our project website: \\href{ https://crisp-real2sim.github.io/CRISP-Real2Sim/}{\\textcolor{cyan}{{crisp-real2sim.github.io/CRISP-Real2Sim}}} .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14696",
      "pdf_url": "https://arxiv.org/pdf/2512.14696",
      "github_links": [
        "https://github.com/Z1hanW/CRISP-Real2Sim"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14696",
      "scraped_at": "2025-12-18T01:44:38.570918"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "paper_url": "https://huggingface.co/papers/2512.14666",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
      "abstract": "EVOLVE-VLA is a test-time training framework that enables Vision-Language-Action models to continuously adapt through environment interaction with minimal or no task-specific demonstrations, overcoming the limitations of static supervised finetuning. By using a learned progress estimator with mechanisms to stabilize noisy feedback, it achieves significant performance gains, cross-task generalization, and emergent adaptive behaviors such as error recovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14666",
      "pdf_url": "https://arxiv.org/pdf/2512.14666",
      "github_links": [
        "https://github.com/showlab/EVOLVE-VLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14666",
      "scraped_at": "2025-12-18T01:44:40.486862"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
    "paper_url": "https://huggingface.co/papers/2512.14550",
    "authors": [
      "Bingzheng Wei",
      "Jian Liang",
      "Yang Yi",
      "Jiaju",
      "upyzwup"
    ],
    "stars": "29",
    "details": {
      "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14550",
      "pdf_url": "https://arxiv.org/pdf/2512.14550",
      "github_links": [
        "https://github.com/Yaziwel/TAT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14550",
      "scraped_at": "2025-12-18T01:44:42.387225"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
    "paper_url": "https://huggingface.co/papers/2512.14273",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
      "abstract": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14273",
      "pdf_url": "https://arxiv.org/pdf/2512.14273",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14273",
      "scraped_at": "2025-12-18T01:44:44.155664"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
    "paper_url": "https://huggingface.co/papers/2512.14067",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
      "abstract": "Proposes Efficient-DLM: converting autoregressive LMs to fast diffusion LMs via block-wise continuous pretraining and token masking, achieving higher accuracy and throughput than AR and existing dLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14067",
      "pdf_url": "https://arxiv.org/pdf/2512.14067",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14067",
      "scraped_at": "2025-12-18T01:44:45.941113"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
    "paper_url": "https://huggingface.co/papers/2512.13525",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
      "abstract": "Paper page: https://arxiv.org/pdf/2512.13525",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13525",
      "pdf_url": "https://arxiv.org/pdf/2512.13525",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13525",
      "scraped_at": "2025-12-18T01:44:47.917365"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "RePo: Language Models with Context Re-Positioning",
    "paper_url": "https://huggingface.co/papers/2512.14391",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RePo: Language Models with Context Re-Positioning",
      "abstract": "TL;DR: We want to give LLMs the architectural ability to reorganize input context just like humans do. Our solution is to incorporate a lightweight RePo module to dynamically assign positions before position encoding functions.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14391",
      "pdf_url": "https://arxiv.org/pdf/2512.14391",
      "github_links": [
        "https://github.com/SakanaAI/repo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14391",
      "scraped_at": "2025-12-18T01:44:49.821387"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
    "paper_url": "https://huggingface.co/papers/2512.14620",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
      "abstract": "‚ÄúBro, Benchmarks like MMMU-Pro are too expensive to build, right?‚Äù One month ago: Yes. Now: No üöÄ Proposing Vibe Benchmark Construction! NanoBanana Pro generates VQA itself, and humans only check or lightly edit prompts for regeneration. üöÄBuilding JMMMU-Pro incredibly quickly! JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, requiring integrated visual-textual understanding through visual perception. üßê Most open-source LMMs seem to perform close to random guessing on JMMMU-Pro. Let's take on the challenge! Paper: https://arxiv.org/pdf/2512.14620 Project Page: https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14620",
      "pdf_url": "https://arxiv.org/pdf/2512.14620",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14620",
      "scraped_at": "2025-12-18T01:44:51.734895"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "paper_url": "https://huggingface.co/papers/2512.14014",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
      "abstract": "A benchmark for world modeling of mobile GUI agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14014",
      "pdf_url": "https://arxiv.org/pdf/2512.14014",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14014",
      "scraped_at": "2025-12-18T01:44:53.520919"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.13655",
    "authors": [
      "richardyoung"
    ],
    "stars": "0",
    "details": {
      "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
      "abstract": "TL;DR We benchmark 4 open-source LLM abliteration implementations across 16 instruction-tuned models. Key results: ‚Ä¢ Coverage differs a lot (Heretic 16/16; DECCP 11/16; ErisForge 9/16; FailSpy 5/16).  Ôøº ‚Ä¢ Single-pass methods preserved capabilities best on the benchmarked subset (avg GSM8K ‚àÜ: DECCP ‚àí0.13 pp, ErisForge ‚àí0.28 pp; Heretic ‚àí7.81 pp avg driven by Yi).  Ôøº ‚Ä¢ Math reasoning is the most sensitive axis (GSM8K swings from +1.51 pp to ‚àí18.81 pp depending on tool/model).  Ôøº",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13655",
      "pdf_url": "https://arxiv.org/pdf/2512.13655",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13655",
      "scraped_at": "2025-12-18T01:44:55.267099"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.13106",
    "authors": [
      "Zhongqi Chen",
      "Yingfan MA",
      "Xing Zheng",
      "Guangcheng Zhu",
      "Shenzhi"
    ],
    "stars": "1",
    "details": {
      "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
      "abstract": "We‚Äôve come up with a semi-supervised RLVR training method that uses just a few labeled examples to help pick out trustworthy samples from the unlabeled ones. Feel free to jump in with thoughts or suggestions‚Äî all feedback is welcome! ü§ó",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13106",
      "pdf_url": "https://arxiv.org/pdf/2512.13106",
      "github_links": [
        "https://github.com/ShenzhiYang2000/TRAPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13106",
      "scraped_at": "2025-12-18T01:44:57.042085"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
    "paper_url": "https://huggingface.co/papers/2512.12941",
    "authors": [
      "Wenqi Ren",
      "Shengjie Li",
      "Taotao Li",
      "Dongxiu Liu",
      "Siyuan Yao"
    ],
    "stars": "0",
    "details": {
      "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
      "abstract": "UAGLNet Repository: https://github.com/Dstate/UAGLNet Paper: ‚ÄúUAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction‚Äù ( arXiv:2512.12941 )",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12941",
      "pdf_url": "https://arxiv.org/pdf/2512.12941",
      "github_links": [
        "https://github.com/Dstate/UAGLNet"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12941",
      "scraped_at": "2025-12-18T01:44:58.969163"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.14440",
    "authors": [
      "Timo Ropinski",
      "phermosilla",
      "xeTaiz",
      "lhoyer",
      "leonsick"
    ],
    "stars": "1",
    "details": {
      "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
      "abstract": "Project page: https://leonsick.github.io/s2d Code: https://github.com/leonsick/s2d",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14440",
      "pdf_url": "https://arxiv.org/pdf/2512.14440",
      "github_links": [
        "https://github.com/leonsick/s2d"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14440",
      "scraped_at": "2025-12-18T01:45:00.811365"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "paper_url": "https://huggingface.co/papers/2512.11934",
    "authors": [
      "Erfan Nourbakhsh",
      "Adeleh Mazaherian"
    ],
    "stars": "0",
    "details": {
      "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
      "abstract": "Hello everyone, I hope you enjoy reading our paper! These are the helpful links: https://arxiv.org/abs/2512.11934 https://github.com/erfan-nourbakhsh/GenAI-EdSent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11934",
      "pdf_url": "https://arxiv.org/pdf/2512.11934",
      "github_links": [
        "https://github.com/erfan-nourbakhsh/GenAI-EdSent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11934",
      "scraped_at": "2025-12-18T01:45:02.577452"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "paper_url": "https://huggingface.co/papers/2512.10952",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
      "abstract": "How do you decide which datasets to train on when data comes from many noisy, heterogeneous sources? In this work, we formalize dataset selection as its own problem and introduce DaSH (Dataset Selection via Hierarchies), a method that models dataset-level utility and group-level utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10952",
      "pdf_url": "https://arxiv.org/pdf/2512.10952",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10952",
      "scraped_at": "2025-12-18T01:45:04.337572"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.10945",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
      "abstract": "MeViSv2 Dataset, Project Page: https://henghuiding.com/MeViS/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10945",
      "pdf_url": "https://arxiv.org/pdf/2512.10945",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10945",
      "scraped_at": "2025-12-18T01:45:06.099138"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
    "paper_url": "https://huggingface.co/papers/2512.10342",
    "authors": [
      "Yogesh S Rawat",
      "Vibhav Vineet",
      "Akash Kumar",
      "Shresth Grover",
      "ppriyank"
    ],
    "stars": "0",
    "details": {
      "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
      "abstract": "LLM benchmark on sequence completion (Spoiler: They can't)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10342",
      "pdf_url": "https://arxiv.org/pdf/2512.10342",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10342",
      "scraped_at": "2025-12-18T01:45:08.978991"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.07328",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
      "abstract": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07328",
      "pdf_url": "https://arxiv.org/pdf/2512.07328",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07328",
      "scraped_at": "2025-12-18T01:45:10.828372"
    },
    "scraped_date": "2025-12-18"
  },
  {
    "title": "Step-GUI Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.15431",
    "authors": [],
    "stars": "1.44k",
    "details": {
      "title": "Step-GUI Technical Report",
      "abstract": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15431",
      "pdf_url": "https://arxiv.org/pdf/2512.15431",
      "github_links": [
        "https://github.com/stepfun-ai/gelab-zero"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15431",
      "scraped_at": "2025-12-19T01:47:15.248931"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
    "paper_url": "https://huggingface.co/papers/2512.15176",
    "authors": [
      "Zhijie Deng",
      "Jia Li",
      "Guo-Wei Yang",
      "Zicong Cheng",
      "menghao22"
    ],
    "stars": "0",
    "details": {
      "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
      "abstract": "Simultaneously leveraging the efficiency of dLLM and the performance of AR models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15176",
      "pdf_url": "https://arxiv.org/pdf/2512.15176",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15176",
      "scraped_at": "2025-12-19T01:47:17.221958"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
    "paper_url": "https://huggingface.co/papers/2512.14681",
    "authors": [
      "Tajana Rosing",
      "Samyam Rajbhandari",
      "Yichao Fu",
      "Siqi Kou",
      "Lanxiang Hu"
    ],
    "stars": "52",
    "details": {
      "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
      "abstract": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from either generation quality or limited wall-clock speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding benchmarks with minimal loss in performance. Based on Jacobi Forcing Model‚Äôs trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14681",
      "pdf_url": "https://arxiv.org/pdf/2512.14681",
      "github_links": [
        "https://github.com/hao-ai-lab/JacobiForcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14681",
      "scraped_at": "2025-12-19T01:47:19.119322"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.14944",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
      "abstract": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14944",
      "pdf_url": "https://arxiv.org/pdf/2512.14944",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14944",
      "scraped_at": "2025-12-19T01:47:20.987926"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
    "paper_url": "https://huggingface.co/papers/2512.14052",
    "authors": [
      "Yuhang Dong",
      "Zhiqiang Xia",
      "Kaiyang Han",
      "Yuchen Liu",
      "HyperAI Team"
    ],
    "stars": "0",
    "details": {
      "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
      "abstract": "üöÄ [New Paper] HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices Current multimodal large language models (MLLMs) possess strong perceptual and reasoning capabilities, but their high computational and memory requirements make them difficult to deploy directly on edge devices. HyperVL aims to tackle this challenge by introducing an efficient multimodal large language model tailored for on-device inference. ‚ú® The Core Intuition: HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: 1Ô∏è‚É£ Visual Resolution Compressor (VRC): Adaptively predicts optimal encoding resolutions to eliminate redundant computation. 2Ô∏è‚É£ Dual Consistency Learning (DCL): Aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. üìà Highlights: State-of-the-Art Performance: HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Resource Efficient: It significantly reduces latency and power consumption on real mobile devices, demonstrating a 6.8x reduction in peak memory overhead. Quantization Robustness: The model demonstrates exceptional robustness to low-bit precision under W4A16 quantization with negligible performance drops. Broad Applications: HyperVL shows strong generalization for on-device tasks such as UI understanding and parsing, intent recommendation, and image-text creation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14052",
      "pdf_url": "https://arxiv.org/pdf/2512.14052",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14052",
      "scraped_at": "2025-12-19T01:47:22.930324"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Universal Reasoning Model",
    "paper_url": "https://huggingface.co/papers/2512.14693",
    "authors": [],
    "stars": "23",
    "details": {
      "title": "Universal Reasoning Model",
      "abstract": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art‚àó 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14693",
      "pdf_url": "https://arxiv.org/pdf/2512.14693",
      "github_links": [
        "https://github.com/zitian-gao/URM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14693",
      "scraped_at": "2025-12-19T01:47:24.889066"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
    "paper_url": "https://huggingface.co/papers/2512.15635",
    "authors": [],
    "stars": "25",
    "details": {
      "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15635",
      "pdf_url": "https://arxiv.org/pdf/2512.15635",
      "github_links": [
        "https://github.com/CUC-MIPG/IC-Effect"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15635",
      "scraped_at": "2025-12-19T01:47:26.801519"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.15693",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
      "abstract": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning https://huggingface.co/papers/2512.15693 Explainable AI-generated video detection with a specialized multimodal LLM. Given an input video, Skyra explicitly identifies human-perceivable spatio-temporal artifacts (e.g., texture/structure inconsistencies, motion irregularities) and uses them as grounded evidence to produce both a real/fake decision and a human-interpretable explanation with localized cues. To train this capability, we introduce ViF-CoT-4K, the first large-scale AI-generated video artifact dataset with fine-grained human annotations, enabling supervised fine-tuning (Skyra-SFT). We further apply a second-stage reinforcement learning procedure to encourage the model to actively mine discriminative artifacts, improving both detection and explanation quality (Skyra-RL). For rigorous evaluation, we release ViF-Bench (3K high-quality samples from 10+ state-of-the-art video generators) with aligned real/fake semantics and formats to reduce shortcut signals, and demonstrate consistent gains over prior binary detectors and MLLM-based baselines. Learn more at https://joeleelyf.github.io/Skyra and https://github.com/JoeLeelyf/Skyra .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15693",
      "pdf_url": "https://arxiv.org/pdf/2512.15693",
      "github_links": [
        "https://github.com/JoeLeelyf/Skyra"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15693",
      "scraped_at": "2025-12-19T01:47:28.689499"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
    "paper_url": "https://huggingface.co/papers/2512.15603",
    "authors": [
      "Xiao Xu",
      "Kaiyuan Gao",
      "Zecheng Tang",
      "Zekai Zhang",
      "Shengming Yin"
    ],
    "stars": "0",
    "details": {
      "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
      "abstract": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15603",
      "pdf_url": "https://arxiv.org/pdf/2512.15603",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15603",
      "scraped_at": "2025-12-19T01:47:30.629216"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
    "paper_url": "https://huggingface.co/papers/2512.15182",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
      "abstract": "The paper ‚ÄúRobust and Calibrated Detection of Authentic Multimedia Content‚Äù presents a new framework for identifying whether multimedia particularly deepfakes produced by generative models is genuinely authentic or can be plausibly denied as fake, addressing key shortcomings of current detection methods which suffer from unbounded false positive rates and are easily defeated by adaptive attackers; by introducing a calibrated resynthesis approach that focuses on high precision and adversarial robustness under realistic (compute-limited) threat models, the authors demonstrate that their method reliably verifies authentic samples with controllable false positive rates while resisting evasion by efficient adversaries, supports multiple modalities, and leverages cutting-edge inversion techniques to improve robustness and calibration compared to prior work.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15182",
      "pdf_url": "https://arxiv.org/pdf/2512.15182",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15182",
      "scraped_at": "2025-12-19T01:47:32.576517"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.13874",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
      "abstract": "üìú explainer thread: https://x.com/allen_ai/status/2001351082916630586 üîó Project page: https://lnkd.in/eff-DjHx üíª Code: github.com/allenai/SAGE üì¶ Models & data: https://lnkd.in/eT9iVVRk üìù Paper: arxiv.org/abs/2512.13874",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13874",
      "pdf_url": "https://arxiv.org/pdf/2512.13874",
      "github_links": [
        "https://github.com/allenai/SAGE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13874",
      "scraped_at": "2025-12-19T01:47:34.472068"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.15687",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
      "abstract": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15687",
      "pdf_url": "https://arxiv.org/pdf/2512.15687",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15687",
      "scraped_at": "2025-12-19T01:47:36.334475"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
    "paper_url": "https://huggingface.co/papers/2512.13884",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
      "abstract": "GitHub Repo: https://github.com/whoisjones/FiNERweb HF Collection: https://huggingface.co/collections/whoisjones/finerweb",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13884",
      "pdf_url": "https://arxiv.org/pdf/2512.13884",
      "github_links": [
        "https://github.com/whoisjones/FiNERweb"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13884",
      "scraped_at": "2025-12-19T01:47:38.197928"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.10863",
    "authors": [
      "Peizhou Cao",
      "Sihan Yang",
      "Shaohao Zhu",
      "Runsen Xu",
      "rbler"
    ],
    "stars": "0",
    "details": {
      "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
      "abstract": "Our homepage: https://rbler1234.github.io/MMSI-VIdeo-Bench.github.io GitHub Page: https://github.com/InternRobotics/MMSI-Video-Bench HuggingFace: https://huggingface.co/datasets/rbler/MMSI-Video-Bench Arxiv: https://arxiv.org/abs/2512.10863",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10863",
      "pdf_url": "https://arxiv.org/pdf/2512.10863",
      "github_links": [
        "https://github.com/InternRobotics/MMSI-Video-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10863",
      "scraped_at": "2025-12-19T01:47:40.150604"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "paper_url": "https://huggingface.co/papers/2512.15713",
    "authors": [],
    "stars": "30",
    "details": {
      "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
      "abstract": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15713",
      "pdf_url": "https://arxiv.org/pdf/2512.15713",
      "github_links": [
        "https://github.com/hustvl/DiffusionVL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15713",
      "scraped_at": "2025-12-19T01:47:42.162776"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
    "paper_url": "https://huggingface.co/papers/2512.12072",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
      "abstract": "Diverse data is ALL you NEED",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12072",
      "pdf_url": "https://arxiv.org/pdf/2512.12072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12072",
      "scraped_at": "2025-12-19T01:47:44.037410"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "paper_url": "https://huggingface.co/papers/2512.15702",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
      "abstract": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15702",
      "pdf_url": "https://arxiv.org/pdf/2512.15702",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15702",
      "scraped_at": "2025-12-19T01:47:45.898986"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.09299",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
      "abstract": "code link: https://github.com/tanABCC/VABench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09299",
      "pdf_url": "https://arxiv.org/pdf/2512.09299",
      "github_links": [
        "https://github.com/tanABCC/VABench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09299",
      "scraped_at": "2025-12-19T01:47:47.763167"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "paper_url": "https://huggingface.co/papers/2512.15715",
    "authors": [
      "Dong Wang",
      "Xinjie Lei",
      "Yang Li",
      "Shang-Wen Li",
      "Lihe Yang"
    ],
    "stars": "50",
    "details": {
      "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
      "abstract": "arXiv lens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/in-pursuit-of-pixel-supervision-for-visual-pre-training-8810-5e30657e Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15715",
      "pdf_url": "https://arxiv.org/pdf/2512.15715",
      "github_links": [
        "https://github.com/facebookresearch/pixio"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15715",
      "scraped_at": "2025-12-19T01:47:49.666981"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
    "paper_url": "https://huggingface.co/papers/2512.15649",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
      "abstract": "A comprehensive benchmark to study VLM's visual text compression ability. Code: https://github.com/Moenupa/VTCBench Huggingface: https://huggingface.co/datasets/MLLM-CL/VTCBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15649",
      "pdf_url": "https://arxiv.org/pdf/2512.15649",
      "github_links": [
        "https://github.com/Moenupa/VTCBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15649",
      "scraped_at": "2025-12-19T01:47:51.545725"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
    "paper_url": "https://huggingface.co/papers/2512.15110",
    "authors": [
      "Yicheng Zhang",
      "Jiaxin Zhu",
      "Hanyu Zhou",
      "Haoyou Deng",
      "Jialong Zuo"
    ],
    "stars": "0",
    "details": {
      "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
      "abstract": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \\textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15110",
      "pdf_url": "https://arxiv.org/pdf/2512.15110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15110",
      "scraped_at": "2025-12-19T01:47:53.384221"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
    "paper_url": "https://huggingface.co/papers/2512.13190",
    "authors": [
      "Sung Won Han",
      "Dongil Park",
      "Wooseok Shin",
      "Hyun Joon Park",
      "sadPororo"
    ],
    "stars": "3",
    "details": {
      "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
      "abstract": "A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13190",
      "pdf_url": "https://arxiv.org/pdf/2512.13190",
      "github_links": [
        "https://github.com/sadPororo/WAY"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13190",
      "scraped_at": "2025-12-19T01:47:55.228395"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.15699",
    "authors": [
      "Shang Zhou",
      "Huanzhi Mao",
      "Zhifei Li",
      "Wenhao Chai",
      "Qiuyang Mang"
    ],
    "stars": "0",
    "details": {
      "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
      "abstract": "https://github.com/FrontierCS/Frontier-CS Introducing FrontierCS. LiveCodeBench Pro is already a challenging competitive programming benchmark, so why do we still need to push one step further? The motivation behind FrontierCS is actually pretty simple: we love measuring intelligence with problems that have a \"single\", \"correct\",  \"optimal\" answer, but what really matters at the frontier in practice is often open-ended problems where the optimum is unknown, yet every step can be objectively scored and verified. In our experiments, we kept running into a sobering pattern: simply scaling up reasoning compute doesn‚Äôt close the gap. Models often settle for a locally feasible \"it runs\" solution, then stall on algorithmic and system choices that are still clearly bad. We still have a long way to go. Let‚Äôs build Evolving Challenges for Evolving Intelligence!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15699",
      "pdf_url": "https://arxiv.org/pdf/2512.15699",
      "github_links": [
        "https://github.com/FrontierCS/Frontier-CS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15699",
      "scraped_at": "2025-12-19T01:47:57.040151"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
    "paper_url": "https://huggingface.co/papers/2512.15374",
    "authors": [
      "Yunhe Wang",
      "Sinno Jialin Pan",
      "Shixiong Kai",
      "Hui-Ling Zhen",
      "Zehua Pei"
    ],
    "stars": "4",
    "details": {
      "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
      "abstract": "We introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution), a framework that automatically evolves agent prompts by learning from execution traces. Try it now: pip install scope-optimizer üìÑ Paper: https://arxiv.org/abs/2512.15374 üíª Code: https://github.com/JarvisPei/SCOPE",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15374",
      "pdf_url": "https://arxiv.org/pdf/2512.15374",
      "github_links": [
        "https://github.com/JarvisPei/SCOPE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15374",
      "scraped_at": "2025-12-19T01:47:58.898057"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.14202",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
      "abstract": "tl;dr : We analytically show that large-norm embeddings destabilize hyperbolic representations in deep RL. In PPO, this coincides with trust-region violations. Existing methods based on SpectralNorm mitigate these issues only partially. We propose a theoretically principled combination of stabilization techniques, Hyper++. Hyper++ substantially outperforms existing hyperbolic agents on ProcGen (PPO) and Atari (DDQN). Because we do not have the power iteration overhead from SpectralNorm, Hyper++ is also faster. Happy to answer any questions :)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14202",
      "pdf_url": "https://arxiv.org/pdf/2512.14202",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14202",
      "scraped_at": "2025-12-19T01:48:00.784957"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
    "paper_url": "https://huggingface.co/papers/2512.14719",
    "authors": [
      "Yuanxing Zhang",
      "Shangyuan Li",
      "Feng Zhang",
      "Zhuoran Zhang",
      "DogNeverSleep"
    ],
    "stars": "0",
    "details": {
      "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
      "abstract": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14719",
      "pdf_url": "https://arxiv.org/pdf/2512.14719",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14719",
      "scraped_at": "2025-12-19T01:48:02.608177"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
    "paper_url": "https://huggingface.co/papers/2512.13077",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
      "abstract": "Memory ‚â† likability. LikeBench shows that models can remember more but still feel worse to talk to, and even SOTA models struggle to become likable over time despite having more information about a user.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13077",
      "pdf_url": "https://arxiv.org/pdf/2512.13077",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13077",
      "scraped_at": "2025-12-19T01:48:04.499671"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
    "paper_url": "https://huggingface.co/papers/2512.09851",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
      "abstract": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
      "arxiv_page_url": "https://arxiv.org/abs/2512.09851",
      "pdf_url": "https://arxiv.org/pdf/2512.09851",
      "github_links": [
        "https://github.com/YuyangLee/TacThru"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.09851",
      "scraped_at": "2025-12-19T01:48:06.352295"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
    "paper_url": "https://huggingface.co/papers/2512.15340",
    "authors": [
      "Kun Li",
      "Qing Zhou",
      "Zhihao Huang",
      "Fei Wang",
      "Junjie Chen"
    ],
    "stars": "6",
    "details": {
      "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
      "abstract": "Human conversation is a continuous exchange of speech and nonverbal cues‚Äîincluding head nods, gaze shifts, and subtle expressions. Most existing approaches, however, treat talking-head and listening-head generation as separate problems, or rely on non-causal full-sequence modeling that is unsuitable for real-time interaction. We propose a causal, turn-level framework for interactive 3D conversational head generation. Our method models dialogue as a sequence of causally linked turns, where each turn accumulates multimodal context from both participants to produce coherent, responsive, and humanlike 3D head dynamics.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15340",
      "pdf_url": "https://arxiv.org/pdf/2512.15340",
      "github_links": [
        "https://github.com/CoderChen01/towards-seamleass-interaction/blob/main/README.md",
        "https://github.com/CoderChen01/towards-seamleass-interaction"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15340",
      "scraped_at": "2025-12-19T01:48:08.178873"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
    "paper_url": "https://huggingface.co/papers/2512.14080",
    "authors": [
      "Tri Dao",
      "Ion Stoica",
      "Xinle Cheng",
      "Mayank Mishra",
      "Wentao Guo"
    ],
    "stars": "0",
    "details": {
      "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
      "abstract": "We propose to co-design the MoE architecture with a GPU kernel tailored to NVIDIA Blackwell and Hopper generation GPUs and a novel routing method. (1) We derive an algorithm to compute the MoE backward pass more efficiently leading to a much smaller activation memory footprint that does not increase with increasing expert granularity. (2) We leverage new hardware features on Blackwell and Hopper GPUs to overlap memory IO with computation which can benefit all MoEs, and, in particular, fine-grained MoEs. (3) We propose a hardware-aware token rounding routing method where the routed number of tokens to an expert is always a multiple of the GEMM tile size. looks amazing!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14080",
      "pdf_url": "https://arxiv.org/pdf/2512.14080",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14080",
      "scraped_at": "2025-12-19T01:48:10.046129"
    },
    "scraped_date": "2025-12-19"
  },
  {
    "title": "Kling-Omni Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kling-Omni Technical Report",
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16776",
      "pdf_url": "https://arxiv.org/pdf/2512.16776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16776",
      "scraped_at": "2025-12-20T01:41:55.235420"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Adaptation of Agentic AI",
    "paper_url": "https://huggingface.co/papers/2512.16301",
    "authors": [
      "XueqiangXu",
      "p-song1",
      "Gabshi",
      "linjc16",
      "pat-jj"
    ],
    "stars": "262",
    "details": {
      "title": "Adaptation of Agentic AI",
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16301",
      "pdf_url": "https://arxiv.org/pdf/2512.16301",
      "github_links": [
        "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16301",
      "scraped_at": "2025-12-20T01:41:57.144311"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "paper_url": "https://huggingface.co/papers/2512.15745",
    "authors": [],
    "stars": "159",
    "details": {
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15745",
      "pdf_url": "https://arxiv.org/pdf/2512.15745",
      "github_links": [
        "https://github.com/inclusionAI/LLaDA2.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15745",
      "scraped_at": "2025-12-20T01:41:59.124430"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "paper_url": "https://huggingface.co/papers/2512.16922",
    "authors": [],
    "stars": "43",
    "details": {
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "abstract": "Make SSL great again.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16922",
      "pdf_url": "https://arxiv.org/pdf/2512.16922",
      "github_links": [
        "https://github.com/SihanXU/nepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16922",
      "scraped_at": "2025-12-20T01:42:01.031451"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "paper_url": "https://huggingface.co/papers/2512.16915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "abstract": "StereoPilot replaces the fragile \"Depth-Warp-Inpaint\" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo dataset with a learnable domain switcher, it provides a unified and efficient solution for both parallel and converged 3D video formats.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16915",
      "pdf_url": "https://arxiv.org/pdf/2512.16915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16915",
      "scraped_at": "2025-12-20T01:42:02.925012"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "paper_url": "https://huggingface.co/papers/2512.13507",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
      "abstract": "Seedance 1.5 pro Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13507",
      "pdf_url": "https://arxiv.org/pdf/2512.13507",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13507",
      "scraped_at": "2025-12-20T01:42:04.842028"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "paper_url": "https://huggingface.co/papers/2512.16913",
    "authors": [
      "Wenxuan Lu",
      "Dizhe Zhang",
      "Meixi Song",
      "Xin Lin",
      "haodongli"
    ],
    "stars": "0",
    "details": {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP website/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16913",
      "pdf_url": "https://arxiv.org/pdf/2512.16913",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16913",
      "scraped_at": "2025-12-20T01:42:06.700701"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.16923",
    "authors": [
      "Yu-Lun Liu",
      "Jia-Bin Huang",
      "rayray9999"
    ],
    "stars": "27",
    "details": {
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16923",
      "pdf_url": "https://arxiv.org/pdf/2512.16923",
      "github_links": [
        "https://github.com/rayray9999/Genfocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16923",
      "scraped_at": "2025-12-20T01:42:08.570320"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16625",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
      "abstract": "‚ú® Image editing is awesome; but it can leak user information! üõ°Ô∏è Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. üìÑ Paper: https://arxiv.org/abs/2512.16625 üíª Code: https://github.com/LinghuiiShen/DeContext üåê Project Page: https://linghuiishen.github.io/decontext_project_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16625",
      "pdf_url": "https://arxiv.org/pdf/2512.16625",
      "github_links": [
        "https://github.com/LinghuiiShen/DeContext"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16625",
      "scraped_at": "2025-12-20T01:42:10.471123"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.16636",
    "authors": [
      "Giorgos Sfikas",
      "Theodoros Giannakopoulos",
      "Bill Psomas",
      "Christos Sgouropoulos",
      "Giorgos Petsangourakis"
    ],
    "stars": "0",
    "details": {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16636",
      "pdf_url": "https://arxiv.org/pdf/2512.16636",
      "github_links": [
        "https://github.com/giorgospets/reglue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16636",
      "scraped_at": "2025-12-20T01:42:12.346186"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "paper_url": "https://huggingface.co/papers/2512.16905",
    "authors": [
      "Jiarong Ou",
      "Miao Yang",
      "Xi Chen",
      "Yang Zhou",
      "Kaixin Ding"
    ],
    "stars": "0",
    "details": {
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "abstract": "data selection",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16905",
      "pdf_url": "https://arxiv.org/pdf/2512.16905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16905",
      "scraped_at": "2025-12-20T01:42:14.238040"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "paper_url": "https://huggingface.co/papers/2512.16924",
    "authors": [],
    "stars": "54",
    "details": {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "abstract": "Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16924",
      "pdf_url": "https://arxiv.org/pdf/2512.16924",
      "github_links": [
        "https://github.com/pPetrichor/WorldCanvas"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16924",
      "scraped_at": "2025-12-20T01:42:16.096041"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.16561",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "abstract": "Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16561",
      "pdf_url": "https://arxiv.org/pdf/2512.16561",
      "github_links": [
        "https://github.com/W-Ted/N3D-VLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16561",
      "scraped_at": "2025-12-20T01:42:17.998665"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "paper_url": "https://huggingface.co/papers/2512.16649",
    "authors": [],
    "stars": "70",
    "details": {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "abstract": "‚ú®What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2√ó less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16649",
      "pdf_url": "https://arxiv.org/pdf/2512.16649",
      "github_links": [
        "https://github.com/thunlp/JustRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16649",
      "scraped_at": "2025-12-20T01:42:19.922832"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "paper_url": "https://huggingface.co/papers/2512.16918",
    "authors": [
      "Zhixun Li",
      "Zhongyu Wang",
      "Dongyang Chen",
      "Kaituo Feng",
      "Chaoyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "abstract": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16918",
      "pdf_url": "https://arxiv.org/pdf/2512.16918",
      "github_links": [
        "https://github.com/CYWang735/AdaTooler-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16918",
      "scraped_at": "2025-12-20T01:42:21.777535"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "paper_url": "https://huggingface.co/papers/2512.16899",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "abstract": "Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta Superintelligence Labs released their reward benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16899",
      "pdf_url": "https://arxiv.org/pdf/2512.16899",
      "github_links": [
        "https://github.com/facebookresearch/MMRB2/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16899",
      "scraped_at": "2025-12-20T01:42:23.598937"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "paper_url": "https://huggingface.co/papers/2512.16920",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16920",
      "pdf_url": "https://arxiv.org/pdf/2512.16920",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16920",
      "scraped_at": "2025-12-20T01:42:25.490781"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "paper_url": "https://huggingface.co/papers/2512.16912",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16912",
      "pdf_url": "https://arxiv.org/pdf/2512.16912",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16912",
      "scraped_at": "2025-12-20T01:42:27.312082"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "paper_url": "https://huggingface.co/papers/2512.16900",
    "authors": [],
    "stars": "51",
    "details": {
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16900",
      "pdf_url": "https://arxiv.org/pdf/2512.16900",
      "github_links": [
        "https://github.com/Francis-Rings/FlashPortrait"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16900",
      "scraped_at": "2025-12-20T01:42:29.189155"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "paper_url": "https://huggingface.co/papers/2512.16864",
    "authors": [
      "Yuqi Liu",
      "Longxiang Tang",
      "Xiaohang Zhan",
      "Lei Ke",
      "TainU"
    ],
    "stars": "8",
    "details": {
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "abstract": "üöß The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). üìâ The Gap: Global Semantic Guidance Existing methods, including unified VLM-diffusion architectures, predominantly rely on Global Semantic Guidance . They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. üöÄ Our Solution: Region-Aligned Guidance RePlan introduces a Plan-then-Execute framework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performs Chain-of-Thought (CoT) reasoning to decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enables precise, multi-region parallel edits in a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities using Group Relative Policy Optimization (GRPO) . Remarkably, we achieve strong planning performance using only ~1k instruction-only samples , bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance is fully editable , enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supports regional negative prompts to prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establish IV-Edit , the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16864",
      "pdf_url": "https://arxiv.org/pdf/2512.16864",
      "github_links": [
        "https://github.com/dvlab-research/RePlan"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16864",
      "scraped_at": "2025-12-20T01:42:31.186498"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "paper_url": "https://huggingface.co/papers/2512.16501",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16501",
      "pdf_url": "https://arxiv.org/pdf/2512.16501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16501",
      "scraped_at": "2025-12-20T01:42:33.122365"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "paper_url": "https://huggingface.co/papers/2512.16106",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "ModelTables: A Corpus of Tables about Models",
      "abstract": "ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and configuration tables. The benchmark defines model and table relatedness using multiple signals, including paper citations, model-card links and inheritance, and shared training datasets, and supports downstream applications such as table discovery and semantic retrieval from both structural and semantic perspectives.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16106",
      "pdf_url": "https://arxiv.org/pdf/2512.16106",
      "github_links": [
        "https://github.com/RJMillerLab/ModelTables"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16106",
      "scraped_at": "2025-12-20T01:42:35.008094"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_url": "https://huggingface.co/papers/2512.16378",
    "authors": [
      "Carlos Escolano",
      "Vil√©m Zouhar",
      "zhopto3",
      "javi8979",
      "spapi"
    ],
    "stars": "13",
    "details": {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "abstract": "Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challenging conditions (noise, accents, disfluencies, long-form). Results show that cascades remain the most reliable overall, while SpeechLLMs close the gap in specific settings (notably noise and code-switching).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16378",
      "pdf_url": "https://arxiv.org/pdf/2512.16378",
      "github_links": [
        "https://github.com/sarapapi/hearing2translate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16378",
      "scraped_at": "2025-12-20T01:42:36.921178"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "paper_url": "https://huggingface.co/papers/2512.16921",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. Project Page: https://auditdm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16921",
      "pdf_url": "https://arxiv.org/pdf/2512.16921",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16921",
      "scraped_at": "2025-12-20T01:42:38.734491"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "paper_url": "https://huggingface.co/papers/2512.11251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
      "abstract": "Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptions.  Inspired by LLaVA, we showed instruction-tuning on TS-Insights can enable LLMs to understand time series as a native input modality and generate textual descriptions. This work was originally done in Summer 2023.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11251",
      "pdf_url": "https://arxiv.org/pdf/2512.11251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11251",
      "scraped_at": "2025-12-20T01:42:40.587194"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "paper_url": "https://huggingface.co/papers/2512.16767",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16767",
      "pdf_url": "https://arxiv.org/pdf/2512.16767",
      "github_links": [
        "https://github.com/jasongzy/Make-It-Poseable"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16767",
      "scraped_at": "2025-12-20T01:42:42.388264"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "paper_url": "https://huggingface.co/papers/2512.16670",
    "authors": [
      "Hendrik P. A. Lensch",
      "Ole Beisswenger",
      "JDihlmann"
    ],
    "stars": "0",
    "details": {
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "abstract": "FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination from a stream of G-buffer data. We overfit SD on single scenes and show that it learns the illumination setting for the scene and can transfer it to OOD views of the scene.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16670",
      "pdf_url": "https://arxiv.org/pdf/2512.16670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16670",
      "scraped_at": "2025-12-20T01:42:44.229155"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16615",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
      "abstract": "Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16615",
      "pdf_url": "https://arxiv.org/pdf/2512.16615",
      "github_links": [
        "https://github.com/SingleZombie/LLSA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16615",
      "scraped_at": "2025-12-20T01:42:46.106366"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.12576",
    "authors": [
      "Ben He",
      "Hongyu Lin",
      "Yanjiang Liu",
      "Jie Lou",
      "Aunderline"
    ],
    "stars": "0",
    "details": {
      "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
      "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12576",
      "pdf_url": "https://arxiv.org/pdf/2512.12576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12576",
      "scraped_at": "2025-12-20T01:42:48.019489"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "paper_url": "https://huggingface.co/papers/2512.16909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "abstract": "Project Page: https://hybridrobotics.github.io/MomaGraph/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16909",
      "pdf_url": "https://arxiv.org/pdf/2512.16909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16909",
      "scraped_at": "2025-12-20T01:42:49.825834"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.15907",
    "authors": [
      "Vivek Gupta",
      "Aparna Garimella",
      "Juhna Park",
      "Tejas Anvekar"
    ],
    "stars": "1",
    "details": {
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15907",
      "pdf_url": "https://arxiv.org/pdf/2512.15907",
      "github_links": [
        "https://github.com/CoRAL-ASU/TabReX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15907",
      "scraped_at": "2025-12-20T01:42:51.619129"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "paper_url": "https://huggingface.co/papers/2512.14884",
    "authors": [
      "Yutong Bai",
      "Michael D. Grossberg",
      "Andrew Lu",
      "Katherine Xu",
      "Huzheng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
      "abstract": "what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14884",
      "pdf_url": "https://arxiv.org/pdf/2512.14884",
      "github_links": [
        "https://github.com/huzeyann/VibeSpace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14884",
      "scraped_at": "2025-12-20T01:42:53.427990"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "paper_url": "https://huggingface.co/papers/2512.10953",
    "authors": [
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Qiao Sun",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "abstract": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10953",
      "pdf_url": "https://arxiv.org/pdf/2512.10953",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10953",
      "scraped_at": "2025-12-20T01:42:55.227843"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "paper_url": "https://huggingface.co/papers/2512.15528",
    "authors": [
      "Can Ma. Yu Zhou",
      "Dongbao Yang",
      "Daiqing Wu"
    ],
    "stars": "1",
    "details": {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "abstract": "Update the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15528",
      "pdf_url": "https://arxiv.org/pdf/2512.15528",
      "github_links": [
        "https://github.com/wdqqdw/EmoCaliber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15528",
      "scraped_at": "2025-12-20T01:42:57.027138"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "paper_url": "https://huggingface.co/papers/2512.15489",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "abstract": "Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization‚Äîespecially on HLE-Math‚Äîwhile maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15489",
      "pdf_url": "https://arxiv.org/pdf/2512.15489",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15489",
      "scraped_at": "2025-12-20T01:42:58.892206"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "paper_url": "https://huggingface.co/papers/2512.14805",
    "authors": [
      "Michael Carbin",
      "Tian Jin",
      "Logan Weber",
      "ellieyhc"
    ],
    "stars": "3",
    "details": {
      "title": "Sharing State Between Prompts and Programs",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14805",
      "pdf_url": "https://arxiv.org/pdf/2512.14805",
      "github_links": [
        "https://github.com/psg-mit/nightjarpy/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14805",
      "scraped_at": "2025-12-20T01:43:00.877943"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "paper_url": "https://huggingface.co/papers/2512.12880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Improving Recursive Transformers with Mixture of LoRAs",
      "abstract": "Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected shared FFNs with a small set of token-routed LoRA experts (sparse routing), allowing conditional computation while keeping the backbone compact. We pretrain ModernALBERT (50M to 120M) with RoPE, GeGLU, FlashAttention, and distillation-based initialisation, and report state-of-the-art results among compact models on GLUE, SQuAD-v2, and BEIR, often surpassing larger fully parameterised baselines. For deployment, we introduce expert merging (including an EMA-based strategy) that compresses MoL into a single adapter at inference, removing routing overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12880",
      "pdf_url": "https://arxiv.org/pdf/2512.12880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12880",
      "scraped_at": "2025-12-20T01:43:02.725068"
    },
    "scraped_date": "2025-12-20"
  },
  {
    "title": "Kling-Omni Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kling-Omni Technical Report",
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16776",
      "pdf_url": "https://arxiv.org/pdf/2512.16776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16776",
      "scraped_at": "2025-12-21T01:53:49.284076"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Adaptation of Agentic AI",
    "paper_url": "https://huggingface.co/papers/2512.16301",
    "authors": [
      "XueqiangXu",
      "p-song1",
      "Gabshi",
      "linjc16",
      "pat-jj"
    ],
    "stars": "274",
    "details": {
      "title": "Adaptation of Agentic AI",
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16301",
      "pdf_url": "https://arxiv.org/pdf/2512.16301",
      "github_links": [
        "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16301",
      "scraped_at": "2025-12-21T01:53:51.185337"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "paper_url": "https://huggingface.co/papers/2512.15745",
    "authors": [],
    "stars": "168",
    "details": {
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15745",
      "pdf_url": "https://arxiv.org/pdf/2512.15745",
      "github_links": [
        "https://github.com/inclusionAI/LLaDA2.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15745",
      "scraped_at": "2025-12-21T01:53:53.118970"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "paper_url": "https://huggingface.co/papers/2512.16922",
    "authors": [],
    "stars": "69",
    "details": {
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "abstract": "Make SSL great again.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16922",
      "pdf_url": "https://arxiv.org/pdf/2512.16922",
      "github_links": [
        "https://github.com/SihanXU/nepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16922",
      "scraped_at": "2025-12-21T01:53:54.966173"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "paper_url": "https://huggingface.co/papers/2512.16915",
    "authors": [],
    "stars": "43",
    "details": {
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "abstract": "StereoPilot replaces the fragile \"Depth-Warp-Inpaint\" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo dataset with a learnable domain switcher, it provides a unified and efficient solution for both parallel and converged 3D video formats.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16915",
      "pdf_url": "https://arxiv.org/pdf/2512.16915",
      "github_links": [
        "https://github.com/KlingTeam/StereoPilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16915",
      "scraped_at": "2025-12-21T01:53:56.824450"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "paper_url": "https://huggingface.co/papers/2512.13507",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
      "abstract": "Seedance 1.5 pro Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13507",
      "pdf_url": "https://arxiv.org/pdf/2512.13507",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13507",
      "scraped_at": "2025-12-21T01:53:58.656454"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.16923",
    "authors": [
      "Yu-Lun Liu",
      "Jia-Bin Huang",
      "rayray9999"
    ],
    "stars": "43",
    "details": {
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16923",
      "pdf_url": "https://arxiv.org/pdf/2512.16923",
      "github_links": [
        "https://github.com/rayray9999/Genfocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16923",
      "scraped_at": "2025-12-21T01:54:00.494837"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "paper_url": "https://huggingface.co/papers/2512.16913",
    "authors": [
      "Wenxuan Lu",
      "Dizhe Zhang",
      "Meixi Song",
      "Xin Lin",
      "haodongli"
    ],
    "stars": "0",
    "details": {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP website/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16913",
      "pdf_url": "https://arxiv.org/pdf/2512.16913",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16913",
      "scraped_at": "2025-12-21T01:54:02.330947"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16625",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
      "abstract": "‚ú® Image editing is awesome; but it can leak user information! üõ°Ô∏è Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. üìÑ Paper: https://arxiv.org/abs/2512.16625 üíª Code: https://github.com/LinghuiiShen/DeContext üåê Project Page: https://linghuiishen.github.io/decontext_project_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16625",
      "pdf_url": "https://arxiv.org/pdf/2512.16625",
      "github_links": [
        "https://github.com/LinghuiiShen/DeContext"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16625",
      "scraped_at": "2025-12-21T01:54:04.614903"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "paper_url": "https://huggingface.co/papers/2512.16905",
    "authors": [
      "Jiarong Ou",
      "Miao Yang",
      "Xi Chen",
      "Yang Zhou",
      "Kaixin Ding"
    ],
    "stars": "0",
    "details": {
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "abstract": "data selection",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16905",
      "pdf_url": "https://arxiv.org/pdf/2512.16905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16905",
      "scraped_at": "2025-12-21T01:54:06.412924"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.16636",
    "authors": [
      "Giorgos Sfikas",
      "Theodoros Giannakopoulos",
      "Bill Psomas",
      "Christos Sgouropoulos",
      "Giorgos Petsangourakis"
    ],
    "stars": "0",
    "details": {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16636",
      "pdf_url": "https://arxiv.org/pdf/2512.16636",
      "github_links": [
        "https://github.com/giorgospets/reglue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16636",
      "scraped_at": "2025-12-21T01:54:08.206625"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "paper_url": "https://huggingface.co/papers/2512.16924",
    "authors": [],
    "stars": "67",
    "details": {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "abstract": "Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16924",
      "pdf_url": "https://arxiv.org/pdf/2512.16924",
      "github_links": [
        "https://github.com/pPetrichor/WorldCanvas"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16924",
      "scraped_at": "2025-12-21T01:54:10.048377"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.16561",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "abstract": "Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16561",
      "pdf_url": "https://arxiv.org/pdf/2512.16561",
      "github_links": [
        "https://github.com/W-Ted/N3D-VLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16561",
      "scraped_at": "2025-12-21T01:54:11.891721"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "paper_url": "https://huggingface.co/papers/2512.16649",
    "authors": [],
    "stars": "73",
    "details": {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "abstract": "‚ú®What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2√ó less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16649",
      "pdf_url": "https://arxiv.org/pdf/2512.16649",
      "github_links": [
        "https://github.com/thunlp/JustRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16649",
      "scraped_at": "2025-12-21T01:54:13.740031"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "paper_url": "https://huggingface.co/papers/2512.16920",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16920",
      "pdf_url": "https://arxiv.org/pdf/2512.16920",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16920",
      "scraped_at": "2025-12-21T01:54:15.580698"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "paper_url": "https://huggingface.co/papers/2512.16918",
    "authors": [
      "Zhixun Li",
      "Zhongyu Wang",
      "Dongyang Chen",
      "Kaituo Feng",
      "Chaoyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "abstract": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16918",
      "pdf_url": "https://arxiv.org/pdf/2512.16918",
      "github_links": [
        "https://github.com/CYWang735/AdaTooler-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16918",
      "scraped_at": "2025-12-21T01:54:17.373399"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "paper_url": "https://huggingface.co/papers/2512.16912",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16912",
      "pdf_url": "https://arxiv.org/pdf/2512.16912",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16912",
      "scraped_at": "2025-12-21T01:54:19.187808"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "paper_url": "https://huggingface.co/papers/2512.16900",
    "authors": [],
    "stars": "83",
    "details": {
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16900",
      "pdf_url": "https://arxiv.org/pdf/2512.16900",
      "github_links": [
        "https://github.com/Francis-Rings/FlashPortrait"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16900",
      "scraped_at": "2025-12-21T01:54:21.037158"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "paper_url": "https://huggingface.co/papers/2512.16899",
    "authors": [],
    "stars": "25",
    "details": {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "abstract": "Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta Superintelligence Labs released their reward benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16899",
      "pdf_url": "https://arxiv.org/pdf/2512.16899",
      "github_links": [
        "https://github.com/facebookresearch/MMRB2/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16899",
      "scraped_at": "2025-12-21T01:54:22.837616"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "paper_url": "https://huggingface.co/papers/2512.16864",
    "authors": [
      "Yuqi Liu",
      "Longxiang Tang",
      "Xiaohang Zhan",
      "Lei Ke",
      "TainU"
    ],
    "stars": "15",
    "details": {
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "abstract": "üöß The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). üìâ The Gap: Global Semantic Guidance Existing methods, including unified VLM-diffusion architectures, predominantly rely on Global Semantic Guidance . They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. üöÄ Our Solution: Region-Aligned Guidance RePlan introduces a Plan-then-Execute framework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performs Chain-of-Thought (CoT) reasoning to decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enables precise, multi-region parallel edits in a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities using Group Relative Policy Optimization (GRPO) . Remarkably, we achieve strong planning performance using only ~1k instruction-only samples , bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance is fully editable , enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supports regional negative prompts to prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establish IV-Edit , the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16864",
      "pdf_url": "https://arxiv.org/pdf/2512.16864",
      "github_links": [
        "https://github.com/dvlab-research/RePlan"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16864",
      "scraped_at": "2025-12-21T01:54:24.647394"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "paper_url": "https://huggingface.co/papers/2512.16501",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16501",
      "pdf_url": "https://arxiv.org/pdf/2512.16501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16501",
      "scraped_at": "2025-12-21T01:54:26.457722"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "paper_url": "https://huggingface.co/papers/2512.16106",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "ModelTables: A Corpus of Tables about Models",
      "abstract": "ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and configuration tables. The benchmark defines model and table relatedness using multiple signals, including paper citations, model-card links and inheritance, and shared training datasets, and supports downstream applications such as table discovery and semantic retrieval from both structural and semantic perspectives.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16106",
      "pdf_url": "https://arxiv.org/pdf/2512.16106",
      "github_links": [
        "https://github.com/RJMillerLab/ModelTables"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16106",
      "scraped_at": "2025-12-21T01:54:28.294571"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_url": "https://huggingface.co/papers/2512.16378",
    "authors": [
      "Carlos Escolano",
      "Vil√©m Zouhar",
      "zhopto3",
      "javi8979",
      "spapi"
    ],
    "stars": "13",
    "details": {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "abstract": "Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challenging conditions (noise, accents, disfluencies, long-form). Results show that cascades remain the most reliable overall, while SpeechLLMs close the gap in specific settings (notably noise and code-switching).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16378",
      "pdf_url": "https://arxiv.org/pdf/2512.16378",
      "github_links": [
        "https://github.com/sarapapi/hearing2translate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16378",
      "scraped_at": "2025-12-21T01:54:30.066165"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "paper_url": "https://huggingface.co/papers/2512.16921",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. Project Page: https://auditdm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16921",
      "pdf_url": "https://arxiv.org/pdf/2512.16921",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16921",
      "scraped_at": "2025-12-21T01:54:31.856850"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16615",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
      "abstract": "Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16615",
      "pdf_url": "https://arxiv.org/pdf/2512.16615",
      "github_links": [
        "https://github.com/SingleZombie/LLSA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16615",
      "scraped_at": "2025-12-21T01:54:33.718460"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "paper_url": "https://huggingface.co/papers/2512.11251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
      "abstract": "Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptions.  Inspired by LLaVA, we showed instruction-tuning on TS-Insights can enable LLMs to understand time series as a native input modality and generate textual descriptions. This work was originally done in Summer 2023.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11251",
      "pdf_url": "https://arxiv.org/pdf/2512.11251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11251",
      "scraped_at": "2025-12-21T01:54:35.514415"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "paper_url": "https://huggingface.co/papers/2512.16670",
    "authors": [
      "Hendrik P. A. Lensch",
      "Ole Beisswenger",
      "JDihlmann"
    ],
    "stars": "0",
    "details": {
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "abstract": "FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination from a stream of G-buffer data. We overfit SD on single scenes and show that it learns the illumination setting for the scene and can transfer it to OOD views of the scene.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16670",
      "pdf_url": "https://arxiv.org/pdf/2512.16670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16670",
      "scraped_at": "2025-12-21T01:54:37.314435"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "paper_url": "https://huggingface.co/papers/2512.16767",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16767",
      "pdf_url": "https://arxiv.org/pdf/2512.16767",
      "github_links": [
        "https://github.com/jasongzy/Make-It-Poseable"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16767",
      "scraped_at": "2025-12-21T01:54:39.135053"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.12576",
    "authors": [
      "Ben He",
      "Hongyu Lin",
      "Yanjiang Liu",
      "Jie Lou",
      "Aunderline"
    ],
    "stars": "0",
    "details": {
      "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
      "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12576",
      "pdf_url": "https://arxiv.org/pdf/2512.12576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12576",
      "scraped_at": "2025-12-21T01:54:40.957154"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "paper_url": "https://huggingface.co/papers/2512.10953",
    "authors": [
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Qiao Sun",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "abstract": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10953",
      "pdf_url": "https://arxiv.org/pdf/2512.10953",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10953",
      "scraped_at": "2025-12-21T01:54:42.674170"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "paper_url": "https://huggingface.co/papers/2512.16909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "abstract": "Project Page: https://hybridrobotics.github.io/MomaGraph/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16909",
      "pdf_url": "https://arxiv.org/pdf/2512.16909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16909",
      "scraped_at": "2025-12-21T01:54:44.423980"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.15907",
    "authors": [
      "Vivek Gupta",
      "Aparna Garimella",
      "Juhna Park",
      "Tejas Anvekar"
    ],
    "stars": "1",
    "details": {
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15907",
      "pdf_url": "https://arxiv.org/pdf/2512.15907",
      "github_links": [
        "https://github.com/CoRAL-ASU/TabReX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15907",
      "scraped_at": "2025-12-21T01:54:46.303716"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "paper_url": "https://huggingface.co/papers/2512.15489",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "abstract": "Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization‚Äîespecially on HLE-Math‚Äîwhile maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15489",
      "pdf_url": "https://arxiv.org/pdf/2512.15489",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15489",
      "scraped_at": "2025-12-21T01:54:48.098606"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "paper_url": "https://huggingface.co/papers/2512.14884",
    "authors": [
      "Yutong Bai",
      "Michael D. Grossberg",
      "Andrew Lu",
      "Katherine Xu",
      "Huzheng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
      "abstract": "what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14884",
      "pdf_url": "https://arxiv.org/pdf/2512.14884",
      "github_links": [
        "https://github.com/huzeyann/VibeSpace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14884",
      "scraped_at": "2025-12-21T01:54:49.958943"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "paper_url": "https://huggingface.co/papers/2512.12880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Improving Recursive Transformers with Mixture of LoRAs",
      "abstract": "Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected shared FFNs with a small set of token-routed LoRA experts (sparse routing), allowing conditional computation while keeping the backbone compact. We pretrain ModernALBERT (50M to 120M) with RoPE, GeGLU, FlashAttention, and distillation-based initialisation, and report state-of-the-art results among compact models on GLUE, SQuAD-v2, and BEIR, often surpassing larger fully parameterised baselines. For deployment, we introduce expert merging (including an EMA-based strategy) that compresses MoL into a single adapter at inference, removing routing overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12880",
      "pdf_url": "https://arxiv.org/pdf/2512.12880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12880",
      "scraped_at": "2025-12-21T01:54:51.739540"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
    "paper_url": "https://huggingface.co/papers/2512.12623",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
      "abstract": "üåê Website: https://mllm-dmlr.github.io üìÑ Paper: https://arxiv.org/abs/2512.12623",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12623",
      "pdf_url": "https://arxiv.org/pdf/2512.12623",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12623",
      "scraped_at": "2025-12-21T01:54:53.533015"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "paper_url": "https://huggingface.co/papers/2512.15528",
    "authors": [
      "Can Ma. Yu Zhou",
      "Dongbao Yang",
      "Daiqing Wu"
    ],
    "stars": "1",
    "details": {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "abstract": "Update the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15528",
      "pdf_url": "https://arxiv.org/pdf/2512.15528",
      "github_links": [
        "https://github.com/wdqqdw/EmoCaliber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15528",
      "scraped_at": "2025-12-21T01:54:55.266546"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "paper_url": "https://huggingface.co/papers/2512.14805",
    "authors": [
      "Michael Carbin",
      "Tian Jin",
      "Logan Weber",
      "ellieyhc"
    ],
    "stars": "3",
    "details": {
      "title": "Sharing State Between Prompts and Programs",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14805",
      "pdf_url": "https://arxiv.org/pdf/2512.14805",
      "github_links": [
        "https://github.com/psg-mit/nightjarpy/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14805",
      "scraped_at": "2025-12-21T01:54:56.988399"
    },
    "scraped_date": "2025-12-21"
  },
  {
    "title": "Kling-Omni Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kling-Omni Technical Report",
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16776",
      "pdf_url": "https://arxiv.org/pdf/2512.16776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16776",
      "scraped_at": "2025-12-22T01:52:28.433456"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Adaptation of Agentic AI",
    "paper_url": "https://huggingface.co/papers/2512.16301",
    "authors": [
      "XueqiangXu",
      "p-song1",
      "Gabshi",
      "linjc16",
      "pat-jj"
    ],
    "stars": "293",
    "details": {
      "title": "Adaptation of Agentic AI",
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16301",
      "pdf_url": "https://arxiv.org/pdf/2512.16301",
      "github_links": [
        "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16301",
      "scraped_at": "2025-12-22T01:52:30.476204"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "paper_url": "https://huggingface.co/papers/2512.16922",
    "authors": [],
    "stars": "87",
    "details": {
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "abstract": "Make SSL great again.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16922",
      "pdf_url": "https://arxiv.org/pdf/2512.16922",
      "github_links": [
        "https://github.com/SihanXU/nepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16922",
      "scraped_at": "2025-12-22T01:52:32.563303"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "paper_url": "https://huggingface.co/papers/2512.15745",
    "authors": [],
    "stars": "172",
    "details": {
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15745",
      "pdf_url": "https://arxiv.org/pdf/2512.15745",
      "github_links": [
        "https://github.com/inclusionAI/LLaDA2.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15745",
      "scraped_at": "2025-12-22T01:52:34.922051"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "paper_url": "https://huggingface.co/papers/2512.16915",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "abstract": "StereoPilot replaces the fragile \"Depth-Warp-Inpaint\" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo dataset with a learnable domain switcher, it provides a unified and efficient solution for both parallel and converged 3D video formats.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16915",
      "pdf_url": "https://arxiv.org/pdf/2512.16915",
      "github_links": [
        "https://github.com/KlingTeam/StereoPilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16915",
      "scraped_at": "2025-12-22T01:52:37.000210"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "paper_url": "https://huggingface.co/papers/2512.13507",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
      "abstract": "Seedance 1.5 pro Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13507",
      "pdf_url": "https://arxiv.org/pdf/2512.13507",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13507",
      "scraped_at": "2025-12-22T01:52:38.879192"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.16923",
    "authors": [
      "Yu-Lun Liu",
      "Jia-Bin Huang",
      "rayray9999"
    ],
    "stars": "64",
    "details": {
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16923",
      "pdf_url": "https://arxiv.org/pdf/2512.16923",
      "github_links": [
        "https://github.com/rayray9999/Genfocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16923",
      "scraped_at": "2025-12-22T01:52:40.769733"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "paper_url": "https://huggingface.co/papers/2512.16913",
    "authors": [
      "Wenxuan Lu",
      "Dizhe Zhang",
      "Meixi Song",
      "Xin Lin",
      "haodongli"
    ],
    "stars": "77",
    "details": {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP website/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16913",
      "pdf_url": "https://arxiv.org/pdf/2512.16913",
      "github_links": [
        "https://github.com/Insta360-Research-Team/DAP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16913",
      "scraped_at": "2025-12-22T01:52:42.690376"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "paper_url": "https://huggingface.co/papers/2512.16905",
    "authors": [
      "Jiarong Ou",
      "Miao Yang",
      "Xi Chen",
      "Yang Zhou",
      "Kaixin Ding"
    ],
    "stars": "0",
    "details": {
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "abstract": "data selection",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16905",
      "pdf_url": "https://arxiv.org/pdf/2512.16905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16905",
      "scraped_at": "2025-12-22T01:52:44.630408"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16625",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
      "abstract": "‚ú® Image editing is awesome; but it can leak user information! üõ°Ô∏è Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. üìÑ Paper: https://arxiv.org/abs/2512.16625 üíª Code: https://github.com/LinghuiiShen/DeContext üåê Project Page: https://linghuiishen.github.io/decontext_project_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16625",
      "pdf_url": "https://arxiv.org/pdf/2512.16625",
      "github_links": [
        "https://github.com/LinghuiiShen/DeContext"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16625",
      "scraped_at": "2025-12-22T01:52:46.565496"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.16636",
    "authors": [
      "Giorgos Sfikas",
      "Theodoros Giannakopoulos",
      "Bill Psomas",
      "Christos Sgouropoulos",
      "Giorgos Petsangourakis"
    ],
    "stars": "1",
    "details": {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16636",
      "pdf_url": "https://arxiv.org/pdf/2512.16636",
      "github_links": [
        "https://github.com/giorgospets/reglue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16636",
      "scraped_at": "2025-12-22T01:52:48.434250"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "paper_url": "https://huggingface.co/papers/2512.16924",
    "authors": [],
    "stars": "74",
    "details": {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "abstract": "Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16924",
      "pdf_url": "https://arxiv.org/pdf/2512.16924",
      "github_links": [
        "https://github.com/pPetrichor/WorldCanvas"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16924",
      "scraped_at": "2025-12-22T01:52:50.342955"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "paper_url": "https://huggingface.co/papers/2512.16649",
    "authors": [],
    "stars": "77",
    "details": {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "abstract": "‚ú®What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2√ó less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16649",
      "pdf_url": "https://arxiv.org/pdf/2512.16649",
      "github_links": [
        "https://github.com/thunlp/JustRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16649",
      "scraped_at": "2025-12-22T01:52:52.263342"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.16561",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "abstract": "Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16561",
      "pdf_url": "https://arxiv.org/pdf/2512.16561",
      "github_links": [
        "https://github.com/W-Ted/N3D-VLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16561",
      "scraped_at": "2025-12-22T01:52:54.191233"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "paper_url": "https://huggingface.co/papers/2512.16920",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16920",
      "pdf_url": "https://arxiv.org/pdf/2512.16920",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16920",
      "scraped_at": "2025-12-22T01:52:56.070423"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "paper_url": "https://huggingface.co/papers/2512.16918",
    "authors": [
      "Zhixun Li",
      "Zhongyu Wang",
      "Dongyang Chen",
      "Kaituo Feng",
      "Chaoyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "abstract": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16918",
      "pdf_url": "https://arxiv.org/pdf/2512.16918",
      "github_links": [
        "https://github.com/CYWang735/AdaTooler-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16918",
      "scraped_at": "2025-12-22T01:52:57.930176"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "paper_url": "https://huggingface.co/papers/2512.16912",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16912",
      "pdf_url": "https://arxiv.org/pdf/2512.16912",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16912",
      "scraped_at": "2025-12-22T01:52:59.800293"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "paper_url": "https://huggingface.co/papers/2512.16899",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "abstract": "Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta Superintelligence Labs released their reward benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16899",
      "pdf_url": "https://arxiv.org/pdf/2512.16899",
      "github_links": [
        "https://github.com/facebookresearch/MMRB2/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16899",
      "scraped_at": "2025-12-22T01:53:01.679382"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "paper_url": "https://huggingface.co/papers/2512.16864",
    "authors": [
      "Yuqi Liu",
      "Longxiang Tang",
      "Xiaohang Zhan",
      "Lei Ke",
      "TainU"
    ],
    "stars": "17",
    "details": {
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "abstract": "üöß The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). üìâ The Gap: Global Semantic Guidance Existing methods, including unified VLM-diffusion architectures, predominantly rely on Global Semantic Guidance . They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. üöÄ Our Solution: Region-Aligned Guidance RePlan introduces a Plan-then-Execute framework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performs Chain-of-Thought (CoT) reasoning to decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enables precise, multi-region parallel edits in a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities using Group Relative Policy Optimization (GRPO) . Remarkably, we achieve strong planning performance using only ~1k instruction-only samples , bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance is fully editable , enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supports regional negative prompts to prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establish IV-Edit , the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16864",
      "pdf_url": "https://arxiv.org/pdf/2512.16864",
      "github_links": [
        "https://github.com/dvlab-research/RePlan"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16864",
      "scraped_at": "2025-12-22T01:53:03.602686"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "paper_url": "https://huggingface.co/papers/2512.16900",
    "authors": [],
    "stars": "92",
    "details": {
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16900",
      "pdf_url": "https://arxiv.org/pdf/2512.16900",
      "github_links": [
        "https://github.com/Francis-Rings/FlashPortrait"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16900",
      "scraped_at": "2025-12-22T01:53:05.498824"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "paper_url": "https://huggingface.co/papers/2512.16501",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16501",
      "pdf_url": "https://arxiv.org/pdf/2512.16501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16501",
      "scraped_at": "2025-12-22T01:53:07.369391"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "paper_url": "https://huggingface.co/papers/2512.16106",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "ModelTables: A Corpus of Tables about Models",
      "abstract": "ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and configuration tables. The benchmark defines model and table relatedness using multiple signals, including paper citations, model-card links and inheritance, and shared training datasets, and supports downstream applications such as table discovery and semantic retrieval from both structural and semantic perspectives.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16106",
      "pdf_url": "https://arxiv.org/pdf/2512.16106",
      "github_links": [
        "https://github.com/RJMillerLab/ModelTables"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16106",
      "scraped_at": "2025-12-22T01:53:09.188933"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_url": "https://huggingface.co/papers/2512.16378",
    "authors": [
      "Carlos Escolano",
      "Vil√©m Zouhar",
      "zhopto3",
      "javi8979",
      "spapi"
    ],
    "stars": "13",
    "details": {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "abstract": "Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challenging conditions (noise, accents, disfluencies, long-form). Results show that cascades remain the most reliable overall, while SpeechLLMs close the gap in specific settings (notably noise and code-switching).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16378",
      "pdf_url": "https://arxiv.org/pdf/2512.16378",
      "github_links": [
        "https://github.com/sarapapi/hearing2translate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16378",
      "scraped_at": "2025-12-22T01:53:11.027318"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "paper_url": "https://huggingface.co/papers/2512.16921",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. Project Page: https://auditdm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16921",
      "pdf_url": "https://arxiv.org/pdf/2512.16921",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16921",
      "scraped_at": "2025-12-22T01:53:12.963299"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "paper_url": "https://huggingface.co/papers/2512.11251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
      "abstract": "Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptions.  Inspired by LLaVA, we showed instruction-tuning on TS-Insights can enable LLMs to understand time series as a native input modality and generate textual descriptions. This work was originally done in Summer 2023.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11251",
      "pdf_url": "https://arxiv.org/pdf/2512.11251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11251",
      "scraped_at": "2025-12-22T01:53:14.895310"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16615",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
      "abstract": "Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16615",
      "pdf_url": "https://arxiv.org/pdf/2512.16615",
      "github_links": [
        "https://github.com/SingleZombie/LLSA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16615",
      "scraped_at": "2025-12-22T01:53:16.820184"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "paper_url": "https://huggingface.co/papers/2512.15489",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "abstract": "Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization‚Äîespecially on HLE-Math‚Äîwhile maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15489",
      "pdf_url": "https://arxiv.org/pdf/2512.15489",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15489",
      "scraped_at": "2025-12-22T01:53:18.671559"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "paper_url": "https://huggingface.co/papers/2512.16767",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16767",
      "pdf_url": "https://arxiv.org/pdf/2512.16767",
      "github_links": [
        "https://github.com/jasongzy/Make-It-Poseable"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16767",
      "scraped_at": "2025-12-22T01:53:20.517544"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "paper_url": "https://huggingface.co/papers/2512.16670",
    "authors": [
      "Hendrik P. A. Lensch",
      "Ole Beisswenger",
      "JDihlmann"
    ],
    "stars": "0",
    "details": {
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "abstract": "FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination from a stream of G-buffer data. We overfit SD on single scenes and show that it learns the illumination setting for the scene and can transfer it to OOD views of the scene.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16670",
      "pdf_url": "https://arxiv.org/pdf/2512.16670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16670",
      "scraped_at": "2025-12-22T01:53:22.394603"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "paper_url": "https://huggingface.co/papers/2512.10953",
    "authors": [
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Qiao Sun",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "abstract": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10953",
      "pdf_url": "https://arxiv.org/pdf/2512.10953",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10953",
      "scraped_at": "2025-12-22T01:53:24.387244"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "paper_url": "https://huggingface.co/papers/2512.12880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Improving Recursive Transformers with Mixture of LoRAs",
      "abstract": "Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected shared FFNs with a small set of token-routed LoRA experts (sparse routing), allowing conditional computation while keeping the backbone compact. We pretrain ModernALBERT (50M to 120M) with RoPE, GeGLU, FlashAttention, and distillation-based initialisation, and report state-of-the-art results among compact models on GLUE, SQuAD-v2, and BEIR, often surpassing larger fully parameterised baselines. For deployment, we introduce expert merging (including an EMA-based strategy) that compresses MoL into a single adapter at inference, removing routing overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12880",
      "pdf_url": "https://arxiv.org/pdf/2512.12880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12880",
      "scraped_at": "2025-12-22T01:53:26.216029"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
    "paper_url": "https://huggingface.co/papers/2512.12623",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
      "abstract": "üåê Website: https://mllm-dmlr.github.io üìÑ Paper: https://arxiv.org/abs/2512.12623",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12623",
      "pdf_url": "https://arxiv.org/pdf/2512.12623",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12623",
      "scraped_at": "2025-12-22T01:53:28.064538"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.12576",
    "authors": [
      "Ben He",
      "Hongyu Lin",
      "Yanjiang Liu",
      "Jie Lou",
      "Aunderline"
    ],
    "stars": "0",
    "details": {
      "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
      "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12576",
      "pdf_url": "https://arxiv.org/pdf/2512.12576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12576",
      "scraped_at": "2025-12-22T01:53:29.883782"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "paper_url": "https://huggingface.co/papers/2512.16909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "abstract": "Project Page: https://hybridrobotics.github.io/MomaGraph/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16909",
      "pdf_url": "https://arxiv.org/pdf/2512.16909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16909",
      "scraped_at": "2025-12-22T01:53:31.722427"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.15907",
    "authors": [
      "Vivek Gupta",
      "Aparna Garimella",
      "Juhna Park",
      "Tejas Anvekar"
    ],
    "stars": "1",
    "details": {
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15907",
      "pdf_url": "https://arxiv.org/pdf/2512.15907",
      "github_links": [
        "https://github.com/CoRAL-ASU/TabReX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15907",
      "scraped_at": "2025-12-22T01:53:33.535002"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "paper_url": "https://huggingface.co/papers/2512.14884",
    "authors": [
      "Yutong Bai",
      "Michael D. Grossberg",
      "Andrew Lu",
      "Katherine Xu",
      "Huzheng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
      "abstract": "what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14884",
      "pdf_url": "https://arxiv.org/pdf/2512.14884",
      "github_links": [
        "https://github.com/huzeyann/VibeSpace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14884",
      "scraped_at": "2025-12-22T01:53:35.652691"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "paper_url": "https://huggingface.co/papers/2512.15528",
    "authors": [
      "Can Ma. Yu Zhou",
      "Dongbao Yang",
      "Daiqing Wu"
    ],
    "stars": "1",
    "details": {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "abstract": "Update the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15528",
      "pdf_url": "https://arxiv.org/pdf/2512.15528",
      "github_links": [
        "https://github.com/wdqqdw/EmoCaliber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15528",
      "scraped_at": "2025-12-22T01:53:37.441726"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "paper_url": "https://huggingface.co/papers/2512.14805",
    "authors": [
      "Michael Carbin",
      "Tian Jin",
      "Logan Weber",
      "ellieyhc"
    ],
    "stars": "3",
    "details": {
      "title": "Sharing State Between Prompts and Programs",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14805",
      "pdf_url": "https://arxiv.org/pdf/2512.14805",
      "github_links": [
        "https://github.com/psg-mit/nightjarpy/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14805",
      "scraped_at": "2025-12-22T01:53:39.288175"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "paper_url": "https://huggingface.co/papers/2512.16969",
    "authors": [
      "Yuhao Zhou",
      "SciYu",
      "VitaCoco",
      "BoKelvin",
      "CoCoOne"
    ],
    "stars": "56",
    "details": {
      "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
      "abstract": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16969",
      "pdf_url": "https://arxiv.org/pdf/2512.16969",
      "github_links": [
        "https://github.com/InternScience/SGI-Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16969",
      "scraped_at": "2025-12-23T01:48:12.405586"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.16793",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16793",
      "pdf_url": "https://arxiv.org/pdf/2512.16793",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16793",
      "scraped_at": "2025-12-23T01:48:14.425346"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "When Reasoning Meets Its Laws",
    "paper_url": "https://huggingface.co/papers/2512.17901",
    "authors": [
      "Liu Ziyin",
      "Jingyan Shen",
      "Tianang Leng",
      "Yifan Sun",
      "jyzhang1208"
    ],
    "stars": "15",
    "details": {
      "title": "When Reasoning Meets Its Laws",
      "abstract": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17901",
      "pdf_url": "https://arxiv.org/pdf/2512.17901",
      "github_links": [
        "https://github.com/ASTRAL-Group/LoRe"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17901",
      "scraped_at": "2025-12-23T01:48:16.357933"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "paper_url": "https://huggingface.co/papers/2512.17260",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
      "abstract": "Github: https://github.com/ByteDance-Seed/Seed-Prover",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17260",
      "pdf_url": "https://arxiv.org/pdf/2512.17260",
      "github_links": [
        "https://github.com/ByteDance-Seed/Seed-Prover"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17260",
      "scraped_at": "2025-12-23T01:48:18.269306"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2512.17909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
      "abstract": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components. Project Page: https://jshilong.github.io/PS-VAE-PAGE/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17909",
      "pdf_url": "https://arxiv.org/pdf/2512.17909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17909",
      "scraped_at": "2025-12-23T01:48:20.166848"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "paper_url": "https://huggingface.co/papers/2512.17012",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
      "abstract": "Project page: https://ca-joe-yang.github.io/resource/projects/4D_RGPT We propose 4D-RGPT , a specialized MLLM that perceives 4D information for enhanced video understanding. We propose the P erceptual 4 D D istillation ( P4D ) training framework to distill 4D perceptual knowledge into 4D-RGPT without introducing additional inference cost. We introduce R4D-Bench , a region-based 4D VQA benchmark that requires region-level 4D understanding. Our 4D-RGPT improves over the baseline on both non-region-based 3D/4D benchmarks ( +5.3% on average across 6 benchmarks ) and our region-based R4D-Bench benchmark ( +4.3% ), while effectively capturing explicit 4D signals.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17012",
      "pdf_url": "https://arxiv.org/pdf/2512.17012",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17012",
      "scraped_at": "2025-12-23T01:48:22.025726"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
    "paper_url": "https://huggingface.co/papers/2512.16041",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
      "abstract": "We argue that evaluating LLM-as-a-Judge is biased by human-annotated ground truth, rethink the evaluation of LLM-as-a-Judge, and design metrics that do not need human annotations.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16041",
      "pdf_url": "https://arxiv.org/pdf/2512.16041",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16041",
      "scraped_at": "2025-12-23T01:48:23.866902"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
    "paper_url": "https://huggingface.co/papers/2512.11362",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
      "abstract": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our https://suyuz1.github.io/VLA-Survey-Anatomy/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11362",
      "pdf_url": "https://arxiv.org/pdf/2512.11362",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11362",
      "scraped_at": "2025-12-23T01:48:25.863004"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
    "paper_url": "https://huggingface.co/papers/2512.17897",
    "authors": [
      "Or Litany",
      "Shengyu Huang",
      "Sanja Fidler",
      "Fangqiang Ding",
      "TomerBo"
    ],
    "stars": "6",
    "details": {
      "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
      "abstract": "Check out radargen.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17897",
      "pdf_url": "https://arxiv.org/pdf/2512.17897",
      "github_links": [
        "https://github.com/tomerborreda/RadarGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17897",
      "scraped_at": "2025-12-23T01:48:27.747323"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.17495",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
      "abstract": "Our new benchmark for evaluating the grounding capabilities of frontier MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17495",
      "pdf_url": "https://arxiv.org/pdf/2512.17495",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17495",
      "scraped_at": "2025-12-23T01:48:29.641583"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "paper_url": "https://huggingface.co/papers/2512.17351",
    "authors": [],
    "stars": "278",
    "details": {
      "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
      "abstract": "https://x.com/ZeyuanAllenZhu/status/2000892470306152701 https://physics.allen-zhu.com/part-4-architecture-design/part-4-1",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17351",
      "pdf_url": "https://arxiv.org/pdf/2512.17351",
      "github_links": [
        "https://github.com/facebookresearch/PhysicsLM4"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17351",
      "scraped_at": "2025-12-23T01:48:31.515651"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
    "paper_url": "https://huggingface.co/papers/2512.17008",
    "authors": [
      "Lihong Li",
      "Meet P. Vadera",
      "Rui Meng",
      "Peng Zhou",
      "ljb121002"
    ],
    "stars": "0",
    "details": {
      "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
      "abstract": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17008",
      "pdf_url": "https://arxiv.org/pdf/2512.17008",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17008",
      "scraped_at": "2025-12-23T01:48:33.437469"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
    "paper_url": "https://huggingface.co/papers/2512.14870",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
      "abstract": "üîó Project page: https://herbench.github.io/ üìÑ  arXiv: https://arxiv.org/abs/2512.14870 ü§ó  HF dataset card: https://huggingface.co/datasets/DanBenAmi/HERBench üñ•  Code (GitHub): https://github.com/DanBenAmi/HERBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14870",
      "pdf_url": "https://arxiv.org/pdf/2512.14870",
      "github_links": [
        "https://github.com/DanBenAmi/HERBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14870",
      "scraped_at": "2025-12-23T01:48:35.301291"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Animate Any Character in Any World",
    "paper_url": "https://huggingface.co/papers/2512.17796",
    "authors": [
      "Yan Lu",
      "Bo Dai",
      "Hongyang Zhang",
      "Fangyun Wei",
      "Yitong Wang"
    ],
    "stars": "28",
    "details": {
      "title": "Animate Any Character in Any World",
      "abstract": "Introducing AniX, a system enables users to provide 3DGS scene along with a 3D or multi-view character, enabling interactive control of the character's behaviors and active exploration of the environment through natural language commands. The system features:",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17796",
      "pdf_url": "https://arxiv.org/pdf/2512.17796",
      "github_links": [
        "https://github.com/snowflakewang/AniX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17796",
      "scraped_at": "2025-12-23T01:48:37.149428"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "paper_url": "https://huggingface.co/papers/2512.17419",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
      "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17419",
      "pdf_url": "https://arxiv.org/pdf/2512.17419",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17419",
      "scraped_at": "2025-12-23T01:48:38.957080"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
    "paper_url": "https://huggingface.co/papers/2512.16483",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
      "abstract": "github: https://github.com/sen-mao/StageVAR",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16483",
      "pdf_url": "https://arxiv.org/pdf/2512.16483",
      "github_links": [
        "https://github.com/sen-mao/StageVAR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16483",
      "scraped_at": "2025-12-23T01:48:40.740165"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Bolmo: Byteifying the Next Generation of Language Models",
    "paper_url": "https://huggingface.co/papers/2512.15586",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Bolmo: Byteifying the Next Generation of Language Models",
      "abstract": "So cool idea to make use of mLSTM and developing this byteifying approach üòç",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15586",
      "pdf_url": "https://arxiv.org/pdf/2512.15586",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15586",
      "scraped_at": "2025-12-23T01:48:42.629718"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Meta-RL Induces Exploration in Language Agents",
    "paper_url": "https://huggingface.co/papers/2512.16848",
    "authors": [
      "Maria Brbic",
      "Michael Moor",
      "Damien Teney",
      "Liangze Jiang",
      "Yulun Jiang"
    ],
    "stars": "0",
    "details": {
      "title": "Meta-RL Induces Exploration in Language Agents",
      "abstract": "üåäLaMer, a general Meta-RL framework that enables LLM agents to explore and learn from the environment feedback at test time.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16848",
      "pdf_url": "https://arxiv.org/pdf/2512.16848",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16848",
      "scraped_at": "2025-12-23T01:48:44.450669"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "paper_url": "https://huggingface.co/papers/2512.17532",
    "authors": [
      "Runtao Liu",
      "Xiaogang Xu",
      "Wei Wei",
      "Jianmin Chen",
      "Jiaqi-hkust"
    ],
    "stars": "0",
    "details": {
      "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
      "abstract": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17532",
      "pdf_url": "https://arxiv.org/pdf/2512.17532",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17532",
      "scraped_at": "2025-12-23T01:48:46.210116"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
    "paper_url": "https://huggingface.co/papers/2512.17459",
    "authors": [
      "Hendrik P. A. Lensch",
      "Tobias Sautter",
      "JDihlmann"
    ],
    "stars": "33",
    "details": {
      "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
      "abstract": "üåê https://3dregen.jdihlmann.com/ üìÉ https://arxiv.org/abs/2512.17459 üíæ https://github.com/cgtuebingen/3D-RE-GEN",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17459",
      "pdf_url": "https://arxiv.org/pdf/2512.17459",
      "github_links": [
        "https://github.com/cgtuebingen/3D-RE-GEN"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17459",
      "scraped_at": "2025-12-23T01:48:48.020152"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
    "paper_url": "https://huggingface.co/papers/2512.16978",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
      "abstract": "üåê Website: https://mbzuai-oryx.github.io/LongShOT/ üíª Github: https://github.com/mbzuai-oryx/longshot ü§ó HuggingFace: https://huggingface.co/datasets/MBZUAI/longshot-bench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16978",
      "pdf_url": "https://arxiv.org/pdf/2512.16978",
      "github_links": [
        "https://github.com/mbzuai-oryx/longshot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16978",
      "scraped_at": "2025-12-23T01:48:49.866724"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.13427",
    "authors": [
      "Tomer Michaeli",
      "Inbar Huberman-Spiegelglas",
      "Nurit Spingarn-Eliezer",
      "Noa Cohen"
    ],
    "stars": "0",
    "details": {
      "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
      "abstract": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13427",
      "pdf_url": "https://arxiv.org/pdf/2512.13427",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13427",
      "scraped_at": "2025-12-23T01:48:51.620855"
    },
    "scraped_date": "2025-12-23"
  },
  {
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "paper_url": "https://huggingface.co/papers/2512.16676",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
      "abstract": "code link: https://github.com/OpenDCAI/DataFlow",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16676",
      "pdf_url": "https://arxiv.org/pdf/2512.16676",
      "github_links": [
        "https://github.com/OpenDCAI/DataFlow"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16676",
      "scraped_at": "2025-12-24T01:46:29.753237"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
    "paper_url": "https://huggingface.co/papers/2512.19693",
    "authors": [
      "Ziwei Liu",
      "Dahua Lin",
      "Quan Wang",
      "Haiwen Diao",
      "Weichen Fan"
    ],
    "stars": "56",
    "details": {
      "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
      "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19693",
      "pdf_url": "https://arxiv.org/pdf/2512.19693",
      "github_links": [
        "https://github.com/WeichenFan/UAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19693",
      "scraped_at": "2025-12-24T01:46:31.698181"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
    "paper_url": "https://huggingface.co/papers/2512.17650",
    "authors": [],
    "stars": "32",
    "details": {
      "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
      "abstract": "Region-Constraint In-Context Generation for Instructional Video Editing Paper: https://arxiv.org/abs/2512.17650 Project Page: https://zhw-zhang.github.io/ReCo-page/ Github: https://github.com/HiDream-ai/ReCo ReCo-Data: https://huggingface.co/datasets/HiDream-ai/ReCo-Data ReCo-Bench: https://huggingface.co/datasets/HiDream-ai/ReCo-Bench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17650",
      "pdf_url": "https://arxiv.org/pdf/2512.17650",
      "github_links": [
        "https://github.com/HiDream-ai/ReCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17650",
      "scraped_at": "2025-12-24T01:46:33.664644"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.17040",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17040",
      "pdf_url": "https://arxiv.org/pdf/2512.17040",
      "github_links": [
        "https://github.com/emjay73/InfCam"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17040",
      "scraped_at": "2025-12-24T01:46:35.583698"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
    "paper_url": "https://huggingface.co/papers/2512.19134",
    "authors": [
      "Lu Cheng",
      "Tongtong Wu",
      "Kailin Zhang",
      "Dehai Min"
    ],
    "stars": "8",
    "details": {
      "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
      "abstract": "A new framework for dynamic retrieval-augmented generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19134",
      "pdf_url": "https://arxiv.org/pdf/2512.19134",
      "github_links": [
        "https://github.com/ZhishanQ/QuCo-RAG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19134",
      "scraped_at": "2025-12-24T01:46:37.568351"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
    "paper_url": "https://huggingface.co/papers/2512.18880",
    "authors": [
      "Hong Jiao",
      "Jian Chen",
      "Yunze Xiao",
      "Han Chen",
      "Ming Li"
    ],
    "stars": "0",
    "details": {
      "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "abstract": "Key Findings of our Human-LLM difficulty alignment study: Systematic Misalignment : Contrary to standard capability metrics, scaling does not reliably translate into alignment. Increasing model scale does not improve difficulty predictions; instead, models form a cohesive Machine Consensus, aligning significantly stronger with each other than with human reality. Limits of Simulation : Neither extrinsic ensembling nor proficiency simulation serves as a reliable fix for the misalignment. Ensemble performance is strictly bounded by weaker models, while proficiency simulation proves highly inconsistent as models struggle to authentically mimic different proficiency levels. The Curse of Knowledge : Our IRT-based analysis reveals a fundamental mechanistic divergence: the difficulty derived from models' actual correctness correlates even worse with humans than their explicit perceptions. Items that are difficult for humans are frequently trivial for models, and this capability exhibits significant inertia even under weak student prompts. Metacognitive Blindness : We identify a critical lack of introspection. With AUROC scores hovering near random guessing, models fail to predict their own limitations, indicating that explicit difficulty estimates are effectively decoupled from the model's actual correctness, lacking the internal signal to ground their predictions.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18880",
      "pdf_url": "https://arxiv.org/pdf/2512.18880",
      "github_links": [
        "https://github.com/MingLiiii/Difficulty_Alignment"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18880",
      "scraped_at": "2025-12-24T01:46:39.452943"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.19678",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "abstract": "Long-range camera-conditioned scene generation from a single image. Project page and code: https://hyokong.github.io/worldwarp-page/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19678",
      "pdf_url": "https://arxiv.org/pdf/2512.19678",
      "github_links": [
        "https://github.com/HyoKong/WorldWarp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19678",
      "scraped_at": "2025-12-24T01:46:41.458062"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
    "paper_url": "https://huggingface.co/papers/2512.19629",
    "authors": [
      "Yuan Shen",
      "Tai Wang",
      "Yuqiang Yang",
      "Wenzhe Cai",
      "Jiaqi Peng"
    ],
    "stars": "0",
    "details": {
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "abstract": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19629",
      "pdf_url": "https://arxiv.org/pdf/2512.19629",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19629",
      "scraped_at": "2025-12-24T01:46:43.484735"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.17385",
    "authors": [
      "Yuqing Ma",
      "Lin Jing",
      "Wei Zhang",
      "Jian Yang",
      "Jiajun Wu"
    ],
    "stars": "0",
    "details": {
      "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
      "abstract": "This paper introduces UCoder, an unsupervised framework for training code-generating large language models without requiring any external datasets, including unlabeled code snippets. The approach, called IPC (Internal Probing of LLMs for Code generation), leverages latent programming knowledge already present in pre-trained models through a six-stage self-bootstrapping process: (1-3) problem space probing that generates diverse algorithmic problems with specifications, (4) test understanding probing to create comprehensive test suites, (5) solution space probing using dense sampling (128 candidates per problem), and (6) knowledge consolidation through supervised fine-tuning on high-quality solutions. The key innovation is execution-driven consensus clustering, which identifies correct implementations by finding clusters of behaviorally identical solutions‚Äîcorrect code naturally clusters together while incorrect solutions fail heterogeneously. Experiments on UCoder models (7B, 14B, 32B parameters) demonstrate competitive performance with supervised baselines across multiple benchmarks (HumanEval, MBPP, BigCodeBench, LiveCodeBench, FullStackBench), with smaller models showing greater improvement gains (inverse scaling). The work proves that self-generated data maintains lexical, semantic, and structural diversity sufficient for effective learning, opening possibilities for resource-efficient LLM training without human annotation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17385",
      "pdf_url": "https://arxiv.org/pdf/2512.17385",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17385",
      "scraped_at": "2025-12-24T01:46:45.428926"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "paper_url": "https://huggingface.co/papers/2512.19682",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19682",
      "pdf_url": "https://arxiv.org/pdf/2512.19682",
      "github_links": [
        "https://github.com/Gen-Verse/GenEnv"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19682",
      "scraped_at": "2025-12-24T01:46:47.338413"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
    "paper_url": "https://huggingface.co/papers/2512.19539",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "abstract": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19539",
      "pdf_url": "https://arxiv.org/pdf/2512.19539",
      "github_links": [
        "https://github.com/Kevin-thu/StoryMem"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19539",
      "scraped_at": "2025-12-24T01:46:49.226679"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
    "paper_url": "https://huggingface.co/papers/2512.16229",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
      "abstract": "üîóPaperÔºö https://arxiv.org/abs/2512.16229 üîóGitHubÔºö https://github.com/zhijie-group/LoPA üîóblog: https://zhijie-group.github.io/blogs/lopa",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16229",
      "pdf_url": "https://arxiv.org/pdf/2512.16229",
      "github_links": [
        "https://github.com/zhijie-group/LoPA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16229",
      "scraped_at": "2025-12-24T01:46:51.047673"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
    "paper_url": "https://huggingface.co/papers/2512.17206",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
      "abstract": "Reasoning Palette addresses the challenge of controlling LLM generation style and enabling effective exploration in RL by introducing a stochastic latent variable that encodes diverse reasoning strategies. This latent, inferred via a VAE from question-answer pairs, is decoded into token prefixes that modulate the model's internal reasoning before generation. A brief SFT phase adapts the model to this conditioning, and during RL, it enables structured, on-demand exploration, boosting both efficiency and performance across reasoning benchmarks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17206",
      "pdf_url": "https://arxiv.org/pdf/2512.17206",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17206",
      "scraped_at": "2025-12-24T01:46:52.865009"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
    "paper_url": "https://huggingface.co/papers/2512.19432",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
      "abstract": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19432",
      "pdf_url": "https://arxiv.org/pdf/2512.19432",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19432",
      "scraped_at": "2025-12-24T01:46:54.727159"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "paper_url": "https://huggingface.co/papers/2512.18658",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
      "abstract": "Most LLMs today are powerful at language but weak at worlds: they generate fluent outputs without maintaining a consistent, verifiable model of reality. As a result, many AI applications plateau at demos or copilots and fail in complex, high-stakes workflows. This paper shows that progress requires shifting from ad-hoc reasoning to explicit, evidence-grounded world models. Cap table tie-out exposes this gap‚Äîand demonstrates how closing it enables genuinely autonomous systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18658",
      "pdf_url": "https://arxiv.org/pdf/2512.18658",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18658",
      "scraped_at": "2025-12-24T01:46:56.999396"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
    "paper_url": "https://huggingface.co/papers/2512.19402",
    "authors": [
      "Liliang Chen",
      "Shengcong Chen",
      "Di Chen",
      "Hongwei Fan",
      "Yujie Zhao"
    ],
    "stars": "0",
    "details": {
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "abstract": "Paper: https://arxiv.org/abs/2512.19402 Project Page: https://real2edit2real.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19402",
      "pdf_url": "https://arxiv.org/pdf/2512.19402",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19402",
      "scraped_at": "2025-12-24T01:46:59.224433"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "paper_url": "https://huggingface.co/papers/2512.19535",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "abstract": "Code: https://github.com/kyutai-labs/casa",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19535",
      "pdf_url": "https://arxiv.org/pdf/2512.19535",
      "github_links": [
        "https://github.com/kyutai-labs/casa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19535",
      "scraped_at": "2025-12-24T01:47:01.059156"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Name That Part: 3D Part Segmentation and Naming",
    "paper_url": "https://huggingface.co/papers/2512.18003",
    "authors": [
      "Alan Yuille",
      "Anand Bhattad",
      "Ankit Vaidya",
      "Prakhar Kaushik",
      "Soumava Paul"
    ],
    "stars": "0",
    "details": {
      "title": "Name That Part: 3D Part Segmentation and Naming",
      "abstract": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18003",
      "pdf_url": "https://arxiv.org/pdf/2512.18003",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18003",
      "scraped_at": "2025-12-24T01:47:02.926590"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "paper_url": "https://huggingface.co/papers/2512.18314",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
      "abstract": "üåê https://matspray.jdihlmann.com/ üìÉ https://arxiv.org/abs/2512.18314 üíæ https://github.com/cgtuebingen/MatSpray",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18314",
      "pdf_url": "https://arxiv.org/pdf/2512.18314",
      "github_links": [
        "https://github.com/cgtuebingen/MatSpray"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18314",
      "scraped_at": "2025-12-24T01:47:04.779920"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
    "paper_url": "https://huggingface.co/papers/2512.12620",
    "authors": [
      "Sujata Ghosh",
      "Saptarshi Sahoo",
      "Aheli Poddar"
    ],
    "stars": "0",
    "details": {
      "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
      "abstract": "arXiv lens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/understanding-syllogistic-reasoning-in-llms-from-formal-and-natural-language-perspectives-822-84433a31 Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12620",
      "pdf_url": "https://arxiv.org/pdf/2512.12620",
      "github_links": [
        "https://github.com/XAheli/Logic-in-LLMs"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12620",
      "scraped_at": "2025-12-24T01:47:06.655524"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
    "paper_url": "https://huggingface.co/papers/2512.19661",
    "authors": [
      "Roni Sengupta",
      "Cary Phillips",
      "Jun Myeong Choi",
      "Jiaye Wu",
      "Luchao Qi"
    ],
    "stars": "0",
    "details": {
      "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19661",
      "pdf_url": "https://arxiv.org/pdf/2512.19661",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19661",
      "scraped_at": "2025-12-24T01:47:08.509366"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "Brain-Grounded Axes for Reading and Steering LLM States",
    "paper_url": "https://huggingface.co/papers/2512.19399",
    "authors": [
      "Sandro Andric"
    ],
    "stars": "0",
    "details": {
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "abstract": "These research supports a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19399",
      "pdf_url": "https://arxiv.org/pdf/2512.19399",
      "github_links": [
        "https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19399",
      "scraped_at": "2025-12-24T01:47:10.322980"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
    "paper_url": "https://huggingface.co/papers/2512.18542",
    "authors": [
      "Scott Thornton"
    ],
    "stars": "1",
    "details": {
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18542",
      "pdf_url": "https://arxiv.org/pdf/2512.18542",
      "github_links": [
        "https://github.com/scthornton/securecode-v2"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18542",
      "scraped_at": "2025-12-24T01:47:12.113503"
    },
    "scraped_date": "2025-12-24"
  },
  {
    "title": "SemanticGen: Video Generation in Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.20619",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SemanticGen: Video Generation in Semantic Space",
      "abstract": "Project Page: https://jianhongbai.github.io/SemanticGen/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20619",
      "pdf_url": "https://arxiv.org/pdf/2512.20619",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20619",
      "scraped_at": "2025-12-25T01:48:29.468881"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "paper_url": "https://huggingface.co/papers/2512.19673",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "abstract": "Bottom-up Policy Optimization (BuPO) provides a novel framework to decompose LLM policies into internal layer and modular policies, reveals distinct reasoning patterns across different model architectures, and introduces a bottom-up optimization algorithm that leverages these insights to enhance complex reasoning. Key Findings: Internal Policies: Decomposes the unified LLM policy into samplable distributions from individual layers and modules (self-attention & FFN). Progressive Reasoning Pattern: Discovered a human-like \"Exploration-Integration-Convergence\" (EIC) pattern in Qwen models, contrasting with the abrupt convergence in Llama models. Bottom-up Policy Optimization (BuPO): A novel two-phase RL algorithm that first optimizes an internal, lower-layer policy to reconstruct foundational reasoning, then fine-tunes the full model. Enhanced Reasoning Performance: BuPO significantly outperforms standard RL on complex reasoning benchmarks. Code: https://github.com/Trae1ounG/BuPO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19673",
      "pdf_url": "https://arxiv.org/pdf/2512.19673",
      "github_links": [
        "https://github.com/Trae1ounG/BuPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19673",
      "scraped_at": "2025-12-25T01:48:31.353002"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "paper_url": "https://huggingface.co/papers/2512.20618",
    "authors": [
      "Renjie Pi",
      "Yue Ma",
      "Jiaqi Tang",
      "Ziyi Liu",
      "Runtao Liu"
    ],
    "stars": "0",
    "details": {
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20618",
      "pdf_url": "https://arxiv.org/pdf/2512.20618",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20618",
      "scraped_at": "2025-12-25T01:48:33.283921"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "paper_url": "https://huggingface.co/papers/2512.20617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "abstract": "Introducing SpatialTree, a four-level hierarchy for spatial abilities in multimodal LLMs, benchmark 27 sub-abilities, reveal transfer patterns, and propose auto-think to improve reinforcement-learning performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20617",
      "pdf_url": "https://arxiv.org/pdf/2512.20617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20617",
      "scraped_at": "2025-12-25T01:48:35.176325"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
    "paper_url": "https://huggingface.co/papers/2512.18746",
    "authors": [
      "Junhao Wang",
      "Zhenhong Zhou",
      "Chong Zhan",
      "Haotian Ren",
      "Guibin Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
      "abstract": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18746",
      "pdf_url": "https://arxiv.org/pdf/2512.18746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18746",
      "scraped_at": "2025-12-25T01:48:37.090337"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Step-DeepResearch Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.20491",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Step-DeepResearch Technical Report",
      "abstract": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20491",
      "pdf_url": "https://arxiv.org/pdf/2512.20491",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20491",
      "scraped_at": "2025-12-25T01:48:38.930062"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
    "paper_url": "https://huggingface.co/papers/2512.17102",
    "authors": [
      "Soumya Smruti Mishra",
      "Yijun Tian",
      "Yawei Wang",
      "Qiaojing Yan",
      "Jiongxiao Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
      "abstract": "Apply RL to Skill Library Agent.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17102",
      "pdf_url": "https://arxiv.org/pdf/2512.17102",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17102",
      "scraped_at": "2025-12-25T01:48:40.786923"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "SAM Audio: Segment Anything in Audio",
    "paper_url": "https://huggingface.co/papers/2512.18099",
    "authors": [],
    "stars": "2.49k",
    "details": {
      "title": "SAM Audio: Segment Anything in Audio",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.18099",
      "pdf_url": "https://arxiv.org/pdf/2512.18099",
      "github_links": [
        "https://github.com/facebookresearch/sam-audio"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18099",
      "scraped_at": "2025-12-25T01:48:42.784641"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "INTELLECT-3: Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16144",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "INTELLECT-3: Technical Report",
      "abstract": "INTELLECT-3: Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16144",
      "pdf_url": "https://arxiv.org/pdf/2512.16144",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16144",
      "scraped_at": "2025-12-25T01:48:44.617656"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
    "paper_url": "https://huggingface.co/papers/2512.20182",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "abstract": "In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20182",
      "pdf_url": "https://arxiv.org/pdf/2512.20182",
      "github_links": [
        "https://github.com/S1s-Z/FaithLens"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20182",
      "scraped_at": "2025-12-25T01:48:46.433500"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "paper_url": "https://huggingface.co/papers/2512.13472",
    "authors": [
      "Aishan Liu",
      "Wei Zhang",
      "Lin Jing",
      "Shawn Guo",
      "Jian Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Scaling Laws for Code: Every Programming Language Matters",
      "abstract": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13472",
      "pdf_url": "https://arxiv.org/pdf/2512.13472",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13472",
      "scraped_at": "2025-12-25T01:48:48.364963"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.19526",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "abstract": "QuantiPhy is the first benchmark that asks vision‚Äìlanguage models to do physics with numerical accuracy. Across 3,300+ video‚Äìtext instances, we show that today‚Äôs VLMs often sound plausible but fail quantitatively on physical reasoning tasks‚Äîthey rely more on memorized world knowledge from pretraining than on the actual video and text inputs. QuantiPhy benchmarks the critical gap between qualitative understanding and quantitative reasoning, providing a rigorous testbed for building input-faithful, physically grounded AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19526",
      "pdf_url": "https://arxiv.org/pdf/2512.19526",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19526",
      "scraped_at": "2025-12-25T01:48:50.210157"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
    "paper_url": "https://huggingface.co/papers/2512.17648",
    "authors": [
      "Luisa Bentivogli",
      "Matteo Negri",
      "Mauro Cettolo",
      "Marco Gaido",
      "spapi"
    ],
    "stars": "0",
    "details": {
      "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
      "abstract": "Already available on PyPi at https://pypi.org/project/simulstream/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17648",
      "pdf_url": "https://arxiv.org/pdf/2512.17648",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17648",
      "scraped_at": "2025-12-25T01:48:52.021839"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.20615",
    "authors": [
      "Cheng Meng",
      "Ruiqi Wu",
      "Ke Cao",
      "Tianyu Yang",
      "Xuanhua He"
    ],
    "stars": "0",
    "details": {
      "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "abstract": "project page: https://xuanhuahe.github.io/ORCA/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20615",
      "pdf_url": "https://arxiv.org/pdf/2512.20615",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20615",
      "scraped_at": "2025-12-25T01:48:53.913538"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
    "paper_url": "https://huggingface.co/papers/2512.20352",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
      "abstract": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa (Œ∫) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (Œ∫=0.907, cosine=95.3%), followed by GPT-4o (Œ∫=0.853, cosine=92.6%) and Claude (Œ∫=0.842, cosine=92.1%). All three models achieve a high agreement (Œ∫>0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20352",
      "pdf_url": "https://arxiv.org/pdf/2512.20352",
      "github_links": [
        "https://github.com/NileshArnaiya/LLM-Thematic-Analysis-Tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20352",
      "scraped_at": "2025-12-25T01:48:55.839956"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
    "paper_url": "https://huggingface.co/papers/2512.20092",
    "authors": [
      "Wenyu Huang",
      "Zhaowei Wang",
      "Yifan Xiang",
      "Baojun Wang",
      "Yiming Du"
    ],
    "stars": "0",
    "details": {
      "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
      "abstract": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20092",
      "pdf_url": "https://arxiv.org/pdf/2512.20092",
      "github_links": [
        "https://github.com/Elvin-Yiming-Du/Memory-T1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20092",
      "scraped_at": "2025-12-25T01:48:57.721739"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
    "paper_url": "https://huggingface.co/papers/2512.15031",
    "authors": [
      "Kostadin Damevski",
      "Preetha Chatterjee",
      "Rahat Rizvi Rahman",
      "Robert Zita",
      "imranraad"
    ],
    "stars": "0",
    "details": {
      "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
      "abstract": "Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecasted by tension triggers, sentiment shifts, and specific conversational patterns. We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15031",
      "pdf_url": "https://arxiv.org/pdf/2512.15031",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15031",
      "scraped_at": "2025-12-25T01:48:59.500303"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Learning to Refocus with Video Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.19823",
    "authors": [
      "Shumian Xin",
      "Xuaner Zhang",
      "Zhoutong Zhang",
      "SaiKiran Tedla"
    ],
    "stars": "5",
    "details": {
      "title": "Learning to Refocus with Video Diffusion Models",
      "abstract": "Learning to Refocus with Video Diffusion Models, SIGGRAPH Asia 2025",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19823",
      "pdf_url": "https://arxiv.org/pdf/2512.19823",
      "github_links": [
        "https://github.com/tedlasai/learn2refocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19823",
      "scraped_at": "2025-12-25T01:49:01.531230"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
    "paper_url": "https://huggingface.co/papers/2512.16093",
    "authors": [],
    "stars": "1.96k",
    "details": {
      "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
      "abstract": "TurboDiffusion : 100‚Äì200√ó acceleration in video generation on a single RTX 5090. A high-quality 5-second video can be generated in just 1.9 seconds . Efficient inference code, as well as model parameters (checkpoints) for TurboWan2.2/2.1 for Text-to-Video and Image-to-Video generation, have been open-sourced for one-click generation. The core techniques are: SageAttention + Sparse-Linear Attention (SLA) + rCM + W8A8. Github: https://github.com/thu-ml/TurboDiffusion Technical Report: https://jt-zhang.github.io/files/TurboDiffusion_Technical_Report.pdf",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16093",
      "pdf_url": "https://arxiv.org/pdf/2512.16093",
      "github_links": [
        "https://github.com/thu-ml/SageAttention",
        "https://github.com/thu-ml/SLA",
        "https://github.com/thu-ml/TurboDiffusion",
        "https://github.com/NVlabs/rcm"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16093",
      "scraped_at": "2025-12-26T01:47:41.832482"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
    "paper_url": "https://huggingface.co/papers/2512.20557",
    "authors": [],
    "stars": "28",
    "details": {
      "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "abstract": "DSR Suite delivers scalable 4D training/evaluation from real-world videos and a lightweight GSM module that injects targeted geometric priors into VLMs, markedly boosting dynamic spatial reasoning while preserving general video understanding. Key Findings: Automated pipeline converts unconstrained real videos into DSR-focused multi-choice QA, leveraging camera poses, local point clouds, object masks/poses, and 3D trajectories to build DSR-Train and the human-refined DSR-Bench. GSM integrates geometry via two stacked Q-Formers: one compresses question semantics; the other selects relevant 4D priors to form compact geometric tokens, reducing noise to the LM. Strong benchmark gains: Qwen2.5-VL-7B + GSM trained on DSR-Train achieves 58.9% on DSR-Bench, outperforming open/closed-source baselines (23.5%‚Äì38.4%), while maintaining performance on general video benchmarks. Better downstream agency: the trained model improves success on dynamic-agent tasks (e.g., MineDojo), excelling at interactions, combat, and resource acquisition in Minecraft.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20557",
      "pdf_url": "https://arxiv.org/pdf/2512.20557",
      "github_links": [
        "https://github.com/TencentARC/DSR_Suite"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20557",
      "scraped_at": "2025-12-26T01:47:43.752792"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.21252",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
      "abstract": "In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. Project Page: https://dreamontage.github.io/DreaMontage/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21252",
      "pdf_url": "https://arxiv.org/pdf/2512.21252",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21252",
      "scraped_at": "2025-12-26T01:47:45.692032"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.21094",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
      "abstract": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21094",
      "pdf_url": "https://arxiv.org/pdf/2512.21094",
      "github_links": [
        "https://github.com/NJU-LINK/T2AV-Compass/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21094",
      "scraped_at": "2025-12-26T01:47:47.669091"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.21337",
    "authors": [
      "Yu-Lun Liu",
      "He Syu",
      "Chia-Jui Chang",
      "Ting-Lin Wu",
      "Li-Zhong Szu-Tu"
    ],
    "stars": "3",
    "details": {
      "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
      "abstract": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21337",
      "pdf_url": "https://arxiv.org/pdf/2512.21337",
      "github_links": [
        "https://github.com/Sytwu/BeyondMemo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21337",
      "scraped_at": "2025-12-26T01:47:49.523545"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
    "paper_url": "https://huggingface.co/papers/2512.21338",
    "authors": [],
    "stars": "21",
    "details": {
      "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
      "abstract": "HiStream is an efficient autoregressive framework for high-resolution video generation that removes the quadratic inference bottleneck of diffusion models by reducing spatial, temporal, and timestep redundancy. It achieves state-of-the-art quality with up to 76√ó speedup at 1080p, and up to 107√ó acceleration with HiStream+, making high-resolution video generation practical and scalable. Project Page: http://haonanqiu.com/projects/HiStream.html",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21338",
      "pdf_url": "https://arxiv.org/pdf/2512.21338",
      "github_links": [
        "https://github.com/arthur-qiu/HiStream"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21338",
      "scraped_at": "2025-12-26T01:47:51.520745"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.20848",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
      "abstract": "Nemotron 3 Nano is a 30B mixture-of-experts hybrid Mamba-Transformer enabling agentic reasoning with 1M context, outperforming models in throughput and accuracy while using only a fraction of parameters per pass.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20848",
      "pdf_url": "https://arxiv.org/pdf/2512.20848",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20848",
      "scraped_at": "2025-12-26T01:47:53.427639"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.20856",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
      "abstract": "Nemotron 3 introduces Mixture-of-Experts Mamba-Transformer with 1M context, LatentMoE, MTP layers, and multi-environment RL for agentic reasoning and tool use, with open weights.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20856",
      "pdf_url": "https://arxiv.org/pdf/2512.20856",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20856",
      "scraped_at": "2025-12-26T01:47:55.389540"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
    "paper_url": "https://huggingface.co/papers/2512.20757",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20757",
      "pdf_url": "https://arxiv.org/pdf/2512.20757",
      "github_links": [
        "https://github.com/r-three/Tokenizers"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20757",
      "scraped_at": "2025-12-26T01:47:57.344257"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
    "paper_url": "https://huggingface.co/papers/2512.21004",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
      "abstract": "Code: https://github.com/Singularity0104/NExT-Vid",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21004",
      "pdf_url": "https://arxiv.org/pdf/2512.21004",
      "github_links": [
        "https://github.com/Singularity0104/NExT-Vid"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21004",
      "scraped_at": "2025-12-26T01:47:59.240235"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
    "paper_url": "https://huggingface.co/papers/2512.18832",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
      "abstract": "Explore the foundation of text-based world model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18832",
      "pdf_url": "https://arxiv.org/pdf/2512.18832",
      "github_links": [
        "https://github.com/X1AOX1A/Word2World"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18832",
      "scraped_at": "2025-12-26T01:48:01.140842"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
    "paper_url": "https://huggingface.co/papers/2512.19012",
    "authors": [
      "Yunqi Huang",
      "jackylin2012",
      "FutureMa"
    ],
    "stars": "44",
    "details": {
      "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
      "abstract": "Hi everyone! I'm Shijian, the first author of DramaBench. We're excited to share our work on evaluating creative writing, specifically drama script continuation. üé≠ Why DramaBench? Traditional \"LLM-as-a-Judge\" metrics often suffer from subjectivity and bias. We introduce a Structured Labeling + Statistical Analysis framework that evaluates scripts across 6 independent dimensions: Format, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. üìä Key Highlights: Scale: 1,103 professionally structured scripts with 8,824 evaluations across 8 SOTA models. Methodology: Instead of direct scoring, we use LLMs as structured data annotators to extract categorical labels (e.g., identifying \"driver beats\" vs \"static beats\"), which are then converted into objective metrics. This ensures high reproducibility and interpretability. üöÄ Key Findings: GPT-5.2 leads the pack: It demonstrates the strongest overall performance, ranking 1st in 3 out of 6 dimensions (Narrative, Character, and Logic), establishing itself as the most well-rounded model for creative continuation. Creative Specialization: While GPT-5.2 dominates in robustness, other models show unique strengths‚ÄîQwen3-Max specializes in Emotional Depth (92.8% arc rate), and Gemini-3-Pro excels in Conflict Handling. Actionable Feedback: Our framework identifies over 10,850 specific errors, providing a roadmap for future model fine-tuning in creative domains. We hope DramaBench establishes a more rigorous standard for evaluating AI‚Äôs creative storytelling capabilities. We‚Äôd love to hear your thoughts! üîó https://github.com/IIIIQIIII/DramaBench",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19012",
      "pdf_url": "https://arxiv.org/pdf/2512.19012",
      "github_links": [
        "https://github.com/IIIIQIIII/DramaBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19012",
      "scraped_at": "2025-12-26T01:48:03.037391"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Streaming Video Instruction Tuning",
    "paper_url": "https://huggingface.co/papers/2512.21334",
    "authors": [
      "Kaiyang Zhou",
      "Xing Sun",
      "Mengdan Zhang",
      "Peixian Chen",
      "Jiaer Xia"
    ],
    "stars": "0",
    "details": {
      "title": "Streaming Video Instruction Tuning",
      "abstract": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21334",
      "pdf_url": "https://arxiv.org/pdf/2512.21334",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21334",
      "scraped_at": "2025-12-26T01:48:04.853670"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Multi-hop Reasoning via Early Knowledge Alignment",
    "paper_url": "https://huggingface.co/papers/2512.20144",
    "authors": [
      "Xuanjing Huang",
      "Qi Luo",
      "Bo Wang",
      "Shicheng Fang",
      "Yuxin Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Multi-hop Reasoning via Early Knowledge Alignment",
      "abstract": "Multi-hop Reasoning via Early Knowledge Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20144",
      "pdf_url": "https://arxiv.org/pdf/2512.20144",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20144",
      "scraped_at": "2025-12-26T01:48:06.714545"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
    "paper_url": "https://huggingface.co/papers/2512.18470",
    "authors": [
      "Nghi D. Q. Bui",
      "Huy Phan Nhat",
      "Dung Nguyen Manh",
      "Tue Le",
      "Minh V. T. Thai"
    ],
    "stars": "0",
    "details": {
      "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
      "abstract": "Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18470",
      "pdf_url": "https://arxiv.org/pdf/2512.18470",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18470",
      "scraped_at": "2025-12-26T01:48:08.676439"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation",
    "paper_url": "https://huggingface.co/papers/2512.21227",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21227",
      "pdf_url": "https://arxiv.org/pdf/2512.21227",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21227",
      "scraped_at": "2025-12-26T01:48:10.461752"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
    "paper_url": "https://huggingface.co/papers/2512.21010",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
      "abstract": "Proposes Competitive Swiss-System Dynamics to rank LLMs across multiple benchmarks using dynamic pairings, Monte Carlo Estimated Win Score, and failure sensitivity analysis for risk-aware evaluation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21010",
      "pdf_url": "https://arxiv.org/pdf/2512.21010",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21010",
      "scraped_at": "2025-12-26T01:48:12.301846"
    },
    "scraped_date": "2025-12-26"
  },
  {
    "title": "Latent Implicit Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.21218",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Latent Implicit Visual Reasoning",
      "abstract": "TL;DR: We introduce a new method that improves visual reasoning by allowing models to implicitly learn latent visual representations, without requiring explicit supervision or additional data for these latents.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.21218",
      "pdf_url": "https://arxiv.org/pdf/2512.21218",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.21218",
      "scraped_at": "2025-12-27T01:45:27.539516"
    },
    "scraped_date": "2025-12-27"
  },
  {
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "paper_url": "https://huggingface.co/papers/2512.20605",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "abstract": "TLDR: This work reveals that autoregressive models inherently learn linearly controllable, temporally abstract action representations within their residual streams, which can be activated and composed to execute long-horizon behaviors. We leverage these emergent abstractions to introduce Internal RL, a method that reinforces semantically meaningful actions inside the residual stream of a sequence model. This enables solving sparse-reward hierarchical tasks that remain intractable for standard token-level approaches like GRPO.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20605",
      "pdf_url": "https://arxiv.org/pdf/2512.20605",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20605",
      "scraped_at": "2025-12-27T01:45:29.478209"
    },
    "scraped_date": "2025-12-27"
  },
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "paper_url": "https://huggingface.co/papers/2512.15716",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Spatia: Video Generation with Updatable Spatial Memory",
      "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as: Explicit Camera Control 3D-Aware Interactive Editing Long-horizon Scene Exploration",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15716",
      "pdf_url": "https://arxiv.org/pdf/2512.15716",
      "github_links": [
        "https://github.com/ZhaoJingjing713/Spatia"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15716",
      "scraped_at": "2025-12-27T01:45:31.478406"
    },
    "scraped_date": "2025-12-27"
  },
  {
    "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "paper_url": "https://huggingface.co/papers/2512.19995",
    "authors": [
      "Tianyi Zhou",
      "Soheil Feizi",
      "Yize Cheng",
      "Chenrui Fan",
      "Ming Li"
    ],
    "stars": "6",
    "details": {
      "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "abstract": "We extend a cognitive science-inspired episode annotation framework to an automatic, scalable, sentence-level representation that supports large-scale analysis of reasoning traces and conduct a systematic study of reasoning dynamics across a diverse set of LLMs. Moreover, we demonstrate the practical utility of episode-level representations through downstream case studies on correctness and efficiency, illustrating how reasoning dynamics can be analyzed beyond outcome-based metrics. Key Findings: When reasoning traces are analyzed at the episode level, a functional progression from abstract reasoning to concrete execution, and finally to evaluative control, consistently emerges. Episodes associated with analysis and exploration use more abstract, conceptual language and decrease steadily as reasoning progresses, while execution-oriented episodes dominate the middle of the trace through sustained concrete operations. In contrast, verification-related episodes are characterized by evaluative and meta-level language and increase toward the end of the reasoning process. Comparing reasoning and non-reasoning models, the difference is not merely how many tokens they generate, but how reasoning is structured. Non-reasoning models allocate most of their response trace to execution , with episode transitions largely following a feed-forward pattern toward implementation. In contrast, reasoning models distribute effort across analysis, exploration, execution, and verification, and exhibit frequent iterative Explore-Monitor/Verify loops. Through our correctness-oriented case study, we find that exploration reflects uncertainty and serves as a critical branching point : correct solutions more often route exploration into monitoring or re-analysis, whereas incorrect solutions tend to continue execution or terminate prematurely after exploration. Through our efficiency-oriented case study, we find that different efficient reasoning methods selectively suppress evaluation-oriented episodes and feedback loops, leading to varying degrees of divergence from the reasoning patterns of the base model. Episode-level analysis thus reveals which episodes can be removed to gain efficiency, beyond token-level pruning.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19995",
      "pdf_url": "https://arxiv.org/pdf/2512.19995",
      "github_links": [
        "https://github.com/MingLiiii/ThinkARM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19995",
      "scraped_at": "2025-12-27T01:45:33.397360"
    },
    "scraped_date": "2025-12-27"
  },
  {
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "paper_url": "https://huggingface.co/papers/2512.19949",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "How Much 3D Do Video Foundation Models Encode?",
      "abstract": "After training on large 2D videos, will video foundation models naturally encode 3D structure and ego-motion? Our study reveals that state-of-the-art video generators develop strong, generalizable 3D understanding even compared to 3D experts, despite being trained only on 2D video data. Project page: https://vidfm-3d-probe.github.io",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19949",
      "pdf_url": "https://arxiv.org/pdf/2512.19949",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19949",
      "scraped_at": "2025-12-27T01:45:35.220043"
    },
    "scraped_date": "2025-12-27"
  },
  {
    "title": "VA-œÄ: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2512.19680",
    "authors": [
      "Yicong Li",
      "Xiaoye Qu",
      "Kai Xu",
      "Qiyuan He",
      "Xinyao Liao"
    ],
    "stars": "4",
    "details": {
      "title": "VA-œÄ: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA- formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA- introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA- enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19680",
      "pdf_url": "https://arxiv.org/pdf/2512.19680",
      "github_links": [
        "https://github.com/Lil-Shake/VA-Pi"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19680",
      "scraped_at": "2025-12-27T01:45:37.023958"
    },
    "scraped_date": "2025-12-27"
  },
  {
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "paper_url": "https://huggingface.co/papers/2512.13043",
    "authors": [
      "Yuanchun Shi",
      "Junliang Xing",
      "Changhao Zhang",
      "Yijun Yang",
      "Tong Wei"
    ],
    "stars": "0",
    "details": {
      "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13043",
      "pdf_url": "https://arxiv.org/pdf/2512.13043",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13043",
      "scraped_at": "2025-12-27T01:45:38.858078"
    },
    "scraped_date": "2025-12-27"
  }
]