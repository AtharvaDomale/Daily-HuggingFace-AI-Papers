[
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-04T20:19:20.961877"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-04T20:19:23.118436"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-04T20:19:25.281652"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-04T20:19:27.476746"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-04T20:19:29.578861"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-04T20:19:31.728245"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-04T20:19:33.755115"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-04T20:19:36.010946"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-04T20:19:38.067484"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-04T20:19:40.618700"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-04T20:19:42.781709"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-04T20:19:44.858222"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-04T20:19:46.947599"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-04T20:19:49.014636"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-04T20:19:51.293128"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-04T20:19:53.370089"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-04T20:19:55.477733"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-04T20:19:57.520691"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-04T20:19:59.592819"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-04T20:20:01.699407"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-04T20:20:03.794116"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-04T20:20:05.813512"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-04T20:20:07.825029"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-04T20:20:09.829762"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-04T21:17:47.069463"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-04T21:17:49.192535"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-04T21:17:51.363234"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-04T21:17:53.492069"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-04T21:17:55.616626"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-04T21:17:57.841621"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-04T21:17:59.919631"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-04T21:18:02.035416"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-04T21:18:04.082998"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-04T21:18:06.180570"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-04T21:18:08.400168"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-04T21:18:10.545247"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-04T21:18:12.576045"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-04T21:18:14.792437"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-04T21:18:16.953344"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-04T21:18:18.990243"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-04T21:18:21.101903"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-04T21:18:23.201891"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-04T21:18:25.299264"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-04T21:18:27.368119"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-04T21:18:29.397458"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-04T21:18:31.533447"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-04T21:18:33.581102"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-04T21:18:35.717215"
    },
    "scraped_date": "2025-12-04"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "paper_url": "https://huggingface.co/papers/2511.21631",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-VL Technical Report",
      "abstract": "Qwen3-VL Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2511.21631",
      "pdf_url": "https://arxiv.org/pdf/2511.21631",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.21631",
      "scraped_at": "2025-12-05T01:45:08.800431"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "paper_url": "https://huggingface.co/papers/2512.02834",
    "authors": [
      "Xiu Li",
      "Ling Pan",
      "Siyuan Yang",
      "haoranhe",
      "breezeyoung"
    ],
    "stars": "7",
    "details": {
      "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
      "abstract": "üöÄ Excited to share our latest work: TACO üåÆ: Test-time Anti-exploration via pseudo-COunts! We found a critical instability in VLA models: even after fine-tuning, the same model can swing from 80% success to 0% just by varying initial noise! üò± Inspired by the Anti-Exploration principle, we tackle the critical instability in VLAs (success rates swinging 80%‚Üí0% due to noise!) by bridging out-of-support inference of VLAs and Offline RL.ü§ù üí° The Theoretical Bridge: We identify VLA inference fragility as an Out-of-Distribution (OOD) problem. üß† Our key insight: Test-Time Scaling is not just a heuristic‚Äîit is a theoretically equivalent realization of the Anti-Exploration principle from Offline RL! Just as Offline RL penalizes OOD actions, TACO steers the VLA policy toward high-density \"success modes\" at test time, effectively filtering out redundant behaviors. ‚ú® Highlights: 1Ô∏è‚É£ Principled: Realizes anti-exploration without gradient updates. 2Ô∏è‚É£ Effective: +16% real-world success rate; fixes distribution shifts. 3Ô∏è‚É£ Efficient: Coupled CFN verifier + KV Cache = Low Latency. 4Ô∏è‚É£ Plug-and-Play: Compatible with $\\pi_0$, OpenVLA, and more. üåê Project: https://vla-anti-exploration.github.io/ üìÑ Paper: https://arxiv.org/abs/2512.02834 üíª Code: https://github.com/breez3young/TACO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02834",
      "pdf_url": "https://arxiv.org/pdf/2512.02834",
      "github_links": [
        "https://github.com/breez3young/TACO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02834",
      "scraped_at": "2025-12-05T01:45:10.948720"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "paper_url": "https://huggingface.co/papers/2512.03442",
    "authors": [
      "Guoqi Li",
      "Jie Lou",
      "debingzhang",
      "Zhiyuan-Fan",
      "xrxing"
    ],
    "stars": "0",
    "details": {
      "title": "PretrainZero: Reinforcement Active Pretraining",
      "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03442",
      "pdf_url": "https://arxiv.org/pdf/2512.03442",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03442",
      "scraped_at": "2025-12-05T01:45:13.120150"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "paper_url": "https://huggingface.co/papers/2512.03405",
    "authors": [
      "jiakaiW",
      "heyween",
      "wrz123",
      "LongoXC",
      "Leexeo"
    ],
    "stars": "0",
    "details": {
      "title": "ViDiC: Video Difference Captioning",
      "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03405",
      "pdf_url": "https://arxiv.org/pdf/2512.03405",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03405",
      "scraped_at": "2025-12-05T01:45:15.330754"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "paper_url": "https://huggingface.co/papers/2512.04069",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
      "abstract": "TL;DR: SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation. Project page: https://spacetools.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04069",
      "pdf_url": "https://arxiv.org/pdf/2512.04069",
      "github_links": [
        "https://github.com/spacetools/SpaceTools"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04069",
      "scraped_at": "2025-12-05T01:45:17.565020"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "paper_url": "https://huggingface.co/papers/2512.03043",
    "authors": [
      "Kaixuan Fan",
      "Hongyu Li",
      "Manyuan Zhang",
      "Kaituo Feng",
      "zhengli1013"
    ],
    "stars": "43",
    "details": {
      "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
      "abstract": "Project page: https://github.com/tulerfeng/OneThinker",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03043",
      "pdf_url": "https://arxiv.org/pdf/2512.03043",
      "github_links": [
        "https://github.com/tulerfeng/OneThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03043",
      "scraped_at": "2025-12-05T01:45:19.723255"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "paper_url": "https://huggingface.co/papers/2512.03534",
    "authors": [
      "Difan Liu",
      "Yiran Xu",
      "Mamshad Nayeem Rizve",
      "Sangwoo Mo",
      "Subin Kim"
    ],
    "stars": "0",
    "details": {
      "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
      "abstract": "Scaling visuals with prompts redesigned for the scaled outputs ‚Üí break the plateau. Simply scaling visuals with a fixed prompt quickly hits a performance ceiling - generations repeatedly exhibit the same recurring failure patterns even as compute grows. By redesigning the prompt to directly address these failure patterns at the new visual scale, we break through this plateau, achieving steadily improving generations and much higher prompt-adherence for both seen and unseen rewards as compute scales.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03534",
      "pdf_url": "https://arxiv.org/pdf/2512.03534",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03534",
      "scraped_at": "2025-12-05T01:45:21.795623"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "paper_url": "https://huggingface.co/papers/2512.04040",
    "authors": [
      "Chongjian Ge",
      "Yiqun Mei",
      "kalyanks",
      "saibi",
      "YicongHong"
    ],
    "stars": "0",
    "details": {
      "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
      "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging‚Äîfor example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine‚Äìrendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04040",
      "pdf_url": "https://arxiv.org/pdf/2512.04040",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04040",
      "scraped_at": "2025-12-05T01:45:23.959532"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "paper_url": "https://huggingface.co/papers/2512.03746",
    "authors": [
      "Tao Jin",
      "Kai Jia",
      "Feng Zhang",
      "Minjie Hong",
      "Zirun Guo"
    ],
    "stars": "0",
    "details": {
      "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization (2025) Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning (2025) DeepEyesV2: Toward Agentic Multimodal Model (2025) Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch (2025) From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning (2025) MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning (2025) Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03746",
      "pdf_url": "https://arxiv.org/pdf/2512.03746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03746",
      "scraped_at": "2025-12-05T01:45:26.057231"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "paper_url": "https://huggingface.co/papers/2511.22345",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22345",
      "pdf_url": "https://arxiv.org/pdf/2511.22345",
      "github_links": [
        "https://github.com/MCG-NJU/FlowBack"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22345",
      "scraped_at": "2025-12-05T01:45:28.203868"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.03540",
    "authors": [
      "Yi Yao",
      "Hongxia Xie",
      "Bin Wen",
      "Ruoxuan Zhang",
      "twbear2024"
    ],
    "stars": "3",
    "details": {
      "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy (2025) ConsistCompose: Unified Multimodal Layout Control for Image Composition (2025) ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation (2025) DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models (2025) CharCom: Composable Identity Control for Multi-Character Story Illustration (2025) The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment (2025) Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03540",
      "pdf_url": "https://arxiv.org/pdf/2512.03540",
      "github_links": [
        "https://github.com/zhangdaxia22/CookAnything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03540",
      "scraped_at": "2025-12-05T01:45:30.339006"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "paper_url": "https://huggingface.co/papers/2512.02924",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
      "abstract": "AutoNeural-VL-1.5B ‚Äî the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software‚Äìhardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: 14√ó faster latency ( 100ms) 3√ó higher visual detail (768¬≤) Up to 7√ó more accurate",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02924",
      "pdf_url": "https://arxiv.org/pdf/2512.02924",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02924",
      "scraped_at": "2025-12-05T01:45:32.465445"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "paper_url": "https://huggingface.co/papers/2512.02807",
    "authors": [
      "Yi Yang",
      "yixuantt"
    ],
    "stars": "0",
    "details": {
      "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "abstract": "ü§Ø We know RLHF relies heavily on external rewards. But what if the model already knows when it's reasoning well? The paper SR-GRPO introduces a simple intrinsic metric, stable rank , which serves as an annotation-free quality signal for LLM output. It measures the richness and coherence of the hidden states. It is like checking the complexity of the model's brain signal to see if it is reasoning clearly. The result: The SR-GRPO alignment method significantly boosts performance on challenging mathematical reasoning and STEM tasks, eliminating the need for expensive human preference data. Just look inside! üßê Paper: SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02807",
      "pdf_url": "https://arxiv.org/pdf/2512.02807",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02807",
      "scraped_at": "2025-12-05T01:45:34.611634"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "paper_url": "https://huggingface.co/papers/2512.04032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Jina-VLM: Small Multilingual Vision Language Model",
      "abstract": "our latest multilingual vlm model at 2b size, about to release soon",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04032",
      "pdf_url": "https://arxiv.org/pdf/2512.04032",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04032",
      "scraped_at": "2025-12-05T01:45:36.734679"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "paper_url": "https://huggingface.co/papers/2512.03073",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
      "abstract": "We document a fundamental rebalancing of economic power: US dominance has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry. We identify statistically significant shifts in model properties alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03073",
      "pdf_url": "https://arxiv.org/pdf/2512.03073",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03073",
      "scraped_at": "2025-12-05T01:45:38.714190"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "paper_url": "https://huggingface.co/papers/2512.03383",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
      "abstract": "üìö  support Transformers, State Space Models (SSMs), and hybrid architectures ‚úÇÔ∏è Efficient, quantization-friendly pruning algorithms üîó One-pass framework for quantization + structured low-rank pruning üì± On-device adaptive pruning driven by real-time memory availability ‚ö° 2.7√ó‚Äì3.4√ó latency speedups üß† 4√ó‚Äì5.7√ó memory reductions",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03383",
      "pdf_url": "https://arxiv.org/pdf/2512.03383",
      "github_links": [
        "https://github.com/enyac-group/UniQL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03383",
      "scraped_at": "2025-12-05T01:45:41.479727"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "paper_url": "https://huggingface.co/papers/2511.20515",
    "authors": [
      "Tosho Hirasawa",
      "Shohei Tanaka",
      "Kuniaki Saito",
      "yushiku",
      "risashinoda"
    ],
    "stars": "0",
    "details": {
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20515",
      "pdf_url": "https://arxiv.org/pdf/2511.20515",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20515",
      "scraped_at": "2025-12-05T01:45:43.560551"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "paper_url": "https://huggingface.co/papers/2512.04072",
    "authors": [
      "Manya Wadhwa",
      "Jack Lu",
      "gregdurrett",
      "sedrickkeh",
      "Zaynes"
    ],
    "stars": "0",
    "details": {
      "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "abstract": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04072",
      "pdf_url": "https://arxiv.org/pdf/2512.04072",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04072",
      "scraped_at": "2025-12-05T01:45:45.641544"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.03979",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
      "abstract": "We present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03979",
      "pdf_url": "https://arxiv.org/pdf/2512.03979",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03979",
      "scraped_at": "2025-12-05T01:45:47.648471"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "In-Context Representation Hijacking",
    "paper_url": "https://huggingface.co/papers/2512.03771",
    "authors": [
      "yossig",
      "MichaelKar",
      "aimir",
      "tux"
    ],
    "stars": "0",
    "details": {
      "title": "In-Context Representation Hijacking",
      "abstract": "We introduce Doublespeak , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb ) with a benign token (e.g., carrot ) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., How to build a carrot? ) are internally interpreted as disallowed instructions (e.g., How to build a bomb? ), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03771",
      "pdf_url": "https://arxiv.org/pdf/2512.03771",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03771",
      "scraped_at": "2025-12-05T01:45:49.740831"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
    "paper_url": "https://huggingface.co/papers/2512.04025",
    "authors": [
      "Bohan Zhuang",
      "Weijie Wang",
      "Xi Lin",
      "Youping Gu",
      "Xiaolong Li"
    ],
    "stars": "4",
    "details": {
      "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
      "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our project page is at: https://ziplab.co/PSA/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04025",
      "pdf_url": "https://arxiv.org/pdf/2512.04025",
      "github_links": [
        "https://github.com/ziplab/Pyramid-Sparse-Attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04025",
      "scraped_at": "2025-12-05T01:45:51.734925"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.04000",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
      "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04000",
      "pdf_url": "https://arxiv.org/pdf/2512.04000",
      "github_links": [
        "https://github.com/Jialuo-Li/DIG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04000",
      "scraped_at": "2025-12-05T01:45:53.791038"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "paper_url": "https://huggingface.co/papers/2512.03794",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
      "abstract": "AdaptVision is an open-source model that leverages agentic visual tool use for dynamic visual token reduction, achieving a sota-level accuracy-efficiency trade-off across multiple VQA benchmarks. Code: https://github.com/AdaptVision/AdaptVision Model: https://huggingface.co/AdaptVision/AdaptVision-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03794",
      "pdf_url": "https://arxiv.org/pdf/2512.03794",
      "github_links": [
        "https://github.com/AdaptVision/AdaptVision"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03794",
      "scraped_at": "2025-12-05T01:45:55.865460"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "paper_url": "https://huggingface.co/papers/2512.04082",
    "authors": [
      "Tianyu Lao",
      "Ken Li",
      "Jiazhe Wei",
      "ChenyangSi",
      "wanghaofan"
    ],
    "stars": "17",
    "details": {
      "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04082",
      "pdf_url": "https://arxiv.org/pdf/2512.04082",
      "github_links": [
        "https://github.com/JiazheWei/PosterCopilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04082",
      "scraped_at": "2025-12-05T01:45:58.004342"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2511.20494",
    "authors": [
      "Artur Janicki",
      "j-hoscilowic"
    ],
    "stars": "0",
    "details": {
      "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
      "abstract": "We introduce the Adversarial Confusion Attack as a new mechanism for protecting websites from MLLM-powered AI Agents. Embedding these ‚ÄúAdversarial CAPTCHAs‚Äù into web content pushes models into systemic decoding failures, from confident hallucinations to full incoherence. The perturbations disrupt all white-box models we test and transfer to proprietary systems like GPT-5 in the full-image setting. Technically, the attack uses PGD to maximize next-token entropy across a small surrogate ensemble of MLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20494",
      "pdf_url": "https://arxiv.org/pdf/2511.20494",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20494",
      "scraped_at": "2025-12-05T01:46:00.069008"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-05T21:45:27.136896"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-05T21:45:29.003036"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-05T21:45:30.836133"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "32",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-05T21:45:32.680328"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "87",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-05T21:45:34.526238"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-05T21:45:36.527491"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "230",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-05T21:45:38.358063"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-05T21:45:40.291130"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-05T21:45:42.094547"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-05T21:45:43.969839"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-05T21:45:45.799525"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-05T21:45:47.632081"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "10",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-05T21:45:49.509345"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-05T21:45:51.439704"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-05T21:45:53.263163"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-05T21:45:55.240172"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "742",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-05T21:45:57.093539"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-05T21:45:58.872854"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-05T21:46:00.832911"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-05T21:46:02.669896"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-05T21:46:04.538104"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-05T21:46:06.330856"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "5",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-05T21:46:08.128956"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-05T21:46:09.903559"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "146",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-05T21:46:11.776860"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-05T21:46:13.573649"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-05T21:46:15.445157"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-05T21:46:17.298562"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-05T21:46:19.102566"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-05T21:46:20.916543"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-05T21:46:22.711806"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-05T21:46:24.551712"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-05T21:46:26.346693"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "1",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-05T21:46:28.138846"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-05T21:46:29.948938"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-05T21:46:31.735937"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-05T21:46:33.515128"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-05T21:46:35.309571"
    },
    "scraped_date": "2025-12-05"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-06T01:39:15.192900"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-06T01:39:17.129412"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "66",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-06T01:39:19.022592"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "33",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-06T01:39:20.928064"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "87",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-06T01:39:22.780299"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-06T01:39:24.694509"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "230",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-06T01:39:26.554770"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "20",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-06T01:39:28.390259"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "38",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-06T01:39:30.642592"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-06T01:39:32.487748"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-06T01:39:34.345229"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-06T01:39:36.204083"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-06T01:39:38.084093"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "10",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-06T01:39:39.989738"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-06T01:39:41.819845"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "742",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-06T01:39:43.747719"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-06T01:39:45.583608"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-06T01:39:47.490249"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-06T01:39:49.355078"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-06T01:39:51.188158"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-06T01:39:54.131792"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-06T01:39:56.410809"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-06T01:39:58.256378"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "5",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-06T01:40:00.105252"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-06T01:40:01.956187"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-06T01:40:03.845611"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "148",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-06T01:40:05.744152"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-06T01:40:07.557068"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-06T01:40:09.385040"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-06T01:40:11.210013"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-06T01:40:13.057458"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-06T01:40:14.900057"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-06T01:40:16.708726"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "1",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-06T01:40:18.512814"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-06T01:40:20.394175"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-06T01:40:22.227829"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-06T01:40:24.083221"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-06T01:40:25.891953"
    },
    "scraped_date": "2025-12-06"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "0",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-07T01:52:46.094380"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-07T01:52:48.210822"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "69",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-07T01:52:50.202899"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/InternLM/ARM-Thinker",
        "https://github.com/open-compass/VLMEvalKit/pull/1334"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-07T01:52:52.202235"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "101",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-07T01:52:54.160261"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "116",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-07T01:52:56.246187"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "350",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-07T01:52:58.305330"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "33",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-07T01:53:00.340224"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "40",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-07T01:53:02.544481"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-07T01:53:04.591650"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-07T01:53:06.578999"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-07T01:53:08.537607"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-07T01:53:10.438186"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-07T01:53:12.449058"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-07T01:53:14.374069"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-07T01:53:16.262997"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-07T01:53:18.204429"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "12",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-07T01:53:20.103764"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "747",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-07T01:53:22.017875"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-07T01:53:23.974486"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-07T01:53:25.852135"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "153",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-07T01:53:27.762613"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-07T01:53:29.694599"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-07T01:53:31.626340"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-07T01:53:33.570046"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-07T01:53:35.457087"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-07T01:53:37.370394"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "17",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-07T01:53:39.404271"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-07T01:53:41.262254"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-07T01:53:43.056982"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-07T01:53:44.927975"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-07T01:53:46.922593"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-07T01:53:48.808339"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-07T01:53:50.767147"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "2",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-07T01:53:52.633019"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-07T01:53:54.486599"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-07T01:53:56.300727"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-07T01:53:58.243503"
    },
    "scraped_date": "2025-12-07"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "paper_url": "https://huggingface.co/papers/2512.04677",
    "authors": [
      "Shifeng Zhang",
      "Fangtai Wu",
      "Hailong Guo",
      "Yubo Huang",
      "jamesliu1217"
    ],
    "stars": "348",
    "details": {
      "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04677",
      "pdf_url": "https://arxiv.org/pdf/2512.04677",
      "github_links": [
        "https://github.com/Alibaba-Quark/LiveAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04677",
      "scraped_at": "2025-12-08T01:45:50.074934"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "paper_url": "https://huggingface.co/papers/2512.04324",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
      "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04324",
      "pdf_url": "https://arxiv.org/pdf/2512.04324",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04324",
      "scraped_at": "2025-12-08T01:45:52.315362"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "paper_url": "https://huggingface.co/papers/2512.04987",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04987",
      "pdf_url": "https://arxiv.org/pdf/2512.04987",
      "github_links": [
        "https://github.com/nex-agi/Nex-N1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04987",
      "scraped_at": "2025-12-08T01:45:54.258639"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.05111",
    "authors": [],
    "stars": "50",
    "details": {
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "abstract": "üè† Github Repo: https://github.com/InternLM/ARM-Thinker ‚≠êÔ∏è For Agent Evaluation: https://github.com/open-compass/VLMEvalKit/pull/1334 (We added a new feature to VLMEvalKit that supports evaluating diverse models within the ARM-Thinker agent flow, including full tool-use and multi-step reasoning.)",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05111",
      "pdf_url": "https://arxiv.org/pdf/2512.05111",
      "github_links": [
        "https://github.com/open-compass/VLMEvalKit/pull/1334",
        "https://github.com/InternLM/ARM-Thinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05111",
      "scraped_at": "2025-12-08T01:45:56.554505"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "paper_url": "https://huggingface.co/papers/2512.02589",
    "authors": [],
    "stars": "670",
    "details": {
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "abstract": "PaperDebugger lives inside Overleaf and rewrites your paper with you in real time! Paper: https://arxiv.org/abs/2512.02589 Github: https://github.com/PaperDebugger/PaperDebugger Enhancer Model XtraGPT: https://huggingface.co/Xtra-Computing/XtraGPT-7B",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02589",
      "pdf_url": "https://arxiv.org/pdf/2512.02589",
      "github_links": [
        "https://github.com/PaperDebugger/PaperDebugger"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02589",
      "scraped_at": "2025-12-08T01:45:58.852851"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "paper_url": "https://huggingface.co/papers/2512.04678",
    "authors": [
      "Hao Ouyang",
      "Haobo Li",
      "Yanhong Zeng",
      "Yunhong Lu",
      "qiuyuu"
    ],
    "stars": "111",
    "details": {
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "abstract": "homepage: https://reward-forcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04678",
      "pdf_url": "https://arxiv.org/pdf/2512.04678",
      "github_links": [
        "https://github.com/JaydenLyh/Reward-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04678",
      "scraped_at": "2025-12-08T01:46:01.038360"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04926",
    "authors": [],
    "stars": "180",
    "details": {
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "abstract": "Why denoise synchronously? SFD introduces an asynchronous paradigm where Semantics lead Texture. Time to rethink the generation ordering in LDMs! Paper: https://arxiv.org/abs/2512.04926 Github: https://github.com/yuemingPAN/SFD",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04926",
      "pdf_url": "https://arxiv.org/pdf/2512.04926",
      "github_links": [
        "https://github.com/yuemingPAN/SFD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04926",
      "scraped_at": "2025-12-08T01:46:03.070805"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.03000",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
      "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03000",
      "pdf_url": "https://arxiv.org/pdf/2512.03000",
      "github_links": [
        "https://github.com/Dynamics-X/DynamicVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03000",
      "scraped_at": "2025-12-08T01:46:05.376820"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "paper_url": "https://huggingface.co/papers/2512.05081",
    "authors": [
      "Heeji Yoon",
      "Jisu Nam",
      "Paul Hyunbin Cho",
      "Wooseok Jang",
      "YJ-142150"
    ],
    "stars": "0",
    "details": {
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "abstract": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05081",
      "pdf_url": "https://arxiv.org/pdf/2512.05081",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05081",
      "scraped_at": "2025-12-08T01:46:07.678018"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "paper_url": "https://huggingface.co/papers/2512.05060",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "abstract": "code: https://github.com/hustvl/4DLangVGGT webpage: https://hustvl.github.io/4DLangVGGT/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05060",
      "pdf_url": "https://arxiv.org/pdf/2512.05060",
      "github_links": [
        "https://github.com/hustvl/4DLangVGGT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05060",
      "scraped_at": "2025-12-08T01:46:09.658067"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "paper_url": "https://huggingface.co/papers/2512.05113",
    "authors": [
      "Yu-Lun Liu",
      "Wei-Lun Chao",
      "Chung-Ho Wu",
      "Yi-Chuan Huang",
      "chien90190"
    ],
    "stars": "0",
    "details": {
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "abstract": "Splannequin transforms imperfect Mannequin-Challenge videos into completely frozen videos.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05113",
      "pdf_url": "https://arxiv.org/pdf/2512.05113",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05113",
      "scraped_at": "2025-12-08T01:46:11.943491"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.04504",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
      "abstract": "Project page is available at https://thu-ml.github.io/ultraimage.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04504",
      "pdf_url": "https://arxiv.org/pdf/2512.04504",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04504",
      "scraped_at": "2025-12-08T01:46:15.287360"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "paper_url": "https://huggingface.co/papers/2512.04797",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
      "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04797",
      "pdf_url": "https://arxiv.org/pdf/2512.04797",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04797",
      "scraped_at": "2025-12-08T01:46:17.530303"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "paper_url": "https://huggingface.co/papers/2512.05106",
    "authors": [
      "Vitor Guizilini",
      "Vishal M. Patel",
      "Mingyuan Zhou",
      "Charles Ochoa",
      "Yu Zeng"
    ],
    "stars": "0",
    "details": {
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05106",
      "pdf_url": "https://arxiv.org/pdf/2512.05106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05106",
      "scraped_at": "2025-12-08T01:46:19.898642"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.05000",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "abstract": "Project page: https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05000",
      "pdf_url": "https://arxiv.org/pdf/2512.05000",
      "github_links": [
        "https://github.com/huawei-bayerlab/windowseat-reflection-removal"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05000",
      "scraped_at": "2025-12-08T01:46:22.128948"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05103",
      "pdf_url": "https://arxiv.org/pdf/2512.05103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05103",
      "scraped_at": "2025-12-08T01:46:24.087696"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "paper_url": "https://huggingface.co/papers/2512.04829",
    "authors": [
      "Jun Wang",
      "Xihan Li",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "alexmaraval"
    ],
    "stars": "0",
    "details": {
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "abstract": "AlphaEvolve is amazing and tackles hard-to-solve and easy-to-evaluate problems! But, many math problems are actually hard-to-solve and hard-to-evaluate! Here, we can't do much trial and error; we need something more efficient - because trying one solution can take DAYS! We propose a framework for that and use it in sphere packing; Hilbert's 18th problem! We recover new bounds not known before! ü•≥ü•≥ The amazing part is we don't use LLMs! We used model-based search!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04829",
      "pdf_url": "https://arxiv.org/pdf/2512.04829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04829",
      "scraped_at": "2025-12-08T01:46:26.031383"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.04746",
    "authors": [],
    "stars": "749",
    "details": {
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abstract": "Extremely low-bit quantization for LLMs. Check out https://github.com/intel/auto-round",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04746",
      "pdf_url": "https://arxiv.org/pdf/2512.04746",
      "github_links": [
        "https://github.com/intel/auto-round"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04746",
      "scraped_at": "2025-12-08T01:46:28.371364"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_url": "https://huggingface.co/papers/2512.05112",
    "authors": [
      "Ziyu Guo",
      "Zhuofan Zong",
      "Renrui Zhang",
      "mickyhimself",
      "CaraJ"
    ],
    "stars": "13",
    "details": {
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "abstract": "üî• Project Page: https://github.com/CaraJ7/DraCo",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05112",
      "pdf_url": "https://arxiv.org/pdf/2512.05112",
      "github_links": [
        "https://github.com/CaraJ7/DraCo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05112",
      "scraped_at": "2025-12-08T01:46:30.273623"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "paper_url": "https://huggingface.co/papers/2512.04220",
    "authors": [
      "Christos Thrampoulidis",
      "Yi Ren",
      "Boying Gong",
      "Yushu Li",
      "Wenlong Deng"
    ],
    "stars": "0",
    "details": {
      "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
      "abstract": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04220",
      "pdf_url": "https://arxiv.org/pdf/2512.04220",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04220",
      "scraped_at": "2025-12-08T01:46:32.517669"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "paper_url": "https://huggingface.co/papers/2512.04356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
      "abstract": "Project page: https://kpc0810.github.io/santa/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04356",
      "pdf_url": "https://arxiv.org/pdf/2512.04356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04356",
      "scraped_at": "2025-12-08T01:46:34.496564"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "paper_url": "https://huggingface.co/papers/2512.05016",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05016",
      "pdf_url": "https://arxiv.org/pdf/2512.05016",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05016",
      "scraped_at": "2025-12-08T01:46:36.498721"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "paper_url": "https://huggingface.co/papers/2512.03052",
    "authors": [
      "Qingxiang Lin",
      "Haolin Liu",
      "Zibo Zhao",
      "Yunfei Zhao",
      "Zeqiang Lai"
    ],
    "stars": "157",
    "details": {
      "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
      "abstract": "3D Shape foundation model",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03052",
      "pdf_url": "https://arxiv.org/pdf/2512.03052",
      "github_links": [
        "https://github.com/Zeqiang-Lai/LATTICE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03052",
      "scraped_at": "2025-12-08T01:46:38.851452"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2512.04981",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "abstract": "We introduce: 1Ô∏è‚É£ A 1,024-prompt benchmark across 4 linguistic complexity levels 2Ô∏è‚É£ Fine-grained, systematic demographic (gender, age, ethnicity, physical appearance) bias diagnostics 3Ô∏è‚É£ FairPRO, a training-free meta-prompting framework that enables self-auditing and fairness-aware system prompts üå≥ Project page: https://fairpro-t2i.github.io üë©üèª‚Äçüíª Github: https://github.com/nahyeonkaty/fairpro",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04981",
      "pdf_url": "https://arxiv.org/pdf/2512.04981",
      "github_links": [
        "https://github.com/nahyeonkaty/fairpro"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04981",
      "scraped_at": "2025-12-08T01:46:41.174546"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "paper_url": "https://huggingface.co/papers/2512.02631",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
      "abstract": "üéØ Code: https://github.com/WzcTHU/SeeNav-Agent ü§ó Model: https://huggingface.co/wangzc9865/SeeNav-Agent",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02631",
      "pdf_url": "https://arxiv.org/pdf/2512.02631",
      "github_links": [
        "https://github.com/WzcTHU/SeeNav-Agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02631",
      "scraped_at": "2025-12-08T01:46:43.171361"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "paper_url": "https://huggingface.co/papers/2511.22826",
    "authors": [
      "Deepti Ghadiyaram",
      "Xavier Thomas",
      "Arjun Reddy Akula",
      "Chaitanya Chakka",
      "Tianle Chen"
    ],
    "stars": "0",
    "details": {
      "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
      "abstract": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration As the famous George Orwell quote goes - \"all animals are equal but some animals are more equal than others\", we indeed find that though present-day MLLMs claim to process all modalities equally, some modalities (like text then visual) are more heavily utilized than others. Consider a video of birds chirping. If blindfolded, could you describe what you hear? Similarly, if wearing noise-canceling headphones, could you describe what you see? . In all these situations, most humans can easily describe the events and rely on the available modality. But regarding present-day Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities?. Most models are trained on datasets that overwhelmingly assume that all available modalities are aligned. To rigorously study this, we introduce MMA-Bench to systematically control the presence or alignment of one modality at a time. We use structured contradictions to reveal if MLLMs are truly multi-modal or take shortcuts during cross-modal reasoning tasks. Key Findings: The Hierarchy of Modalities Our analysis revealed a critical flaw: despite being trained as multimodal systems, all MLLMs collapse when modalities conflict. Text Dominance: We found that small text distractions cripple models, ignoring clear audio-visual cues. White-box interpretability reveals that textual tokens consistently exhibit the highest attention magnitudes. This indicates that textual priors frequently override multimodal signals. Seeing Without Listening: Models rely almost exclusively on vision, i.e., \"seeing without listening,\" resulting in strong degradation of audio performance when modalities conflict. Visual Fallback: When the requested modality is absent (e.g., asking about audio in a silent video), models do not reliably follow the desired modality instructions and instead fall back on whichever modality remains informative - typically the visual stream. Brittle Modality Integration: MLLMs fail ungracefully if any one modality is perturbed. This brittleness makes these models susceptible to simple text-, vision-, and audio-based data poisoning or prompt attacks. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. This prompt-conditioned supervision yields adaptive attention redistribution toward the queried modality rather than static bias suppression. We show that targeted modality alignment is more effective than scaling alone. Code and dataset will be available at cskyl.github.io/MMA-Bench/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2511.22826",
      "pdf_url": "https://arxiv.org/pdf/2511.22826",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.22826",
      "scraped_at": "2025-12-08T01:46:45.529468"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "paper_url": "https://huggingface.co/papers/2512.04390",
    "authors": [
      "Munchurl Kim",
      "Jihyong Oh",
      "Geunhyuk Youk"
    ],
    "stars": "22",
    "details": {
      "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
      "abstract": "Project page: https://kaist-viclab.github.io/fmanetpp_site/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04390",
      "pdf_url": "https://arxiv.org/pdf/2512.04390",
      "github_links": [
        "https://github.com/KAIST-VICLab/FMA-Net-PlusPlus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04390",
      "scraped_at": "2025-12-08T01:46:47.448319"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.05076",
    "authors": [
      "Jan Ackermann",
      "Tong Wu",
      "Shengqu Cai",
      "Qihang Zhang",
      "Yiming Wang"
    ],
    "stars": "0",
    "details": {
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05076",
      "pdf_url": "https://arxiv.org/pdf/2512.05076",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05076",
      "scraped_at": "2025-12-08T01:46:49.455630"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.04515",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
      "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04515",
      "pdf_url": "https://arxiv.org/pdf/2512.04515",
      "github_links": [
        "https://github.com/AIGeeksGroup/EgoLCD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04515",
      "scraped_at": "2025-12-08T01:46:52.138190"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "paper_url": "https://huggingface.co/papers/2512.01803",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "abstract": "Project webpage: https://xthomasbu.github.io/video-gen-evals/ Dataset: https://huggingface.co/datasets/dghadiya/TAG-Bench-Video",
      "arxiv_page_url": "https://arxiv.org/abs/2512.01803",
      "pdf_url": "https://arxiv.org/pdf/2512.01803",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.01803",
      "scraped_at": "2025-12-08T01:46:54.052433"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "paper_url": "https://huggingface.co/papers/2512.04844",
    "authors": [
      "Nikolaos Aletras",
      "Aline Villavicencio",
      "Terufumi Morishita",
      "atsuki-yamaguchi"
    ],
    "stars": "0",
    "details": {
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "abstract": "Our code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our proposed method (SSU) and all baselines are available on GitHub: https://github.com/gucci-j/ssu .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04844",
      "pdf_url": "https://arxiv.org/pdf/2512.04844",
      "github_links": [
        "https://github.com/gucci-j/ssu"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04844",
      "scraped_at": "2025-12-08T01:46:56.366922"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "paper_url": "https://huggingface.co/papers/2512.03683",
    "authors": [
      "Sezer Karaoglu",
      "Ngo Anh Vien",
      "Yue Li",
      "Xiaoyan Xing",
      "melisocal"
    ],
    "stars": "0",
    "details": {
      "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
      "abstract": "Project page: https://gaussianblender.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03683",
      "pdf_url": "https://arxiv.org/pdf/2512.03683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03683",
      "scraped_at": "2025-12-08T01:46:58.622222"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_url": "https://huggingface.co/papers/2512.05110",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05110",
      "pdf_url": "https://arxiv.org/pdf/2512.05110",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05110",
      "scraped_at": "2025-12-08T01:47:00.882199"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "paper_url": "https://huggingface.co/papers/2512.05049",
    "authors": [
      "Nan-Yow Chen",
      "Kuo-Chung Peng",
      "Chun-Hua Lin",
      "Yu-Chao Hsu",
      "Jim137"
    ],
    "stars": "0",
    "details": {
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "abstract": "A follow-up to our earlier QKAN research, this work explores how quantum-inspired activations can enhance classical LSTM models. With single-qubit DARUAN modules and QKAN-based gating, QKAN-LSTM cuts parameters by up to 79% while improving performance on physics-based and real-world telecom datasets. We also introduce HQKAN-LSTM for hierarchical sequence modeling. Excited to share this with the community!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05049",
      "pdf_url": "https://arxiv.org/pdf/2512.05049",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05049",
      "scraped_at": "2025-12-08T01:47:03.075799"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "paper_url": "https://huggingface.co/papers/2512.03125",
    "authors": [
      "Radu Marculescu",
      "Mustafa Munir",
      "Xiwen Wei"
    ],
    "stars": "2",
    "details": {
      "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
      "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03125",
      "pdf_url": "https://arxiv.org/pdf/2512.03125",
      "github_links": [
        "https://github.com/Christina200/MoDE-official"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03125",
      "scraped_at": "2025-12-08T01:47:05.025805"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "paper_url": "https://huggingface.co/papers/2512.04124",
    "authors": [
      "Gilbert Fridgen",
      "Igor Tchappi",
      "Amir Sartipi",
      "Hanna Marxen",
      "akhadangi"
    ],
    "stars": "0",
    "details": {
      "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
      "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04124",
      "pdf_url": "https://arxiv.org/pdf/2512.04124",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04124",
      "scraped_at": "2025-12-08T01:47:07.334262"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "paper_url": "https://huggingface.co/papers/2511.20233",
    "authors": [
      "Yaxin Fan",
      "Hongzhan Lin",
      "Jing Ma",
      "Gao Wei",
      "Chuyi Kong"
    ],
    "stars": "0",
    "details": {
      "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
      "abstract": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in the backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2511.20233",
      "pdf_url": "https://arxiv.org/pdf/2511.20233",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2511.20233",
      "scraped_at": "2025-12-08T01:47:09.230588"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "paper_url": "https://huggingface.co/papers/2512.03915",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
      "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03915",
      "pdf_url": "https://arxiv.org/pdf/2512.03915",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03915",
      "scraped_at": "2025-12-08T01:47:11.482221"
    },
    "scraped_date": "2025-12-08"
  },
  {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "paper_url": "https://huggingface.co/papers/2512.05150",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
      "abstract": "Taming 20B full-parameter few-step training with self-adversarial flows! üëèüèª One-model Simplicity: We eliminate the need for auxiliary networks (discriminators, teachers, fake score estimators...), everything in one model! Scalability on Large Models: We transform Qwen-Image-20B into high-quality few-step generators by full-parameter training (Optimized for human figure generation!). Checkout our 2-NFE images generated by our TwinFlow-Qwen-Image! üëá We are also working on Z-Image-Turbo , stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05150",
      "pdf_url": "https://arxiv.org/pdf/2512.05150",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05150",
      "scraped_at": "2025-12-09T01:44:30.991822"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
    "paper_url": "https://huggingface.co/papers/2512.05965",
    "authors": [
      "Ziyu Guo",
      "Manyuan Zhang",
      "Longin-Yu",
      "zhengli1013",
      "appletea2333"
    ],
    "stars": "25",
    "details": {
      "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
      "abstract": "Instruction-based image editing has emerged as a prominent research area. Benefiting from image generation foundation models, it has achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to \"think\" while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions, followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produces the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05965",
      "pdf_url": "https://arxiv.org/pdf/2512.05965",
      "github_links": [
        "https://github.com/appletea233/EditThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05965",
      "scraped_at": "2025-12-09T01:44:32.917190"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "paper_url": "https://huggingface.co/papers/2512.02580",
    "authors": [
      "Yang Li",
      "Shuai Zhang",
      "Yuchen Liu",
      "Jinyang Wu",
      "Changpeng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
      "abstract": "üöÄ [New Paper] CAPO: From Imitation to Discrimination ‚Äì Rethinking Advantage in RL Early RL training often suffers from instability due to \"mixed signals\" (simultaneous positive & negative feedback). Inspired by child cognitive development, we propose CAPO (Curriculum Advantage Policy Optimization) . ‚ú® The Core Intuition: Instead of a static curriculum, we leverage Advantage values to create a dynamic, two-phase process: 1Ô∏è‚É£ Imitation Phase: Train on Positive Advantage only. This reduces variance and establishes a stable behavioral foundation (Imitate to learn). 2Ô∏è‚É£ Discrimination Phase: Introduce Negative Signals later. [cite_start]This restores unbiased estimation and refines decision boundaries (Discriminate to generalize). üìà Highlights: Plug-and-Play: Delivers consistent gains when compatible with GRPO, PPO, RLOO, & Reinforce++ . Cross-Domain: Not just for Math! CAPO demonstrates impressive generalization on Multimodal GUI Agent tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02580",
      "pdf_url": "https://arxiv.org/pdf/2512.02580",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02580",
      "scraped_at": "2025-12-09T01:44:34.806051"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
    "paper_url": "https://huggingface.co/papers/2512.04810",
    "authors": [
      "Qi Tian",
      "Lingxi Xie",
      "Jianbo Ouyang",
      "Longhui Wei",
      "Xin He"
    ],
    "stars": "13",
    "details": {
      "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
      "abstract": "The project page https://emma-umm.github.io/emma/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04810",
      "pdf_url": "https://arxiv.org/pdf/2512.04810",
      "github_links": [
        "https://github.com/umm-emma/emma"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04810",
      "scraped_at": "2025-12-09T01:44:36.824643"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
    "paper_url": "https://huggingface.co/papers/2512.04784",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
      "abstract": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04784",
      "pdf_url": "https://arxiv.org/pdf/2512.04784",
      "github_links": [
        "https://github.com/X-GenGroup/PaCo-RL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04784",
      "scraped_at": "2025-12-09T01:44:38.814591"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
    "paper_url": "https://huggingface.co/papers/2512.05905",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
      "abstract": "SCAIL is a new framework for studio-grade character animation that uses a novel 3D pose representation and full-sequence context injection to deliver more stable, realistic motion transfer under complex and cross-identity scenarios.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05905",
      "pdf_url": "https://arxiv.org/pdf/2512.05905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05905",
      "scraped_at": "2025-12-09T01:44:40.683227"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.05591",
    "authors": [
      "Zijia Lin",
      "Tiehua Mei",
      "Minxuan Lv",
      "Leiyu Pan",
      "Suu"
    ],
    "stars": "0",
    "details": {
      "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
      "abstract": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05591",
      "pdf_url": "https://arxiv.org/pdf/2512.05591",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05591",
      "scraped_at": "2025-12-09T01:44:42.544097"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.05044",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
      "abstract": "Project Page: https://ivg-yanranzhang.github.io/MoRe4D/ Github Repo: https://github.com/Zhangyr2022/MoRe4D The dataset is coming soon. Stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05044",
      "pdf_url": "https://arxiv.org/pdf/2512.05044",
      "github_links": [
        "https://github.com/Zhangyr2022/MoRe4D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05044",
      "scraped_at": "2025-12-09T01:44:44.580407"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04563",
    "authors": [
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Hengzhu Tang",
      "CUDAOUTOFMEMORY",
      "Starrrrrry"
    ],
    "stars": "5",
    "details": {
      "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
      "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04563",
      "pdf_url": "https://arxiv.org/pdf/2512.04563",
      "github_links": [
        "https://github.com/zhangzef/COOPER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04563",
      "scraped_at": "2025-12-09T01:44:46.516932"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
    "paper_url": "https://huggingface.co/papers/2512.00473",
    "authors": [
      "Zilong Huang",
      "Dongzhi Jiang",
      "Yuncheng Guo",
      "Leiqi Zhu",
      "Junyan Ye"
    ],
    "stars": "65",
    "details": {
      "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
      "abstract": "arXiv: https://arxiv.org/abs/2512.00473 code: https://github.com/yejy53/RealGen project page: https://yejy53.github.io/RealGen/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.00473",
      "pdf_url": "https://arxiv.org/pdf/2512.00473",
      "github_links": [
        "https://github.com/yejy53/RealGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.00473",
      "scraped_at": "2025-12-09T01:44:48.413384"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Self-Improving VLM Judges Without Human Annotations",
    "paper_url": "https://huggingface.co/papers/2512.05145",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving VLM Judges Without Human Annotations",
      "abstract": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05145",
      "pdf_url": "https://arxiv.org/pdf/2512.05145",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05145",
      "scraped_at": "2025-12-09T01:44:50.242883"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "paper_url": "https://huggingface.co/papers/2512.05927",
    "authors": [
      "Anirudha Majumdar",
      "Ola Shorinwa",
      "Micah Baker",
      "Tenny Yin",
      "Zhiting Mei"
    ],
    "stars": "0",
    "details": {
      "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
      "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05927",
      "pdf_url": "https://arxiv.org/pdf/2512.05927",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05927",
      "scraped_at": "2025-12-09T01:44:52.149809"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "paper_url": "https://huggingface.co/papers/2512.05343",
    "authors": [
      "Marc Pollefeys",
      "Or Litany",
      "Ian Huang",
      "Francis Engelmann",
      "efedele"
    ],
    "stars": "0",
    "details": {
      "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
      "abstract": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at this https URL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05343",
      "pdf_url": "https://arxiv.org/pdf/2512.05343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05343",
      "scraped_at": "2025-12-09T01:44:54.015630"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.02835",
    "authors": [
      "Shengju Qian",
      "Weikai Chen",
      "Lingting Zhu",
      "Yingda Yin",
      "Tangerine24"
    ],
    "stars": "5",
    "details": {
      "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
      "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02835",
      "pdf_url": "https://arxiv.org/pdf/2512.02835",
      "github_links": [
        "https://github.com/Clementine24/ReVSeg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02835",
      "scraped_at": "2025-12-09T01:44:55.832519"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
    "paper_url": "https://huggingface.co/papers/2512.05356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
      "abstract": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05356",
      "pdf_url": "https://arxiv.org/pdf/2512.05356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05356",
      "scraped_at": "2025-12-09T01:44:57.637941"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
    "paper_url": "https://huggingface.co/papers/2512.03514",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
      "abstract": "Can we build universal document retrievers that maintain strong results across typologically diverse languages without losing English performance. This question led us to design synthetic training data and multilingual benchmarks to teach a model to match documents across scripts and formats. We are excited to launch NetraEmbed our SoTA model for multimodal multilingual document retrieval along with the M3DR: Towards Universal Multilingual Multimodal Document Retrieval paper. The release includes the NetraEmbed model which produces a single dense embedding with matryoshka support at 768,1536 and 2560 dimensions and the ColNetraEmbed model which produces patch level multivector embeddings. Both models are finetuned on Gemma3-4B-it and gained ~150% improvement over baselines. To measure progress we also built the NayanaIR Benchmark with 22 multilingual and 1 cross lingual dataset and documented the full framework in the M3DR paper . Links Blog: https://www.cognitivelab.in/blog/introducing-netraembed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03514",
      "pdf_url": "https://arxiv.org/pdf/2512.03514",
      "github_links": [
        "https://github.com/adithya-s-k/colpali"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03514",
      "scraped_at": "2025-12-09T01:44:59.738369"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
    "paper_url": "https://huggingface.co/papers/2512.05277",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
      "abstract": "This work introduces TAD, the first benchmark targeting temporal understanding in ego-centric autonomous-driving videos, evaluates SoTA VLMs, and boosts their performance with two training-free motion-reasoning methods (Scene-CoT and TCogMap).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05277",
      "pdf_url": "https://arxiv.org/pdf/2512.05277",
      "github_links": [
        "https://github.com/vbdi/tad_bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05277",
      "scraped_at": "2025-12-09T01:45:01.601687"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "paper_url": "https://huggingface.co/papers/2512.05564",
    "authors": [
      "Yuhao Cheng",
      "Terry Jingchen Zhang",
      "Jing Wang",
      "Panwen Hu",
      "Zijun Wang"
    ],
    "stars": "0",
    "details": {
      "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
      "abstract": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05564",
      "pdf_url": "https://arxiv.org/pdf/2512.05564",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05564",
      "scraped_at": "2025-12-09T01:45:04.335515"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.05409",
    "authors": [
      "Minghui Yu",
      "Jinyuan Shi",
      "Hantao Huang",
      "Hao Zeng",
      "Ruixuan Huang"
    ],
    "stars": "0",
    "details": {
      "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
      "abstract": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05409",
      "pdf_url": "https://arxiv.org/pdf/2512.05409",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05409",
      "scraped_at": "2025-12-09T01:45:06.133971"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
    "paper_url": "https://huggingface.co/papers/2512.04694",
    "authors": [
      "Salih Tileylioglu",
      "Erdem Akag√ºnd√ºz",
      "Bevan Deniz Cilgin",
      "Barisylmz"
    ],
    "stars": "0",
    "details": {
      "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
      "abstract": "This work presents a transformer-based generative model for complex time-series signals, with experiments on seismic accelerometer data. Key idea: treat seismic waveforms as structured high-dimensional sequences and learn a latent trajectory that captures both physical dynamics and long-range temporal dependencies.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04694",
      "pdf_url": "https://arxiv.org/pdf/2512.04694",
      "github_links": [
        "https://github.com/brsylmz23/TimesNet-Gen/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04694",
      "scraped_at": "2025-12-09T01:45:07.983185"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.03667",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
      "abstract": "Colonoscopy saves lives ‚Äî but AI for colonoscopy is still far from intelligent. We are excited to launch the Colon-X project, an open initiative aimed at advancing multimodal intelligence in colonoscopy and beyond. Beyond serving as a community-wide data foundation, we're focused on a critical yet under-explored transition ‚Äì evolving from multimodal understanding to clinical reasoning. Keywords: Multimodal Colonoscopy Analysis, Multimodal Understanding, Clinical Reasoning, Reinforcement Learning, Multimodal Benchmark, AI Healthcare, and Abdomen",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03667",
      "pdf_url": "https://arxiv.org/pdf/2512.03667",
      "github_links": [
        "https://github.com/ai4colonoscopy/Colon-X"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03667",
      "scraped_at": "2025-12-09T01:45:09.862566"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.05774",
    "authors": [
      "Caiming Xiong",
      "Junnan Li",
      "Shijie Wang",
      "Honglu Zhou",
      "Ziyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05774",
      "pdf_url": "https://arxiv.org/pdf/2512.05774",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05774",
      "scraped_at": "2025-12-09T01:45:11.724493"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04142",
    "authors": [
      "Aimee van Wynsberghe",
      "Lisa Biber-Freudenberger",
      "Sasha Luccioni",
      "nicholasKluge",
      "sophia-falk"
    ],
    "stars": "0",
    "details": {
      "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
      "abstract": "The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4 üå±üçÉ",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04142",
      "pdf_url": "https://arxiv.org/pdf/2512.04142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04142",
      "scraped_at": "2025-12-09T01:45:13.587428"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.05339",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
      "abstract": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05339",
      "pdf_url": "https://arxiv.org/pdf/2512.05339",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05339",
      "scraped_at": "2025-12-09T01:45:15.478241"
    },
    "scraped_date": "2025-12-09"
  }
]