[
  {
    "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
    "paper_url": "https://huggingface.co/papers/2601.20833",
    "authors": [],
    "stars": "226",
    "details": {
      "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
      "abstract": "arXivLens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/idea2story-an-automated-pipeline-for-transforming-research-concepts-into-complete-scientific-narratives-2345-6407a884 Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20833",
      "pdf_url": "https://arxiv.org/pdf/2601.20833",
      "github_links": [
        "https://github.com/AgentAlphaAGI/Idea2Paper"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20833",
      "scraped_at": "2026-02-01T02:33:03.597644"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2601.20354",
    "authors": [],
    "stars": "97",
    "details": {
      "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
      "abstract": "A very interesting benchmark (ICLR2026) for T2I models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20354",
      "pdf_url": "https://arxiv.org/pdf/2601.20354",
      "github_links": [
        "https://github.com/AMAP-ML/SpatialGenEval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20354",
      "scraped_at": "2026-02-01T02:33:05.477273"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21204",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "abstract": "Embedding scaling can outperform mixture of experts for sparse language models, aided by system optimizations and speculative decoding, with LongCat-Flash-Lite achieving strong competitiveness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21204",
      "pdf_url": "https://arxiv.org/pdf/2601.21204",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21204",
      "scraped_at": "2026-02-01T02:33:07.391167"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.22153",
    "authors": [],
    "stars": "68",
    "details": {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "abstract": "TL; DR: DynamicVLA enables open-ended dynamic object manipulation by pairing a compact 0.4B VLM with low-latency Continuous Inference and Latent-aware Action Streaming, evaluated at scale through the new DOM benchmark in both simulation and the real world. GitHub: https://github.com/hzxie/DynamicVLA Project Page: https://haozhexie.com/project/dynamic-vla Spotlight Video: https://www.youtube.com/watch?v=NmJnHcI04_Q",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22153",
      "pdf_url": "https://arxiv.org/pdf/2601.22153",
      "github_links": [
        "https://github.com/hzxie/DynamicVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22153",
      "scraped_at": "2026-02-01T02:33:09.315341"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
    "paper_url": "https://huggingface.co/papers/2601.21821",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
      "abstract": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21821",
      "pdf_url": "https://arxiv.org/pdf/2601.21821",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21821",
      "scraped_at": "2026-02-01T02:33:11.246878"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21639",
    "authors": [
      "Liming Zheng",
      "Wenkang Han",
      "Xuanle Zhao",
      "Lei Chen",
      "Albert-Zhong"
    ],
    "stars": "19",
    "details": {
      "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
      "abstract": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21639",
      "pdf_url": "https://arxiv.org/pdf/2601.21639",
      "github_links": [
        "https://github.com/DocTron-hub/OCRVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21639",
      "scraped_at": "2026-02-01T02:33:13.120944"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
    "paper_url": "https://huggingface.co/papers/2601.21420",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
      "abstract": "ConceptMoE shifts language model processing from uniform token-level to adaptive concept-level computation. By learning to merge semantically similar tokens into unified concepts while preserving fine-grained granularity for complex tokens, it performs implicit compute allocation‚Äîautomatically investing computation where needed. Key results: (1) Fair comparison under identical parameters and FLOPs shows consistent gains across language (+0.9), vision-language (+0.6, +2.3 on long context), and continual training (+5.5 with layer loops, +6.4 from scratch). (2) Inherent efficiency: at compression ratio R=2, attention computation reduces by R¬≤√ó and KV cache by R√ó, achieving prefill speedups up to 175% and decoding speedups up to 117%. (3) Minimal architectural changes (chunk module + decoder QKV projectors) enable straightforward deployment in existing MoE systems. Represents a paradigm shift toward hierarchical semantic processing in LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21420",
      "pdf_url": "https://arxiv.org/pdf/2601.21420",
      "github_links": [
        "https://github.com/ZihaoHuang-notabot/ConceptMoE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21420",
      "scraped_at": "2026-02-01T02:33:14.976727"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.22046",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "abstract": "PLANING introduces a loosely coupled triangle-Gaussian representation and a monocular streaming framework that jointly achieves accurate geometry, high-fidelity rendering, and efficient planar abstraction for embodied AI applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22046",
      "pdf_url": "https://arxiv.org/pdf/2601.22046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22046",
      "scraped_at": "2026-02-01T02:33:16.978270"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Qwen3-ASR Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.21337",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-ASR Technical Report",
      "abstract": "Qwen3-ASR delivers two all-in-one ASR models with 52-language support and a non-autoregressive forced-aligner; achieves competitive SOTA accuracy, fast TTFT, and open-source Apache 2.0 release.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21337",
      "pdf_url": "https://arxiv.org/pdf/2601.21337",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21337",
      "scraped_at": "2026-02-01T02:33:18.892437"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Exploring Reasoning Reward Model for Agents",
    "paper_url": "https://huggingface.co/papers/2601.22154",
    "authors": [
      "Zhixun Li",
      "Tianshuo Peng",
      "Manyuan Zhang",
      "Kaituo Feng",
      "bunny127"
    ],
    "stars": "20",
    "details": {
      "title": "Exploring Reasoning Reward Model for Agents",
      "abstract": "Github: https://github.com/kxfan2002/Reagent Paper: https://arxiv.org/pdf/2601.22154",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22154",
      "pdf_url": "https://arxiv.org/pdf/2601.22154",
      "github_links": [
        "https://github.com/kxfan2002/Reagent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22154",
      "scraped_at": "2026-02-01T02:33:20.830139"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
    "paper_url": "https://huggingface.co/papers/2601.20730",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
      "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \\textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues. The code is available at https://github.com/euReKa025/AgentLongBench .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20730",
      "pdf_url": "https://arxiv.org/pdf/2601.20730",
      "github_links": [
        "https://github.com/euReKa025/AgentLongBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20730",
      "scraped_at": "2026-02-01T02:33:22.810822"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "paper_url": "https://huggingface.co/papers/2601.17883",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
      "abstract": "We propose fair and comprehensive benchmarking for open source EEG foundation models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17883",
      "pdf_url": "https://arxiv.org/pdf/2601.17883",
      "github_links": [
        "https://github.com/Dingkun0817/EEG-FM-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17883",
      "scraped_at": "2026-02-01T02:33:24.698613"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
    "paper_url": "https://huggingface.co/papers/2601.16914",
    "authors": [
      "Xiaojie Li",
      "Tao Yang",
      "Ming Li",
      "Jie Wu",
      "Justin Cui"
    ],
    "stars": "0",
    "details": {
      "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
      "abstract": "Scaling up video generation to hour long, please checkout our paper at: https://arxiv.org/abs/2601.16914 Project Page and code will released at: https://github.com/justincui03/LoL",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16914",
      "pdf_url": "https://arxiv.org/pdf/2601.16914",
      "github_links": [
        "https://github.com/justincui03/LoL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16914",
      "scraped_at": "2026-02-01T02:33:26.670368"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "paper_url": "https://huggingface.co/papers/2601.22157",
    "authors": [
      "Yedid Hoshen",
      "Eliahu Horwitz",
      "Jonathan Kahana"
    ],
    "stars": "0",
    "details": {
      "title": "Discovering Hidden Gems in Model Repositories",
      "abstract": "An investigation of the available fine-tunes of popular foundation models. While over 90% of downloads are directed to the official base versions the paper shows the existence of other, rarely downloaded fine-tunes that significantly outperform them.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22157",
      "pdf_url": "https://arxiv.org/pdf/2601.22157",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22157",
      "scraped_at": "2026-02-01T02:33:28.571777"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
    "paper_url": "https://huggingface.co/papers/2601.21754",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
      "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21754",
      "pdf_url": "https://arxiv.org/pdf/2601.21754",
      "github_links": [
        "https://github.com/Harry-mic/SCOUT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21754",
      "scraped_at": "2026-02-01T02:33:30.462705"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Shaping capabilities with token-level data filtering",
    "paper_url": "https://huggingface.co/papers/2601.21571",
    "authors": [],
    "stars": "42",
    "details": {
      "title": "Shaping capabilities with token-level data filtering",
      "abstract": "Key Findings: 1. Token-level Filtering vs Document-level Filtering (Figure 3) Token filtering Pareto-dominates document filtering : Can achieve equal reduction in undesired capabilities (equal medical loss) at lower cost to desired capabilities (lower biology loss) More precise filtering preserves beneficial content better 2. Scaling Effects (Figures 1, 4, 5, 6) Filtering gets more effective with scale : 1.8B parameter models see 7,000√ó compute slowdown on medical domain Document filtering: ~30√ó slowdown Token removal: >7,000√ó slowdown Multiple choice evaluation : Models score near chance on MedMCQA and MedQA-USMLE (medical), but maintain performance on retain domains Free response : Token filtering reduces medical answer correctness up to 20√ó, relevance/coherence 3√ó compared to baseline 3. Robustness to Attacks (Figure 7) 10√ó more robust than unlearning against adversarial finetuning attacks for 1.8B models State-of-the-art unlearning (RMU) requires 13√ó fewer tokens to recover capabilities compared to token removal 4. Alignment Compatibility (Figures 8, 9) Models can still be aligned on forget domain : Token-level filtering makes refusal training easier (2√ó better refusal generalization) Document filtering struggles with alignment generalization Linear probes show models can distinguish forget vs. retain tokens despite filtering 5. Classifier Training (Table 1, Figure 11) Small, task-specific models outperform large general ones : 224M parameter biLM achieves 0.894 F1 on test set Outperforms 395M ModernBERT-large (0.794 F1) Domain-specific pretraining improves performance 6. Label Quality Tolerance (Figures 12, 13, 14, 15) Robust to imperfect labels : Aggressive filtering with sufficient compute can overcome label noise Token-level classifiers generalize from weak labels better than document-level Can trade precision for recall to maintain effectiveness",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21571",
      "pdf_url": "https://arxiv.org/pdf/2601.21571",
      "github_links": [
        "https://github.com/neilrathi/token-filtering"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21571",
      "scraped_at": "2026-02-01T02:33:32.442711"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
    "paper_url": "https://huggingface.co/papers/2601.21590",
    "authors": [
      "Haitham Bou Ammar",
      "Matthieu Zimmer",
      "Rasul Tutunov",
      "xtongji"
    ],
    "stars": "0",
    "details": {
      "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
      "abstract": "What if RL isn‚Äôt teaching LLMs how to reason, but just sharpening what‚Äôs already there? Most recent progress in LLM reasoning comes from RL post-training (GRPO, verifiers, rewards). But there‚Äôs growing evidence that these gains may come less from learning new capabilities and more from reshaping the distribution of outputs. In our new work, we take that idea seriously. We show that: Reasoning trajectories already exist in base models What matters is how you sample, not how you retrain The global power distribution can be approximated autoregressively, without MCMC The result is a training-free, verifier-free inference-time method that: ‚ö° Matches GRPO-style post-training ‚è± Is ~10√ó faster than MCMC-based power sampling üß™ Requires no rewards, no finetuning, no verifier Conceptually, the key insight is simple: Power sampling ‚âà low-temperature sampling √ó future-aware token scaling This lets us recover global reasoning behaviour token by token, without expensive trajectory-level inference.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21590",
      "pdf_url": "https://arxiv.org/pdf/2601.21590",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21590",
      "scraped_at": "2026-02-01T02:33:34.343811"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "paper_url": "https://huggingface.co/papers/2601.22083",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Latent Adversarial Regularization for Offline Preference Optimization",
      "abstract": "Most offline preference optimization methods (e.g., DPO) constrain policy updates using token-level divergences. However, token-space similarity is often a weak proxy for semantic or structural behavior. We propose GANPO, a plug-and-play regularizer that introduces latent-space adversarial regularization, aligning the latent representation distributions of a policy and a reference model via a principled GAN-style divergence. We find consistent performance improvements. GANPO yields consistent gains across model architectures when integrated into OPO-style methods on AlpacaEval. We also find that structure is preserved. The adversarial objective acts as a geometry-preserving regularizer. Unlike DPO, which often degrades at high sampling temperatures (T ‚â• 1.0), GANPO maintains structural coherence in high-entropy settings. If you‚Äôre interested in alignment, GANs, or the limitations of KL-divergence‚Äìbased regularization, feel free to check out the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22083",
      "pdf_url": "https://arxiv.org/pdf/2601.22083",
      "github_links": [
        "https://github.com/enyijiang/GANPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22083",
      "scraped_at": "2026-02-01T02:33:36.241397"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.21051",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "abstract": "Model card: https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21051",
      "pdf_url": "https://arxiv.org/pdf/2601.21051",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21051",
      "scraped_at": "2026-02-01T02:33:38.147164"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.18129",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
      "abstract": "Code: https://github.com/scb-10x/typhoon-s Artifact: https://huggingface.co/collections/typhoon-ai/typhoon-s",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18129",
      "pdf_url": "https://arxiv.org/pdf/2601.18129",
      "github_links": [
        "https://github.com/scb-10x/typhoon-s"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18129",
      "scraped_at": "2026-02-01T02:33:40.132735"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
    "paper_url": "https://huggingface.co/papers/2601.21343",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
      "abstract": "Streaming pretraining uses a strong post-trained model to judge next-token generations with RL, improving quality, safety, and factuality earlier in training.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21343",
      "pdf_url": "https://arxiv.org/pdf/2601.21343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21343",
      "scraped_at": "2026-02-01T02:33:42.076876"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.22069",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
      "abstract": "We propose VTC-R1, an efficient long-context reasoning paradigm that integrates vision-text compression into iterative reasoning. By rendering previous reasoning segments into compact visual representations, VTC-R1 replaces long textual contexts with significantly fewer vision tokens in a lightweight and model-free manner. Extensive experiments show that VTC-R1 consistently improves reasoning accuracy across multiple benchmarks while achieving up to 3.4x token compression and 2.7x end-to-end inference speedup. The results demonstrate that VTC-R1 provides an effective alternative representation for scalable long-context reasoning. We hope our work would inspire further exploration of efficient reasoning beyond pure text-based paradigms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22069",
      "pdf_url": "https://arxiv.org/pdf/2601.22069",
      "github_links": [
        "https://github.com/w-yibo/VTC-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22069",
      "scraped_at": "2026-02-01T02:33:44.003367"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21181",
    "authors": [
      "Yong Man Ro",
      "Youngchae Chee",
      "Se Yeon Kim",
      "topyun"
    ],
    "stars": "0",
    "details": {
      "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8% and 2.0% improvements for VideoLLaMA2-AV, 8.7% and 4.7% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21181",
      "pdf_url": "https://arxiv.org/pdf/2601.21181",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21181",
      "scraped_at": "2026-02-01T02:33:45.991896"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "paper_url": "https://huggingface.co/papers/2601.22158",
    "authors": [
      "Zhicheng Jiang",
      "Hanhong Zhao",
      "Qiao Sun",
      "Susie Lu",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "abstract": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22158",
      "pdf_url": "https://arxiv.org/pdf/2601.22158",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22158",
      "scraped_at": "2026-02-01T02:33:47.801410"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
    "paper_url": "https://huggingface.co/papers/2601.21598",
    "authors": [
      "Wee Sun Lee",
      "zz1358m"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
      "abstract": "Our recent work on Latent Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21598",
      "pdf_url": "https://arxiv.org/pdf/2601.21598",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21598",
      "scraped_at": "2026-02-01T02:33:49.693160"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
    "paper_url": "https://huggingface.co/papers/2601.20975",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
      "abstract": "Proposes DeepSearchQA, a 900-prompt benchmark across 17 fields to test long-horizon search, info synthesis, deduplication, and stopping criteria for open-web research agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20975",
      "pdf_url": "https://arxiv.org/pdf/2601.20975",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20975",
      "scraped_at": "2026-02-01T02:33:51.875835"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "paper_url": "https://huggingface.co/papers/2601.22156",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "abstract": "Code: https://www.github.com/THUNLP/hybrid-linear-attention",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22156",
      "pdf_url": "https://arxiv.org/pdf/2601.22156",
      "github_links": [
        "https://github.com/thunlp/hybrid-linear-attention",
        "https://www.github.com/THUNLP/hybrid-linear-attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22156",
      "scraped_at": "2026-02-01T02:33:53.733483"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
    "paper_url": "https://huggingface.co/papers/2601.22146",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "abstract": "@ AjayP13 and @ craffel really interesting work and approach, do you plan to add support for multilingual instructions ü§î",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22146",
      "pdf_url": "https://arxiv.org/pdf/2601.22146",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22146",
      "scraped_at": "2026-02-01T02:33:55.671646"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
    "paper_url": "https://huggingface.co/papers/2601.21579",
    "authors": [
      "Danilo Mandic",
      "Giorgos Iacovides",
      "Yuxuan Gu",
      "WuyangZzzz"
    ],
    "stars": "3",
    "details": {
      "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
      "abstract": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21579",
      "pdf_url": "https://arxiv.org/pdf/2601.21579",
      "github_links": [
        "https://github.com/wz1119/KromHC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21579",
      "scraped_at": "2026-02-01T02:33:57.494868"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "ECO: Quantized Training without Full-Precision Master Weights",
    "paper_url": "https://huggingface.co/papers/2601.22101",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ECO: Quantized Training without Full-Precision Master Weights",
      "abstract": "We present Error-Compensating Optimizer (ECO), which integrates with standard optimizers and, for the first time, enables quantized training of large-scale LLMs without requiring high-precision master weights.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22101",
      "pdf_url": "https://arxiv.org/pdf/2601.22101",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22101",
      "scraped_at": "2026-02-01T02:33:59.289520"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "paper_url": "https://huggingface.co/papers/2601.22054",
    "authors": [
      "Jianxun Cui",
      "Xuancheng Zhang",
      "Donglin Di",
      "Baorui Ma",
      "yjh001"
    ],
    "stars": "62",
    "details": {
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "abstract": "Project Page: https://metric-anything.github.io/metric-anything-io/ Code: https: https://github.com/metric-anything/metric-anything",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22054",
      "pdf_url": "https://arxiv.org/pdf/2601.22054",
      "github_links": [
        "https://github.com/metric-anything/metric-anything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22054",
      "scraped_at": "2026-02-01T02:34:01.121705"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
    "paper_url": "https://huggingface.co/papers/2601.21996",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "abstract": "We introduce Mechanistic Data Attribution (MDA), a new paradigm that shifts the focus of mechanistic interpretability from post-hoc circuit analysis to the causal formation of these mechanisms during training.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21996",
      "pdf_url": "https://arxiv.org/pdf/2601.21996",
      "github_links": [
        "https://github.com/chenjianhuii/Mechanistic-Data-Attribution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21996",
      "scraped_at": "2026-02-01T02:34:02.973491"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
    "paper_url": "https://huggingface.co/papers/2601.21406",
    "authors": [
      "Guanhua Chen",
      "Yong Wang",
      "Kangrui Cen",
      "Hongyang Wei",
      "Zihan Su"
    ],
    "stars": "8",
    "details": {
      "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
      "abstract": "Paper: https://arxiv.org/abs/2601.21406 Github: https://github.com/Sugewud/UniMRG Project: https://sugewud.github.io/UniMRG-Project/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21406",
      "pdf_url": "https://arxiv.org/pdf/2601.21406",
      "github_links": [
        "https://github.com/Sugewud/UniMRG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21406",
      "scraped_at": "2026-02-01T02:34:04.835065"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
    "paper_url": "https://huggingface.co/papers/2601.20465",
    "authors": [
      "Mingkun Xu",
      "Yujie Wu",
      "Yusong Wang",
      "Jiaxiang Liu",
      "innovation64"
    ],
    "stars": "2",
    "details": {
      "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
      "abstract": "We introduce BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture designed to solve \"soul erosion\"‚Äîthe loss of temporal grounding and consistency in long-term agent interactions. üß† Key Innovations: Cognitive-inspired Architecture: Decomposes memory into episodic, semantic, salience-aware, and control-oriented components. Temporal Grounding: Operates at complementary time scales to maintain behavioral consistency. Plug-and-play: A general framework for LLM-based multi-agent systems. Check out our preprint for details on how we bridge the gap between biological memory systems and AI agents!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20465",
      "pdf_url": "https://arxiv.org/pdf/2601.20465",
      "github_links": [
        "https://github.com/innovation64/BMAM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20465",
      "scraped_at": "2026-02-01T02:34:06.608828"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.22143",
    "authors": [
      "Urska Jelercic",
      "Matan Ben Yosef",
      "Tavi Halperin",
      "Naomi Ken Korem",
      "Anthony Chen"
    ],
    "stars": "0",
    "details": {
      "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22143",
      "pdf_url": "https://arxiv.org/pdf/2601.22143",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22143",
      "scraped_at": "2026-02-01T02:34:08.453033"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.19001",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
      "abstract": "ICLR2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19001",
      "pdf_url": "https://arxiv.org/pdf/2601.19001",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19001",
      "scraped_at": "2026-02-01T02:34:10.289576"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
    "paper_url": "https://huggingface.co/papers/2601.21268",
    "authors": [
      "Jesse Roberts",
      "Micah Rentschler"
    ],
    "stars": "0",
    "details": {
      "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
      "abstract": "We present Reinforcement Learning from Meta-Evaluation (RLME), a label-free RL framework that trains LLMs using evaluator judgments to natural-language meta-questions, achieving performance comparable to supervised rewards while scaling to ambiguous, open-domain tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21268",
      "pdf_url": "https://arxiv.org/pdf/2601.21268",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21268",
      "scraped_at": "2026-02-01T02:34:12.180581"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
    "paper_url": "https://huggingface.co/papers/2601.20103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
      "abstract": "We show that contrasting reward hacks in an outlier detection setting helps LLMs detect code hacking behaviors. We further show that a cluster's benign-to-hacked trajectory ratio influences this detection rate. Finally we perform thorough QA and show that semantically contextualized hacks are more difficult to detect as compared to syntactic ones. We release TRACE, a synthetic, human verified dataset of 517 trajectories spanning 54 code reward hack categories to help the community build robust automated RL orchestration pipelines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20103",
      "pdf_url": "https://arxiv.org/pdf/2601.20103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20103",
      "scraped_at": "2026-02-01T02:34:14.074758"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Flow-based Extremal Mathematical Structure Discovery",
    "paper_url": "https://huggingface.co/papers/2601.18005",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Flow-based Extremal Mathematical Structure Discovery",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18005",
      "pdf_url": "https://arxiv.org/pdf/2601.18005",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18005",
      "scraped_at": "2026-02-01T02:34:15.977827"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
    "paper_url": "https://huggingface.co/papers/2601.17690",
    "authors": [
      "Melody Ma",
      "Iram Kamdar",
      "Yunyan Ouyang",
      "Ziling Gong",
      "Franck-Dernoncourt"
    ],
    "stars": "0",
    "details": {
      "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning (2026) BEST-STD2.0: Balanced and Efficient Speech Tokenizer for Spoken Term Detection (2025) VIBEVOICE-ASR Technical Report (2026) DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification (2026) Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding (2025) LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech (2026) SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17690",
      "pdf_url": "https://arxiv.org/pdf/2601.17690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17690",
      "scraped_at": "2026-02-01T02:34:17.832401"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
    "paper_url": "https://huggingface.co/papers/2601.11747",
    "authors": [
      "Stefano Petrangeli",
      "Yu Shen",
      "Sunav Choudhary",
      "Huaxiaoyue Wang",
      "Franck-Dernoncourt"
    ],
    "stars": "0",
    "details": {
      "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing (2026) Styles + Persona-plug = Customized LLMs (2026) Step-by-step Layered Design Generation (2025) Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs (2025) Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation (2025) ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement (2025) Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11747",
      "pdf_url": "https://arxiv.org/pdf/2601.11747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11747",
      "scraped_at": "2026-02-01T02:34:19.617722"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
    "paper_url": "https://huggingface.co/papers/2601.21872",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
      "abstract": "Accepted at ICLR 2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21872",
      "pdf_url": "https://arxiv.org/pdf/2601.21872",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21872",
      "scraped_at": "2026-02-01T02:34:21.390092"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.21416",
    "authors": [
      "Liming Chen",
      "Emmanuel Dellandr√©a",
      "Bruno Machado",
      "Beegbrain"
    ],
    "stars": "0",
    "details": {
      "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
      "abstract": "The ability of visuomotor policies to generalize across tasks and environments critically depends on the structure of the underlying visual representations. While most state-of-the-art robot policies rely on either global or dense features from pre-trained vision models, these approaches often entangle task-relevant and irrelevant signals, limiting their robustness to visual distribution shifts. In this paper, we investigate Slot-based Object-Centric Representations (SOCRs) as a structured alternative that decomposes scenes into a finite set of object-like entities. We present the first large-scale, systematic comparison of global, dense, and object-centric representations across diverse simulated (METAWORLD, LIBERO) and real-world robotic manipulation tasks. Our results show that SOCRs enable superior policy performance and significantly improve generalization under shifts in lighting, texture, and clutter‚Äî even without task-specific tuning. We further demonstrate that pretraining SOCRs on robotic video datasets amplifies these benefits. Our findings highlight the importance of structured visual abstraction in robotic perception and suggest that SOCRs offer a promising pathway toward more robust and generaliz- able manipulation policies.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21416",
      "pdf_url": "https://arxiv.org/pdf/2601.21416",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21416",
      "scraped_at": "2026-02-01T02:34:23.263171"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "paper_url": "https://huggingface.co/papers/2601.21282",
    "authors": [
      "Pranay Boreddy",
      "Ayush Agrawal",
      "Jim Solomon",
      "Howard Zhang",
      "Rishi Upadhyay"
    ],
    "stars": "0",
    "details": {
      "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
      "abstract": "WorldBench provides a disentangled, concept-specific video benchmark to rigorously evaluate physical reasoning in world models and their video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21282",
      "pdf_url": "https://arxiv.org/pdf/2601.21282",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21282",
      "scraped_at": "2026-02-01T02:34:25.095867"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.20381",
    "authors": [
      "Liming Chen",
      "Emmanuel Dellandr√©a",
      "Beegbrain"
    ],
    "stars": "0",
    "details": {
      "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
      "abstract": "We introduce a slot-based object-centric method with a \"task-awareness\" alignment in order to learn robotic manipulation. Our method obtains strong generalization improvements over existing VFM by simply adding a few layers of structure and keeping the backbone frozen. We hope this work can lead to more work going in the direction of adding structure in the visual inputs for robotics manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20381",
      "pdf_url": "https://arxiv.org/pdf/2601.20381",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20381",
      "scraped_at": "2026-02-01T02:34:26.937043"
    },
    "scraped_date": "2026-02-01"
  },
  {
    "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
    "paper_url": "https://huggingface.co/papers/2601.20833",
    "authors": [],
    "stars": "322",
    "details": {
      "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
      "abstract": "arXivLens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/idea2story-an-automated-pipeline-for-transforming-research-concepts-into-complete-scientific-narratives-2345-6407a884 Executive Summary Detailed Breakdown Practical Applications",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20833",
      "pdf_url": "https://arxiv.org/pdf/2601.20833",
      "github_links": [
        "https://github.com/AgentAlphaAGI/Idea2Paper"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20833",
      "scraped_at": "2026-02-02T02:24:08.763828"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
    "paper_url": "https://huggingface.co/papers/2601.20354",
    "authors": [],
    "stars": "98",
    "details": {
      "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
      "abstract": "A very interesting benchmark (ICLR2026) for T2I models!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20354",
      "pdf_url": "https://arxiv.org/pdf/2601.20354",
      "github_links": [
        "https://github.com/AMAP-ML/SpatialGenEval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20354",
      "scraped_at": "2026-02-02T02:24:10.746396"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21204",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "abstract": "Embedding scaling can outperform mixture of experts for sparse language models, aided by system optimizations and speculative decoding, with LongCat-Flash-Lite achieving strong competitiveness.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21204",
      "pdf_url": "https://arxiv.org/pdf/2601.21204",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21204",
      "scraped_at": "2026-02-02T02:24:12.849094"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.22153",
    "authors": [],
    "stars": "79",
    "details": {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "abstract": "TL; DR: DynamicVLA enables open-ended dynamic object manipulation by pairing a compact 0.4B VLM with low-latency Continuous Inference and Latent-aware Action Streaming, evaluated at scale through the new DOM benchmark in both simulation and the real world. GitHub: https://github.com/hzxie/DynamicVLA Project Page: https://haozhexie.com/project/dynamic-vla Spotlight Video: https://www.youtube.com/watch?v=NmJnHcI04_Q",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22153",
      "pdf_url": "https://arxiv.org/pdf/2601.22153",
      "github_links": [
        "https://github.com/hzxie/DynamicVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22153",
      "scraped_at": "2026-02-02T02:24:14.944949"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
    "paper_url": "https://huggingface.co/papers/2601.21821",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
      "abstract": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21821",
      "pdf_url": "https://arxiv.org/pdf/2601.21821",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21821",
      "scraped_at": "2026-02-02T02:24:16.945932"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21639",
    "authors": [
      "Liming Zheng",
      "Wenkang Han",
      "Xuanle Zhao",
      "Lei Chen",
      "Albert-Zhong"
    ],
    "stars": "20",
    "details": {
      "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
      "abstract": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21639",
      "pdf_url": "https://arxiv.org/pdf/2601.21639",
      "github_links": [
        "https://github.com/DocTron-hub/OCRVerse"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21639",
      "scraped_at": "2026-02-02T02:24:18.907249"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
    "paper_url": "https://huggingface.co/papers/2601.21420",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
      "abstract": "ConceptMoE shifts language model processing from uniform token-level to adaptive concept-level computation. By learning to merge semantically similar tokens into unified concepts while preserving fine-grained granularity for complex tokens, it performs implicit compute allocation‚Äîautomatically investing computation where needed. Key results: (1) Fair comparison under identical parameters and FLOPs shows consistent gains across language (+0.9), vision-language (+0.6, +2.3 on long context), and continual training (+5.5 with layer loops, +6.4 from scratch). (2) Inherent efficiency: at compression ratio R=2, attention computation reduces by R¬≤√ó and KV cache by R√ó, achieving prefill speedups up to 175% and decoding speedups up to 117%. (3) Minimal architectural changes (chunk module + decoder QKV projectors) enable straightforward deployment in existing MoE systems. Represents a paradigm shift toward hierarchical semantic processing in LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21420",
      "pdf_url": "https://arxiv.org/pdf/2601.21420",
      "github_links": [
        "https://github.com/ZihaoHuang-notabot/ConceptMoE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21420",
      "scraped_at": "2026-02-02T02:24:20.852622"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Qwen3-ASR Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.21337",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Qwen3-ASR Technical Report",
      "abstract": "Qwen3-ASR delivers two all-in-one ASR models with 52-language support and a non-autoregressive forced-aligner; achieves competitive SOTA accuracy, fast TTFT, and open-source Apache 2.0 release.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21337",
      "pdf_url": "https://arxiv.org/pdf/2601.21337",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21337",
      "scraped_at": "2026-02-02T02:24:22.868564"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Exploring Reasoning Reward Model for Agents",
    "paper_url": "https://huggingface.co/papers/2601.22154",
    "authors": [
      "Zhixun Li",
      "Tianshuo Peng",
      "Manyuan Zhang",
      "Kaituo Feng",
      "bunny127"
    ],
    "stars": "22",
    "details": {
      "title": "Exploring Reasoning Reward Model for Agents",
      "abstract": "Github: https://github.com/kxfan2002/Reagent Paper: https://arxiv.org/pdf/2601.22154",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22154",
      "pdf_url": "https://arxiv.org/pdf/2601.22154",
      "github_links": [
        "https://github.com/kxfan2002/Reagent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22154",
      "scraped_at": "2026-02-02T02:24:24.837601"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
    "paper_url": "https://huggingface.co/papers/2601.22046",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "abstract": "PLANING introduces a loosely coupled triangle-Gaussian representation and a monocular streaming framework that jointly achieves accurate geometry, high-fidelity rendering, and efficient planar abstraction for embodied AI applications.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22046",
      "pdf_url": "https://arxiv.org/pdf/2601.22046",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22046",
      "scraped_at": "2026-02-02T02:24:26.850927"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
    "paper_url": "https://huggingface.co/papers/2601.20730",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
      "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \\textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues. The code is available at https://github.com/euReKa025/AgentLongBench .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20730",
      "pdf_url": "https://arxiv.org/pdf/2601.20730",
      "github_links": [
        "https://github.com/euReKa025/AgentLongBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20730",
      "scraped_at": "2026-02-02T02:24:28.792252"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Shaping capabilities with token-level data filtering",
    "paper_url": "https://huggingface.co/papers/2601.21571",
    "authors": [],
    "stars": "49",
    "details": {
      "title": "Shaping capabilities with token-level data filtering",
      "abstract": "Key Findings: 1. Token-level Filtering vs Document-level Filtering (Figure 3) Token filtering Pareto-dominates document filtering : Can achieve equal reduction in undesired capabilities (equal medical loss) at lower cost to desired capabilities (lower biology loss) More precise filtering preserves beneficial content better 2. Scaling Effects (Figures 1, 4, 5, 6) Filtering gets more effective with scale : 1.8B parameter models see 7,000√ó compute slowdown on medical domain Document filtering: ~30√ó slowdown Token removal: >7,000√ó slowdown Multiple choice evaluation : Models score near chance on MedMCQA and MedQA-USMLE (medical), but maintain performance on retain domains Free response : Token filtering reduces medical answer correctness up to 20√ó, relevance/coherence 3√ó compared to baseline 3. Robustness to Attacks (Figure 7) 10√ó more robust than unlearning against adversarial finetuning attacks for 1.8B models State-of-the-art unlearning (RMU) requires 13√ó fewer tokens to recover capabilities compared to token removal 4. Alignment Compatibility (Figures 8, 9) Models can still be aligned on forget domain : Token-level filtering makes refusal training easier (2√ó better refusal generalization) Document filtering struggles with alignment generalization Linear probes show models can distinguish forget vs. retain tokens despite filtering 5. Classifier Training (Table 1, Figure 11) Small, task-specific models outperform large general ones : 224M parameter biLM achieves 0.894 F1 on test set Outperforms 395M ModernBERT-large (0.794 F1) Domain-specific pretraining improves performance 6. Label Quality Tolerance (Figures 12, 13, 14, 15) Robust to imperfect labels : Aggressive filtering with sufficient compute can overcome label noise Token-level classifiers generalize from weak labels better than document-level Can trade precision for recall to maintain effectiveness",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21571",
      "pdf_url": "https://arxiv.org/pdf/2601.21571",
      "github_links": [
        "https://github.com/neilrathi/token-filtering"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21571",
      "scraped_at": "2026-02-02T02:24:30.794548"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "paper_url": "https://huggingface.co/papers/2601.17883",
    "authors": [],
    "stars": "48",
    "details": {
      "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
      "abstract": "We propose fair and comprehensive benchmarking for open source EEG foundation models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17883",
      "pdf_url": "https://arxiv.org/pdf/2601.17883",
      "github_links": [
        "https://github.com/Dingkun0817/EEG-FM-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17883",
      "scraped_at": "2026-02-02T02:24:32.727889"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "paper_url": "https://huggingface.co/papers/2601.22157",
    "authors": [
      "Yedid Hoshen",
      "Eliahu Horwitz",
      "Jonathan Kahana"
    ],
    "stars": "0",
    "details": {
      "title": "Discovering Hidden Gems in Model Repositories",
      "abstract": "An investigation of the available fine-tunes of popular foundation models. While over 90% of downloads are directed to the official base versions the paper shows the existence of other, rarely downloaded fine-tunes that significantly outperform them.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22157",
      "pdf_url": "https://arxiv.org/pdf/2601.22157",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22157",
      "scraped_at": "2026-02-02T02:24:34.629118"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
    "paper_url": "https://huggingface.co/papers/2601.21754",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
      "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21754",
      "pdf_url": "https://arxiv.org/pdf/2601.21754",
      "github_links": [
        "https://github.com/Harry-mic/SCOUT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21754",
      "scraped_at": "2026-02-02T02:24:36.580671"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
    "paper_url": "https://huggingface.co/papers/2601.16914",
    "authors": [
      "Xiaojie Li",
      "Tao Yang",
      "Ming Li",
      "Jie Wu",
      "Justin Cui"
    ],
    "stars": "0",
    "details": {
      "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
      "abstract": "Scaling up video generation to hour long, please checkout our paper at: https://arxiv.org/abs/2601.16914 Project Page and code will released at: https://github.com/justincui03/LoL",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16914",
      "pdf_url": "https://arxiv.org/pdf/2601.16914",
      "github_links": [
        "https://github.com/justincui03/LoL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16914",
      "scraped_at": "2026-02-02T02:24:38.465226"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "paper_url": "https://huggingface.co/papers/2601.22083",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Latent Adversarial Regularization for Offline Preference Optimization",
      "abstract": "Most offline preference optimization methods (e.g., DPO) constrain policy updates using token-level divergences. However, token-space similarity is often a weak proxy for semantic or structural behavior. We propose GANPO, a plug-and-play regularizer that introduces latent-space adversarial regularization, aligning the latent representation distributions of a policy and a reference model via a principled GAN-style divergence. We find consistent performance improvements. GANPO yields consistent gains across model architectures when integrated into OPO-style methods on AlpacaEval. We also find that structure is preserved. The adversarial objective acts as a geometry-preserving regularizer. Unlike DPO, which often degrades at high sampling temperatures (T ‚â• 1.0), GANPO maintains structural coherence in high-entropy settings. If you‚Äôre interested in alignment, GANs, or the limitations of KL-divergence‚Äìbased regularization, feel free to check out the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22083",
      "pdf_url": "https://arxiv.org/pdf/2601.22083",
      "github_links": [
        "https://github.com/enyijiang/GANPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22083",
      "scraped_at": "2026-02-02T02:24:40.449934"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
    "paper_url": "https://huggingface.co/papers/2601.21590",
    "authors": [
      "Haitham Bou Ammar",
      "Matthieu Zimmer",
      "Rasul Tutunov",
      "xtongji"
    ],
    "stars": "0",
    "details": {
      "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
      "abstract": "What if RL isn‚Äôt teaching LLMs how to reason, but just sharpening what‚Äôs already there? Most recent progress in LLM reasoning comes from RL post-training (GRPO, verifiers, rewards). But there‚Äôs growing evidence that these gains may come less from learning new capabilities and more from reshaping the distribution of outputs. In our new work, we take that idea seriously. We show that: Reasoning trajectories already exist in base models What matters is how you sample, not how you retrain The global power distribution can be approximated autoregressively, without MCMC The result is a training-free, verifier-free inference-time method that: ‚ö° Matches GRPO-style post-training ‚è± Is ~10√ó faster than MCMC-based power sampling üß™ Requires no rewards, no finetuning, no verifier Conceptually, the key insight is simple: Power sampling ‚âà low-temperature sampling √ó future-aware token scaling This lets us recover global reasoning behaviour token by token, without expensive trajectory-level inference.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21590",
      "pdf_url": "https://arxiv.org/pdf/2601.21590",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21590",
      "scraped_at": "2026-02-02T02:24:42.439814"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
    "paper_url": "https://huggingface.co/papers/2601.21051",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "abstract": "Model card: https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21051",
      "pdf_url": "https://arxiv.org/pdf/2601.21051",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21051",
      "scraped_at": "2026-02-02T02:24:44.602632"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
    "paper_url": "https://huggingface.co/papers/2601.21343",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
      "abstract": "Streaming pretraining uses a strong post-trained model to judge next-token generations with RL, improving quality, safety, and factuality earlier in training.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21343",
      "pdf_url": "https://arxiv.org/pdf/2601.21343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21343",
      "scraped_at": "2026-02-02T02:24:46.525589"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.18129",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
      "abstract": "Code: https://github.com/scb-10x/typhoon-s Artifact: https://huggingface.co/collections/typhoon-ai/typhoon-s",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18129",
      "pdf_url": "https://arxiv.org/pdf/2601.18129",
      "github_links": [
        "https://github.com/scb-10x/typhoon-s"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18129",
      "scraped_at": "2026-02-02T02:24:48.451524"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "paper_url": "https://huggingface.co/papers/2601.22158",
    "authors": [
      "Zhicheng Jiang",
      "Hanhong Zhao",
      "Qiao Sun",
      "Susie Lu",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "abstract": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22158",
      "pdf_url": "https://arxiv.org/pdf/2601.22158",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22158",
      "scraped_at": "2026-02-02T02:24:50.401436"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "paper_url": "https://huggingface.co/papers/2601.22156",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "abstract": "Code: https://www.github.com/THUNLP/hybrid-linear-attention",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22156",
      "pdf_url": "https://arxiv.org/pdf/2601.22156",
      "github_links": [
        "https://www.github.com/THUNLP/hybrid-linear-attention",
        "https://github.com/thunlp/hybrid-linear-attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22156",
      "scraped_at": "2026-02-02T02:24:52.320457"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.22069",
    "authors": [],
    "stars": "16",
    "details": {
      "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
      "abstract": "We propose VTC-R1, an efficient long-context reasoning paradigm that integrates vision-text compression into iterative reasoning. By rendering previous reasoning segments into compact visual representations, VTC-R1 replaces long textual contexts with significantly fewer vision tokens in a lightweight and model-free manner. Extensive experiments show that VTC-R1 consistently improves reasoning accuracy across multiple benchmarks while achieving up to 3.4x token compression and 2.7x end-to-end inference speedup. The results demonstrate that VTC-R1 provides an effective alternative representation for scalable long-context reasoning. We hope our work would inspire further exploration of efficient reasoning beyond pure text-based paradigms.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22069",
      "pdf_url": "https://arxiv.org/pdf/2601.22069",
      "github_links": [
        "https://github.com/w-yibo/VTC-R1"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22069",
      "scraped_at": "2026-02-02T02:24:54.249712"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.21181",
    "authors": [
      "Yong Man Ro",
      "Youngchae Chee",
      "Se Yeon Kim",
      "topyun"
    ],
    "stars": "0",
    "details": {
      "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8% and 2.0% improvements for VideoLLaMA2-AV, 8.7% and 4.7% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21181",
      "pdf_url": "https://arxiv.org/pdf/2601.21181",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21181",
      "scraped_at": "2026-02-02T02:24:56.143073"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
    "paper_url": "https://huggingface.co/papers/2601.20975",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
      "abstract": "Proposes DeepSearchQA, a 900-prompt benchmark across 17 fields to test long-horizon search, info synthesis, deduplication, and stopping criteria for open-web research agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20975",
      "pdf_url": "https://arxiv.org/pdf/2601.20975",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20975",
      "scraped_at": "2026-02-02T02:24:58.309116"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
    "paper_url": "https://huggingface.co/papers/2601.21598",
    "authors": [
      "Wee Sun Lee",
      "zz1358m"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
      "abstract": "Our recent work on Latent Reasoning",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21598",
      "pdf_url": "https://arxiv.org/pdf/2601.21598",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21598",
      "scraped_at": "2026-02-02T02:25:00.362508"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
    "paper_url": "https://huggingface.co/papers/2601.21579",
    "authors": [
      "Danilo Mandic",
      "Giorgos Iacovides",
      "Yuxuan Gu",
      "WuyangZzzz"
    ],
    "stars": "3",
    "details": {
      "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
      "abstract": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21579",
      "pdf_url": "https://arxiv.org/pdf/2601.21579",
      "github_links": [
        "https://github.com/wz1119/KromHC"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21579",
      "scraped_at": "2026-02-02T02:25:02.250183"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
    "paper_url": "https://huggingface.co/papers/2601.22146",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "abstract": "@ AjayP13 and @ craffel really interesting work and approach, do you plan to add support for multilingual instructions ü§î",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22146",
      "pdf_url": "https://arxiv.org/pdf/2601.22146",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22146",
      "scraped_at": "2026-02-02T02:25:04.504788"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "ECO: Quantized Training without Full-Precision Master Weights",
    "paper_url": "https://huggingface.co/papers/2601.22101",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ECO: Quantized Training without Full-Precision Master Weights",
      "abstract": "We present Error-Compensating Optimizer (ECO), which integrates with standard optimizers and, for the first time, enables quantized training of large-scale LLMs without requiring high-precision master weights.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22101",
      "pdf_url": "https://arxiv.org/pdf/2601.22101",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22101",
      "scraped_at": "2026-02-02T02:25:06.378922"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
    "paper_url": "https://huggingface.co/papers/2601.22143",
    "authors": [
      "Urska Jelercic",
      "Matan Ben Yosef",
      "Tavi Halperin",
      "Naomi Ken Korem",
      "Anthony Chen"
    ],
    "stars": "0",
    "details": {
      "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22143",
      "pdf_url": "https://arxiv.org/pdf/2601.22143",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22143",
      "scraped_at": "2026-02-02T02:25:08.517305"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "paper_url": "https://huggingface.co/papers/2601.22054",
    "authors": [
      "Jianxun Cui",
      "Xuancheng Zhang",
      "Donglin Di",
      "Baorui Ma",
      "yjh001"
    ],
    "stars": "67",
    "details": {
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "abstract": "Project Page: https://metric-anything.github.io/metric-anything-io/ Code: https: https://github.com/metric-anything/metric-anything",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22054",
      "pdf_url": "https://arxiv.org/pdf/2601.22054",
      "github_links": [
        "https://github.com/metric-anything/metric-anything"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22054",
      "scraped_at": "2026-02-02T02:25:10.763168"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
    "paper_url": "https://huggingface.co/papers/2601.21996",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "abstract": "We introduce Mechanistic Data Attribution (MDA), a new paradigm that shifts the focus of mechanistic interpretability from post-hoc circuit analysis to the causal formation of these mechanisms during training.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21996",
      "pdf_url": "https://arxiv.org/pdf/2601.21996",
      "github_links": [
        "https://github.com/chenjianhuii/Mechanistic-Data-Attribution"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21996",
      "scraped_at": "2026-02-02T02:25:13.113858"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
    "paper_url": "https://huggingface.co/papers/2601.21406",
    "authors": [
      "Guanhua Chen",
      "Yong Wang",
      "Kangrui Cen",
      "Hongyang Wei",
      "Zihan Su"
    ],
    "stars": "10",
    "details": {
      "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
      "abstract": "Paper: https://arxiv.org/abs/2601.21406 Github: https://github.com/Sugewud/UniMRG Project: https://sugewud.github.io/UniMRG-Project/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21406",
      "pdf_url": "https://arxiv.org/pdf/2601.21406",
      "github_links": [
        "https://github.com/Sugewud/UniMRG"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21406",
      "scraped_at": "2026-02-02T02:25:15.252387"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
    "paper_url": "https://huggingface.co/papers/2601.20465",
    "authors": [
      "Mingkun Xu",
      "Yujie Wu",
      "Yusong Wang",
      "Jiaxiang Liu",
      "innovation64"
    ],
    "stars": "2",
    "details": {
      "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
      "abstract": "We introduce BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture designed to solve \"soul erosion\"‚Äîthe loss of temporal grounding and consistency in long-term agent interactions. üß† Key Innovations: Cognitive-inspired Architecture: Decomposes memory into episodic, semantic, salience-aware, and control-oriented components. Temporal Grounding: Operates at complementary time scales to maintain behavioral consistency. Plug-and-play: A general framework for LLM-based multi-agent systems. Check out our preprint for details on how we bridge the gap between biological memory systems and AI agents!",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20465",
      "pdf_url": "https://arxiv.org/pdf/2601.20465",
      "github_links": [
        "https://github.com/innovation64/BMAM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20465",
      "scraped_at": "2026-02-02T02:25:17.419648"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.19001",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
      "abstract": "ICLR2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.19001",
      "pdf_url": "https://arxiv.org/pdf/2601.19001",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.19001",
      "scraped_at": "2026-02-02T02:25:19.341557"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
    "paper_url": "https://huggingface.co/papers/2601.21268",
    "authors": [
      "Jesse Roberts",
      "Micah Rentschler"
    ],
    "stars": "0",
    "details": {
      "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
      "abstract": "We present Reinforcement Learning from Meta-Evaluation (RLME), a label-free RL framework that trains LLMs using evaluator judgments to natural-language meta-questions, achieving performance comparable to supervised rewards while scaling to ambiguous, open-domain tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21268",
      "pdf_url": "https://arxiv.org/pdf/2601.21268",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21268",
      "scraped_at": "2026-02-02T02:25:21.212465"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
    "paper_url": "https://huggingface.co/papers/2601.20103",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
      "abstract": "We show that contrasting reward hacks in an outlier detection setting helps LLMs detect code hacking behaviors. We further show that a cluster's benign-to-hacked trajectory ratio influences this detection rate. Finally we perform thorough QA and show that semantically contextualized hacks are more difficult to detect as compared to syntactic ones. We release TRACE, a synthetic, human verified dataset of 517 trajectories spanning 54 code reward hack categories to help the community build robust automated RL orchestration pipelines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20103",
      "pdf_url": "https://arxiv.org/pdf/2601.20103",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20103",
      "scraped_at": "2026-02-02T02:25:23.283574"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Flow-based Extremal Mathematical Structure Discovery",
    "paper_url": "https://huggingface.co/papers/2601.18005",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Flow-based Extremal Mathematical Structure Discovery",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18005",
      "pdf_url": "https://arxiv.org/pdf/2601.18005",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18005",
      "scraped_at": "2026-02-02T02:25:25.204882"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
    "paper_url": "https://huggingface.co/papers/2601.17690",
    "authors": [
      "Melody Ma",
      "Iram Kamdar",
      "Yunyan Ouyang",
      "Ziling Gong",
      "Franck-Dernoncourt"
    ],
    "stars": "0",
    "details": {
      "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning (2026) BEST-STD2.0: Balanced and Efficient Speech Tokenizer for Spoken Term Detection (2025) VIBEVOICE-ASR Technical Report (2026) DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification (2026) Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding (2025) LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech (2026) SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.17690",
      "pdf_url": "https://arxiv.org/pdf/2601.17690",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.17690",
      "scraped_at": "2026-02-02T02:25:27.048523"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
    "paper_url": "https://huggingface.co/papers/2601.11747",
    "authors": [
      "Stefano Petrangeli",
      "Yu Shen",
      "Sunav Choudhary",
      "Huaxiaoyue Wang",
      "Franck-Dernoncourt"
    ],
    "stars": "0",
    "details": {
      "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing (2026) Styles + Persona-plug = Customized LLMs (2026) Step-by-step Layered Design Generation (2025) Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs (2025) Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation (2025) ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement (2025) Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.11747",
      "pdf_url": "https://arxiv.org/pdf/2601.11747",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.11747",
      "scraped_at": "2026-02-02T02:25:29.819440"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
    "paper_url": "https://huggingface.co/papers/2601.21872",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
      "abstract": "Accepted at ICLR 2026",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21872",
      "pdf_url": "https://arxiv.org/pdf/2601.21872",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21872",
      "scraped_at": "2026-02-02T02:25:31.993094"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.21416",
    "authors": [
      "Liming Chen",
      "Emmanuel Dellandr√©a",
      "Bruno Machado",
      "Beegbrain"
    ],
    "stars": "0",
    "details": {
      "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
      "abstract": "The ability of visuomotor policies to generalize across tasks and environments critically depends on the structure of the underlying visual representations. While most state-of-the-art robot policies rely on either global or dense features from pre-trained vision models, these approaches often entangle task-relevant and irrelevant signals, limiting their robustness to visual distribution shifts. In this paper, we investigate Slot-based Object-Centric Representations (SOCRs) as a structured alternative that decomposes scenes into a finite set of object-like entities. We present the first large-scale, systematic comparison of global, dense, and object-centric representations across diverse simulated (METAWORLD, LIBERO) and real-world robotic manipulation tasks. Our results show that SOCRs enable superior policy performance and significantly improve generalization under shifts in lighting, texture, and clutter‚Äî even without task-specific tuning. We further demonstrate that pretraining SOCRs on robotic video datasets amplifies these benefits. Our findings highlight the importance of structured visual abstraction in robotic perception and suggest that SOCRs offer a promising pathway toward more robust and generaliz- able manipulation policies.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21416",
      "pdf_url": "https://arxiv.org/pdf/2601.21416",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21416",
      "scraped_at": "2026-02-02T02:25:33.857071"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "paper_url": "https://huggingface.co/papers/2601.21282",
    "authors": [
      "Pranay Boreddy",
      "Ayush Agrawal",
      "Jim Solomon",
      "Howard Zhang",
      "Rishi Upadhyay"
    ],
    "stars": "0",
    "details": {
      "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
      "abstract": "WorldBench provides a disentangled, concept-specific video benchmark to rigorously evaluate physical reasoning in world models and their video generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21282",
      "pdf_url": "https://arxiv.org/pdf/2601.21282",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21282",
      "scraped_at": "2026-02-02T02:25:35.699878"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
    "paper_url": "https://huggingface.co/papers/2601.20381",
    "authors": [
      "Liming Chen",
      "Emmanuel Dellandr√©a",
      "Beegbrain"
    ],
    "stars": "0",
    "details": {
      "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
      "abstract": "We introduce a slot-based object-centric method with a \"task-awareness\" alignment in order to learn robotic manipulation. Our method obtains strong generalization improvements over existing VFM by simply adding a few layers of structure and keeping the backbone frozen. We hope this work can lead to more work going in the direction of adding structure in the visual inputs for robotics manipulation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20381",
      "pdf_url": "https://arxiv.org/pdf/2601.20381",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20381",
      "scraped_at": "2026-02-02T02:25:37.542856"
    },
    "scraped_date": "2026-02-02"
  },
  {
    "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
    "paper_url": "https://huggingface.co/papers/2601.21558",
    "authors": [
      "Hao Zhou",
      "Shuaiting Chen",
      "Haotian Wang",
      "jade0101",
      "Emperorizzis"
    ],
    "stars": "84",
    "details": {
      "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
      "abstract": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21558",
      "pdf_url": "https://arxiv.org/pdf/2601.21558",
      "github_links": [
        "https://github.com/LianjiaTech/astra"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21558",
      "scraped_at": "2026-02-03T02:15:28.439188"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation",
    "paper_url": "https://huggingface.co/papers/2601.22813",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation",
      "abstract": "A SOTA NVFP4 LLM pre-training method based on MS-EDEN unbiased gradient estimation. Code is available on GitHub .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22813",
      "pdf_url": "https://arxiv.org/pdf/2601.22813",
      "github_links": [
        "https://github.com/IST-DASLab/Quartet-II"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22813",
      "scraped_at": "2026-02-03T02:15:30.368866"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text",
    "paper_url": "https://huggingface.co/papers/2601.22975",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text",
      "abstract": "TL;DR: We introduce Golden Goose ü¶¢, a simple method that synthesizes unlimited RLVR tasks from unverifiable internet text by constructing multiple-choice fill-in-the-middle problems. This enables the use of reasoning-rich unverifiable corpora typically excluded from prior RLVR data curation (e.g., science textbooks), allowing RL to scale beyond the data saturation of existing RLVR datasets and achieving new SoTA results on 1.5B and 4B-Instruct models . In a real-world deployment to cybersecurity, where no prior RLVR data exists, Golden Goose synthesizes RLVR tasks from raw FineWeb scrapes, yielding a new SoTA 4B cybersecurity LLM that surpasses a 7B domain-specialized model .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22975",
      "pdf_url": "https://arxiv.org/pdf/2601.22975",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22975",
      "scraped_at": "2026-02-03T02:15:32.373504"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
    "paper_url": "https://huggingface.co/papers/2601.23143",
    "authors": [
      "Minki Kang",
      "Gyeongman Kim",
      "YuminChoi",
      "Sangsang",
      "Seanie-lee"
    ],
    "stars": "3",
    "details": {
      "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
      "abstract": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23143",
      "pdf_url": "https://arxiv.org/pdf/2601.23143",
      "github_links": [
        "https://github.com/seanie12/ThinkSafe.git"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23143",
      "scraped_at": "2026-02-03T02:15:34.303739"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
    "paper_url": "https://huggingface.co/papers/2601.22628",
    "authors": [
      "Chengsong Huang",
      "Zongpei Teng",
      "Yunbo Tang",
      "Zhishang Xiang",
      "ChengyiYang"
    ],
    "stars": "19",
    "details": {
      "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
      "abstract": "TTCS, a new paradigm for self-evolving",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22628",
      "pdf_url": "https://arxiv.org/pdf/2601.22628",
      "github_links": [
        "https://github.com/XMUDeepLIT/TTCS"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22628",
      "scraped_at": "2026-02-03T02:15:36.227648"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
    "paper_url": "https://huggingface.co/papers/2601.23265",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
      "abstract": "PaperBanana automates publication-ready AI research illustrations via an agentic framework using VLMs and image models, orchestrating reference retrieval, planning, rendering, and self-critique with a benchmarking suite.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23265",
      "pdf_url": "https://arxiv.org/pdf/2601.23265",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23265",
      "scraped_at": "2026-02-03T02:15:38.164377"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Do Reasoning Models Enhance Embedding Models?",
    "paper_url": "https://huggingface.co/papers/2601.21192",
    "authors": [
      "Elton Chun-Chai Li",
      "Kwun Hang Lau",
      "Huihao Jing",
      "Shaojin Chen",
      "lucaswychan"
    ],
    "stars": "5",
    "details": {
      "title": "Do Reasoning Models Enhance Embedding Models?",
      "abstract": "Our analysis revealed a phenomenon we term Manifold Realignment. RLVR is a Trajectory Optimizer : We found that RLVR irreversibly reorganizes the local geometry of the latent manifold but largely preserves the global manifold geometry (the overall map of knowledge) and the linear readout of base models. Coordinate basis will alter substantially and reversibly only under prolonged RLVR. Contrastive Learning acts as an Equalizer : When we fine-tune these models to embedding models, the contrastive loss forces the base and reasoning models to align strongly again. The Takeaway for Practitioners: Our results suggest that the \"reasoning\" capability in current models is a learned policy for navigating the latent manifold , rather than a fundamental restructuring of the knowledge map itself. As latent-space-centric paradigms such as World Models and JEPA gain prominence, our findings point to a practical trade-off: RLVR tends to preserve the base model‚Äôs representational backbone (which may help retain broad generalization), yet on its own is unlikely to fundamentally improve the underlying global organization of the latent manifold. If RLVR‚Äôs distinctive footprint is local geometry reorganization under global geometry stability, then similar behavior might be achievable via SFT augmented with geometry- and basis-aware regularization .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21192",
      "pdf_url": "https://arxiv.org/pdf/2601.21192",
      "github_links": [
        "https://github.com/HKUST-KnowComp/Reasoning-Embedding"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21192",
      "scraped_at": "2026-02-03T02:15:40.123818"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
    "paper_url": "https://huggingface.co/papers/2601.23182",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
      "abstract": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct. Code is available at https://github.com/ShirleYoung/FourierSampler .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23182",
      "pdf_url": "https://arxiv.org/pdf/2601.23182",
      "github_links": [
        "https://github.com/ShirleYoung/FourierSampler"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23182",
      "scraped_at": "2026-02-03T02:15:42.069386"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Causal World Modeling for Robot Control",
    "paper_url": "https://huggingface.co/papers/2601.21998",
    "authors": [
      "Ruilin Wang",
      "Shuai Yang",
      "Yiming Luo",
      "Qihang Zhang",
      "Lin Li"
    ],
    "stars": "0",
    "details": {
      "title": "Causal World Modeling for Robot Control",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21998",
      "pdf_url": "https://arxiv.org/pdf/2601.21998",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21998",
      "scraped_at": "2026-02-03T02:15:44.064946"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
    "paper_url": "https://huggingface.co/papers/2601.23184",
    "authors": [
      "Zhifeng Gao",
      "Hongteng Xu",
      "Guojiang Zhao",
      "Haotian Liu",
      "FanmengWang"
    ],
    "stars": "15",
    "details": {
      "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
      "abstract": "Introduces ReGuLaR, a variational latent reasoning framework that renders reasoning as images to regularize posterior inference, achieving efficient multimodal reasoning beyond traditional chain of thought.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23184",
      "pdf_url": "https://arxiv.org/pdf/2601.23184",
      "github_links": [
        "https://github.com/FanmengWang/ReGuLaR"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23184",
      "scraped_at": "2026-02-03T02:15:46.003959"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
    "paper_url": "https://huggingface.co/papers/2601.21716",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
      "abstract": "Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21716",
      "pdf_url": "https://arxiv.org/pdf/2601.21716",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21716",
      "scraped_at": "2026-02-03T02:15:47.915924"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
    "paper_url": "https://huggingface.co/papers/2601.22491",
    "authors": [
      "Bolin Ni",
      "Fangzhi Xu",
      "Yuhao Shen",
      "Jinyang Wu",
      "thkelper"
    ],
    "stars": "0",
    "details": {
      "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22491",
      "pdf_url": "https://arxiv.org/pdf/2601.22491",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22491",
      "scraped_at": "2026-02-03T02:15:49.836577"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
    "paper_url": "https://huggingface.co/papers/2601.20218",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
      "abstract": "A dense reward for RL in flow matching models.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20218",
      "pdf_url": "https://arxiv.org/pdf/2601.20218",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20218",
      "scraped_at": "2026-02-03T02:15:51.734118"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
    "paper_url": "https://huggingface.co/papers/2601.22636",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
      "abstract": "Real-world jailbreak attackers don‚Äôt usually try once. They try many times, in parallel, until the model slips. That‚Äôs why adversarial risk can‚Äôt be captured by attack success rate on a single attempt (ASR@1). As the number of attempts N grows, risk can amplify, often at very different rates across models and settings. So what actually governs this amplification rate? In work, we derive a theoretically grounded scaling law for adversarial risk under Best-of-N sampling, and introduce SABER, a scaling-aware estimator that predicts large-budget risk from small-budget measurements.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22636",
      "pdf_url": "https://arxiv.org/pdf/2601.22636",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22636",
      "scraped_at": "2026-02-03T02:15:53.630169"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
    "paper_url": "https://huggingface.co/papers/2601.22904",
    "authors": [
      "Jong Chul Ye",
      "Byunghee Cha",
      "Hun Chang"
    ],
    "stars": "0",
    "details": {
      "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
      "abstract": "DINO-SAE bridges semantic directions and pixel fidelity via spherical latent diffusion with hierarchical patch embedding and cosine alignment, achieving state-of-the-art reconstruction while preserving semantic alignment.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22904",
      "pdf_url": "https://arxiv.org/pdf/2601.22904",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22904",
      "scraped_at": "2026-02-03T02:15:55.539632"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
    "paper_url": "https://huggingface.co/papers/2601.21957",
    "authors": [
      "Zelun Zhang",
      "Tingquan Gao",
      "Suyin Liang",
      "sunflowerting78",
      "ChengCui"
    ],
    "stars": "0",
    "details": {
      "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21957",
      "pdf_url": "https://arxiv.org/pdf/2601.21957",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21957",
      "scraped_at": "2026-02-03T02:15:57.474033"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.13097",
    "authors": [
      "Mikhail Klementev",
      "doooori",
      "rmndrnts",
      "dangrebenkin",
      "brucheselena"
    ],
    "stars": "5",
    "details": {
      "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
      "abstract": "RM -RF: Reward Model for Run-Free Unit Test Evaluation proposes a novel lightweight reward model  that predicts unit test quality without compiling or executing code by inferring three execution-derived signals directly from source and test code: whether the augmented test suite would compile and run, whether the new tests increase code coverage, whether they improve mutation kill rate. This work is motivated by the high computational cost of traditional compile-and-run validation in automated test generation, especially when using large language models (LLMs) for code tasks. RM-RF is trained on a multilingual dataset across Java, Python, and Go that pairs code + test files with execution-derived labels, and various model families and tuning regimes (zero-shot, fine-tuned, PEFT/LoRA) achieve ~0.69 F1 on the three targets. Key contributions include: a scalable ‚Äúrun-free‚Äù reward model that reduces latency and infrastructure needs compared to execution-based evaluation, a curated dataset and methodology for comparative assessment of unit test quality signals, empirical analysis of RM-RF with different model sizes and fine-tuning strategies. This approach can provide rapid feedback for large-scale test generation and RL-based code optimization, bridging a gap between high-fidelity execution feedback and scalable automated unit test evaluation.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.13097",
      "pdf_url": "https://arxiv.org/pdf/2601.13097",
      "github_links": [
        "https://github.com/trndcenter/RM-RF-unit-tests"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.13097",
      "scraped_at": "2026-02-03T02:15:59.396853"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
    "paper_url": "https://huggingface.co/papers/2601.23161",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "abstract": "DIFFA-2 provides a practical diffusion-based large audio language model with semantic/acoustic adapters and a four-stage curriculum, improving general audio understanding under practical budgets.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23161",
      "pdf_url": "https://arxiv.org/pdf/2601.23161",
      "github_links": [
        "https://github.com/NKU-HLT/DIFFA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23161",
      "scraped_at": "2026-02-03T02:16:01.336883"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
    "paper_url": "https://huggingface.co/papers/2601.22837",
    "authors": [
      "Zhendong Mao",
      "Weinan Jia",
      "Mengqi Huang",
      "Bin Wu"
    ],
    "stars": "4",
    "details": {
      "title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
      "abstract": "Introduces native visual tokenization with causal dependencies, via NativeTok (MIT and MoCET) and hierarchical training for efficient, coherent image reconstruction with relational token constraints.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22837",
      "pdf_url": "https://arxiv.org/pdf/2601.22837",
      "github_links": [
        "https://github.com/wangbei1/Nativetok"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22837",
      "scraped_at": "2026-02-03T02:16:03.310657"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
    "paper_url": "https://huggingface.co/papers/2601.22642",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22642",
      "pdf_url": "https://arxiv.org/pdf/2601.22642",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22642",
      "scraped_at": "2026-02-03T02:16:05.313485"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
    "paper_url": "https://huggingface.co/papers/2601.21468",
    "authors": [
      "Yuxin Chen",
      "Wenyu Mao",
      "Yu Yang",
      "Shugui Liu",
      "Yaorui Shi"
    ],
    "stars": "0",
    "details": {
      "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
      "abstract": "MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21468",
      "pdf_url": "https://arxiv.org/pdf/2601.21468",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21468",
      "scraped_at": "2026-02-03T02:16:07.319616"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance",
    "paper_url": "https://huggingface.co/papers/2601.18241",
    "authors": [
      "Vadim Alperovich",
      "dangrebenkin",
      "rmndrnts",
      "doooori",
      "brucheselena"
    ],
    "stars": "6",
    "details": {
      "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance",
      "abstract": "üß™ TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance What‚Äôs new: Large Language Models (LLMs) have been widely explored for unit test generation , but real-world test suite maintenance ‚Äî like creating, updating, and repairing tests as code evolves ‚Äî hasn‚Äôt been systematically evaluated. This paper introduces TAM-Eval , the benchmark and evaluation framework that targets exactly these maintenance tasks in realistic software contexts. Core contributions: üîπ Benchmark and framework: TAM-Eval evaluates LLMs on three maintenance scenarios ‚Äî creation , repair , and updating of test suites ‚Äî at the test file level with actual context (not isolated function snippets). üîπ Real-world dataset: The benchmark is built from 1,539 automatically extracted and validated scenarios from open-source projects in Python, Java, and Go. üîπ Evaluation metrics: Instead of simple accuracy scores, the framework uses reference-free metrics reflecting real software quality: Test suite pass rate Code coverage change Mutation testing outcomes These align closer to developer goals in maintenance. üîπ Empirical results: State-of-the-art LLMs show only limited improvements on realistic maintenance workflows ‚Äî indicating that current models still struggle with practical test suite evolution. Why it matters: Automated testing and maintenance are essential for high-quality software. Most benchmarks have focused on test generation at function level; TAM-Eval shifts the focus to maintenance workflows developers actually deal with , providing a new community standard for evaluating LLMs in software engineering contexts. Open science: The TAM-Eval code and dataset are fully open-source, enabling future research and direct integration into evaluation pipelines.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.18241",
      "pdf_url": "https://arxiv.org/pdf/2601.18241",
      "github_links": [
        "https://github.com/trndcenter/TAM-Eval"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.18241",
      "scraped_at": "2026-02-03T02:16:09.235196"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Scaling Multiagent Systems with Process Rewards",
    "paper_url": "https://huggingface.co/papers/2601.23228",
    "authors": [],
    "stars": "43",
    "details": {
      "title": "Scaling Multiagent Systems with Process Rewards",
      "abstract": "Define and train your own multiagent system @ our github repo !",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23228",
      "pdf_url": "https://arxiv.org/pdf/2601.23228",
      "github_links": [
        "https://github.com/ltjed/multiagent-coaching"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23228",
      "scraped_at": "2026-02-03T02:16:11.132610"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
    "paper_url": "https://huggingface.co/papers/2601.21358",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Forest Before Trees: Latent Superposition for Efficient Visual Reasoning (2026) Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning (2026) Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs (2025) Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models (2026) Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge (2026) LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning (2026) iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21358",
      "pdf_url": "https://arxiv.org/pdf/2601.21358",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21358",
      "scraped_at": "2026-02-03T02:16:13.058828"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
    "paper_url": "https://huggingface.co/papers/2601.23188",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
      "abstract": "üöÄ New Research Alert! üß†‚ú® We‚Äôre excited to share our latest work: Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience üîç What‚Äôs the key idea? Deep search agents powered by LLMs excel at multi-step reasoning and retrieval‚Äîbut they often fail silently when small errors accumulate under uncertainty. Drawing direct inspiration from human cognitive neuroscience , we propose a hierarchical meta-cognitive monitoring framework that mirrors how humans detect anomalies, reflect on mistakes, and adapt their behavior during complex problem solving. üß† Neuroscience-inspired design Human metacognition is not monolithic: it combines fast, automatic error signals with slower, experience-driven reflection . We translate this principle into deep search agents via two complementary monitors: üß© Our approach introduces two complementary monitors: ‚ö° Fast Consistency Monitor Inspired by rapid neural mechanisms for conflict and surprise detection (e.g., early mismatch and uncertainty signals), this module performs lightweight, always-on checks to detect misalignment between external evidence and internal reasoning confidence . üê¢ Slow Experience-Driven Monitor Inspired by higher-order reflective processes and memory-based control in the human brain, this module is selectively activated . It leverages experience from past trajectories to guide deliberate correction and strategic adjustment. üìà Why does this matter? By embedding neuroscience-inspired meta-cognitive monitoring directly into the ReAct loop of a Deep Search Agent, our framework: Enhances robustness and reliability in long-horizon reasoning Enables early detection and correction of cascading errors Establishes a tighter conceptual bridge between human metacognition and agentic AI system design üß™ Results Experiments across multiple deep search benchmarks and backbone models demonstrate consistent improvements in both performance and robustness , validating the effectiveness of this cognitively grounded design.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23188",
      "pdf_url": "https://arxiv.org/pdf/2601.23188",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23188",
      "scraped_at": "2026-02-03T02:16:14.971476"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Revisiting Diffusion Model Predictions Through Dimensionality",
    "paper_url": "https://huggingface.co/papers/2601.21419",
    "authors": [
      "Chaoyang Wang",
      "Qing Jin"
    ],
    "stars": "0",
    "details": {
      "title": "Revisiting Diffusion Model Predictions Through Dimensionality",
      "abstract": "not sure which to choose: x0 prediction or velocity prediction? this paper provides a universal solution to find the optimal solution for you",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21419",
      "pdf_url": "https://arxiv.org/pdf/2601.21419",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21419",
      "scraped_at": "2026-02-03T02:16:16.886031"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Real-Time Aligned Reward Model beyond Semantics",
    "paper_url": "https://huggingface.co/papers/2601.22664",
    "authors": [
      "Jianbin Zheng",
      "Yuxi Ren",
      "Xin Xia",
      "Yikunb",
      "hzxllll"
    ],
    "stars": "0",
    "details": {
      "title": "Real-Time Aligned Reward Model beyond Semantics",
      "abstract": "RLHF is central to aligning LLMs with human preferences, but it often suffers from reward overoptimization: the policy learns to game the reward model instead of truly following human intent. A key reason? Distribution shift‚Äîthe policy keeps changing, while the reward model stays static. R2M (Real-Time Aligned Reward Model) tackles this head-on. Instead of relying only on fixed semantic features, R2M feeds on the policy‚Äôs evolving hidden states, aligning the reward model in real time with the policy‚Äôs trajectory during RL. üîë Takeaway: Reward models shouldn‚Äôt be passive judges. They should co-evolve with the policy.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22664",
      "pdf_url": "https://arxiv.org/pdf/2601.22664",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22664",
      "scraped_at": "2026-02-03T02:16:18.885339"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
    "paper_url": "https://huggingface.co/papers/2601.21525",
    "authors": [
      "Yulong Li",
      "Parul Awasthy",
      "Aashka Trivedi",
      "vishwajeetkumar",
      "meetdoshi90"
    ],
    "stars": "0",
    "details": {
      "title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs (2026) Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models (2026) CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding (2026) ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System (2026) Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings (2025) BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics (2026) Next-Embedding Prediction Makes Strong Vision Learners (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21525",
      "pdf_url": "https://arxiv.org/pdf/2601.21525",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21525",
      "scraped_at": "2026-02-03T02:16:20.790330"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Continual GUI Agents",
    "paper_url": "https://huggingface.co/papers/2601.20732",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Continual GUI Agents",
      "abstract": "Some of the observations founded are :- -- Static GUI training breaks under real world change : GUI agents trained on fixed datasets degrade badly when UI domains (mobile --> desktop --> web) or resolutions (1080p -> 4K) shift, mainly due to unstable grounding of interaction points and regions. -- SFT overfits ; RFT is more suitable for continual learning : Supervised Fine Tuning memorizes current layouts and forgets prior skills, while Reinforcement Fine Tuning ( RFT ) better preserves knowledge via KL regularized updates making it a stronger base for continual GUI agents. -- Grounding fails because interaction points and scales are in flux : Domain and resolution shifts cause large changes in element locations and sizes, and existing reward designs over adapt to static coordinates or scales, leading to poor generalization. -- GUI AiF stabilizes learning by rewarding diversity, not fixation : The proposed GUI AiF framework introduces two rewards APR-iF ( diverse interaction points ) and ARR-iF ( diverse element regions ) which prevent agents from collapsing onto single layouts. -- Generalizing interaction points matters more than scale : Ablations show APR-iF contributes more than ARR-iF, indicating that adapting to where to interact is more critical than adapting to how big elements are in continual GUI environments. ..... many more...",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20732",
      "pdf_url": "https://arxiv.org/pdf/2601.20732",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20732",
      "scraped_at": "2026-02-03T02:16:22.654011"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
    "paper_url": "https://huggingface.co/papers/2601.15625",
    "authors": [
      "Bin Liang",
      "Zezhong Wang",
      "Rui Wang",
      "Zhiwei Zhang",
      "Hiiamein"
    ],
    "stars": "0",
    "details": {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "abstract": "Robust Tool Use via FISSION-GRPO: Learning to Recover from Execution Errors",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15625",
      "pdf_url": "https://arxiv.org/pdf/2601.15625",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15625",
      "scraped_at": "2026-02-03T02:16:24.533675"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
    "paper_url": "https://huggingface.co/papers/2601.22141",
    "authors": [
      "Michal Byra",
      "Alberto Presta",
      "GrzegorzStefanski"
    ],
    "stars": "0",
    "details": {
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "abstract": "This work challenges a core assumption of the Lottery Ticket Hypothesis: that a single sparse subnetwork can serve all data. The authors show that under heterogeneity, multiple specialized winning tickets outperform a universal one, reframing pruning as a mechanism for structural specialization rather than pure compression.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22141",
      "pdf_url": "https://arxiv.org/pdf/2601.22141",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22141",
      "scraped_at": "2026-02-03T02:16:26.440644"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
    "paper_url": "https://huggingface.co/papers/2601.22032",
    "authors": [],
    "stars": "32",
    "details": {
      "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
      "abstract": "End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22032",
      "pdf_url": "https://arxiv.org/pdf/2601.22032",
      "github_links": [
        "https://github.com/linhanwang/Drive-JEPA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22032",
      "scraped_at": "2026-02-03T02:16:28.394820"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis",
    "paper_url": "https://huggingface.co/papers/2601.21709",
    "authors": [
      "Xialiang Tong",
      "Yinqi Bai",
      "Xing Li",
      "Jie Wang",
      "Qingyue Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis",
      "abstract": "We systematically analyze attention patterns from a unified temporal perspective and find that embedding temporal self-similarity and RoPE are key factors underlying streaming, retrieval, seasonal, and reaccess attention patterns. We further apply time-series analysis methodologies to study attention scores and their dynamics, which is interesting.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21709",
      "pdf_url": "https://arxiv.org/pdf/2601.21709",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21709",
      "scraped_at": "2026-02-03T02:16:30.682423"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Memorization Dynamics in Knowledge Distillation for Language Models",
    "paper_url": "https://huggingface.co/papers/2601.15394",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Memorization Dynamics in Knowledge Distillation for Language Models",
      "abstract": "We show that knowledge distillation in language models can give both improved generalization and reduced memorization.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.15394",
      "pdf_url": "https://arxiv.org/pdf/2601.15394",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.15394",
      "scraped_at": "2026-02-03T02:16:32.546033"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Machine Learning for Energy-Performance-aware Scheduling",
    "paper_url": "https://huggingface.co/papers/2601.23134",
    "authors": [
      "Yifei Shi",
      "Peter2023HuggingFace"
    ],
    "stars": "1",
    "details": {
      "title": "Machine Learning for Energy-Performance-aware Scheduling",
      "abstract": "Machine Learning for Energy-Performance-aware Scheduling. @ misc {HuShi2026mlcpusched,\n      title={Machine Learning for Energy-Performance-aware Scheduling}, \n      author={Zheyuan Hu and Yifei Shi},\n      year={2026},\n      eprint={2601.23134},\n      archivePrefix={arXiv},\n      primaryClass={cs.AR},\n      url={https://arxiv.org/abs/2601.23134}, \n}",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23134",
      "pdf_url": "https://arxiv.org/pdf/2601.23134",
      "github_links": [
        "https://github.com/PeterHUistyping/ml-cpu-sched"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23134",
      "scraped_at": "2026-02-03T02:16:34.420823"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Visual Personalization Turing Test",
    "paper_url": "https://huggingface.co/papers/2601.22680",
    "authors": [
      "Kuan-Chieh Jackson Wang",
      "Sergey Tulyakov",
      "James Burgess",
      "Rameen Abdal"
    ],
    "stars": "0",
    "details": {
      "title": "Visual Personalization Turing Test",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22680",
      "pdf_url": "https://arxiv.org/pdf/2601.22680",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22680",
      "scraped_at": "2026-02-03T02:16:36.286484"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding",
    "paper_url": "https://huggingface.co/papers/2601.22666",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding",
      "abstract": "Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attentionbased soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangianconstrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 APron the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference efficient.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22666",
      "pdf_url": "https://arxiv.org/pdf/2601.22666",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22666",
      "scraped_at": "2026-02-03T02:16:38.163159"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Value-Based Pre-Training with Downstream Feedback",
    "paper_url": "https://huggingface.co/papers/2601.22108",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "abstract": "We‚Äôre entering the age of research, not just the age of scaling. Bigger models gave us horsepower. But pretraining still has almost no steering wheel. Today‚Äôs foundation models learn in an open loop: pick a proxy objective (next‚Äëtoken / fixed augmentations) ‚Üí burn trillions of tokens ‚Üí hope the capabilities we care about ‚Äúemerge‚Äù. That hope is getting expensive. If the ‚ÄúAGI won‚Äôt happen from brute-force scaling‚Äù camp is even partly right, then the bottleneck is clear: value per gradient step. So we asked a practical question: Can a small amount of verified goal information steer the massive unlabeled pretraining phase‚Äîwithout turning pretraining into supervised finetuning? A potential answer: V‚ÄëPretraining (Value‚ÄëBased Pre‚ÄëTraining with Downstream Feedback). https://arxiv.org/abs/2601.22108 . Idea: add a lightweight task designer that reshapes the self‚Äësupervised task so each unlabeled update is more useful downstream.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22108",
      "pdf_url": "https://arxiv.org/pdf/2601.22108",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22108",
      "scraped_at": "2026-02-03T02:16:40.023794"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding",
    "paper_url": "https://huggingface.co/papers/2601.21666",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding",
      "abstract": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal LLMs on Audio-Video Understanding SONIC-O1 is a fully human-verified benchmark for real-world audio‚Äìvideo conversations: 13 conversational domains, 4,958 annotated instances, plus demographic metadata for group-wise analysis. It evaluates models on open-ended summarization, MCQ QA, and temporal localization with supporting rationales‚Äîand we find temporal grounding is still a major pain point (e.g., a 22.6% gap between the best closed vs. open model families on temporal localization).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21666",
      "pdf_url": "https://arxiv.org/pdf/2601.21666",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21666",
      "scraped_at": "2026-02-03T02:16:41.889521"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
    "paper_url": "https://huggingface.co/papers/2601.21526",
    "authors": [],
    "stars": "56",
    "details": {
      "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
      "abstract": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes. KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance. Code Available at: https://github.com/Leeroo-AI/kapso",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21526",
      "pdf_url": "https://arxiv.org/pdf/2601.21526",
      "github_links": [
        "https://github.com/Leeroo-AI/kapso"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21526",
      "scraped_at": "2026-02-03T02:16:43.760977"
    },
    "scraped_date": "2026-02-03"
  },
  {
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "paper_url": "https://huggingface.co/papers/2602.00919",
    "authors": [],
    "stars": "24",
    "details": {
      "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
      "abstract": "TL;DR: Scaling VLA isn‚Äôt enough‚Äîyou need quality-aligned trajectories + a unified action interface + staged RL refinement to get reliable cross-robot generalization. This work (1) introduces a unified R64 action space with a fixed semantic layout plus embodiment/control-type prompts and a masked BC loss so unused DoFs don‚Äôt inject spurious gradients, (2) normalizes heterogeneous demonstration speeds via optical-flow‚Äìbased temporal resampling to align motion statistics across datasets, and (3) follows a staged recipe R0 ‚Üí R1 ‚Üí R2, where R2 RL alignment explicitly targets long-horizon consistency and error recovery. On real bimanual table cleaning (ALOHA), it reaches 69.5% first-item success vs 35.6% for the baseline and is ~2√ó faster (1m35s vs 2m59s). On Simpler (Google Robot), performance improves from 60.2 (R0) to 71.8 (R2). A nice practical touch: an episode-end prediction head reduces ‚Äúpost-success fidgeting‚Äù that can flip successes into failures. Project Page: https://greenvla.github.io/ Code: https://github.com/greenvla/GreenVLA",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00919",
      "pdf_url": "https://arxiv.org/pdf/2602.00919",
      "github_links": [
        "https://github.com/greenvla/GreenVLA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00919",
      "scraped_at": "2026-02-04T02:09:27.200013"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Kimi K2.5: Visual Agentic Intelligence",
    "paper_url": "https://huggingface.co/papers/2602.02276",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kimi K2.5: Visual Agentic Intelligence",
      "abstract": "amazing <3",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02276",
      "pdf_url": "https://arxiv.org/pdf/2602.02276",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02276",
      "scraped_at": "2026-02-04T02:09:29.260821"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.22060",
    "authors": [
      "Zhen Fang",
      "Qiuchen Wang",
      "Lin-Chen",
      "YuZeng260",
      "Osilly"
    ],
    "stars": "96",
    "details": {
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "abstract": "We present the first long-horizon multimodal deep-research MLLM, introducing multi-turn, multi-entity, and multi-scale visual/textual search, and scaling up to dozens of reasoning steps and hundreds of search-engine interactions. By combining VQA synthesis, trajectory synthesis, cold-start supervision, and reinforcement learning, we internalize deep-research capabilities into the MLLM. With 8B and 30B-A3B models, we achieve state-of-the-art performance on six mainstream fact-centric VQA benchmarks, surpassing deep-research systems built on strong proprietary models such as GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22060",
      "pdf_url": "https://arxiv.org/pdf/2601.22060",
      "github_links": [
        "https://github.com/Osilly/Vision-DeepResearch"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22060",
      "scraped_at": "2026-02-04T02:09:31.283956"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
    "paper_url": "https://huggingface.co/papers/2602.02185",
    "authors": [
      "gaotiexinqu",
      "chocckaka",
      "Lin-Chen",
      "Osilly",
      "YuZeng260"
    ],
    "stars": "0",
    "details": {
      "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
      "abstract": "We introduce the Vision-DeepResearch Benchmark (VDR-Bench) to address two key limitations of existing multimodal deep-research benchmarks: (1) they are not visual-search-centric, allowing many instances to be solved without genuine visual retrieval; and (2) they rely on overly idealized retrieval settings that fail to reflect noisy, real-world search engines. To this end, VDR-Bench comprises 2,000 instances curated with full human involvement and rigorous solvability verification, complementing existing benchmarks by enforcing realistic visual search and evidence-grounded reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02185",
      "pdf_url": "https://arxiv.org/pdf/2602.02185",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02185",
      "scraped_at": "2026-02-04T02:09:33.226296"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
    "paper_url": "https://huggingface.co/papers/2602.02084",
    "authors": [
      "Steven Liu",
      "Qingtao Li",
      "Xin Zhang",
      "Cipherxzc",
      "Luo2003"
    ],
    "stars": "59",
    "details": {
      "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
      "abstract": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG‚Äôs high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02084",
      "pdf_url": "https://arxiv.org/pdf/2602.02084",
      "github_links": [
        "https://github.com/microsoft/RPG-ZeroRepo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02084",
      "scraped_at": "2026-02-04T02:09:35.128933"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2602.02437",
    "authors": [
      "Size Wu",
      "Feng Han",
      "Chaofan Ma",
      "Dianyi Wang",
      "CodeGoat24"
    ],
    "stars": "0",
    "details": {
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Unified Thinker: A General Reasoning Modular Core for Image Generation (2026) ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning (2025) Loom: Diffusion-Transformer for Interleaved Generation (2025) Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing (2026) Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders (2026) CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation (2026) ThinkGen: Generalized Thinking for Visual Generation (2025) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02437",
      "pdf_url": "https://arxiv.org/pdf/2602.02437",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02437",
      "scraped_at": "2026-02-04T02:09:37.088820"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
    "paper_url": "https://huggingface.co/papers/2602.02053",
    "authors": [],
    "stars": "6",
    "details": {
      "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
      "abstract": "hi",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02053",
      "pdf_url": "https://arxiv.org/pdf/2602.02053",
      "github_links": [
        "https://github.com/BstWPY/WildGraphBench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02053",
      "scraped_at": "2026-02-04T02:09:39.087666"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
    "paper_url": "https://huggingface.co/papers/2602.01566",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
      "abstract": "A deep research agent with file system as the scaling substrate, allowing external and persistent context. Although achieving maximal performance on downstream tasks still requires a lot of task-specific design at this stage, we believe that a file system has the potential to become a standard component of LLM agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01566",
      "pdf_url": "https://arxiv.org/pdf/2602.01566",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01566",
      "scraped_at": "2026-02-04T02:09:40.986852"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
    "paper_url": "https://huggingface.co/papers/2602.02361",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02361",
      "pdf_url": "https://arxiv.org/pdf/2602.02361",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02361",
      "scraped_at": "2026-02-04T02:09:43.014980"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
    "paper_url": "https://huggingface.co/papers/2602.01590",
    "authors": [],
    "stars": "4",
    "details": {
      "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
      "abstract": "Hi everyone, we have released the Wiki Live Challenge, a benchmark that uses Wikipedia Good Articles as a high-level human baseline. It is designed to evaluate the writing quality and information-gathering capabilities of Deep Research Agents in authoring Wikipedia content. Our results indicate that there is still a gap between current DRAs and real-world human experts in this domain.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01590",
      "pdf_url": "https://arxiv.org/pdf/2602.01590",
      "github_links": [
        "https://github.com/WangShao2000/Wiki_Live_Challenge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01590",
      "scraped_at": "2026-02-04T02:09:44.973680"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
    "paper_url": "https://huggingface.co/papers/2602.02493",
    "authors": [],
    "stars": "51",
    "details": {
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen .",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02493",
      "pdf_url": "https://arxiv.org/pdf/2602.02493",
      "github_links": [
        "https://github.com/Zehong-Ma/PixelGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02493",
      "scraped_at": "2026-02-04T02:09:46.942656"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2602.01058",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
      "abstract": "A good objective for supervised post-training is commonly taken as one that optimizes for performance after supervised stage. But when this supervised stage is followed by an online RL stage,  SFT stage gains may not be preserved after online RL .  This paper experiments with a variety of supervised objectives, and finds that the out-of-the-box performance of these objectives often change after subsequent RL. This highlights a mismatch between these two goals. This paper proposes a reweighing mechanism for standard supervised losses designed to weigh each token using the effect of learning on that token on RL stage. This paper presents an approach based inspired by off-policy evaluation to compute weights based on the likelihood of continuation from each starting point. The paper includes multiple practical variants based on that principle and demonstrates the effectiveness.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01058",
      "pdf_url": "https://arxiv.org/pdf/2602.01058",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01058",
      "scraped_at": "2026-02-04T02:09:49.173825"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
    "paper_url": "https://huggingface.co/papers/2602.02488",
    "authors": [],
    "stars": "193",
    "details": {
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "abstract": "Code and model: https://github.com/Gen-Verse/Open-AgentRL",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02488",
      "pdf_url": "https://arxiv.org/pdf/2602.02488",
      "github_links": [
        "https://github.com/Gen-Verse/Open-AgentRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02488",
      "scraped_at": "2026-02-04T02:09:51.193498"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
    "paper_url": "https://huggingface.co/papers/2602.02383",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
      "abstract": "We introduce SLIME, a reference-free preference optimization objective designed to decouple preference learning from generation quality. Our approach uses a three-pronged objective: Likelihood Anchoring: An explicit term to maximize the likelihood of the preferred response, preventing quality degradation. Token-Level Stabilization: A softplus-based penalty that prevents rejected token probabilities from collapsing to zero, preserving linguistic fluency. Dual-Margin Mechanism: A novel combination of hard and soft margins for precise boundary shaping without vanishing gradients.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02383",
      "pdf_url": "https://arxiv.org/pdf/2602.02383",
      "github_links": [
        "https://github.com/fpsigma/trl-slime"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02383",
      "scraped_at": "2026-02-04T02:09:53.118260"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
    "paper_url": "https://huggingface.co/papers/2602.01624",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
      "abstract": "PISCES is an annotation-free post-training framework for text-to-video models. We tackle a key bottleneck in VLM-based rewards, text/video embedding misalignment, by using Optimal Transport (OT) to align text embeddings to the video space. This yields dual OT-aligned rewards that mimic how humans judge T2V: a distributional quality reward and a token-level semantic reward, improving fidelity and prompt faithfulness across short and long video generators.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01624",
      "pdf_url": "https://arxiv.org/pdf/2602.01624",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01624",
      "scraped_at": "2026-02-04T02:09:55.125376"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
    "paper_url": "https://huggingface.co/papers/2602.01756",
    "authors": [
      "Chenjue Zhang",
      "Dongzhi Jiang",
      "Junyan Ye",
      "Jun He",
      "SereinH"
    ],
    "stars": "36",
    "details": {
      "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
      "abstract": "üß† Mind-Brush Framework: A novel agentic paradigm that unifies Intent Analysis, Multi-modal Search, and Knowledge Reasoning into a seamless \"Think-Research-Create\" workflow for image generation. üìä Mind-Bench: A specialized benchmark designed to evaluate generative models on dynamic external knowledge and complex logical deduction, exposing the reasoning gaps in current SOTA multimodal models. üèÜ Superior Performance: Elevates Qwen-Image baseline accuracy from 0.02 to 0.31 on Mind-Bench. Outperforms existing baselines on WISE (+25.8% WiScore) and RISEBench (+27.3% Accuracy).",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01756",
      "pdf_url": "https://arxiv.org/pdf/2602.01756",
      "github_links": [
        "https://github.com/PicoTrex/Mind-Brush"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01756",
      "scraped_at": "2026-02-04T02:09:57.095306"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
    "paper_url": "https://huggingface.co/papers/2602.02214",
    "authors": [],
    "stars": "164",
    "details": {
      "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
      "abstract": "Causal Forcing exposes a mathematical fallacy in Self Forcing and significantly outperforms it in both visual quality and motion dynamics , while maintaining the same training budget and inference efficiency , enabling real time streaming video generation on a single RTX 4090. Code (full-stack open source): https://github.com/thu-ml/Causal-Forcing Page: https://thu-ml.github.io/CausalForcing.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02214",
      "pdf_url": "https://arxiv.org/pdf/2602.02214",
      "github_links": [
        "https://github.com/thu-ml/Causal-Forcing"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02214",
      "scraped_at": "2026-02-04T02:09:59.088987"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
    "paper_url": "https://huggingface.co/papers/2602.01801",
    "authors": [
      "Gal Chechik",
      "Micahel Green",
      "Matan Levy",
      "Issar Tzachor",
      "Dvir Samuel"
    ],
    "stars": "0",
    "details": {
      "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
      "abstract": "Project Page: https://dvirsamuel.github.io/fast-auto-regressive-video/",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01801",
      "pdf_url": "https://arxiv.org/pdf/2602.01801",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01801",
      "scraped_at": "2026-02-04T02:10:01.051872"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Rethinking Selective Knowledge Distillation",
    "paper_url": "https://huggingface.co/papers/2602.01395",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Rethinking Selective Knowledge Distillation",
      "abstract": "For the GitHub repo: https://github.com/almogtavor/SE-KD3x",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01395",
      "pdf_url": "https://arxiv.org/pdf/2602.01395",
      "github_links": [
        "https://github.com/almogtavor/SE-KD3x"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01395",
      "scraped_at": "2026-02-04T02:10:03.048964"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Generative Visual Code Mobile World Models",
    "paper_url": "https://huggingface.co/papers/2602.01576",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Generative Visual Code Mobile World Models",
      "abstract": "Coming soon: Website, demo video, github, and eval dataset",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01576",
      "pdf_url": "https://arxiv.org/pdf/2602.01576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01576",
      "scraped_at": "2026-02-04T02:10:05.069765"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
    "paper_url": "https://huggingface.co/papers/2602.02092",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation (2026) OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion (2026) Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context (2025) VideoAR: Autoregressive Video Generation via Next-Frame&Scale Prediction (2026) SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices (2026) Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance (2025) EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02092",
      "pdf_url": "https://arxiv.org/pdf/2602.02092",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02092",
      "scraped_at": "2026-02-04T02:10:07.064459"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
    "paper_url": "https://huggingface.co/papers/2602.01541",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01541",
      "pdf_url": "https://arxiv.org/pdf/2602.01541",
      "github_links": [
        "https://github.com/PediaMedAI/Cognition-MLLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01541",
      "scraped_at": "2026-02-04T02:10:09.121705"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
    "paper_url": "https://huggingface.co/papers/2602.01479",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
      "abstract": "Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01479",
      "pdf_url": "https://arxiv.org/pdf/2602.01479",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01479",
      "scraped_at": "2026-02-04T02:10:11.080868"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
    "paper_url": "https://huggingface.co/papers/2602.01335",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
      "abstract": "This paper introduces Visual Metaphor Transfer (VMT), a new task that goes beyond pixel-level editing to model abstract, cross-domain creative logic in visual generation. Inspired by Conceptual Blending Theory, the authors propose a schema-based, multi-agent framework that explicitly decouples metaphorical essence from visual appearance and re-instantiates it on new subjects. Extensive human studies show clear gains in metaphor consistency and creative quality, highlighting strong potential for high-impact applications in advertising and media.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01335",
      "pdf_url": "https://arxiv.org/pdf/2602.01335",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01335",
      "scraped_at": "2026-02-04T02:10:12.999163"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
    "paper_url": "https://huggingface.co/papers/2602.01851",
    "authors": [
      "Haochen Tian",
      "Chen Liang",
      "Chengzu Li",
      "Xuehai Bai",
      "Huanyu Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
      "abstract": "üöÄ Introducing VIBE: The Visual Instruction Benchmark for Image Editing! Why limit image editing to text? Human intent is multimodal. We‚Äôre filling the gap with VIBE , a new benchmark designed to evaluate how models follow visual instructions. Check out our full analysis and dataset below!üëá üìÑ Paper: https://arxiv.org/abs/2602.01851 üíª Github: https://github.com/hwanyu112/VIBE-Benchmark üåê Project Page: https://vibe-benchmark.github.io/ ü§ó Huggingface: https://huggingface.co/datasets/VIBE-Benchmark/VIBE-Benchmark",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01851",
      "pdf_url": "https://arxiv.org/pdf/2602.01851",
      "github_links": [
        "https://github.com/hwanyu112/VIBE-Benchmark"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01851",
      "scraped_at": "2026-02-04T02:10:14.953196"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
    "paper_url": "https://huggingface.co/papers/2602.01538",
    "authors": [
      "Teng Hu",
      "Ziyao Huang",
      "Zhentao Yu",
      "Zhengguang Zhou",
      "youliang1233214"
    ],
    "stars": "0",
    "details": {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "abstract": "link: https://arxiv.org/abs/2602.01538",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01538",
      "pdf_url": "https://arxiv.org/pdf/2602.01538",
      "github_links": [
        "https://github.com/angzong/InteractAvatar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01538",
      "scraped_at": "2026-02-04T02:10:16.894226"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
    "paper_url": "https://huggingface.co/papers/2602.02486",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "abstract": "We proposed RE-TRAC, a recursive framework addresses the inefficiency of disjointed traditional agent search by compressing historical trajectories to guide subsequent steps. Experiments demonstrate that this  explicit guidance mechanism not only significantly outperforms ReAct but also enables remarkable performance leaps on smaller models (e.g., 4B) via fine-tuning.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02486",
      "pdf_url": "https://arxiv.org/pdf/2602.02486",
      "github_links": [
        "https://github.com/microsoft/InfoAgent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02486",
      "scraped_at": "2026-02-04T02:10:18.854834"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
    "paper_url": "https://huggingface.co/papers/2602.02472",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
      "abstract": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing Signal Preservation And symmetRy breaKing for width-progressive LearnING), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2√ó width expansion.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02472",
      "pdf_url": "https://arxiv.org/pdf/2602.02472",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02472",
      "scraped_at": "2026-02-04T02:10:20.780970"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "paper_url": "https://huggingface.co/papers/2602.02343",
    "authors": [],
    "stars": "2.71k",
    "details": {
      "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
      "abstract": "We unify LLM control methods as dynamic weight updates, analyze their trade-offs between preference (targeted behavior) and utility (task-valid generation) via a shared log-odds framework, explain these effects through activation manifolds, and introduce SPLIT, a steering method that enhances preference while better preserving utility.",
      "arxiv_page_url": "https://arxiv.org/abs/2508.11290",
      "pdf_url": "https://arxiv.org/pdf/2602.02343",
      "github_links": [
        "https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02343",
      "scraped_at": "2026-02-04T02:10:22.726096"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
    "paper_url": "https://huggingface.co/papers/2602.02227",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
      "abstract": "Code: https://github.com/EnVision-Research/LatentMorph",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02227",
      "pdf_url": "https://arxiv.org/pdf/2602.02227",
      "github_links": [
        "https://github.com/EnVision-Research/LatentMorph"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02227",
      "scraped_at": "2026-02-04T02:10:24.666506"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "CUA-Skill: Develop Skills for Computer Using Agent",
    "paper_url": "https://huggingface.co/papers/2601.21123",
    "authors": [],
    "stars": "8",
    "details": {
      "title": "CUA-Skill: Develop Skills for Computer Using Agent",
      "abstract": "Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at this .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21123",
      "pdf_url": "https://arxiv.org/pdf/2601.21123",
      "github_links": [
        "https://github.com/microsoft/cua_skill"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21123",
      "scraped_at": "2026-02-04T02:10:26.583113"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
    "paper_url": "https://huggingface.co/papers/2602.02156",
    "authors": [
      "Yexin Liu",
      "Rui-Jie Zhu",
      "Wen-Jie Shu",
      "Harold328",
      "Xuerui123"
    ],
    "stars": "0",
    "details": {
      "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
      "abstract": "Code: https://github.com/WenjieShu/LoopViT",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02156",
      "pdf_url": "https://arxiv.org/pdf/2602.02156",
      "github_links": [
        "https://github.com/WenjieShu/LoopViT"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02156",
      "scraped_at": "2026-02-04T02:10:28.500608"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
    "paper_url": "https://huggingface.co/papers/2602.02453",
    "authors": [
      "Muyun Yang",
      "Yuchen Song",
      "Qiuyu Ding",
      "Wenxin Zhu",
      "AndongChen"
    ],
    "stars": "4",
    "details": {
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02453",
      "pdf_url": "https://arxiv.org/pdf/2602.02453",
      "github_links": [
        "https://github.com/andongBlue/Think-with-Comics"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02453",
      "scraped_at": "2026-02-04T02:10:30.460982"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
    "paper_url": "https://huggingface.co/papers/2602.01322",
    "authors": [
      "Mihalis Nicolaou",
      "Yannis Panagakis",
      "James Oldfield",
      "Andreas D. Demou",
      "Panagiotis Koromilas"
    ],
    "stars": "0",
    "details": {
      "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
      "abstract": "PolySAE: Modeling Feature Interactions via Polynomial Decoding What: We generalize Sparse Autoencoders to capture feature interactions via polynomial decoding while preserving linear, interpretable encodings. Why: Standard SAEs assume features combine additively through linear reconstruction, which conflates compositional concepts with co-occurrence‚Äîthey can't distinguish whether \"Starbucks\" emerges from composing \"star\" + \"coffee\" features or merely their frequent overlap, forcing monolithic features for compound concepts. How: We extend the decoder with quadratic and cubic terms to model interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization, PolySAE captures pairwise and triple feature interactions with minimal parameter overhead (~3% on GPT-2 Small). Results: Across 4 LLMs and 3 sparsifiers, we achieve an average 8% improvement in probing F1 and 2‚Äì10√ó larger Wasserstein distances between class-conditional distributions, while maintaining comparable reconstruction error. Beyond Co-occurrence: Learned interaction weights show negligible correlation with co-occurrence frequency (r = 0.06 vs. r = 0.82 for SAE feature covariance), suggesting the model captures compositional semantics rather than surface statistics. Interpretable Interactions: Qualitative analysis reveals meaningful interaction patterns with examples \"invest\" + \"-ing\" + \"stock\" ‚Üí financial contexts, \"star\" + \"coffee\" ‚Üí Starbucks, or \"nuclear\" + \"test\" + \"radiation\" ‚Üí weapons testing. Plug-and-Play: PolySAE is a strict generalization of standard SAEs‚Äîsetting interaction coefficients to zero recovers vanilla behavior‚Äîand is compatible with SAE architectures (i.e., TopK, BatchTopK, Matryoshka, etc.).",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01322",
      "pdf_url": "https://arxiv.org/pdf/2602.01322",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01322",
      "scraped_at": "2026-02-04T02:10:32.367379"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Sparse Reward Subsystem in Large Language Models",
    "paper_url": "https://huggingface.co/papers/2602.00986",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Sparse Reward Subsystem in Large Language Models",
      "abstract": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00986",
      "pdf_url": "https://arxiv.org/pdf/2602.00986",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00986",
      "scraped_at": "2026-02-04T02:10:34.285628"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
    "paper_url": "https://huggingface.co/papers/2602.01675",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
      "abstract": "We are motivated by the gap between existing LLM-agent benchmarks and real deployment needs, where agents must handle long, multi-turn interactions, satisfy global constraints, and coordinate tools under frequent user revisions. We introduce TRIP-Bench, a realistic travel-planning benchmark with 18 tools, 40+ constraint types, and automated evaluation across difficulty splits, and show that even strong models degrade sharply on harder long-horizon dialogues. Finally, we propose GTPO, an online multi-turn RL method that improves constraint satisfaction and robustness on TRIP-Bench.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01675",
      "pdf_url": "https://arxiv.org/pdf/2602.01675",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01675",
      "scraped_at": "2026-02-04T02:10:36.376078"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
    "paper_url": "https://huggingface.co/papers/2602.01511",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
      "abstract": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01511",
      "pdf_url": "https://arxiv.org/pdf/2602.01511",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01511",
      "scraped_at": "2026-02-04T02:10:38.302166"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
    "paper_url": "https://huggingface.co/papers/2601.20613",
    "authors": [
      "Tianhao Tang",
      "Taiyu Hou",
      "Qimin Wu",
      "Kaiyuan Chen",
      "YuanshuoZhang"
    ],
    "stars": "0",
    "details": {
      "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
      "abstract": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.20613",
      "pdf_url": "https://arxiv.org/pdf/2601.20613",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.20613",
      "scraped_at": "2026-02-04T02:10:40.268939"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
    "paper_url": "https://huggingface.co/papers/2602.01660",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
      "abstract": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01660",
      "pdf_url": "https://arxiv.org/pdf/2602.01660",
      "github_links": [
        "https://github.com/ALEX-nlp/CoDiQ"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01660",
      "scraped_at": "2026-02-04T02:10:42.338922"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models",
    "paper_url": "https://huggingface.co/papers/2602.00269",
    "authors": [
      "Stephanie Wang",
      "Rohan Kadekodi",
      "Atindra Jha",
      "Wei-Tzu Lee",
      "Keisuke Kamahori"
    ],
    "stars": "29",
    "details": {
      "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models",
      "abstract": "A serving system for SpeechLMs https://github.com/vox-serve/vox-serve",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00269",
      "pdf_url": "https://arxiv.org/pdf/2602.00269",
      "github_links": [
        "https://github.com/vox-serve/vox-serve"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00269",
      "scraped_at": "2026-02-04T02:10:44.331707"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
    "paper_url": "https://huggingface.co/papers/2602.02477",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
      "abstract": "We introduce an end-to-end RL framework to endow LLMs with divide-and-conquer reasoning capabilities, enabling a higher performance ceiling and stronger test-time scalability.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02477",
      "pdf_url": "https://arxiv.org/pdf/2602.02477",
      "github_links": [
        "https://github.com/MasterVito/DAC-RL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02477",
      "scraped_at": "2026-02-04T02:10:46.344215"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
    "paper_url": "https://huggingface.co/papers/2602.02338",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2602.02338",
      "pdf_url": "https://arxiv.org/pdf/2602.02338",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02338",
      "scraped_at": "2026-02-04T02:10:48.310838"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation",
    "paper_url": "https://huggingface.co/papers/2602.01382",
    "authors": [],
    "stars": "59",
    "details": {
      "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation",
      "abstract": "Image Editing Text-to-Image Generation",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01382",
      "pdf_url": "https://arxiv.org/pdf/2602.01382",
      "github_links": [
        "https://github.com/G-U-N/UniRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01382",
      "scraped_at": "2026-02-04T02:10:50.268582"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2602.00759",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "abstract": "This is a new work by Renmin University of China and ByteDance Seed. We introduce a novel RLVR algorithm that allows a single base model to evolve into two complementary models i.e., Decomposer and Reasoner, which can mutually reinforce each other. We warmly welcome the community‚Äôs interest and feedback!",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00759",
      "pdf_url": "https://arxiv.org/pdf/2602.00759",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00759",
      "scraped_at": "2026-02-04T02:10:52.197984"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "An Empirical Study of World Model Quantization",
    "paper_url": "https://huggingface.co/papers/2602.02110",
    "authors": [],
    "stars": "930",
    "details": {
      "title": "An Empirical Study of World Model Quantization",
      "abstract": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at this https URL .",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02110",
      "pdf_url": "https://arxiv.org/pdf/2602.02110",
      "github_links": [
        "https://github.com/huawei-noah/noah-research/tree/master/QuantWM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02110",
      "scraped_at": "2026-02-04T02:10:54.060979"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
    "paper_url": "https://huggingface.co/papers/2602.02039",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
      "abstract": "This paper introduces Deep Data Research , shifting from executional intelligence , which focuses on completing assigned tasks, to investigatory intelligence , where agents autonomously set goals and explore. Under this paradigm, Agentic LLMs are allowed to explore databases freely, discovering insights behind the data, without any predefined queries, questions, or objectives. The LLM-generated insights are evaluated against a fact checklist derived from the freeform components of the database, which naturally yields an aligned and objective evaluation. Beyond presenting DDR-Bench, the paper's experimental analysis reveals some insights into investigatory intelligence: Inference-time Scaling Dynamics : Top models exhibit \"quality over quantity\". They delay commitment and concentrate reasoning into a few high-value late-stage interactions. Token scaling shows flat-then-sharp patterns where final-stage tokens deliver disproportionate value, signalling depth-first exploration after breadth-oriented search. Balanced Exploration Regime : Entropy-based visualisation reveals advanced models consistently operate in a balanced regime combining coverage with focus, supporting an \"implicit planning hypothesis\" where strong models maintain coherent exploration strategies without explicit scaffolding. Training Trumps Scaling : Analysis of the Qwen family shows parameter scaling alone yields marginal gains (under 3% from 10√ó parameters), and longer context windows don't consistently help. However, cross-generation models with agentic-first training, including targeted pre-training and reinforcement learning, achieve substantially higher ceilings despite fewer activated parameters, demonstrating that meaningful agency requires intentional training strategies over mere scale. Scaffolding Paradox : Adding sophisticated frameworks like reasoning or memory mechanisms leads to unpredictable behaviours and may degrade performance. Proactive versus reactive comparison shows substantial gaps, confirming that autonomous goal-setting demands far exceed executing predefined objectives. Failure Modes : 58% of errors stem from insufficient exploration breadth/depth, while 40% involve other issues: powerful models over-reason with unsupported assumptions, weaker models struggle with instruction-following.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02039",
      "pdf_url": "https://arxiv.org/pdf/2602.02039",
      "github_links": [
        "https://github.com/thinkwee/DDR_Bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02039",
      "scraped_at": "2026-02-04T02:10:56.024349"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
    "paper_url": "https://huggingface.co/papers/2601.22588",
    "authors": [
      "Yiming Zeng",
      "Yuelyu Ji",
      "Ming Li",
      "Zhuochun Li",
      "ReRaWo"
    ],
    "stars": "7",
    "details": {
      "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
      "abstract": "The paper motivates a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22588",
      "pdf_url": "https://arxiv.org/pdf/2601.22588",
      "github_links": [
        "https://github.com/zhuochunli/Representation-as-a-judge"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22588",
      "scraped_at": "2026-02-04T02:10:58.102904"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling",
    "paper_url": "https://huggingface.co/papers/2602.01984",
    "authors": [
      "Seong Joon Oh",
      "Yejin Kim",
      "Dongjun Hwang",
      "Yeji Park",
      "MYMY-young"
    ],
    "stars": "2",
    "details": {
      "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling",
      "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01984",
      "pdf_url": "https://arxiv.org/pdf/2602.01984",
      "github_links": [
        "https://github.com/MYMY-young/DelimScaling"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01984",
      "scraped_at": "2026-02-04T02:10:59.964784"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
    "paper_url": "https://huggingface.co/papers/2602.01842",
    "authors": [
      "Qingyu Shi",
      "Yi Xin",
      "Yuchen Zhu",
      "Yixuan Li",
      "BryanW"
    ],
    "stars": "5",
    "details": {
      "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
      "abstract": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at this https URL .",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01842",
      "pdf_url": "https://arxiv.org/pdf/2602.01842",
      "github_links": [
        "https://github.com/viiika/Prism"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01842",
      "scraped_at": "2026-02-04T02:11:01.864285"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Interacted Planes Reveal 3D Line Mapping",
    "paper_url": "https://huggingface.co/papers/2602.01296",
    "authors": [],
    "stars": "19",
    "details": {
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "abstract": "Structured 3D reconstruction",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01296",
      "pdf_url": "https://arxiv.org/pdf/2602.01296",
      "github_links": [
        "https://github.com/calmke/LiPMAP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01296",
      "scraped_at": "2026-02-04T02:11:03.793960"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2602.01077",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
      "abstract": "Code: https://github.com/xie-lab-ml/piecewise-sparse-attention",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01077",
      "pdf_url": "https://arxiv.org/pdf/2602.01077",
      "github_links": [
        "https://github.com/xie-lab-ml/piecewise-sparse-attention"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01077",
      "scraped_at": "2026-02-04T02:11:05.795195"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
    "paper_url": "https://huggingface.co/papers/2601.22674",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
      "abstract": "An efficient vision token compression framework with two modules, Dominant Vision Token Selection (DVTS) and Text-Guided Vision Complement (TGVC).",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22674",
      "pdf_url": "https://arxiv.org/pdf/2601.22674",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22674",
      "scraped_at": "2026-02-04T02:11:07.699775"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
    "paper_url": "https://huggingface.co/papers/2602.00130",
    "authors": [
      "rockerritesh"
    ],
    "stars": "0",
    "details": {
      "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
      "abstract": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks.",
      "arxiv_page_url": "https://arxiv.org/abs/2505.08727",
      "pdf_url": "https://arxiv.org/pdf/2602.00130",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00130",
      "scraped_at": "2026-02-04T02:11:09.584525"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
    "paper_url": "https://huggingface.co/papers/2602.01997",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
      "abstract": "TLDR; Layer pruning compresses LLMs with little impact on classification, but severely degrades generative reasoning by disrupting core algorithmic abilities like arithmetic and syntax. Self-generated supervision improves recovery, yet explains clear practical limits to depth reduction under realistic training constraints.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01997",
      "pdf_url": "https://arxiv.org/pdf/2602.01997",
      "github_links": [
        "https://github.com/safal312/on-the-limits-of-layer-pruning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01997",
      "scraped_at": "2026-02-04T02:11:11.509574"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
    "paper_url": "https://huggingface.co/papers/2602.01983",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
      "abstract": "This paper introduces UCT, a training-free framework that enables LLM agents to evolve during inference by transforming reasoning experience into reusable tools. Unlike prior tool-augmented methods that rely on fixed or single-use tools, UCT allows agents to autonomously create, validate, reuse, and refine tools, supported by an online build loop and offline memory consolidation. Experiments across math, science, and multimodal reasoning benchmarks show significant performance gains, demonstrating a practical path toward self-evolving tool-using agents without additional training.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01983",
      "pdf_url": "https://arxiv.org/pdf/2602.01983",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01983",
      "scraped_at": "2026-02-04T02:11:13.479603"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Mano: Restriking Manifold Optimization for LLM Training",
    "paper_url": "https://huggingface.co/papers/2601.23000",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Mano: Restriking Manifold Optimization for LLM Training",
      "abstract": "Code: https://github.com/xie-lab-ml/Mano-Restriking-Manifold-Optimization-for-LLM-Training",
      "arxiv_page_url": "https://arxiv.org/abs/2601.23000",
      "pdf_url": "https://arxiv.org/pdf/2601.23000",
      "github_links": [
        "https://github.com/xie-lab-ml/Mano-Restriking-Manifold-Optimization-for-LLM-Training"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.23000",
      "scraped_at": "2026-02-04T02:11:15.368949"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Implicit neural representation of textures",
    "paper_url": "https://huggingface.co/papers/2602.02354",
    "authors": [
      "Dounia Hammou",
      "Albert Kwok",
      "Peter2023HuggingFace"
    ],
    "stars": "1",
    "details": {
      "title": "Implicit neural representation of textures",
      "abstract": "Implicit neural representation of textures @ misc {KH2026INR-Tex,\n      title={Implicit neural representation of textures}, \n      author={Albert Kwok and Zheyuan Hu and Dounia Hammou},\n      year={2026},\n      eprint={2602.02354},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2602.02354}, \n}",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02354",
      "pdf_url": "https://arxiv.org/pdf/2602.02354",
      "github_links": [
        "https://github.com/PeterHUistyping/INR-Tex"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02354",
      "scraped_at": "2026-02-04T02:11:17.244969"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages",
    "paper_url": "https://huggingface.co/papers/2602.02287",
    "authors": [
      "Linda Freienthal",
      "Isaac Chung"
    ],
    "stars": "0",
    "details": {
      "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages",
      "abstract": "Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences. This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/ cross-lingual-stability-judges.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.02287",
      "pdf_url": "https://arxiv.org/pdf/2602.02287",
      "github_links": [
        "https://github.com/isaac-chung/cross-lingual-stability-judges",
        "https://github.com/isaac-chung/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.02287",
      "scraped_at": "2026-02-04T02:11:19.241742"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
    "paper_url": "https://huggingface.co/papers/2602.01970",
    "authors": [
      "Yuhang Jiang",
      "Heming Zou",
      "Yixiu Mao",
      "Qi Wang",
      "yunqu"
    ],
    "stars": "0",
    "details": {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "abstract": "This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01970",
      "pdf_url": "https://arxiv.org/pdf/2602.01970",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01970",
      "scraped_at": "2026-02-04T02:11:21.144336"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
    "paper_url": "https://huggingface.co/papers/2602.00521",
    "authors": [
      "Bugeun Kim",
      "Hyeonchu Park",
      "Chanhee Cho",
      "Sohhyung Park",
      "Junhyuk Choi"
    ],
    "stars": "0",
    "details": {
      "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
      "abstract": ",",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00521",
      "pdf_url": "https://arxiv.org/pdf/2602.00521",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00521",
      "scraped_at": "2026-02-04T02:11:23.008184"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Clipping-Free Policy Optimization for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2601.22801",
    "authors": [
      "Xuandong Zhao",
      "G√∂zde G√ºl ≈ûahin",
      "Barƒ±≈ü Akg√ºn",
      "√ñmer Veysel √áaƒüatan"
    ],
    "stars": "0",
    "details": {
      "title": "Clipping-Free Policy Optimization for Large Language Models",
      "abstract": "Clipping-Free Policy Optimization for Large Language Models",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22801",
      "pdf_url": "https://arxiv.org/pdf/2601.22801",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22801",
      "scraped_at": "2026-02-04T02:11:24.866533"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
    "paper_url": "https://huggingface.co/papers/2602.00192",
    "authors": [
      "Adrian Popescu",
      "Elif Nebioglu",
      "emirhanbilgic"
    ],
    "stars": "0",
    "details": {
      "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
      "abstract": "Key takeaway: ùêÇùêÆùê´ùê´ùêûùêßùê≠ ùêõùêûùêßùêúùê°ùê¶ùêöùê´ùê§ùê¨ for AI-generated Image Detection, ùêúùêöùêß ùêùùê´ùêöùê¶ùêöùê≠ùê¢ùêúùêöùê•ùê•ùê≤ ùê®ùêØùêûùê´ùêûùê¨ùê≠ùê¢ùê¶ùêöùê≠ùêû ùê´ùê®ùêõùêÆùê¨ùê≠ùêßùêûùê¨ùê¨. ùêìùê´ùêöùê¢ùêßùê¢ùêßùê† ùê∞ùê¢ùê≠ùê° ùêàùêçùêè-ùêó ùêüùê®ùê´ùêúùêûùê¨ ùêùùêûùê≠ùêûùêúùê≠ùê®ùê´ùê¨ ùê≠ùê® ùêüùê®ùêúùêÆùê¨ ùê®ùêß ùê†ùêûùêßùêûùê´ùêöùê≠ùêûùêù ùêúùê®ùêßùê≠ùêûùêßùê≠, ùê¢ùê¶ùê©ùê´ùê®ùêØùê¢ùêßùê† ùê†ùêûùêßùêûùê´ùêöùê•ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß ùêöùêßùêù ùê•ùê®ùêúùêöùê•ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00192",
      "pdf_url": "https://arxiv.org/pdf/2602.00192",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00192",
      "scraped_at": "2026-02-04T02:11:26.759703"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
    "paper_url": "https://huggingface.co/papers/2601.22599",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
      "abstract": "Query-based universal sound separation is a cornerstone capability for intelligent auditory systems, yet progress is often hindered by a data bottleneck: in-the-wild datasets typically come with weak labels and heavy event co-occurrence, encouraging models to learn spurious background-category correlations rather than robust acoustic cues. This work introduces an automated data pipeline that mines high-purity single-event segments from unconstrained recordings and synthesizes mixtures using semantically consistent strategies, effectively reducing co-occurrence noise at the source. Based on this pipeline, the authors release Hive, a 2,000-hour high-quality synthetic dataset for data-efficient training. Despite using only ~0.2% of the data scale of million-hour baselines, models trained on Hive achieve competitive separation accuracy and perceptual quality, and show strong zero-shot generalization on out-of-distribution benchmarks such as MUSDB18-HQ and USS-Bench. The key takeaway is clear: prioritizing supervision purity can dramatically improve data efficiency, offering a practical path toward robust auditory foundation models with reduced compute and annotation costs.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22599",
      "pdf_url": "https://arxiv.org/pdf/2601.22599",
      "github_links": [
        "https://github.com/ShandaAI/Hive"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22599",
      "scraped_at": "2026-02-04T02:11:28.664268"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
    "paper_url": "https://huggingface.co/papers/2601.22296",
    "authors": [
      "Claudio Gallicchio",
      "Andrea Ceni",
      "Giacomo Lagomarsini",
      "nennomp"
    ],
    "stars": "0",
    "details": {
      "title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
      "abstract": "TL;DR: We revisit the Reservoir Computing paradigm through the lens of structured operators and state space modelling, introducing Parallel Echo State Networks (ParalESN). ParalESN enables the construction of high-dimensional, efficient, and parallelizable randomized Recurrent Neural Networks.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.22296",
      "pdf_url": "https://arxiv.org/pdf/2601.22296",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.22296",
      "scraped_at": "2026-02-04T02:11:30.534160"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "OVD: On-policy Verbal Distillation",
    "paper_url": "https://huggingface.co/papers/2601.21968",
    "authors": [
      "Jianghan Shen",
      "Yuxin Cheng",
      "Shansan Gong",
      "Hui Shen",
      "Jing Xiong"
    ],
    "stars": "0",
    "details": {
      "title": "OVD: On-policy Verbal Distillation",
      "abstract": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model‚Äôs exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0‚Äì9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io .",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21968",
      "pdf_url": "https://arxiv.org/pdf/2601.21968",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21968",
      "scraped_at": "2026-02-04T02:11:32.417007"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
    "paper_url": "https://huggingface.co/papers/2601.21759",
    "authors": [
      "Jaydeep Sen",
      "Yulong Li",
      "vishwajeetkumar",
      "meetdoshi90"
    ],
    "stars": "0",
    "details": {
      "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning (2025) CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval (2026) Fine-Tuned In-Context Learners for Efficient Adaptation (2025) SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines (2026) DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation (2026) LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum (2026) InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2601.21759",
      "pdf_url": "https://arxiv.org/pdf/2601.21759",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.21759",
      "scraped_at": "2026-02-04T02:11:34.282716"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Competing Visions of Ethical AI: A Case Study of OpenAI",
    "paper_url": "https://huggingface.co/papers/2601.16513",
    "authors": [
      "Madelyn Rose Sanfilippo",
      "Mengting Ai",
      "Melissa Wilfley"
    ],
    "stars": "0",
    "details": {
      "title": "Competing Visions of Ethical AI: A Case Study of OpenAI",
      "abstract": "AI Ethics is framed distinctly across actors and stakeholder groups. We report results from a case study of OpenAI analysing ethical AI discourse. Research addressed: How has OpenAI's public discourse leveraged 'ethics', 'safety', 'alignment', and adjacent related concepts over time, and what does discourse signal about framing in practice? A structured corpus, differentiating between communication for a general audience and communication with an academic audience, was assembled from public documentation. Qualitative content analysis of ethical themes combined inductively derived and deductively applied codes. Quantitative analysis leveraged computational content analysis methods via NLP to model topics and quantify changes in rhetoric over time. Visualizations report aggregate results. Results indicate that safety and risk discourse dominate OpenAI's public communication and documentation, without applying academic and advocacy ethics frameworks or vocabularies. Implications for governance are presented, along with a discussion of ethics-washing practices in industry.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.16513",
      "pdf_url": "https://arxiv.org/pdf/2601.16513",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.16513",
      "scraped_at": "2026-02-04T02:11:36.377220"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
    "paper_url": "https://huggingface.co/papers/2602.01897",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
      "abstract": "This repository implements Internal Flow Signatures, a training-free method for auditing and refining LLM decisions by analyzing depthwise hidden-state dynamics. The approach enables lightweight self-checking and targeted refinement without modifying the base model.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01897",
      "pdf_url": "https://arxiv.org/pdf/2602.01897",
      "github_links": [
        "https://github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01897",
      "scraped_at": "2026-02-04T02:11:38.254908"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
    "paper_url": "https://huggingface.co/papers/2602.01815",
    "authors": [
      "Sungsoo Ahn",
      "Jaehyung Kim",
      "Seonghyun Park",
      "Yunhui Jang"
    ],
    "stars": "0",
    "details": {
      "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
      "abstract": "This paper suggests constructing agent persona based on the research trajectory instead of static role-based prompting or keywords. This enhances the individuality of each agent, which guarantees high diversity and fact-grounding agents.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01815",
      "pdf_url": "https://arxiv.org/pdf/2602.01815",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01815",
      "scraped_at": "2026-02-04T02:11:40.136399"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
    "paper_url": "https://huggingface.co/papers/2602.01618",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
      "abstract": "Model: https://huggingface.co/collections/aisingapore/sea-guard",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01618",
      "pdf_url": "https://arxiv.org/pdf/2602.01618",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01618",
      "scraped_at": "2026-02-04T02:11:42.035572"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas",
    "paper_url": "https://huggingface.co/papers/2602.01418",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas",
      "abstract": "Parabolic Position Encoding (PaPE) We propose a position encoding that is designed from the ground up for vision modalities. It works by treating relative positions as the dependent variable in a sum of parabolas. PaPE is the highest scoring position encoding on 7 out of 8 datasets and the extrapolation beyond the training resolutions is very strong. Links Paper: https://arxiv.org/abs/2602.01418 Website: https://chrisohrstrom.github.io/parabolic-position-encoding Code: https://github.com/DTU-PAS/parabolic-position-encoding",
      "arxiv_page_url": "https://arxiv.org/abs/2602.01418",
      "pdf_url": "https://arxiv.org/pdf/2602.01418",
      "github_links": [
        "https://github.com/DTU-PAS/parabolic-position-encoding"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.01418",
      "scraped_at": "2026-02-04T02:11:43.933780"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation",
    "paper_url": "https://huggingface.co/papers/2602.00168",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation",
      "abstract": "This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architecturAl contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.00168",
      "pdf_url": "https://arxiv.org/pdf/2602.00168",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.00168",
      "scraped_at": "2026-02-04T02:11:45.866823"
    },
    "scraped_date": "2026-02-04"
  },
  {
    "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
    "paper_url": "https://huggingface.co/papers/2601.14691",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
      "abstract": "TL;DR: The assumption that an agent's Chain-of-Thought (CoT) faithfully reflects its internal reasoning and environment state is brittle, which can reflect badly on the reliability of LLM judges that use the agent reasoning for evaluation. Our key finding is that simply rewriting agent CoTs while keeping actions and observations fixed, the false positive rates of state-of-the-art VLM judges can be inflated by up to 90%.  Specialized prompting and scaling judge-time compute can reduce susceptibility; they do not fully eliminate the vulnerability to manipulation. üìù  Paper: https://arxiv.org/abs/2601.14691 Code and trajectories will be released soon.",
      "arxiv_page_url": "https://arxiv.org/abs/2601.14691",
      "pdf_url": "https://arxiv.org/pdf/2601.14691",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2601.14691",
      "scraped_at": "2026-02-04T02:11:47.726406"
    },
    "scraped_date": "2026-02-04"
  }
]