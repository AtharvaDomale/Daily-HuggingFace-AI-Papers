[
  {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "paper_url": "https://huggingface.co/papers/2512.05150",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
      "abstract": "Taming 20B full-parameter few-step training with self-adversarial flows! üëèüèª One-model Simplicity: We eliminate the need for auxiliary networks (discriminators, teachers, fake score estimators...), everything in one model! Scalability on Large Models: We transform Qwen-Image-20B into high-quality few-step generators by full-parameter training (Optimized for human figure generation!). Checkout our 2-NFE images generated by our TwinFlow-Qwen-Image! üëá We are also working on Z-Image-Turbo , stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05150",
      "pdf_url": "https://arxiv.org/pdf/2512.05150",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05150",
      "scraped_at": "2025-12-09T01:44:30.991822"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
    "paper_url": "https://huggingface.co/papers/2512.05965",
    "authors": [
      "Ziyu Guo",
      "Manyuan Zhang",
      "Longin-Yu",
      "zhengli1013",
      "appletea2333"
    ],
    "stars": "25",
    "details": {
      "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
      "abstract": "Instruction-based image editing has emerged as a prominent research area. Benefiting from image generation foundation models, it has achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to \"think\" while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions, followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produces the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05965",
      "pdf_url": "https://arxiv.org/pdf/2512.05965",
      "github_links": [
        "https://github.com/appletea233/EditThinker"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05965",
      "scraped_at": "2025-12-09T01:44:32.917190"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "paper_url": "https://huggingface.co/papers/2512.02580",
    "authors": [
      "Yang Li",
      "Shuai Zhang",
      "Yuchen Liu",
      "Jinyang Wu",
      "Changpeng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
      "abstract": "üöÄ [New Paper] CAPO: From Imitation to Discrimination ‚Äì Rethinking Advantage in RL Early RL training often suffers from instability due to \"mixed signals\" (simultaneous positive & negative feedback). Inspired by child cognitive development, we propose CAPO (Curriculum Advantage Policy Optimization) . ‚ú® The Core Intuition: Instead of a static curriculum, we leverage Advantage values to create a dynamic, two-phase process: 1Ô∏è‚É£ Imitation Phase: Train on Positive Advantage only. This reduces variance and establishes a stable behavioral foundation (Imitate to learn). 2Ô∏è‚É£ Discrimination Phase: Introduce Negative Signals later. [cite_start]This restores unbiased estimation and refines decision boundaries (Discriminate to generalize). üìà Highlights: Plug-and-Play: Delivers consistent gains when compatible with GRPO, PPO, RLOO, & Reinforce++ . Cross-Domain: Not just for Math! CAPO demonstrates impressive generalization on Multimodal GUI Agent tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02580",
      "pdf_url": "https://arxiv.org/pdf/2512.02580",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02580",
      "scraped_at": "2025-12-09T01:44:34.806051"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
    "paper_url": "https://huggingface.co/papers/2512.04810",
    "authors": [
      "Qi Tian",
      "Lingxi Xie",
      "Jianbo Ouyang",
      "Longhui Wei",
      "Xin He"
    ],
    "stars": "13",
    "details": {
      "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
      "abstract": "The project page https://emma-umm.github.io/emma/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04810",
      "pdf_url": "https://arxiv.org/pdf/2512.04810",
      "github_links": [
        "https://github.com/umm-emma/emma"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04810",
      "scraped_at": "2025-12-09T01:44:36.824643"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
    "paper_url": "https://huggingface.co/papers/2512.04784",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
      "abstract": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04784",
      "pdf_url": "https://arxiv.org/pdf/2512.04784",
      "github_links": [
        "https://github.com/X-GenGroup/PaCo-RL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04784",
      "scraped_at": "2025-12-09T01:44:38.814591"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
    "paper_url": "https://huggingface.co/papers/2512.05905",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
      "abstract": "SCAIL is a new framework for studio-grade character animation that uses a novel 3D pose representation and full-sequence context injection to deliver more stable, realistic motion transfer under complex and cross-identity scenarios.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05905",
      "pdf_url": "https://arxiv.org/pdf/2512.05905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05905",
      "scraped_at": "2025-12-09T01:44:40.683227"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.05591",
    "authors": [
      "Zijia Lin",
      "Tiehua Mei",
      "Minxuan Lv",
      "Leiyu Pan",
      "Suu"
    ],
    "stars": "0",
    "details": {
      "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
      "abstract": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05591",
      "pdf_url": "https://arxiv.org/pdf/2512.05591",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05591",
      "scraped_at": "2025-12-09T01:44:42.544097"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.05044",
    "authors": [],
    "stars": "37",
    "details": {
      "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
      "abstract": "Project Page: https://ivg-yanranzhang.github.io/MoRe4D/ Github Repo: https://github.com/Zhangyr2022/MoRe4D The dataset is coming soon. Stay tuned!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05044",
      "pdf_url": "https://arxiv.org/pdf/2512.05044",
      "github_links": [
        "https://github.com/Zhangyr2022/MoRe4D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05044",
      "scraped_at": "2025-12-09T01:44:44.580407"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04563",
    "authors": [
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Hengzhu Tang",
      "CUDAOUTOFMEMORY",
      "Starrrrrry"
    ],
    "stars": "5",
    "details": {
      "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
      "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04563",
      "pdf_url": "https://arxiv.org/pdf/2512.04563",
      "github_links": [
        "https://github.com/zhangzef/COOPER"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04563",
      "scraped_at": "2025-12-09T01:44:46.516932"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
    "paper_url": "https://huggingface.co/papers/2512.00473",
    "authors": [
      "Zilong Huang",
      "Dongzhi Jiang",
      "Yuncheng Guo",
      "Leiqi Zhu",
      "Junyan Ye"
    ],
    "stars": "65",
    "details": {
      "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
      "abstract": "arXiv: https://arxiv.org/abs/2512.00473 code: https://github.com/yejy53/RealGen project page: https://yejy53.github.io/RealGen/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.00473",
      "pdf_url": "https://arxiv.org/pdf/2512.00473",
      "github_links": [
        "https://github.com/yejy53/RealGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.00473",
      "scraped_at": "2025-12-09T01:44:48.413384"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Self-Improving VLM Judges Without Human Annotations",
    "paper_url": "https://huggingface.co/papers/2512.05145",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Self-Improving VLM Judges Without Human Annotations",
      "abstract": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05145",
      "pdf_url": "https://arxiv.org/pdf/2512.05145",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05145",
      "scraped_at": "2025-12-09T01:44:50.242883"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "paper_url": "https://huggingface.co/papers/2512.05927",
    "authors": [
      "Anirudha Majumdar",
      "Ola Shorinwa",
      "Micah Baker",
      "Tenny Yin",
      "Zhiting Mei"
    ],
    "stars": "0",
    "details": {
      "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
      "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05927",
      "pdf_url": "https://arxiv.org/pdf/2512.05927",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05927",
      "scraped_at": "2025-12-09T01:44:52.149809"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "paper_url": "https://huggingface.co/papers/2512.05343",
    "authors": [
      "Marc Pollefeys",
      "Or Litany",
      "Ian Huang",
      "Francis Engelmann",
      "efedele"
    ],
    "stars": "0",
    "details": {
      "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
      "abstract": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at this https URL",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05343",
      "pdf_url": "https://arxiv.org/pdf/2512.05343",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05343",
      "scraped_at": "2025-12-09T01:44:54.015630"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.02835",
    "authors": [
      "Shengju Qian",
      "Weikai Chen",
      "Lingting Zhu",
      "Yingda Yin",
      "Tangerine24"
    ],
    "stars": "5",
    "details": {
      "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
      "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.02835",
      "pdf_url": "https://arxiv.org/pdf/2512.02835",
      "github_links": [
        "https://github.com/Clementine24/ReVSeg"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.02835",
      "scraped_at": "2025-12-09T01:44:55.832519"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
    "paper_url": "https://huggingface.co/papers/2512.05356",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
      "abstract": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05356",
      "pdf_url": "https://arxiv.org/pdf/2512.05356",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05356",
      "scraped_at": "2025-12-09T01:44:57.637941"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
    "paper_url": "https://huggingface.co/papers/2512.03514",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
      "abstract": "Can we build universal document retrievers that maintain strong results across typologically diverse languages without losing English performance. This question led us to design synthetic training data and multilingual benchmarks to teach a model to match documents across scripts and formats. We are excited to launch NetraEmbed our SoTA model for multimodal multilingual document retrieval along with the M3DR: Towards Universal Multilingual Multimodal Document Retrieval paper. The release includes the NetraEmbed model which produces a single dense embedding with matryoshka support at 768,1536 and 2560 dimensions and the ColNetraEmbed model which produces patch level multivector embeddings. Both models are finetuned on Gemma3-4B-it and gained ~150% improvement over baselines. To measure progress we also built the NayanaIR Benchmark with 22 multilingual and 1 cross lingual dataset and documented the full framework in the M3DR paper . Links Blog: https://www.cognitivelab.in/blog/introducing-netraembed",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03514",
      "pdf_url": "https://arxiv.org/pdf/2512.03514",
      "github_links": [
        "https://github.com/adithya-s-k/colpali"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03514",
      "scraped_at": "2025-12-09T01:44:59.738369"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
    "paper_url": "https://huggingface.co/papers/2512.05277",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
      "abstract": "This work introduces TAD, the first benchmark targeting temporal understanding in ego-centric autonomous-driving videos, evaluates SoTA VLMs, and boosts their performance with two training-free motion-reasoning methods (Scene-CoT and TCogMap).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05277",
      "pdf_url": "https://arxiv.org/pdf/2512.05277",
      "github_links": [
        "https://github.com/vbdi/tad_bench"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05277",
      "scraped_at": "2025-12-09T01:45:01.601687"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "paper_url": "https://huggingface.co/papers/2512.05564",
    "authors": [
      "Yuhao Cheng",
      "Terry Jingchen Zhang",
      "Jing Wang",
      "Panwen Hu",
      "Zijun Wang"
    ],
    "stars": "0",
    "details": {
      "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
      "abstract": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05564",
      "pdf_url": "https://arxiv.org/pdf/2512.05564",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05564",
      "scraped_at": "2025-12-09T01:45:04.335515"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
    "paper_url": "https://huggingface.co/papers/2512.05409",
    "authors": [
      "Minghui Yu",
      "Jinyuan Shi",
      "Hantao Huang",
      "Hao Zeng",
      "Ruixuan Huang"
    ],
    "stars": "0",
    "details": {
      "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
      "abstract": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05409",
      "pdf_url": "https://arxiv.org/pdf/2512.05409",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05409",
      "scraped_at": "2025-12-09T01:45:06.133971"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
    "paper_url": "https://huggingface.co/papers/2512.04694",
    "authors": [
      "Salih Tileylioglu",
      "Erdem Akag√ºnd√ºz",
      "Bevan Deniz Cilgin",
      "Barisylmz"
    ],
    "stars": "0",
    "details": {
      "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
      "abstract": "This work presents a transformer-based generative model for complex time-series signals, with experiments on seismic accelerometer data. Key idea: treat seismic waveforms as structured high-dimensional sequences and learn a latent trajectory that captures both physical dynamics and long-range temporal dependencies.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04694",
      "pdf_url": "https://arxiv.org/pdf/2512.04694",
      "github_links": [
        "https://github.com/brsylmz23/TimesNet-Gen/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04694",
      "scraped_at": "2025-12-09T01:45:07.983185"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.03667",
    "authors": [],
    "stars": "3",
    "details": {
      "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
      "abstract": "Colonoscopy saves lives ‚Äî but AI for colonoscopy is still far from intelligent. We are excited to launch the Colon-X project, an open initiative aimed at advancing multimodal intelligence in colonoscopy and beyond. Beyond serving as a community-wide data foundation, we're focused on a critical yet under-explored transition ‚Äì evolving from multimodal understanding to clinical reasoning. Keywords: Multimodal Colonoscopy Analysis, Multimodal Understanding, Clinical Reasoning, Reinforcement Learning, Multimodal Benchmark, AI Healthcare, and Abdomen",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03667",
      "pdf_url": "https://arxiv.org/pdf/2512.03667",
      "github_links": [
        "https://github.com/ai4colonoscopy/Colon-X"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03667",
      "scraped_at": "2025-12-09T01:45:09.862566"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "paper_url": "https://huggingface.co/papers/2512.05774",
    "authors": [
      "Caiming Xiong",
      "Junnan Li",
      "Shijie Wang",
      "Honglu Zhou",
      "Ziyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05774",
      "pdf_url": "https://arxiv.org/pdf/2512.05774",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05774",
      "scraped_at": "2025-12-09T01:45:11.724493"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
    "paper_url": "https://huggingface.co/papers/2512.04142",
    "authors": [
      "Aimee van Wynsberghe",
      "Lisa Biber-Freudenberger",
      "Sasha Luccioni",
      "nicholasKluge",
      "sophia-falk"
    ],
    "stars": "0",
    "details": {
      "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
      "abstract": "The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4 üå±üçÉ",
      "arxiv_page_url": "https://arxiv.org/abs/2512.04142",
      "pdf_url": "https://arxiv.org/pdf/2512.04142",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.04142",
      "scraped_at": "2025-12-09T01:45:13.587428"
    },
    "scraped_date": "2025-12-09"
  },
  {
    "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "paper_url": "https://huggingface.co/papers/2512.05339",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
      "abstract": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.05339",
      "pdf_url": "https://arxiv.org/pdf/2512.05339",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.05339",
      "scraped_at": "2025-12-09T01:45:15.478241"
    },
    "scraped_date": "2025-12-09"
  }
]