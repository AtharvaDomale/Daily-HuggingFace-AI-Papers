[
  {
    "title": "Kling-Omni Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16776",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Kling-Omni Technical Report",
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16776",
      "pdf_url": "https://arxiv.org/pdf/2512.16776",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16776",
      "scraped_at": "2025-12-22T01:52:28.433456"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Adaptation of Agentic AI",
    "paper_url": "https://huggingface.co/papers/2512.16301",
    "authors": [
      "XueqiangXu",
      "p-song1",
      "Gabshi",
      "linjc16",
      "pat-jj"
    ],
    "stars": "293",
    "details": {
      "title": "Adaptation of Agentic AI",
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16301",
      "pdf_url": "https://arxiv.org/pdf/2512.16301",
      "github_links": [
        "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16301",
      "scraped_at": "2025-12-22T01:52:30.476204"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "paper_url": "https://huggingface.co/papers/2512.16922",
    "authors": [],
    "stars": "87",
    "details": {
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "abstract": "Make SSL great again.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16922",
      "pdf_url": "https://arxiv.org/pdf/2512.16922",
      "github_links": [
        "https://github.com/SihanXU/nepa"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16922",
      "scraped_at": "2025-12-22T01:52:32.563303"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "paper_url": "https://huggingface.co/papers/2512.15745",
    "authors": [],
    "stars": "172",
    "details": {
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15745",
      "pdf_url": "https://arxiv.org/pdf/2512.15745",
      "github_links": [
        "https://github.com/inclusionAI/LLaDA2.0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15745",
      "scraped_at": "2025-12-22T01:52:34.922051"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "paper_url": "https://huggingface.co/papers/2512.16915",
    "authors": [],
    "stars": "47",
    "details": {
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "abstract": "StereoPilot replaces the fragile \"Depth-Warp-Inpaint\" pipeline with a single-step feed-forward model that leverages video diffusion priors for efficient, high-fidelity monocular-to-stereo conversion. By training on the large-scale UniStereo dataset with a learnable domain switcher, it provides a unified and efficient solution for both parallel and converged 3D video formats.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16915",
      "pdf_url": "https://arxiv.org/pdf/2512.16915",
      "github_links": [
        "https://github.com/KlingTeam/StereoPilot"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16915",
      "scraped_at": "2025-12-22T01:52:37.000210"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "paper_url": "https://huggingface.co/papers/2512.13507",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
      "abstract": "Seedance 1.5 pro Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13507",
      "pdf_url": "https://arxiv.org/pdf/2512.13507",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13507",
      "scraped_at": "2025-12-22T01:52:38.879192"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "paper_url": "https://huggingface.co/papers/2512.16923",
    "authors": [
      "Yu-Lun Liu",
      "Jia-Bin Huang",
      "rayray9999"
    ],
    "stars": "64",
    "details": {
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16923",
      "pdf_url": "https://arxiv.org/pdf/2512.16923",
      "github_links": [
        "https://github.com/rayray9999/Genfocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16923",
      "scraped_at": "2025-12-22T01:52:40.769733"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "paper_url": "https://huggingface.co/papers/2512.16913",
    "authors": [
      "Wenxuan Lu",
      "Dizhe Zhang",
      "Meixi Song",
      "Xin Lin",
      "haodongli"
    ],
    "stars": "77",
    "details": {
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP website/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16913",
      "pdf_url": "https://arxiv.org/pdf/2512.16913",
      "github_links": [
        "https://github.com/Insta360-Research-Team/DAP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16913",
      "scraped_at": "2025-12-22T01:52:42.690376"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "paper_url": "https://huggingface.co/papers/2512.16905",
    "authors": [
      "Jiarong Ou",
      "Miao Yang",
      "Xi Chen",
      "Yang Zhou",
      "Kaixin Ding"
    ],
    "stars": "0",
    "details": {
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "abstract": "data selection",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16905",
      "pdf_url": "https://arxiv.org/pdf/2512.16905",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16905",
      "scraped_at": "2025-12-22T01:52:44.630408"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16625",
    "authors": [],
    "stars": "10",
    "details": {
      "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
      "abstract": "‚ú® Image editing is awesome; but it can leak user information! üõ°Ô∏è Introducing DeContext : the first method to safeguard input images from unauthorized DiT-based in-context editing. üìÑ Paper: https://arxiv.org/abs/2512.16625 üíª Code: https://github.com/LinghuiiShen/DeContext üåê Project Page: https://linghuiishen.github.io/decontext_project_page/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16625",
      "pdf_url": "https://arxiv.org/pdf/2512.16625",
      "github_links": [
        "https://github.com/LinghuiiShen/DeContext"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16625",
      "scraped_at": "2025-12-22T01:52:46.565496"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "paper_url": "https://huggingface.co/papers/2512.16636",
    "authors": [
      "Giorgos Sfikas",
      "Theodoros Giannakopoulos",
      "Bill Psomas",
      "Christos Sgouropoulos",
      "Giorgos Petsangourakis"
    ],
    "stars": "1",
    "details": {
      "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
      "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16636",
      "pdf_url": "https://arxiv.org/pdf/2512.16636",
      "github_links": [
        "https://github.com/giorgospets/reglue"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16636",
      "scraped_at": "2025-12-22T01:52:48.434250"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "paper_url": "https://huggingface.co/papers/2512.16924",
    "authors": [],
    "stars": "74",
    "details": {
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "abstract": "Demo page: https://worldcanvas.github.io/ Github: https://github.com/pPetrichor/WorldCanvas",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16924",
      "pdf_url": "https://arxiv.org/pdf/2512.16924",
      "github_links": [
        "https://github.com/pPetrichor/WorldCanvas"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16924",
      "scraped_at": "2025-12-22T01:52:50.342955"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "paper_url": "https://huggingface.co/papers/2512.16649",
    "authors": [],
    "stars": "77",
    "details": {
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "abstract": "‚ú®What if the simplest RL recipe is all you need? Introducing JustRL: new SOTA among 1.5B reasoning models with 2√ó less compute. Stable improvement over 4,000+ steps. No multi-stage pipelines. No dynamic schedules. Just simple RL at scale.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16649",
      "pdf_url": "https://arxiv.org/pdf/2512.16649",
      "github_links": [
        "https://github.com/thunlp/JustRL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16649",
      "scraped_at": "2025-12-22T01:52:52.263342"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.16561",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
      "abstract": "Project page: https://n3d-vlm.github.io/ Code: https://github.com/W-Ted/N3D-VLM",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16561",
      "pdf_url": "https://arxiv.org/pdf/2512.16561",
      "github_links": [
        "https://github.com/W-Ted/N3D-VLM"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16561",
      "scraped_at": "2025-12-22T01:52:54.191233"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "paper_url": "https://huggingface.co/papers/2512.16920",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16920",
      "pdf_url": "https://arxiv.org/pdf/2512.16920",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16920",
      "scraped_at": "2025-12-22T01:52:56.070423"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "paper_url": "https://huggingface.co/papers/2512.16918",
    "authors": [
      "Zhixun Li",
      "Zhongyu Wang",
      "Dongyang Chen",
      "Kaituo Feng",
      "Chaoyang Wang"
    ],
    "stars": "0",
    "details": {
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "abstract": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16918",
      "pdf_url": "https://arxiv.org/pdf/2512.16918",
      "github_links": [
        "https://github.com/CYWang735/AdaTooler-V"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16918",
      "scraped_at": "2025-12-22T01:52:57.930176"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "paper_url": "https://huggingface.co/papers/2512.16912",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16912",
      "pdf_url": "https://arxiv.org/pdf/2512.16912",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16912",
      "scraped_at": "2025-12-22T01:52:59.800293"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "paper_url": "https://huggingface.co/papers/2512.16899",
    "authors": [],
    "stars": "31",
    "details": {
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "abstract": "Reward models are the most critical part of post-training for Omni models like nano banana, but it is barely studied in the open-source world. To build the foundation for future research on better post-training and RL for Omni models, FAIR at Meta Superintelligence Labs released their reward benchmark.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16899",
      "pdf_url": "https://arxiv.org/pdf/2512.16899",
      "github_links": [
        "https://github.com/facebookresearch/MMRB2/tree/main"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16899",
      "scraped_at": "2025-12-22T01:53:01.679382"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "paper_url": "https://huggingface.co/papers/2512.16864",
    "authors": [
      "Yuqi Liu",
      "Longxiang Tang",
      "Xiaohang Zhan",
      "Lei Ke",
      "TainU"
    ],
    "stars": "17",
    "details": {
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "abstract": "üöß The Challenge: IV-Complexity Current instruction-based editing models struggle when intricate instructions meet cluttered, realistic scenes‚Äîa challenge we define as Instruction-Visual Complexity (IV-Complexity) . In these scenarios, high-level global context is insufficient to distinguish specific targets from semantically similar objects (e.g., distinguishing a \"used cup\" from a clean glass on a messy desk). üìâ The Gap: Global Semantic Guidance Existing methods, including unified VLM-diffusion architectures, predominantly rely on Global Semantic Guidance . They compress instructions into global feature vectors, lacking spatial grounding. Consequently, edits often \"spill over\" into unrelated areas or modify the wrong targets, failing to preserve background consistency. üöÄ Our Solution: Region-Aligned Guidance RePlan introduces a Plan-then-Execute framework that explicitly links text to pixels. Our key contributions include: üß± Reasoning-Guided Planning A VLM planner performs Chain-of-Thought (CoT) reasoning to decompose complex instructions into structured, region-specific guidance (Bounding Boxes + Local Hints). üéØ Training-Free Attention Injection We introduce a mechanism tailored for Multimodal DiT (MMDiT) that executes edits via region-constrained attention. This enables precise, multi-region parallel edits in a single pass while preserving the background, without requiring any training of the DiT backbone. ‚ö° Efficient GRPO Training We enhance the planner's reasoning capabilities using Group Relative Policy Optimization (GRPO) . Remarkably, we achieve strong planning performance using only ~1k instruction-only samples , bypassing the need for large-scale paired image datasets. üéõÔ∏è Interactive & Flexible Editing RePlan's intermediate region guidance is fully editable , enabling user-in-the-loop intervention. Users can adjust bounding boxes or hints directly to refine results. Furthermore, our attention mechanism supports regional negative prompts to prevent bleeding effects. üìä IV-Edit Benchmark To foster future research, we establish IV-Edit , the first benchmark specifically designed to evaluate IV-Complex editing, filling the gap left by current subject-dominated evaluation sets.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16864",
      "pdf_url": "https://arxiv.org/pdf/2512.16864",
      "github_links": [
        "https://github.com/dvlab-research/RePlan"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16864",
      "scraped_at": "2025-12-22T01:53:03.602686"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "paper_url": "https://huggingface.co/papers/2512.16900",
    "authors": [],
    "stars": "92",
    "details": {
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16900",
      "pdf_url": "https://arxiv.org/pdf/2512.16900",
      "github_links": [
        "https://github.com/Francis-Rings/FlashPortrait"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16900",
      "scraped_at": "2025-12-22T01:53:05.498824"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "paper_url": "https://huggingface.co/papers/2512.16501",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
      "abstract": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16501",
      "pdf_url": "https://arxiv.org/pdf/2512.16501",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16501",
      "scraped_at": "2025-12-22T01:53:07.369391"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "paper_url": "https://huggingface.co/papers/2512.16106",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "ModelTables: A Corpus of Tables about Models",
      "abstract": "ModelTables is a large-scale benchmark of structured tables in model lakes, built from model cards, READMEs, and papers. It is motivated by the intuition that models addressing similar tasks tend to exhibit structurally similar performance and configuration tables. The benchmark defines model and table relatedness using multiple signals, including paper citations, model-card links and inheritance, and shared training datasets, and supports downstream applications such as table discovery and semantic retrieval from both structural and semantic perspectives.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16106",
      "pdf_url": "https://arxiv.org/pdf/2512.16106",
      "github_links": [
        "https://github.com/RJMillerLab/ModelTables"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16106",
      "scraped_at": "2025-12-22T01:53:09.188933"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_url": "https://huggingface.co/papers/2512.16378",
    "authors": [
      "Carlos Escolano",
      "Vil√©m Zouhar",
      "zhopto3",
      "javi8979",
      "spapi"
    ],
    "stars": "13",
    "details": {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "abstract": "Hearing to Translate presents the first large-scale, phenomenon-aware benchmark of SpeechLLMs for speech-to-text translation, comparing 5 SpeechLLMs against strong direct and cascaded systems across 16 benchmarks, 13 language pairs, and challenging conditions (noise, accents, disfluencies, long-form). Results show that cascades remain the most reliable overall, while SpeechLLMs close the gap in specific settings (notably noise and code-switching).",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16378",
      "pdf_url": "https://arxiv.org/pdf/2512.16378",
      "github_links": [
        "https://github.com/sarapapi/hearing2translate"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16378",
      "scraped_at": "2025-12-22T01:53:11.027318"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "paper_url": "https://huggingface.co/papers/2512.16921",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. Project Page: https://auditdm.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16921",
      "pdf_url": "https://arxiv.org/pdf/2512.16921",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16921",
      "scraped_at": "2025-12-22T01:53:12.963299"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "paper_url": "https://huggingface.co/papers/2512.11251",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
      "abstract": "Can LLMs natively understand time series? We constructed TS-Insights, the first large-scale alignment dataset containing 100k time series and text pairs across 20 domains. We use a novel agentic workflow to synthesize high-quality trend descriptions.  Inspired by LLaVA, we showed instruction-tuning on TS-Insights can enable LLMs to understand time series as a native input modality and generate textual descriptions. This work was originally done in Summer 2023.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.11251",
      "pdf_url": "https://arxiv.org/pdf/2512.11251",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.11251",
      "scraped_at": "2025-12-22T01:53:14.895310"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "paper_url": "https://huggingface.co/papers/2512.16615",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
      "abstract": "Paper: https://arxiv.org/abs/2512.16615 GitHub: https://github.com/SingleZombie/LLSA",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16615",
      "pdf_url": "https://arxiv.org/pdf/2512.16615",
      "github_links": [
        "https://github.com/SingleZombie/LLSA"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16615",
      "scraped_at": "2025-12-22T01:53:16.820184"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "paper_url": "https://huggingface.co/papers/2512.15489",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
      "abstract": "Nemotron-Math is a large-scale mathematical reasoning dataset with 7.5 million solution traces spanning high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR). It combines 85K curated AoPS competition problems with 262K community-sourced StackExchange-Math questions, balancing structured tasks and real-world mathematical queries. Experiments show that Nemotron-Math consistently outperforms prior datasets, improving robustness and generalization‚Äîespecially on HLE-Math‚Äîwhile maintaining strong performance on competition benchmarks. With an efficient long-context training strategy, it enables state-of-the-art results, including 100% maj@16 accuracy on AIME 2024 and 2025 using Python TIR.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15489",
      "pdf_url": "https://arxiv.org/pdf/2512.15489",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15489",
      "scraped_at": "2025-12-22T01:53:18.671559"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "paper_url": "https://huggingface.co/papers/2512.16767",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16767",
      "pdf_url": "https://arxiv.org/pdf/2512.16767",
      "github_links": [
        "https://github.com/jasongzy/Make-It-Poseable"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16767",
      "scraped_at": "2025-12-22T01:53:20.517544"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "paper_url": "https://huggingface.co/papers/2512.16670",
    "authors": [
      "Hendrik P. A. Lensch",
      "Ole Beisswenger",
      "JDihlmann"
    ],
    "stars": "0",
    "details": {
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "abstract": "FrameDiffuser: GBuffer-Conditioned Diffusion for Neural Forward Frame Rendering Let any interactive scene be illuminated by an image diffusion model. We show that Stable Diffusion 1.5 can be adapted to autoregressively predict global illumination from a stream of G-buffer data. We overfit SD on single scenes and show that it learns the illumination setting for the scene and can transfer it to OOD views of the scene.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16670",
      "pdf_url": "https://arxiv.org/pdf/2512.16670",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16670",
      "scraped_at": "2025-12-22T01:53:22.394603"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "paper_url": "https://huggingface.co/papers/2512.10953",
    "authors": [
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Qiao Sun",
      "Yiyang Lu"
    ],
    "stars": "0",
    "details": {
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "abstract": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "arxiv_page_url": "https://arxiv.org/abs/2512.10953",
      "pdf_url": "https://arxiv.org/pdf/2512.10953",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.10953",
      "scraped_at": "2025-12-22T01:53:24.387244"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "paper_url": "https://huggingface.co/papers/2512.12880",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Improving Recursive Transformers with Mixture of LoRAs",
      "abstract": "Recursive transformers cut model size by sharing parameters across layers, but this sharing tends to collapse layer-wise expressivity and makes the model less flexible. We propose Mixture of LoRAs (MoL), a lightweight fix that replaces selected shared FFNs with a small set of token-routed LoRA experts (sparse routing), allowing conditional computation while keeping the backbone compact. We pretrain ModernALBERT (50M to 120M) with RoPE, GeGLU, FlashAttention, and distillation-based initialisation, and report state-of-the-art results among compact models on GLUE, SQuAD-v2, and BEIR, often surpassing larger fully parameterised baselines. For deployment, we introduce expert merging (including an EMA-based strategy) that compresses MoL into a single adapter at inference, removing routing overhead.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12880",
      "pdf_url": "https://arxiv.org/pdf/2512.12880",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12880",
      "scraped_at": "2025-12-22T01:53:26.216029"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
    "paper_url": "https://huggingface.co/papers/2512.12623",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
      "abstract": "üåê Website: https://mllm-dmlr.github.io üìÑ Paper: https://arxiv.org/abs/2512.12623",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12623",
      "pdf_url": "https://arxiv.org/pdf/2512.12623",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12623",
      "scraped_at": "2025-12-22T01:53:28.064538"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.12576",
    "authors": [
      "Ben He",
      "Hongyu Lin",
      "Yanjiang Liu",
      "Jie Lou",
      "Aunderline"
    ],
    "stars": "0",
    "details": {
      "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
      "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4% over the base model and achieves an additional 2.3% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.12576",
      "pdf_url": "https://arxiv.org/pdf/2512.12576",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.12576",
      "scraped_at": "2025-12-22T01:53:29.883782"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "paper_url": "https://huggingface.co/papers/2512.16909",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "abstract": "Project Page: https://hybridrobotics.github.io/MomaGraph/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16909",
      "pdf_url": "https://arxiv.org/pdf/2512.16909",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16909",
      "scraped_at": "2025-12-22T01:53:31.722427"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.15907",
    "authors": [
      "Vivek Gupta",
      "Aparna Garimella",
      "Juhna Park",
      "Tejas Anvekar"
    ],
    "stars": "1",
    "details": {
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15907",
      "pdf_url": "https://arxiv.org/pdf/2512.15907",
      "github_links": [
        "https://github.com/CoRAL-ASU/TabReX"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15907",
      "scraped_at": "2025-12-22T01:53:33.535002"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "paper_url": "https://huggingface.co/papers/2512.14884",
    "authors": [
      "Yutong Bai",
      "Michael D. Grossberg",
      "Andrew Lu",
      "Katherine Xu",
      "Huzheng Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
      "abstract": "what's the vibe? vibe is the shared attributes among images, e.g., similar instruments, same hairstyle, etc.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14884",
      "pdf_url": "https://arxiv.org/pdf/2512.14884",
      "github_links": [
        "https://github.com/huzeyann/VibeSpace"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14884",
      "scraped_at": "2025-12-22T01:53:35.652691"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "paper_url": "https://huggingface.co/papers/2512.15528",
    "authors": [
      "Can Ma. Yu Zhou",
      "Dongbao Yang",
      "Daiqing Wu"
    ],
    "stars": "1",
    "details": {
      "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
      "abstract": "Update the paper.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15528",
      "pdf_url": "https://arxiv.org/pdf/2512.15528",
      "github_links": [
        "https://github.com/wdqqdw/EmoCaliber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15528",
      "scraped_at": "2025-12-22T01:53:37.441726"
    },
    "scraped_date": "2025-12-22"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "paper_url": "https://huggingface.co/papers/2512.14805",
    "authors": [
      "Michael Carbin",
      "Tian Jin",
      "Logan Weber",
      "ellieyhc"
    ],
    "stars": "3",
    "details": {
      "title": "Sharing State Between Prompts and Programs",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.14805",
      "pdf_url": "https://arxiv.org/pdf/2512.14805",
      "github_links": [
        "https://github.com/psg-mit/nightjarpy/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.14805",
      "scraped_at": "2025-12-22T01:53:39.288175"
    },
    "scraped_date": "2025-12-22"
  }
]