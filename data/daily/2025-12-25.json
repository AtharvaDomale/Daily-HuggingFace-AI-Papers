[
  {
    "title": "SemanticGen: Video Generation in Semantic Space",
    "paper_url": "https://huggingface.co/papers/2512.20619",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SemanticGen: Video Generation in Semantic Space",
      "abstract": "Project Page: https://jianhongbai.github.io/SemanticGen/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20619",
      "pdf_url": "https://arxiv.org/pdf/2512.20619",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20619",
      "scraped_at": "2025-12-25T01:48:29.468881"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "paper_url": "https://huggingface.co/papers/2512.19673",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "abstract": "Bottom-up Policy Optimization (BuPO) provides a novel framework to decompose LLM policies into internal layer and modular policies, reveals distinct reasoning patterns across different model architectures, and introduces a bottom-up optimization algorithm that leverages these insights to enhance complex reasoning. Key Findings: Internal Policies: Decomposes the unified LLM policy into samplable distributions from individual layers and modules (self-attention & FFN). Progressive Reasoning Pattern: Discovered a human-like \"Exploration-Integration-Convergence\" (EIC) pattern in Qwen models, contrasting with the abrupt convergence in Llama models. Bottom-up Policy Optimization (BuPO): A novel two-phase RL algorithm that first optimizes an internal, lower-layer policy to reconstruct foundational reasoning, then fine-tunes the full model. Enhanced Reasoning Performance: BuPO significantly outperforms standard RL on complex reasoning benchmarks. Code: https://github.com/Trae1ounG/BuPO",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19673",
      "pdf_url": "https://arxiv.org/pdf/2512.19673",
      "github_links": [
        "https://github.com/Trae1ounG/BuPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19673",
      "scraped_at": "2025-12-25T01:48:31.353002"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "paper_url": "https://huggingface.co/papers/2512.20618",
    "authors": [
      "Renjie Pi",
      "Yue Ma",
      "Jiaqi Tang",
      "Ziyi Liu",
      "Runtao Liu"
    ],
    "stars": "0",
    "details": {
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/ .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20618",
      "pdf_url": "https://arxiv.org/pdf/2512.20618",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20618",
      "scraped_at": "2025-12-25T01:48:33.283921"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "paper_url": "https://huggingface.co/papers/2512.20617",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "abstract": "Introducing SpatialTree, a four-level hierarchy for spatial abilities in multimodal LLMs, benchmark 27 sub-abilities, reveal transfer patterns, and propose auto-think to improve reinforcement-learning performance.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20617",
      "pdf_url": "https://arxiv.org/pdf/2512.20617",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20617",
      "scraped_at": "2025-12-25T01:48:35.176325"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
    "paper_url": "https://huggingface.co/papers/2512.18746",
    "authors": [
      "Junhao Wang",
      "Zhenhong Zhou",
      "Chong Zhan",
      "Haotian Ren",
      "Guibin Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
      "abstract": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.18746",
      "pdf_url": "https://arxiv.org/pdf/2512.18746",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18746",
      "scraped_at": "2025-12-25T01:48:37.090337"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Step-DeepResearch Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.20491",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Step-DeepResearch Technical Report",
      "abstract": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20491",
      "pdf_url": "https://arxiv.org/pdf/2512.20491",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20491",
      "scraped_at": "2025-12-25T01:48:38.930062"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
    "paper_url": "https://huggingface.co/papers/2512.17102",
    "authors": [
      "Soumya Smruti Mishra",
      "Yijun Tian",
      "Yawei Wang",
      "Qiaojing Yan",
      "Jiongxiao Wang"
    ],
    "stars": "0",
    "details": {
      "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
      "abstract": "Apply RL to Skill Library Agent.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17102",
      "pdf_url": "https://arxiv.org/pdf/2512.17102",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17102",
      "scraped_at": "2025-12-25T01:48:40.786923"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "SAM Audio: Segment Anything in Audio",
    "paper_url": "https://huggingface.co/papers/2512.18099",
    "authors": [],
    "stars": "2.49k",
    "details": {
      "title": "SAM Audio: Segment Anything in Audio",
      "abstract": null,
      "arxiv_page_url": "https://arxiv.org/abs/2512.18099",
      "pdf_url": "https://arxiv.org/pdf/2512.18099",
      "github_links": [
        "https://github.com/facebookresearch/sam-audio"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.18099",
      "scraped_at": "2025-12-25T01:48:42.784641"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "INTELLECT-3: Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.16144",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "INTELLECT-3: Technical Report",
      "abstract": "INTELLECT-3: Technical Report",
      "arxiv_page_url": "https://arxiv.org/abs/2512.16144",
      "pdf_url": "https://arxiv.org/pdf/2512.16144",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.16144",
      "scraped_at": "2025-12-25T01:48:44.617656"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
    "paper_url": "https://huggingface.co/papers/2512.20182",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "abstract": "In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20182",
      "pdf_url": "https://arxiv.org/pdf/2512.20182",
      "github_links": [
        "https://github.com/S1s-Z/FaithLens"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20182",
      "scraped_at": "2025-12-25T01:48:46.433500"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "paper_url": "https://huggingface.co/papers/2512.13472",
    "authors": [
      "Aishan Liu",
      "Wei Zhang",
      "Lin Jing",
      "Shawn Guo",
      "Jian Yang"
    ],
    "stars": "0",
    "details": {
      "title": "Scaling Laws for Code: Every Programming Language Matters",
      "abstract": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.13472",
      "pdf_url": "https://arxiv.org/pdf/2512.13472",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.13472",
      "scraped_at": "2025-12-25T01:48:48.364963"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
    "paper_url": "https://huggingface.co/papers/2512.19526",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "abstract": "QuantiPhy is the first benchmark that asks vision–language models to do physics with numerical accuracy. Across 3,300+ video–text instances, we show that today’s VLMs often sound plausible but fail quantitatively on physical reasoning tasks—they rely more on memorized world knowledge from pretraining than on the actual video and text inputs. QuantiPhy benchmarks the critical gap between qualitative understanding and quantitative reasoning, providing a rigorous testbed for building input-faithful, physically grounded AI.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19526",
      "pdf_url": "https://arxiv.org/pdf/2512.19526",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19526",
      "scraped_at": "2025-12-25T01:48:50.210157"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
    "paper_url": "https://huggingface.co/papers/2512.17648",
    "authors": [
      "Luisa Bentivogli",
      "Matteo Negri",
      "Mauro Cettolo",
      "Marco Gaido",
      "spapi"
    ],
    "stars": "0",
    "details": {
      "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
      "abstract": "Already available on PyPi at https://pypi.org/project/simulstream/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.17648",
      "pdf_url": "https://arxiv.org/pdf/2512.17648",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.17648",
      "scraped_at": "2025-12-25T01:48:52.021839"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "paper_url": "https://huggingface.co/papers/2512.20615",
    "authors": [
      "Cheng Meng",
      "Ruiqi Wu",
      "Ke Cao",
      "Tianyu Yang",
      "Xuanhua He"
    ],
    "stars": "0",
    "details": {
      "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "abstract": "project page: https://xuanhuahe.github.io/ORCA/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20615",
      "pdf_url": "https://arxiv.org/pdf/2512.20615",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20615",
      "scraped_at": "2025-12-25T01:48:53.913538"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
    "paper_url": "https://huggingface.co/papers/2512.20352",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
      "abstract": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa (κ) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (κ=0.907, cosine=95.3%), followed by GPT-4o (κ=0.853, cosine=92.6%) and Claude (κ=0.842, cosine=92.1%). All three models achieve a high agreement (κ>0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20352",
      "pdf_url": "https://arxiv.org/pdf/2512.20352",
      "github_links": [
        "https://github.com/NileshArnaiya/LLM-Thematic-Analysis-Tool"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20352",
      "scraped_at": "2025-12-25T01:48:55.839956"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
    "paper_url": "https://huggingface.co/papers/2512.20092",
    "authors": [
      "Wenyu Huang",
      "Zhaowei Wang",
      "Yifan Xiang",
      "Baojun Wang",
      "Yiming Du"
    ],
    "stars": "0",
    "details": {
      "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
      "abstract": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.20092",
      "pdf_url": "https://arxiv.org/pdf/2512.20092",
      "github_links": [
        "https://github.com/Elvin-Yiming-Du/Memory-T1/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.20092",
      "scraped_at": "2025-12-25T01:48:57.721739"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
    "paper_url": "https://huggingface.co/papers/2512.15031",
    "authors": [
      "Kostadin Damevski",
      "Preetha Chatterjee",
      "Rahat Rizvi Rahman",
      "Robert Zita",
      "imranraad"
    ],
    "stars": "0",
    "details": {
      "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
      "abstract": "Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecasted by tension triggers, sentiment shifts, and specific conversational patterns. We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.15031",
      "pdf_url": "https://arxiv.org/pdf/2512.15031",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.15031",
      "scraped_at": "2025-12-25T01:48:59.500303"
    },
    "scraped_date": "2025-12-25"
  },
  {
    "title": "Learning to Refocus with Video Diffusion Models",
    "paper_url": "https://huggingface.co/papers/2512.19823",
    "authors": [
      "Shumian Xin",
      "Xuaner Zhang",
      "Zhoutong Zhang",
      "SaiKiran Tedla"
    ],
    "stars": "5",
    "details": {
      "title": "Learning to Refocus with Video Diffusion Models",
      "abstract": "Learning to Refocus with Video Diffusion Models, SIGGRAPH Asia 2025",
      "arxiv_page_url": "https://arxiv.org/abs/2512.19823",
      "pdf_url": "https://arxiv.org/pdf/2512.19823",
      "github_links": [
        "https://github.com/tedlasai/learn2refocus"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.19823",
      "scraped_at": "2025-12-25T01:49:01.531230"
    },
    "scraped_date": "2025-12-25"
  }
]