[
  {
    "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.07461",
    "authors": [],
    "stars": "18",
    "details": {
      "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
      "abstract": "Paper: https://arxiv.org/abs/2512.07461 Code: https://github.com/bigai-nlco/Native-Parallel-Reasoner Model & Data: https://huggingface.co/bigai-NPR Website: https://bigai-nlco.github.io/Native-Parallel-Reasoner",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07461",
      "pdf_url": "https://arxiv.org/pdf/2512.07461",
      "github_links": [
        "https://github.com/bigai-nlco/Native-Parallel-Reasoner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07461",
      "scraped_at": "2025-12-10T01:46:58.731334"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
    "paper_url": "https://huggingface.co/papers/2512.07525",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
      "abstract": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07525",
      "pdf_url": "https://arxiv.org/pdf/2512.07525",
      "github_links": [
        "https://github.com/OpenMOSS/rope_pp"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07525",
      "scraped_at": "2025-12-10T01:47:00.619948"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Unified Video Editing with Temporal Reasoner",
    "paper_url": "https://huggingface.co/papers/2512.07469",
    "authors": [],
    "stars": "25",
    "details": {
      "title": "Unified Video Editing with Temporal Reasoner",
      "abstract": "A Chain of Frames video editing method enbale temporal reasoning and 4x video length extrapolation with just 50k training pairs! ğŸ  Page: videocof.github.io/ ğŸ“„ Paper: arxiv.org/abs/2512.07469 ğŸ’» Code: github.com/knightyxp/VideoCoF",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07469",
      "pdf_url": "https://arxiv.org/pdf/2512.07469",
      "github_links": [
        "https://github.com/knightyxp/VideoCoF"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07469",
      "scraped_at": "2025-12-10T01:47:02.532632"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "paper_url": "https://huggingface.co/papers/2512.07834",
    "authors": [
      "Yu-Lun Liu",
      "chien90190",
      "JiewenChan",
      "YiChuanH"
    ],
    "stars": "0",
    "details": {
      "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
      "abstract": "Stylized voxel art is widely used in games and digital media, but turning 3D meshes into visually appealing voxel forms remains challenging and often requires manual effort. Existing methods struggle to preserve semantic structure and offer limited control over stylization, particularly in discrete color and abstraction. We present Voxify3D, a differentiable two-stage framework for generating stylized voxel art from 3D meshes. In the first stage, we initialize a coarse voxel grid via neural volume rendering. In the second stage, we refine the grid under six-view orthographic pixel art supervision, guided by a discrete color palette derived from clustering strategies (e.g., K-means, Max-Min, Median Cut). To support differentiable palette-based quantization, we design a rendering mechanism based on Gumbel-Softmax and incorporate a CLIP-based perceptual loss to enforce semantic alignment between voxel renderings and the original mesh.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07834",
      "pdf_url": "https://arxiv.org/pdf/2512.07834",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07834",
      "scraped_at": "2025-12-10T01:47:04.380187"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Scaling Zero-Shot Reference-to-Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.06905",
    "authors": [],
    "stars": "5",
    "details": {
      "title": "Scaling Zero-Shot Reference-to-Video Generation",
      "abstract": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06905",
      "pdf_url": "https://arxiv.org/pdf/2512.06905",
      "github_links": [
        "https://github.com/franciszzj/Saber"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06905",
      "scraped_at": "2025-12-10T01:47:06.279606"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "paper_url": "https://huggingface.co/papers/2512.06749",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
      "abstract": "Project website with an intro video is available at: https://aka.ms/DoVer .",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06749",
      "pdf_url": "https://arxiv.org/pdf/2512.06749",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06749",
      "scraped_at": "2025-12-10T01:47:08.156914"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Distribution Matching Variational AutoEncoder",
    "paper_url": "https://huggingface.co/papers/2512.07778",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Distribution Matching Variational AutoEncoder",
      "abstract": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoderâ€™s latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at \\url{ https://github.com/sen-ye/dmvae}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07778",
      "pdf_url": "https://arxiv.org/pdf/2512.07778",
      "github_links": [
        "https://github.com/sen-ye/dmvae%7D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07778",
      "scraped_at": "2025-12-10T01:47:10.004095"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "paper_url": "https://huggingface.co/papers/2512.06065",
    "authors": [],
    "stars": "15",
    "details": {
      "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
      "abstract": "We propose a framework for real-time egocentric video editing. Our system is composed of: EgoEditData, a manually curated dataset of 100k video editing pairs focusing on the egocentric case and featuring object substitution and removal under challenging hand occlusions, interactions, and large egomotion; EgoEdit the first real-time autoregressive model for egocentric video editing running in real time on a single H100 with 855ms first-frame latency and enabling live augmented reality (AR) interactions; EgoEditBench, a comprehensive benchmark for evaluation of egocentric video editing systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06065",
      "pdf_url": "https://arxiv.org/pdf/2512.06065",
      "github_links": [
        "https://github.com/snap-research/EgoEdit"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06065",
      "scraped_at": "2025-12-10T01:47:11.841255"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Relational Visual Similarity",
    "paper_url": "https://huggingface.co/papers/2512.07833",
    "authors": [
      "Jing Shi",
      "Yilin Wang",
      "Krishna Kumar Singh",
      "Sicheng Mo",
      "thaoshibe"
    ],
    "stars": "14",
    "details": {
      "title": "Relational Visual Similarity",
      "abstract": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07833",
      "pdf_url": "https://arxiv.org/pdf/2512.07833",
      "github_links": [
        "https://github.com/thaoshibe/relsim"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07833",
      "scraped_at": "2025-12-10T01:47:13.783773"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "paper_url": "https://huggingface.co/papers/2512.07806",
    "authors": [
      "Jungwoo Kim",
      "Younggeun Lee",
      "Seungtae Nam",
      "Seungkwon Yang",
      "Gynjn"
    ],
    "stars": "56",
    "details": {
      "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
      "abstract": "We are excited to share our recent work \"Multi-view Pyramid Transformer: Look Coarser to See Broader\" Paper: https://arxiv.org/abs/2512.07806 Project page: https://gynjn.github.io/MVP/ Code: https://github.com/Gynjn/MVP",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07806",
      "pdf_url": "https://arxiv.org/pdf/2512.07806",
      "github_links": [
        "https://github.com/Gynjn/MVP"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07806",
      "scraped_at": "2025-12-10T01:47:15.632640"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "LongCat-Image Technical Report",
    "paper_url": "https://huggingface.co/papers/2512.07584",
    "authors": [],
    "stars": "307",
    "details": {
      "title": "LongCat-Image Technical Report",
      "abstract": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07584",
      "pdf_url": "https://arxiv.org/pdf/2512.07584",
      "github_links": [
        "https://github.com/meituan-longcat/LongCat-Image"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07584",
      "scraped_at": "2025-12-10T01:47:17.470709"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.07831",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
      "abstract": "Project Website https://jackailab.github.io/Projects/UnityVideo/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07831",
      "pdf_url": "https://arxiv.org/pdf/2512.07831",
      "github_links": [
        "https://github.com/dvlab-research/UnityVideo"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07831",
      "scraped_at": "2025-12-10T01:47:19.511510"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "paper_url": "https://huggingface.co/papers/2512.07783",
    "authors": [],
    "stars": "7",
    "details": {
      "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
      "abstract": "We develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the modelâ€™s edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07783",
      "pdf_url": "https://arxiv.org/pdf/2512.07783",
      "github_links": [
        "https://github.com/Interplay-LM-Reasoning/Interplay-LM-Reasoning"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07783",
      "scraped_at": "2025-12-10T01:47:21.314074"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.03244",
    "authors": [
      "Nanyun Peng",
      "Swastik Roy",
      "Arpit Gupta",
      "Sruthi Gorantla",
      "Salman Rahman"
    ],
    "stars": "0",
    "details": {
      "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
      "abstract": "Please find our paper on training process reward models without ground truth by leveraging inference-time scaling methods, enabling reinforcement learning in domains where verifiable answers are unavailable.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03244",
      "pdf_url": "https://arxiv.org/pdf/2512.03244",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03244",
      "scraped_at": "2025-12-10T01:47:23.147318"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "paper_url": "https://huggingface.co/papers/2512.03621",
    "authors": [
      "Taojun Ding",
      "Jiehui Huang",
      "Mantang Guo",
      "wangshx",
      "Iron-lyk"
    ],
    "stars": "23",
    "details": {
      "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
      "abstract": "Project page: https://recamdriving.github.io/",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03621",
      "pdf_url": "https://arxiv.org/pdf/2512.03621",
      "github_links": [
        "https://github.com/Iron-LYK/ReCamDriving"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03621",
      "scraped_at": "2025-12-10T01:47:24.938487"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.06533",
    "authors": [
      "Jiacheng Chen",
      "Ziniu Li",
      "Sheng Tang",
      "Ming Chen",
      "trxcc2002"
    ],
    "stars": "0",
    "details": {
      "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06533",
      "pdf_url": "https://arxiv.org/pdf/2512.06533",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06533",
      "scraped_at": "2025-12-10T01:47:26.723851"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2512.06373",
    "authors": [
      "Yansong Tang",
      "Haoji Zhang",
      "Jingxuan Niu",
      "Wenlong Liu",
      "VoyageWang"
    ],
    "stars": "11",
    "details": {
      "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
      "abstract": "The project page is https://github.com/VoyageWang/VG-Refiner",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06373",
      "pdf_url": "https://arxiv.org/pdf/2512.06373",
      "github_links": [
        "https://github.com/VoyageWang/VG-Refiner"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06373",
      "scraped_at": "2025-12-10T01:47:28.523139"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
    "paper_url": "https://huggingface.co/papers/2512.06589",
    "authors": [
      "Simeng Qin",
      "Teng Ma",
      "Qi Guo",
      "Jie Liao",
      "jiaxiaojunQAQ"
    ],
    "stars": "0",
    "details": {
      "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
      "abstract": "This work presents OmniSafeBench-MM, a unified, open-source benchmark and toolbox designed for comprehensive evaluation of multimodal jailbreak attack and defense methods. It integrates 13 representative attack techniques, 15 defense strategies, and a diverse dataset spanning 9 risk domains and 50 fine-grained categories, covering real-worldâ€“relevant query types (consultative, imperative, declarative). We also propose a three-dimensional evaluation protocol measuring harmfulness (from low-impact individual harm to societal-level threats), intent alignment, and response detail â€” allowing nuanced safety-utility tradeoff analysis. Our extensive experiments across 10 open-source and 8 closed-source multimodal LLMs reveal widespread vulnerabilities to multimodal jailbreak attacks. By unifying data, methods, and evaluation, OmniSafeBench-MM offers a standardized, reproducible platform â€” which we hope will become a foundational resource for future research on safe, robust multimodal LLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06589",
      "pdf_url": "https://arxiv.org/pdf/2512.06589",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06589",
      "scraped_at": "2025-12-10T01:47:30.348829"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "paper_url": "https://huggingface.co/papers/2512.07829",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
      "abstract": "We proposed FAE which adapts pretrained ViT as the latent space for visual generative models",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07829",
      "pdf_url": "https://arxiv.org/pdf/2512.07829",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07829",
      "scraped_at": "2025-12-10T01:47:32.150504"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Group Representational Position Encoding",
    "paper_url": "https://huggingface.co/papers/2512.07805",
    "authors": [],
    "stars": "30",
    "details": {
      "title": "Group Representational Position Encoding",
      "abstract": "Introducing GRAPE: Group Representational Position Encoding. Embracing General Relative Law of Position Encoding, unifying and improving Multiplicative and Additive Position Encoding, such as RoPE and Alibi! Better performance with a clear theoretical formulation! Project Page: https://model-architectures.github.io/GRAPE/ Paper: https://model-architectures.github.io/GRAPE/GRAPE.pdf Devoted to the frontier of superintelligence, hope you will enjoy it!",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07805",
      "pdf_url": "https://arxiv.org/pdf/2512.07805",
      "github_links": [
        "https://github.com/model-architectures/GRAPE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07805",
      "scraped_at": "2025-12-10T01:47:33.925881"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
    "paper_url": "https://huggingface.co/papers/2512.06835",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
      "abstract": "Experiment Results ğŸ“Š We evaluate DoGe on 7 benchmarks covering: General visual reasoning & hallucination (MMMU, MMStar, HallBench) Specialized domain reasoning (MathVision, MathVista, ChemBench, MSEarthMCQ) 3B-level Models Performance Method MMMU MMStar HallBench MathVision MathVista ChemBench MSEarthMCQ Avg. InternVL2.5-2B 43.6 53.7 42.6 13.5 51.3 - - - Visionary-3B 40.7 50.5 59.8 17.1 54.7 40.8 38.2 43.1 Qwen2.5VL-3B* (Base) 41.0 49.3 60.6 18.7 48.8 43.4 40.8 43.2 DoGe-3B (Iter1) 46.6 54.5 61.5 21.7 ğŸ¥‡57.9 45.8 ğŸ¥‡48.3 48.0 DoGe-3B (Iter2) 48.9 52.5 ğŸ¥‡62.5 23.1 54.2 ğŸ¥‡47.7 46.2 47.9 DoGe-3B (Iter3) ğŸ¥‡50.2 ğŸ¥‡54.7 61.8 ğŸ¥‡24.2 57.0 46.9 47.3 ğŸ¥‡48.9 â¬†ï¸ Max Gain (vs. Base) +9.2 +5.4 +1.9 +5.5 +9.1 +4.3 +7.5 +5.7 7B-level Models Performance Method MMMU MMStar HallBench MathVision MathVista ChemBench MSEarthMCQ Avg. InternVL2.5-8B 48.9 62.8 50.1 22.0 64.4 - - - Vision-R1-7B 46.9 60.8 66.7 ğŸ¥‡29.0 68.5 46.0 44.1 51.7 Qwen2.5VL-7B* (Base) 49.9 60.7 66.3 23.6 64.1 48.6 43.3 50.9 DoGe-7B (Iter1) 53.1 ğŸ¥‡63.2 54.4 24.3 62.1 48.7 46.4 50.3 DoGe-7B (Iter2) 50.9 60.0 ğŸ¥‡68.3 25.3 ğŸ¥‡68.8 ğŸ¥‡49.0 ğŸ¥‡46.5 52.7 DoGe-7B (Iter3) ğŸ¥‡53.6 63.0 68.0 25.2 68.3 48.5 45.8 ğŸ¥‡53.2 â¬†ï¸ Max Gain (vs. Base) +3.7 +2.5 +2.0 +1.7 +4.7 +0.4 +3.2 +2.3 Key Takeaways âœ¨ Stable Self-Evolution : DoGe achieves consistent performance improvement across 3 iterations for both 3B and 7B models Domain Generalization : 3B models: Average +5.7% performance gain across all benchmarks 7B models: Average +2.3% performance gain (maintains superiority over strong baselines) Hallucination Reduction : +2.0% average improvement on HallBench, mitigating visual hallucination Data Efficiency : Excels in data-scarce domains (Chemistry, Earth Science) with limited manual annotations",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06835",
      "pdf_url": "https://arxiv.org/pdf/2512.06835",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06835",
      "scraped_at": "2025-12-10T01:47:35.762943"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "paper_url": "https://huggingface.co/papers/2512.06963",
    "authors": [
      "Yaobo Liang",
      "Zhiying Du",
      "Fangyun Wei",
      "godjiaolongge",
      "ys3197"
    ],
    "stars": "0",
    "details": {
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "abstract": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06963",
      "pdf_url": "https://arxiv.org/pdf/2512.06963",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06963",
      "scraped_at": "2025-12-10T01:47:37.689331"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
    "paper_url": "https://huggingface.co/papers/2512.06421",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
      "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06421",
      "pdf_url": "https://arxiv.org/pdf/2512.06421",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06421",
      "scraped_at": "2025-12-10T01:47:39.463814"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
    "paper_url": "https://huggingface.co/papers/2512.06791",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
      "abstract": "Gradient methods in games are usually proven to converge only under strong monotonicity in the Euclidean geometry (Rosen-style assumptions). That fails even for simple coupled quadratic games, yet in practice we still often see convergence. This paper takes a structural â€œdesign the geometryâ€ viewpoint. Small-Gain Nash (SGN) uses per-player curvature and cross-player coupling bounds to build a block-weighted metric where the pseudo-gradient becomes strongly monotone and the joint gradient flow is contracting on a certified region. Once SGN holds on a region, you get: â€“ existence + uniqueness of a Nash equilibrium there, â€“ exponential contraction of the continuous flow, â€“ explicit safe step-size bounds for projected Euler and RK4 via a game-theoretic CFL number, and â€“ a finite â€œtimescale bandâ€ that plays a TTUR-like role i.e instead of vanishing two-timescale step sizes, it tells you for which relative player weights a single global step size is provably stable. The paper ends with an offline certification pipeline that estimates curvature/couplings on compact regions, optimizes the metric to enlarge the SGN margin, and certifies convergence in non-monotone quadratic and Markov games (including mirror/Fisher geometries for entropy-regularized policy gradient). code: https://github.com/AashVed/SmallGainNash",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06791",
      "pdf_url": "https://arxiv.org/pdf/2512.06791",
      "github_links": [
        "https://github.com/AashVed/SmallGainNash"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06791",
      "scraped_at": "2025-12-10T01:47:41.245034"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "paper_url": "https://huggingface.co/papers/2512.06609",
    "authors": [
      "Wendi Zheng",
      "jerytang",
      "Ya-Qin",
      "jmhernandezlobato",
      "xutongda"
    ],
    "stars": "10",
    "details": {
      "title": "Vector Quantization using Gaussian Variational Autoencoder",
      "abstract": "State-of-the-Art VQ-VAE from Gaussian VAE without Training! We train a Gaussian VAE, convert it into VQ-VAE with almost 100% codebook usage, and keeps reconstruction performance! As flexible to setup as VQ-VAE, supporting: codebook size, codebook dimension, codebook number. Pre-trained models can be found in [Huggingface] Paper can be found in [Arxiv] Code can be found in [Github] Quick Start Install dependency dependency in environment.yaml conda env create --file=environment.yaml\nconda activate tokenizer Install this package from source pip install -e . [optional] CUDA kernel for fast run time cd gq_cuda_extension\npip install --no-build-isolation -e . Download pre-trained model Download model \"sd3unet_gq_0.25.ckpt\" from [Huggingface] : mkdir model_256 mv \"sd3unet_gq_0.25.ckpt\" ./model_256 This is a VQ-VAE with codebook_size=2**16=65536 and codebook_dim=16 Infer the model as VQ-VAE Then use the model as follows from PIL import Image from torchvision import transforms from omegaconf import OmegaConf from pit.util import instantiate_from_config import torch\n\ntransform = transforms.Compose([\n    transforms.Resize(( 256 , 256 )),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[ 0.5 , 0.5 , 0.5 ],\n                        std=[ 0.5 , 0.5 , 0.5 ])\n])\n\nimg = transform(Image. open ( \"demo.png\" )).unsqueeze( 0 ).cuda()\nconfig = OmegaConf.load( \"./configs/sd3unet_gq_0.25.yaml\" )\nvae = instantiate_from_config(config.model)\nvae.load_state_dict(\n    torch.load( \"models_256/sd3unet_gq_0.25.ckpt\" ,\n        map_location=torch.device( 'cpu' ))[ \"state_dict\" ],strict= False )\nvae = vae. eval ().cuda()\n\nvae. eval ()\nz, log = vae.encode(img, return_reg_log= True ) \nimg_hat = vae.dequant(log[ \"indices\" ]) # discrete indices img_hat = vae.decode(z) # quantized latent Infer the model as Gaussian VAE Alternatively, the model can be used as a Vanilla Gaussian VAE: from PIL import Image from torchvision import transforms from omegaconf import OmegaConf from pit.util import instantiate_from_config import torch\n\ntransform = transforms.Compose([\n    transforms.Resize(( 256 , 256 )),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[ 0.5 , 0.5 , 0.5 ],\n                        std=[ 0.5 , 0.5 , 0.5 ])\n])\n\nimg = transform(Image. open ( \"demo.png\" )).unsqueeze( 0 ).cuda()\nconfig = OmegaConf.load( \"./configs/sd3unet_gq_0.25.yaml\" )\nvae = instantiate_from_config(config.model)\nvae.load_state_dict(\n    torch.load( \"models_256/sd3unet_gq_0.25.ckpt\" ,\n        map_location=torch.device( 'cpu' ))[ \"state_dict\" ],strict= False )\nvae = vae. eval ().cuda()\n\nvae. eval ()\n\nz = vae.encode(img, return_reg_log= True )[ 1 ][ \"zhat_noquant\" ] # Gaussian VAE latents img_hat = vae.decode(z) Train your own VQ-VAE Determine the VQ-VAE parameters: codebook_size: the codebook size, must be 2**N codebook_dimension: the dimension for each codebook codebook_number: number of sub codebook per spatial dimension Setup \"sd3unet_gq_0.25.yaml\" according to VQ-VAE parameters: n_samples: = codebook_size size, must be 2**N group: = codebook_dimension, dim of each codebook z_channels: = codebook_dimension * codebook_number, total dim of codebook Setup \"sd3unet_gq_0.25.yaml\" according to dataset path root: dataset root image_size: target image size batch_size: batch size Run the training! The default \"sd3unet_gq_0.25.yaml\" is setup for codebook_dimension=16, codebook_number=1, codebook_size=2**16=65536 export WANDB_API_KEY= $YOUR_WANDB_API_KEY python main.py --base configs/sd3unet_gq_0.25.yaml --wandb Run the evaluation! After the training, obtain the ckpt in $CKPT_PATH. Then, evaluate the model as python -m torch.distributed.launch --standalone --use-env \\\n    --nproc-per-node=8 eval.py \\\n    --bs=16 \\\n    --img_size 256 \\\n    --base=/workspace/cogview_dev/xutd/xu/pytorch-image-tokenizer/configs/sd3unet_gq_0.25.yaml \\\n    --ckpt= $CKPT_PATH \\\n    --dataset= $IMAGE_FOLDER_PATH Train with VAVAE Like Alignment See \"configs/sd3unet_gq_0.25_vf.yaml\". Why it Works? The only difference between our Gaussian VAE and vanilla Gaussian VAE is the KL divergence penralization. The key difference is class \"GaussianQuantRegularizer\" in \"./pit/quantization/gaussian.py\". During training, GaussianQuantRegularizer forces each dimension of KL be the same and achieve log(codebook_size). kl2 = 1.4426 * 0.5 * (torch. pow (mu, 2 ) + var - 1.0 - logvar)\nkl2 = kl2.reshape(b,l,self.group,c//self.group)\nkl2 = torch. sum (kl2,dim= 2 ) # sum over group dimension kl2_mean, kl2_min, kl2_max = torch.mean(kl2), torch. min (kl2), torch. max (kl2)\n\nge = (kl2 > self.log_n_samples + self.tolerance). type (kl2.dtype) * self.lam_max\neq = (kl2 <= self.log_n_samples + self.tolerance). type (kl2.dtype) * (\n    kl2 >= self.log_n_samples - self.tolerance\n). type (kl2.dtype)\nle = (kl2 < self.log_n_samples - self.tolerance). type (kl2.dtype) * self.lam_min\nkl_loss = torch. sum ((ge * kl2 + eq * kl2 + le * kl2), dim=[ 1 , 2 ])\nkl_loss = torch. sum (kl_loss) / kl_loss.shape[ 0 ] During inference, GaussianQuantRegularizer create a codebook of iid Gaussian, and find the cloest sample to posterior mean. q_normal_dist = Normal(mu_q[:, None , :], std_q[:, None , :])\nlog_ratios = (\n    q_normal_dist.log_prob(self.prior_samples[ None ])\n    - self.normal_log_prob[ None ] * self.beta\n)\nperturbed = torch. sum (log_ratios, dim= 2 )\nargmax_indices = torch.argmax(perturbed, dim= 1 )\nzhat[i : i + bs] = torch.index_select(self.prior_samples, 0 , argmax_indices)\nindices[i : i + bs] = argmax_indices Basically we limit the KL divergence of Gaussian VAE close to log2 codebook size. Once this constraint is met, the Gaussian VAE can be converted to VQ-VAE without much loss. For more information, see our paper! Contact & Ack Largely from https://github.com/Stability-AI/generative-models Any questions or comments goes to: x.tongda@nyu.edu Or if you have wechat: 18510201763 Reference @ misc {xu2025vectorquantizationusinggaussian,\n      title={Vector Quantization using Gaussian Variational Autoencoder}, \n      author={Tongda Xu and Wendi Zheng and Jiajun He and Jose Miguel Hernandez-Lobato and Yan Wang and Ya-Qin Zhang and Jie Tang},\n      year={2025},\n      eprint={2512.06609},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2512.06609}, \n}",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06609",
      "pdf_url": "https://arxiv.org/pdf/2512.06609",
      "github_links": [
        "https://github.com/Stability-AI/generative-models",
        "https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06609",
      "scraped_at": "2025-12-10T01:47:43.136663"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
    "paper_url": "https://huggingface.co/papers/2512.03704",
    "authors": [
      "YijunLiao"
    ],
    "stars": "2",
    "details": {
      "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
      "abstract": "ğŸ”¥ Solving \"State Inertia\" in Long-Context LLMs! We introduce DZ-TDPO, a non-destructive alignment framework. Problem: Standard DPO causes \"Alignment Tax\" (PPL explosion >100) when updating user states in long context. Solution: Dynamic KL Constraints + Dual-Zone Temporal Attention. Result: SOTA 55.4% Win Rate on MSC dataset with Zero PPL degradation (PPL ~26.0). ğŸš€ Code & SOTA Model (Phi-3.5) are released! Check the Linked Models section.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.03704",
      "pdf_url": "https://arxiv.org/pdf/2512.03704",
      "github_links": [
        "https://github.com/lyj20071013/DZ-TDPO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.03704",
      "scraped_at": "2025-12-10T01:47:44.960333"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
    "paper_url": "https://huggingface.co/papers/2512.07168",
    "authors": [
      "Linsey Pang",
      "Aaron Elkins",
      "Aman Chadha",
      "Christos Constantinou",
      "Georgios Ioannides"
    ],
    "stars": "0",
    "details": {
      "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
      "abstract": "This paper introduces JEPA+DAAM , a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Gaussian mixtureâ€“based Density Adaptive Attention Mechanism (DAAM) to learn semantically rich and highly compressible speech representations, achieving reversible neural tokenization at 47.5 tokens/sec with strong reconstruction quality. â¡ï¸ ğŠğğ² ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬ ğ¨ğŸ ğ‰ğ„ğğ€+ğƒğ€ğ€ğŒ: ğŸ§  ğ‘±ğ‘¬ğ‘·ğ‘¨ ğ‘­ğ’ğ’“ ğ‘ºğ’†ğ’ğ’‡-ğ‘ºğ’–ğ’‘ğ’†ğ’“ğ’—ğ’Šğ’”ğ’†ğ’… ğ‘ºğ’‘ğ’†ğ’†ğ’„ğ’‰ ğ‘¬ğ’ğ’„ğ’ğ’…ğ’Šğ’ğ’ˆ: Decouples representation learning from waveform reconstruction using a masked-prediction JEPA objective in latent space. The encoder learns semantically meaningful embeddings at 2.5 Hz without requiring low-level waveform loss, enabling robust self-supervised pretraining and cross-task adaptability (ASR, TTS, voice conversion). ğŸ¯ ğ‘«ğ’†ğ’ğ’”ğ’Šğ’•ğ’š ğ‘¨ğ’…ğ’‚ğ’‘ğ’•ğ’Šğ’—ğ’† ğ‘¨ğ’•ğ’•ğ’†ğ’ğ’•ğ’Šğ’ğ’ (ğ‘«ğ‘¨ğ‘¨ğ‘´): Introduces a Gaussian mixtureâ€“based gating attention mechanism that modulates temporal features based on local statistical salience rather than pairwise dot-products. This allows adaptive feature selection and hierarchical speech-structure discovery with linear complexity, improving JEPA convergence (loss 0.09 vs 0.17 without DAAM). ğŸ§© ğ‘­ğ‘ºğ‘¸ + ğ‘´ğ’Šğ’™ğ’†ğ’…-ğ‘¹ğ’‚ğ’…ğ’Šğ’™ ğ‘»ğ’ğ’Œğ’†ğ’ğ’Šğ’›ğ’‚ğ’•ğ’Šğ’ğ’: Implements Finite Scalar Quantization (FSQ) with mixed-radix integer packing for reversible, codebook-free tokenization (47.5 tokens/sec, 16 384-way vocabulary). Combined with a HiFi-GAN decoder, it achieves high-fidelity waveform reconstruction, outperforming or matching neural audio codecs like SoundStream and EnCodec at dramatically lower frame rates.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.07168",
      "pdf_url": "https://arxiv.org/pdf/2512.07168",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.07168",
      "scraped_at": "2025-12-10T01:47:46.822030"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "paper_url": "https://huggingface.co/papers/2512.06558",
    "authors": [
      "Ganesh Nanduru",
      "amanchadha",
      "Anubis91",
      "alexiglad",
      "mmiakashs"
    ],
    "stars": "0",
    "details": {
      "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
      "abstract": "The paper introduces Refer360 , a comprehensive multimodal dataset for embodied referring expression comprehension in human-robot interaction (HRI), and proposes MuRes , a lightweight guided residual module that selectively reinforces modality-specific features to improve multimodal grounding performance in real-world scenarios. â¡ï¸ ğŠğğ² ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬ ğ¨ğŸ ğ­ğ¡ğ ğ‘ğğŸğğ«ğŸ‘ğŸ”ğŸ ğğğ§ğœğ¡ğ¦ğšğ«ğ¤ + ğŒğ®ğ‘ğğ¬ ğŒğ¨ğğ®ğ¥ğ : ğŸ§  ğ‘¹ğ’†ğ’‡ğ’†ğ’“ğŸ‘ğŸ”ğŸ: ğ‘­ğ’Šğ’“ğ’”ğ’• ğ‘¬ğ’ğ’ƒğ’ğ’…ğ’Šğ’†ğ’… ğ‘¹ğ‘¬ ğ‘«ğ’‚ğ’•ğ’‚ğ’”ğ’†ğ’• ğ’˜ğ’Šğ’•ğ’‰ ğ‘´ğ’–ğ’ğ’•ğ’Š-ğ‘½ğ’Šğ’†ğ’˜, ğ‘´ğ’–ğ’ğ’•ğ’Š-ğ‘ºğ’†ğ’ğ’”ğ’ğ’“ ğ‘´ğ’ğ’…ğ’‚ğ’ğ’Šğ’•ğ’Šğ’†ğ’” : Introduces a dataset with synchronized egocentric and exocentric views , RGB, depth, infrared, 3D skeleton , eye gaze , and audio , across indoor and outdoor environments. With 13,990 annotated interactions (3.2M frames), it overcomes biases in existing datasets (e.g., single view, indoor-only, no gesture/gaze integration). ğŸ” ğ‘´ğ’–ğ‘¹ğ’†ğ’”: ğ‘®ğ’–ğ’Šğ’…ğ’†ğ’… ğ‘¹ğ’†ğ’”ğ’Šğ’…ğ’–ğ’‚ğ’ ğ‘©ğ’ğ’•ğ’•ğ’ğ’†ğ’ğ’†ğ’„ğ’Œ ğ’‡ğ’ğ’“ ğ‘´ğ’–ğ’ğ’•ğ’Šğ’ğ’ğ’…ğ’‚ğ’ ğ‘­ğ’–ğ’”ğ’Šğ’ğ’ : Proposes a novel residual architecture that uses cross-attention to guide modality-specific signals (visual/language) through an information bottleneck , preventing feature dilution during fusion and outperforming both vanilla residuals and attention-only fusion across 4 datasets. ğŸ“ˆ ğ‘ºğ’Šğ’ˆğ’ğ’Šğ’‡ğ’Šğ’„ğ’‚ğ’ğ’• ğ‘®ğ’‚ğ’Šğ’ğ’” ğ’‚ğ’„ğ’“ğ’ğ’”ğ’” ğ‘¯ğ‘¹ğ‘° ğ’‚ğ’ğ’… ğ‘½ğ‘¸ğ‘¨ ğ‘»ğ’‚ğ’”ğ’Œğ’” : On Refer360, integrating MuRes into CLIP improved IOU-25 by +3.4% , and on CAESAR-PRO by +4.99% . For broader VQA tasks like ScienceQA and A-OKVQA, MuRes boosted model accuracy by up to +30% , highlighting its generalization ability across task domains.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06558",
      "pdf_url": "https://arxiv.org/pdf/2512.06558",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06558",
      "scraped_at": "2025-12-10T01:47:48.625230"
    },
    "scraped_date": "2025-12-10"
  },
  {
    "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
    "paper_url": "https://huggingface.co/papers/2512.06032",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
      "abstract": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3 (also called SAMv2 and SAMv3). We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAMv3. SAMv2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAMv3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAMv2 with multimodal fusion and text-conditioned mask generation of SAMv3; (2) Architectural Divergence, detailing pure vision-temporal design of SAMv2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAMv3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAMv3; (4) Training and Hyperparameter Distinctions, showing why SAMv2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAMv3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.",
      "arxiv_page_url": "https://arxiv.org/abs/2512.06032",
      "pdf_url": "https://arxiv.org/pdf/2512.06032",
      "github_links": [
        "https://github.com/Applied-AI-Research-Lab/The-SAM2-to-SAM3-Gap-in-the-Segment-Anything-Model-Family"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2512.06032",
      "scraped_at": "2025-12-10T01:47:50.436796"
    },
    "scraped_date": "2025-12-10"
  }
]