[
  {
    "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
    "paper_url": "https://huggingface.co/papers/2602.09877",
    "authors": [
      "Jinyu Hou",
      "Zejian Chen",
      "Songyang Liu",
      "Chaozhuo Li",
      "xunyoyo"
    ],
    "stars": "0",
    "details": {
      "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
      "abstract": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.09877",
      "pdf_url": "https://arxiv.org/pdf/2602.09877",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.09877",
      "scraped_at": "2026-02-15T02:27:14.316655"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
    "paper_url": "https://huggingface.co/papers/2602.12036",
    "authors": [],
    "stars": "14",
    "details": {
      "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
      "abstract": "Models and datasets are available at https://huggingface.co/collections/xx18/composition-rl",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12036",
      "pdf_url": "https://arxiv.org/pdf/2602.12036",
      "github_links": [
        "https://github.com/XinXU-USTC/Composition-RL"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12036",
      "scraped_at": "2026-02-15T02:27:16.567944"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
    "paper_url": "https://huggingface.co/papers/2602.12205",
    "authors": [],
    "stars": "71",
    "details": {
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "abstract": "Models: https://huggingface.co/deepgenteam/DeepGen-1.0 Datasets: https://huggingface.co/datasets/DeepGenTeam/DeepGen-1.0",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12205",
      "pdf_url": "https://arxiv.org/pdf/2602.12205",
      "github_links": [
        "https://github.com/DeepGenTeam/DeepGen"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12205",
      "scraped_at": "2026-02-15T02:27:18.735764"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "paper_url": "https://huggingface.co/papers/2602.12125",
    "authors": [],
    "stars": "13",
    "details": {
      "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
      "abstract": "We propose G-OPD, a generalized on-policy distillation framework. Building on G-OPD, we propose ExOPD (Generalized On-Policy Distillation with Reward Extrapolation), which enables a unified student to surpass all domain teachers in the multi-teacher distillation setting.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12125",
      "pdf_url": "https://arxiv.org/pdf/2602.12125",
      "github_links": [
        "https://github.com/RUCBM/G-OPD"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12125",
      "scraped_at": "2026-02-15T02:27:20.833968"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
    "paper_url": "https://huggingface.co/papers/2602.10934",
    "authors": [],
    "stars": "84",
    "details": {
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "abstract": "Start discussion in this paper",
      "arxiv_page_url": "https://arxiv.org/abs/2602.10934",
      "pdf_url": "https://arxiv.org/pdf/2602.10934",
      "github_links": [
        "https://github.com/OpenMOSS/MOSS-Audio-Tokenizer"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.10934",
      "scraped_at": "2026-02-15T02:27:22.877185"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2602.12099",
    "authors": [],
    "stars": "2.3k",
    "details": {
      "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
      "abstract": "GigaBrain-0.5M* is a VLA That Learns From World Model-Based Reinforcement Learning. GigaBrain-0.5M* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12099",
      "pdf_url": "https://arxiv.org/pdf/2602.12099",
      "github_links": [
        "https://github.com/open-gigaai/giga-brain-0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12099",
      "scraped_at": "2026-02-15T02:27:25.062377"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
    "paper_url": "https://huggingface.co/papers/2602.09070",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2602.09070",
      "pdf_url": "https://arxiv.org/pdf/2602.09070",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.09070",
      "scraped_at": "2026-02-15T02:27:27.192364"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
    "paper_url": "https://huggingface.co/papers/2602.12056",
    "authors": [],
    "stars": "23",
    "details": {
      "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
      "abstract": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12056",
      "pdf_url": "https://arxiv.org/pdf/2602.12056",
      "github_links": [
        "https://github.com/yxy-919/LawThinker-agent"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12056",
      "scraped_at": "2026-02-15T02:27:29.403965"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
    "paper_url": "https://huggingface.co/papers/2602.11731",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
      "abstract": "The core idea of Thinking with Drafting (TwD) is super refreshing: instead of letting a multimodal model ‚Äúguess the answer‚Äù with fluent CoT or pretty-looking diagrams, it forces the model to draft its reasoning into executable structure. Not vibes. Not plausible pixels. But strict, renderable DSL code. The ‚Äúoptical decompression‚Äù framing is also üî• ‚Äî OCR gives you symbols, but not logical topology. TwD says: real understanding = reconstructing the hidden structure behind those symbols. And the moment the model has to commit to aligned segments, brackets, and cross-row constraints, hallucination becomes much harder. What I like most is the shift from: generate explanation ‚Üí hope it‚Äôs right to generate structure ‚Üí verify it deterministically That feels like a big step toward trustworthy multimodal reasoning.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11731",
      "pdf_url": "https://arxiv.org/pdf/2602.11731",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11731",
      "scraped_at": "2026-02-15T02:27:31.456479"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
    "paper_url": "https://huggingface.co/papers/2602.12280",
    "authors": [],
    "stars": "26",
    "details": {
      "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
      "abstract": "Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace\" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12280",
      "pdf_url": "https://arxiv.org/pdf/2602.12280",
      "github_links": [
        "https://github.com/stroke-of-surprise/Stroke-Of-Surprise"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12280",
      "scraped_at": "2026-02-15T02:27:33.658643"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2602.11748",
    "authors": [],
    "stars": "11",
    "details": {
      "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
      "abstract": "üîó Code: https://github.com/LINs-lab/LIE üîó Paper: https://arxiv.org/abs/2602.11748",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11748",
      "pdf_url": "https://arxiv.org/pdf/2602.11748",
      "github_links": [
        "https://github.com/LINs-lab/LIE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11748",
      "scraped_at": "2026-02-15T02:27:35.801868"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
    "paper_url": "https://huggingface.co/papers/2602.11075",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
      "abstract": "The first study on leveraging world models as an effective learning environment for challenging real-world manipulation, bootstrapping performance on tasks requiring high dynamics, dexterity, and precision.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11075",
      "pdf_url": "https://arxiv.org/pdf/2602.11075",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11075",
      "scraped_at": "2026-02-15T02:27:37.942150"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "œá_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
    "paper_url": "https://huggingface.co/papers/2602.09021",
    "authors": [],
    "stars": "182",
    "details": {
      "title": "œá_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
      "abstract": "üß• Live-stream robotic teamwork that folds clothes. 6 clothes in 3 minutes straight. œá‚ÇÄ = 20hrs data + 8 A100s + 3 key insights: Mode Consistency: align your distributions Model Arithmetic: merge, don't retrain Stage Advantage: pivot wisely üîó http://mmlab.hk/research/kai0 üì¶ Data + checkpoints + code: https://github.com/OpenDriveLab/KAI0 [2/5] Problem: Distribution Mismatch Training data ‚â† Model behavior ‚â† Real-world execution This gap causes failures. Solution ‚Üí Mode Consistency: ‚Ä¢ DAgger for failure recovery ‚Ä¢ Augmentation for coverage ‚Ä¢ Inference smoothing for clean execution [3/5] Problem: Expensive Iteration Collect new data ‚Üí Retrain everything ‚Üí Repeat Slow yet expensive. How? Model Arithmetic: ‚Ä¢ Train only on new data ‚Ä¢ Merge via weight interpolation ‚Ä¢ Merged model > full-dataset model Models trained separately preserve distinct modes. [4/5] Problem: Long-Horizon Credit Assignment 6-minute tasks. Which actions actually helped? Solution ‚Üí Stage Advantage: ‚Ä¢ Decompose into semantic stages ‚Ä¢ Predict advantage directly (not value-diff) ‚Ä¢ Smoother supervision, less error compounding [5/5] Bottom Line ‚Ä¢ Not all robot data is equally valuable ‚Ä¢ Fast iteration > bruteforce scaling ‚Ä¢ Weight-space merging can outperform joint training ‚Ä¢ Stage-aware advantage estimation helps long-horizon tasks",
      "arxiv_page_url": "https://arxiv.org/abs/2602.09021",
      "pdf_url": "https://arxiv.org/pdf/2602.09021",
      "github_links": [
        "https://github.com/OpenDriveLab/KAI0"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.09021",
      "scraped_at": "2026-02-15T02:27:41.217280"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "dVoting: Fast Voting for dLLMs",
    "paper_url": "https://huggingface.co/papers/2602.12153",
    "authors": [],
    "stars": "22",
    "details": {
      "title": "dVoting: Fast Voting for dLLMs",
      "abstract": "The first efficient test-time scaling strategy for dLLMs. Welcome any discussion!",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12153",
      "pdf_url": "https://arxiv.org/pdf/2602.12153",
      "github_links": [
        "https://github.com/fscdc/dVoting"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12153",
      "scraped_at": "2026-02-15T02:27:43.361815"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
    "paper_url": "https://huggingface.co/papers/2602.10106",
    "authors": [
      "Yinghui Li",
      "Haoran Jiang",
      "Jin Chen",
      "Shijia Peng",
      "Modi Shi"
    ],
    "stars": "0",
    "details": {
      "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
      "abstract": "Project page: https://opendrivelab.com/EgoHumanoid",
      "arxiv_page_url": "https://arxiv.org/abs/2602.10106",
      "pdf_url": "https://arxiv.org/pdf/2602.10106",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.10106",
      "scraped_at": "2026-02-15T02:27:45.396178"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "paper_url": "https://huggingface.co/papers/2602.05827",
    "authors": [
      "Yukuan Xu",
      "Yuxian Li",
      "Li Chen",
      "Siqi Liang",
      "Hai Zhang"
    ],
    "stars": "35",
    "details": {
      "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
      "abstract": "SparseVideoNav introduces video generation models to real-world beyond-the-view vision-language navigation for the first time. It achieves sub-second trajectory inference with a sparse future spanning a 20-second horizon, yielding a remarkable 27√ó speed-up. Real-world zero-shot experiments show 2.5√ó higher success rate than state-of-the-art LLM baselines and mark the first realization in challenging night scenes. Highlights We investigate beyond-the-view navigation tasks in the real world by introducing video generation model to this field for the first time. We pioneer a paradigm shift from continuous to sparse video generation for longer prediction horizon. We achieve sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. We achieve the first realization of beyond-the-view navigation in challenging night scenes with a 17.5% success rate.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.05827",
      "pdf_url": "https://arxiv.org/pdf/2602.05827",
      "github_links": [
        "https://github.com/opendrivelab/sparsevideonav"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.05827",
      "scraped_at": "2026-02-15T02:27:47.430463"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "paper_url": "https://huggingface.co/papers/2602.12092",
    "authors": [],
    "stars": "34",
    "details": {
      "title": "DeepSight: An All-in-One LM Safety Toolkit",
      "abstract": "We propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12092",
      "pdf_url": "https://arxiv.org/pdf/2602.12092",
      "github_links": [
        "https://github.com/AI45Lab/DeepSafe",
        "https://github.com/AI45Lab/DeepScan/"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12092",
      "scraped_at": "2026-02-15T02:27:49.539160"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "paper_url": "https://huggingface.co/papers/2602.05548",
    "authors": [],
    "stars": "9",
    "details": {
      "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
      "abstract": "See https://github.com/HKU-HealthAI/A-GRAE for the code base, and https://yu7-code.github.io/A-GRAE-web/ for the project page",
      "arxiv_page_url": "https://arxiv.org/abs/2602.05548",
      "pdf_url": "https://arxiv.org/pdf/2602.05548",
      "github_links": [
        "https://github.com/HKU-HealthAI/A-GRAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.05548",
      "scraped_at": "2026-02-15T02:27:51.537723"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Adapting Vision-Language Models for E-commerce Understanding at Scale",
    "paper_url": "https://huggingface.co/papers/2602.11733",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Adapting Vision-Language Models for E-commerce Understanding at Scale",
      "abstract": "Figure 1: Output of our E-commerce Adapted VLMs compared against same size LLaVA-OneVision . We show our models ability to more faithfully extract attributes from e-commerce items. In red, we highlight wrong model predictions that are neither tied to the image nor valid item attributes.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11733",
      "pdf_url": "https://arxiv.org/pdf/2602.11733",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11733",
      "scraped_at": "2026-02-15T02:27:53.688459"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Voxtral Realtime",
    "paper_url": "https://huggingface.co/papers/2602.11298",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Voxtral Realtime",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Streaming Speech Recognition with Decoder-Only Large Language Models and Latency Optimization (2026) MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models (2026) Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization (2026) Qwen3-TTS Technical Report (2026) Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models (2026) FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation (2026) dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11298",
      "pdf_url": "https://arxiv.org/pdf/2602.11298",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11298",
      "scraped_at": "2026-02-15T02:27:55.733797"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
    "paper_url": "https://huggingface.co/papers/2602.08277",
    "authors": [],
    "stars": "27",
    "details": {
      "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
      "abstract": "PISCO: Precise Video Instance Insertion with Sparse Control",
      "arxiv_page_url": "https://arxiv.org/abs/2602.08277",
      "pdf_url": "https://arxiv.org/pdf/2602.08277",
      "github_links": [
        "https://github.com/taco-group/PISCO"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.08277",
      "scraped_at": "2026-02-15T02:27:57.870152"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "paper_url": "https://huggingface.co/papers/2602.12262",
    "authors": [
      "Xiaoxiao He",
      "Haizhou Shi",
      "Ligong Han",
      "Xinxi Zhang",
      "Tyrion279"
    ],
    "stars": "13",
    "details": {
      "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
      "abstract": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12262",
      "pdf_url": "https://arxiv.org/pdf/2602.12262",
      "github_links": [
        "https://github.com/Tyrion58/T3D"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12262",
      "scraped_at": "2026-02-15T02:27:59.968818"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
    "paper_url": "https://huggingface.co/papers/2602.11964",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API IDRBench: Interactive Deep Research Benchmark (2026) Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning (2026) AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts (2026) From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents (2026) Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios (2026) Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction (2026) LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11964",
      "pdf_url": "https://arxiv.org/pdf/2602.11964",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11964",
      "scraped_at": "2026-02-15T02:28:02.026025"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck",
    "paper_url": "https://huggingface.co/papers/2602.07885",
    "authors": [
      "Wei Xue",
      "Zhenbo Song",
      "Zhiqin Yang",
      "Xianzhang Jia",
      "Zhenyuan Zhang"
    ],
    "stars": "0",
    "details": {
      "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2602.07885",
      "pdf_url": "https://arxiv.org/pdf/2602.07885",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.07885",
      "scraped_at": "2026-02-15T02:28:04.216333"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Single-minus gluon tree amplitudes are nonzero",
    "paper_url": "https://huggingface.co/papers/2602.12176",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Single-minus gluon tree amplitudes are nonzero",
      "abstract": "A group of theoretical physicists derive a new result in quantum field theory using GPT-5.2 Pro.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12176",
      "pdf_url": "https://arxiv.org/pdf/2602.12176",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12176",
      "scraped_at": "2026-02-15T02:28:06.338099"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
    "paper_url": "https://huggingface.co/papers/2602.11683",
    "authors": [
      "Julian McAuley",
      "Haoliang Wang",
      "Xiang Chen",
      "Tong Yu",
      "XinXuNLPer"
    ],
    "stars": "0",
    "details": {
      "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
      "abstract": "This paper proposes ThinkRouter, a confidence-aware routing mechanism to improve reasoning performance for large reasoning models (LRMs), which routes LRMs thinking between latent and discrete token spaces based on model confidence at inference time.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11683",
      "pdf_url": "https://arxiv.org/pdf/2602.11683",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11683",
      "scraped_at": "2026-02-15T02:28:08.436984"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
    "paper_url": "https://huggingface.co/papers/2602.08194",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
      "abstract": "Large Language Models that \"dream\" and materialize executable environment code to scaffold learning in open-ended worlds.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.08194",
      "pdf_url": "https://arxiv.org/pdf/2602.08194",
      "github_links": [
        "https://github.com/konstantinosmitsides/dreaming-in-code"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.08194",
      "scraped_at": "2026-02-15T02:28:10.587741"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
    "paper_url": "https://huggingface.co/papers/2602.11761",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts (2026) HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing (2026) STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs (2026) CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling (2026) SPLA: Block Sparse Plus Linear Attention for Long Context Modeling (2026) HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference (2026) Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11761",
      "pdf_url": "https://arxiv.org/pdf/2602.11761",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11761",
      "scraped_at": "2026-02-15T02:28:12.682205"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
    "paper_url": "https://huggingface.co/papers/2602.11337",
    "authors": [],
    "stars": "92",
    "details": {
      "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes (2026) Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot (2026) SAGE: Scalable Agentic 3D Scene Generation for Embodied AI (2026) MobileManiBench: Simplifying Model Verification for Mobile Manipulation (2026) IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments (2025) REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation (2025) VirtualEnv: A Platform for Embodied AI Research (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11337",
      "pdf_url": "https://arxiv.org/pdf/2602.11337",
      "github_links": [
        "https://github.com/allenai/molmospaces"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11337",
      "scraped_at": "2026-02-15T02:28:14.700003"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
    "paper_url": "https://huggingface.co/papers/2602.11543",
    "authors": [],
    "stars": "17",
    "details": {
      "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
      "abstract": "We propose SPES, a decentralized framework for pretraining MoE LLMs. SPES supports sparse training on weakly connected nodes, reducing memory and communication costs and enabling efficient pretraining on resource-constrained devices.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11543",
      "pdf_url": "https://arxiv.org/pdf/2602.11543",
      "github_links": [
        "https://github.com/zjr2000/SPES"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11543",
      "scraped_at": "2026-02-15T02:28:16.879614"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "paper_url": "https://huggingface.co/papers/2602.10575",
    "authors": [
      "Hongsheng Li",
      "Yazhe Niu",
      "Chenhao Zhang"
    ],
    "stars": "4",
    "details": {
      "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
      "abstract": "Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench. Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.10575",
      "pdf_url": "https://arxiv.org/pdf/2602.10575",
      "github_links": [
        "https://github.com/MING-ZCH/MetaphorStar"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.10575",
      "scraped_at": "2026-02-15T02:28:18.882879"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
    "paper_url": "https://huggingface.co/papers/2602.12203",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
      "abstract": "We introduce ExStrucTiny, a new benchmark for structured information extraction from document images that unifies in one task (1) key entity extraction, (2) relation extraction, and (3) visual question answering across diverse input schemas and document types. Results show that current VLMs still struggle with schema adaptation, underspecified queries, and answer localization.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12203",
      "pdf_url": "https://arxiv.org/pdf/2602.12203",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12203",
      "scraped_at": "2026-02-15T02:28:20.950320"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
    "paper_url": "https://huggingface.co/papers/2602.12164",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
      "abstract": "We‚Äôre excited to share our new work, Sci-CoE ! üéâ In this project, we tackle a fundamental challenge: üß© How can we train LLMs with RL when there is no explicit final answer and no way to compute outcome rewards via unit tests or exact matching? This setting is common in üî¨ scientific reasoning and üìê mathematical proofs, where correct solutions are often non unique and expressed as structured natural language rather than fixed outputs. Traditional reward signals simply do not work in these scenarios. To address this, we propose a ‚ú® geometric consensus reward that models agreement, reliability, and diversity, enabling RL training without ground truth final answers. Starting from sparse supervision, Sci CoE transitions to large scale self evolution üöÄ, allowing models to improve as both solver and verifier in open ended scientific tasks.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12164",
      "pdf_url": "https://arxiv.org/pdf/2602.12164",
      "github_links": [
        "https://github.com/InternScience/Sci-CoE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12164",
      "scraped_at": "2026-02-15T02:28:23.136408"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
    "paper_url": "https://huggingface.co/papers/2602.12116",
    "authors": [],
    "stars": "12",
    "details": {
      "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
      "abstract": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.12116",
      "pdf_url": "https://arxiv.org/pdf/2602.12116",
      "github_links": [
        "https://github.com/Tongyi-ConvAI/Qwen-Character"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.12116",
      "scraped_at": "2026-02-15T02:28:25.184053"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
    "paper_url": "https://huggingface.co/papers/2602.11541",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
      "abstract": "Don't Let Your Agent Max Out Your Credit Card! üí≥ Most current research pushes the boundaries of agent performance but often overlooks the actual economic cost. Can agents still make rational decisions when every tool call comes with a price tag? We introduce INTENT, a lightweight inference-time framework that predicts future spending through \"intention-level\" simulations. It allows agents to remain highly efficient even under strict budget constraints‚Äîwithout the need for retraining or exhaustive searches. This is more than just a technical improvement; it's our initial exploration into the burgeoning field of Agent Economics .",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11541",
      "pdf_url": "https://arxiv.org/pdf/2602.11541",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11541",
      "scraped_at": "2026-02-15T02:28:27.213357"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "paper_url": "https://huggingface.co/papers/2602.11509",
    "authors": [
      "Hyunji Lee",
      "Elias Stengel-Eskin",
      "Ziyang Wang",
      "David Wan",
      "HanNight"
    ],
    "stars": "3",
    "details": {
      "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
      "abstract": "",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11509",
      "pdf_url": "https://arxiv.org/pdf/2602.11509",
      "github_links": [
        "https://github.com/meetdavidwan/murgat"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11509",
      "scraped_at": "2026-02-15T02:28:29.282666"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
    "paper_url": "https://huggingface.co/papers/2602.11636",
    "authors": [],
    "stars": "2",
    "details": {
      "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
      "abstract": "ScalSelect: Training-Free and Scalable Data Selection for Visual Instruction Tuning Large-scale visual instruction tuning datasets are highly redundant, yet full-data training remains the default ‚Äî leading to substantial computational waste. This paper introduces ScalSelect, a training-free and gradient-free data selection method with linear-time complexity, specifically designed for visual instruction tuning. üí° Key Ideas Leverages instruction-conditioned attention representations to better capture instruction-relevant visual signals Selects samples from a dominant global subspace, rather than relying on pairwise similarity or proxy model training Scalable ‚Äî no additional models, no additional datasets, training-free, no gradient training, linear-time complexity with respect to data size,  required for selection üìä Results Using only 16% of the data, ScalSelect retains ‚â•97.5% of full-data performance, and in some cases even surpasses training on the entire dataset. üöÄ Why It Matters ScalSelect provides a practical and scalable solution for multimodal training. As VLMs continue to scale, data curation becomes increasingly critical ‚Äî and this work offers a simple yet principled direction forward. Code: https://github.com/ChangtiWu/ScalSelect",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11636",
      "pdf_url": "https://arxiv.org/pdf/2602.11636",
      "github_links": [
        "https://github.com/ChangtiWu/ScalSelect"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11636",
      "scraped_at": "2026-02-15T02:28:31.569224"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
    "paper_url": "https://huggingface.co/papers/2602.11598",
    "authors": [
      "Minghua Luo",
      "Yanfen Shen",
      "Xiaolong Wu",
      "Shichao Xie",
      "Zedong Chu"
    ],
    "stars": "37",
    "details": {
      "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
      "abstract": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments (2025) Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization (2026) VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation (2025) Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training (2025) RoboBrain 2.5: Depth in Sight, Time in Mind (2026) VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory (2026) AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation (2026) Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkout this Space You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: @ librarian-bot recommend",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11598",
      "pdf_url": "https://arxiv.org/pdf/2602.11598",
      "github_links": [
        "https://github.com/amap-cvlab/ABot-Navigation"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11598",
      "scraped_at": "2026-02-15T02:28:33.653073"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
    "paper_url": "https://huggingface.co/papers/2602.10585",
    "authors": [
      "Aidong Zhang",
      "Sanchit Sinha",
      "Guangzhi Xiong"
    ],
    "stars": "2",
    "details": {
      "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
      "abstract": "TL;DR: We introduce Neural Additive Experts (NAEs), a context-gated mixture-of-experts extension of generalized additive models that preserves per-feature explanations while capturing interactions when needed, using a single tunable regularizer to trade off interpretability vs. accuracy.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.10585",
      "pdf_url": "https://arxiv.org/pdf/2602.10585",
      "github_links": [
        "https://github.com/Teddy-XiongGZ/NAE"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.10585",
      "scraped_at": "2026-02-15T02:28:35.714426"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
    "paper_url": "https://huggingface.co/papers/2602.09891",
    "authors": [],
    "stars": "0",
    "details": {
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "abstract": "Check out intro & demo video here! https://youtu.be/IrGD3CHaPYU?si=nutPyU5sz5iHfES5 More sound examples: https://stemphonic-demo.vercel.app",
      "arxiv_page_url": "https://arxiv.org/abs/2602.09891",
      "pdf_url": "https://arxiv.org/pdf/2602.09891",
      "github_links": [],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.09891",
      "scraped_at": "2026-02-15T02:28:37.817607"
    },
    "scraped_date": "2026-02-15"
  },
  {
    "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
    "paper_url": "https://huggingface.co/papers/2602.11792",
    "authors": [],
    "stars": "1",
    "details": {
      "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-kNN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the k smallest nearest-neighbor edit distances. Min-kNN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.",
      "arxiv_page_url": "https://arxiv.org/abs/2602.11792",
      "pdf_url": "https://arxiv.org/pdf/2602.11792",
      "github_links": [
        "https://github.com/StevenZHB/Detect_RLVR_Data"
      ],
      "metadata": {},
      "page_url": "https://huggingface.co/papers/2602.11792",
      "scraped_at": "2026-02-15T02:28:39.928224"
    },
    "scraped_date": "2026-02-15"
  }
]